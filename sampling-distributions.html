<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
               "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"
lang="en" xml:lang="en">
<head>
<title>Sampling Distributions</title>
<meta http-equiv="Content-Type" content="text/html;charset=iso-8859-1"/>
<meta name="generator" content="Org-mode"/>
<meta name="generated" content="2011-09-21 18:51:16 EDT"/>
<meta name="author" content="G. Jay Kerns"/>
<meta name="description" content="This chapter is the bridge from multivariate distributions to estimation."/>
<meta name="keywords" content="sampling distribution statistics R probability"/>
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  html { font-family: Times, serif; font-size: 12pt; }
  .title  { text-align: center; }
  .todo   { color: red; }
  .done   { color: green; }
  .tag    { background-color: #add8e6; font-weight:normal }
  .target { }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  {margin-left:auto; margin-right:0px;  text-align:right;}
  .left   {margin-left:0px;  margin-right:auto; text-align:left;}
  .center {margin-left:auto; margin-right:auto; text-align:center;}
  p.verse { margin-left: 3% }
  pre {
	border: 1pt solid #AEBDCC;
	background-color: #F3F5F7;
	padding: 5pt;
	font-family: courier, monospace;
        font-size: 90%;
        overflow:auto;
  }
  table { border-collapse: collapse; }
  td, th { vertical-align: top;  }
  th.right  { text-align:center;  }
  th.left   { text-align:center;   }
  th.center { text-align:center; }
  td.right  { text-align:right;  }
  td.left   { text-align:left;   }
  td.center { text-align:center; }
  dt { font-weight: bold; }
  div.figure { padding: 0.5em; }
  div.figure p { text-align: center; }
  div.inlinetask {
    padding:10px;
    border:2px solid gray;
    margin:10px;
    background: #ffffcc;
  }
  textarea { overflow-x: auto; }
  .linenr { font-size:smaller }
  .code-highlighted {background-color:#ffff00;}
  .org-info-js_info-navigation { border-style:none; }
  #org-info-js_console-label { font-size:10px; font-weight:bold;
                               white-space:nowrap; }
  .org-info-js_search-highlight {background-color:#ffff00; color:#000000;
                                 font-weight:bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="org-div-home-and-up" style="text-align:right;font-size:70%;white-space:nowrap;">
 <a accesskey="h" href="index.html   "> UP </a>
 |
 <a accesskey="H" href="http://ipsur.org/index.html"> HOME </a>
</div>

<div id="preamble">

</div>

<div id="content">
<h1 class="title">Sampling Distributions</h1>




---
layout: chapter
title: Sampling Distributions
previous: multivariate-distributions.html
next: estimation.html
description: This chapter is the bridge from multivariate distributions to estimation.
keywords: sampling distribution statistics R probability
---


<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">Sampling Distributions</a>
<ul>
<li><a href="#sec-1-1">Simple Random Samples</a>
<ul>
<li><a href="#sec-1-1-1">Simple Random Samples</a></li>
</ul>
</li>
<li><a href="#sec-1-2">Sampling from a Normal Distribution</a>
<ul>
<li><a href="#sec-1-2-1">The Distribution of the Sample Mean</a></li>
<li><a href="#sec-1-2-2">The Distribution of the Sample Variance</a></li>
<li><a href="#sec-1-2-3">The Distribution of Student's \(T\) Statistic</a></li>
</ul>
</li>
<li><a href="#sec-1-3">The Central Limit Theorem</a></li>
<li><a href="#sec-1-4">Sampling Distributions of Two-Sample Statistics</a>
<ul>
<li><a href="#sec-1-4-1">Difference of Independent Sample Means</a></li>
<li><a href="#sec-1-4-2">Difference of Independent Sample Proportions</a></li>
<li><a href="#sec-1-4-3">Ratio of Independent Sample Variances</a></li>
</ul>
</li>
<li><a href="#sec-1-5">Simulated Sampling Distributions</a>
<ul>
<li><a href="#sec-1-5-1">The Interquartile Range</a></li>
<li><a href="#sec-1-5-2">The Median Absolute Deviation</a></li>
</ul>
</li>
<li><a href="#sec-1-6">Exercises</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-1" class="outline-2">
<h2 id="sec-1">Sampling Distributions</h2>
<div class="outline-text-2" id="text-1">


<p>
This is an important chapter; it is the bridge from probability and descriptive statistics that we studied in Chapters <a href="#cha-Describing-Data-Distributions">Describing-Data-Distributions</a> through <a href="#cha-Multivariable-Distributions">Multivariable-Distributions</a> to inferential statistics which forms the latter part of this book.
</p>
<p>
Here is the link: we are presented with a <i>population</i> about which we would like to learn. And while it would be desirable to examine every single member of the population, we find that it is either impossible or infeasible to for us to do so, thus, we resort to collecting a <i>sample</i> instead. We do not lose heart. Our method will suffice, provided the sample is <i>representative</i> of the population. A good way to achieve this is to sample <i>randomly</i> from the population.
</p>
<p>
Supposing for the sake of argument that we have collected a random sample, the next task is to make some <i>sense</i> out of the data because the complete list of sample information is usually cumbersome, unwieldy. We summarize the data set with a descriptive <i>statistic</i>, a quantity calculated from the data (we saw many examples of these in Chapter <a href="#cha-Describing-Data-Distributions">Describing-Data-Distributions</a>). But our sample was random&hellip; therefore, it stands to reason that our statistic will be random, too. How is the statistic distributed?
</p>
<p>
The probability distribution associated with the population (from which we sample) is called the <i>population distribution</i>, and the probability distribution associated with our statistic is called its <i>sampling distribution</i>; clearly, the two are interrelated. To learn about the population distribution, it is imperative to know everything we can about the sampling distribution. Such is the goal of this chapter.
</p>
<p>
We begin by introducing the notion of simple random samples and cataloguing some of their more convenient mathematical properties. Next we focus on what happens in the special case of sampling from the normal distribution (which, again, has several convenient mathematical properties), and in particular, we meet the sampling distribution of \(\overline{X}\) and \(S^{2}\). Then we explore what happens to \(\overline{X}\)'s sampling distribution when the population is not normal and prove one of the most remarkable theorems in statistics, the Central Limit Theorem (CLT).
</p>
<p>
With the CLT in hand, we then investigate the sampling distributions of several other popular statistics, taking full advantage of those with a tractable form. We finish the chapter with an exploration of statistics whose sampling distributions are not quite so tractable, and to accomplish this goal we will use simulation methods that are grounded in all of our work in the previous four chapters.
</p>

<ul>
<li>the notion of population versus simple random sample, parameter versus statistic, and population distribution versus sampling distribution
</li>
<li>the classical sampling distributions of the standard one and two sample statistics
</li>
<li>how to generate a simulated sampling distribution when the statistic is crazy
</li>
<li>the Central Limit Theorem, period.
</li>
<li>some basic concepts related to sampling distribution utility, such as bias and variance
</li>
</ul>



</div>

<div id="outline-container-1-1" class="outline-3">
<h3 id="sec-1-1">Simple Random Samples</h3>
<div class="outline-text-3" id="text-1-1">



</div>

<div id="outline-container-1-1-1" class="outline-4">
<h4 id="sec-1-1-1">Simple Random Samples</h4>
<div class="outline-text-4" id="text-1-1-1">


<p>
<div class="defn">
If \(X_{1}\), \(X_{2}\), &hellip;, \(X_{n}\) are independent with \(X_{i}\sim f\) for \(i=1,2,\ldots,n\), then we say that \(X_{1}\), \(X_{2}\), &hellip;, \(X_{n}\) are <i>independent and identically distributed</i> (IID) from the population \(f\) or alternatively we say that \(X_{1}\), \(X_{2}\), &hellip;, \(X_{n}\) are a <i>simple random sample of size</i> \(n\), denoted \(SRS(n)\), from the population \(f\). 
</div>
</p>
<p>
<div class="prop">
Let \(X_{1}\), \(X_{2}\), &hellip;, \(X_{n}\) be a \(SRS(n)\) from a population distribution with mean \(\mu\) and finite standard deviation \(\sigma\). Then the mean and standard deviation of \(\overline{X}\) are given by the formulas \(\mu_{\overline{X}}=\mu\) and \(\sigma_{\overline{X}}=\sigma/\sqrt{n}\).
</div>
</p>
<p>
<div class="proof">
Plug in \(a_{1}=a_{2}=\cdots=a_{n}=1/n\) in Proposition <a href="#pro-mean-sd-lin-comb">mean-sd-lin-comb</a>.
</div>
</p>
<p>
The next fact will be useful to us when it comes time to prove the Central Limit Theorem in Section <a href="#sec-Central-Limit-Theorem">Central-Limit-Theorem</a>.
</p>
<p>
<div class="prop">
Let \(X_{1}\), \(X_{2}\), &hellip;, \(X_{n}\) be a \(SRS(n)\) from a population distribution with MGF \(M(t)\). Then the MGF of \(\overline{X}\) is given by
\begin{equation}
M_{\overline{X}}(t)=\left[M\left(\frac{t}{n}\right)\right]^{n}.
\end{equation}
</div>
</p>
<p>
<div class="proof">
Go from the definition:
\begin{eqnarray*}
M_{\overline{X}}(t) & = & \mathbb{E}\,\mathrm{e}^{t\overline{X}},\\
 & = & \mathbb{E}\,\mathrm{e}^{t(X_{1}+\cdots+X_{n})/n},\\
 & = & \mathbb{E}\,\mathrm{e}^{tX_{1}/n}\mathrm{e}^{tX_{2}/n}\cdots\mathrm{e}^{tX_{n}/n}.
\end{eqnarray*}
And because \(X_{1}\), \(X_{2}\), &hellip;, \(X_{n}\) are independent, Proposition <a href="#pro-indep-implies-prodexpect">indep-implies-prodexpect</a> allows us to distribute the expectation among each term in the product, which is
\[
\mathbb{E}\mathrm{e}^{tX_{1}/n}\,\mathbb{E}\mathrm{e}^{tX_{2}/n}\cdots\mathbb{E}\mathrm{e}^{tX_{n}/n}.
\]
The last step is to recognize that each term in last product above is exactly \(M(t/n)\).
</div>
</p>
</div>
</div>

</div>

<div id="outline-container-1-2" class="outline-3">
<h3 id="sec-1-2">Sampling from a Normal Distribution</h3>
<div class="outline-text-3" id="text-1-2">



</div>

<div id="outline-container-1-2-1" class="outline-4">
<h4 id="sec-1-2-1">The Distribution of the Sample Mean</h4>
<div class="outline-text-4" id="text-1-2-1">


<p>
<div class="prop">
Let \(X_{1}\), \(X_{2}\), &hellip;, \(X_{n}\) be a \(SRS(n)\) from a \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) distribution. Then the sample mean \(\overline{X}\) has a \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma/\sqrt{n})\) sampling distribution.
</div>
</p>
<p>
<div class="proof">
The mean and standard deviation of \(\overline{X}\) follow directly from Proposition <a href="#pro-mean-sd-xbar">mean-sd-xbar</a>. To address the shape, first remember from Section <a href="#sec-The-Normal-Distribution">The-Normal-Distribution</a> that the \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) MGF is of the form
\[
M(t)=\exp\left[ \mu t+\sigma^{2}t^{2}/2\right] .
\]
Now use Proposition <a href="#pro-mgf-xbar">mgf-xbar</a> to find
\begin{eqnarray*}
M_{\overline{X}}(t) & = & \left[M\left(\frac{t}{n}\right)\right]^{n},\\
 & = & \left[\exp\left( \mu(t/n)+\sigma^{2}(t/n)^{2}/2\right) \right]^{n},\\
 & = & \exp\left( \, n\cdot\left[\mu(t/n)+\sigma^{2}(t/n)^{2}/2\right]\right) ,\\
 & = & \exp\left( \mu t+(\sigma/\sqrt{n})^{2}t^{2}/2\right),
\end{eqnarray*}
and we recognize this last quantity as the MGF of a \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma/\sqrt{n})\) distribution.
</div>
</p>
</div>

</div>

<div id="outline-container-1-2-2" class="outline-4">
<h4 id="sec-1-2-2">The Distribution of the Sample Variance</h4>
<div class="outline-text-4" id="text-1-2-2">


<p>
<div class="thm">
Let \(X_{1}\), \(X_{2}\), &hellip;, \(X_{n}\) be a \(SRS(n)\) from a \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) distribution, and let
\begin{equation}
\overline{X}=\sum_{i=1}^{n}X_{i}\quad\mbox{and}\quad S^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}.
\end{equation}
Then
</p><ol>
<li>\(\overline{X}\) and \(S^{2}\) are independent, and
</li>
<li>The rescaled sample variance
    \begin{equation}
    \frac{(n-1)}{\sigma^{2}}S^{2}=\frac{\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}}{\sigma^{2}}
    \end{equation}
    has a \(\mathsf{chisq}(\mathtt{df}=n-1)\) sampling distribution.
</li>
</ol>


<p>
</div>
</p>
<p>
<div class="proof">
The proof is beyond the scope of the present book, but the theorem is simply too important to be omitted. The interested reader could consult Casella and Berger \cite{Casella2002}, or Hogg <i>et al</i> \cite{Hogg2005}. 
</div>
</p>
</div>

</div>

<div id="outline-container-1-2-3" class="outline-4">
<h4 id="sec-1-2-3">The Distribution of Student's \(T\) Statistic</h4>
<div class="outline-text-4" id="text-1-2-3">


<p>
<div class="prop">
Let \(X_{1}\), \(X_{2}\), &hellip;, \(X_{n}\) be a \(SRS(n)\) from a \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) distribution. Then the quantity 
\begin{equation}
T=\frac{\overline{X}-\mu}{S/\sqrt{n}}
\end{equation}
has a \(\mathsf{t}(\mathtt{df}=n-1)\) sampling distribution.
</div>
</p>
<p>
<div class="proof">
Divide the numerator and denominator by \(\sigma\) and rewrite
\[
T=\frac{\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}}{S/\sigma}=\frac{\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}}{\sqrt{\left.\frac{(n-1)S^{2}}{\sigma^{2}}\right\slash (n-1)}}.
\]
Now let 
\[
Z=\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\quad\mbox{and}\quad V=\frac{(n-1)S^{2}}{\sigma^{2}},
\]
so that
\begin{equation}
T=\frac{Z}{\sqrt{V/r}},
\end{equation}
where \(r=n-1\).
</p>
<p>
We know from Section <a href="#sub-samp-mean-dist-of">samp-mean-dist-of</a> that \(Z\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) and we know from Section <a href="#sub-Samp-Var-Dist">Samp-Var-Dist</a> that \(V\sim\mathsf{chisq}(\mathtt{df}=n-1)\). Further, since we are sampling from a normal distribution, Theorem <a href="#thm-Xbar-andS">Xbar-andS</a> gives that \(\overline{X}\) and \(S^{2}\) are independent and by Fact <a href="#fac-indep-then-function-indep">indep-then-function-indep</a> so are \(Z\) and \(V\). In summary, the distribution of \(T\) is the same as the distribution of the quantity \(Z/\sqrt{V/r}\), where \(Z\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) and \(V\sim\mathsf{chisq}(\mathtt{df}=r)\) are independent. This is in fact the definition of Student's \(t\) distribution.
</div>
</p>
<p>
This distribution was first published by W. S. Gosset (1900) under the pseudonym Student, and the distribution has consequently come to be known as Student's \(t\) distribution. The PDF of \(T\) can be derived explicitly using the techniques of Section <a href="#sec-Functions-of-Continuous">Functions-of-Continuous</a>; it takes the form 
\begin{equation}
f_{X}(x)=\frac{\Gamma[(r+1)/2]}{\sqrt{r\pi}\ \Gamma(r/2)}\left(1+\frac{x^{2}}{r}\right)^{-(r+1)/2},\quad-\infty<x<\infty.
\end{equation}
Any random variable \(X\) with the preceding PDF is said to have Student's \(t\) distribution with \(r\) <i>degrees of freedom</i>, and we write \(X\sim\mathsf{t}(\mathtt{df}=r)\). The shape of the PDF is similar to the normal, but the tails are considerably heavier. See Figure <a href="#fig-Student-s-t-dist-vary-df">Student's-t-dist-vary-df</a>. As with the normal distribution, there are four functions in \(\mathsf{R}\) associated with the \(t\) distribution, namely <code>dt</code>, <code>pt</code>,=qt=, and <code>rt</code>, which compute the PDF, CDF, quantile function, and generate random variates, respectively.
</p>

<p>
The code to produce Figure <a href="#fig-Student-s-t-dist-vary-df">Student's-t-dist-vary-df</a> is
</p>



<pre class="src src-R">curve(dt(x, df = 30), from = -3, to = 3, lwd = 3, ylab = <span style="color: #8b2252;">"y"</span>)
ind <span style="color: #008b8b;">&lt;-</span> c(1, 2, 3, 5, 10)
<span style="color: #a020f0;">for</span> (i <span style="color: #a020f0;">in</span> ind) curve(dt(x, df = i), -3, 3, add = <span style="color: #228b22;">TRUE</span>)
</pre>







<div id="fig-Student's-t-dist-vary-df" class="figure">
  <p><img src="svg/Student's-t-dist-vary-df.svg" width=500 alt="svg/Student's-t-dist-vary-df.svg" /></p>
  <p>A plot of Student's t distribution for various degrees of freedom.</p>
</div>

<p>
Similar to that done for the normal we may define \(\mathsf{t}_{\alpha}(\mathtt{df}=n-1)\) as the number on the \(x\)-axis such that there is exactly \(\alpha\) area under the \(\mathsf{t}(\mathtt{df}=n-1)\) curve to its right.
</p>
<p>
Find \(\mathsf{t}{}_{0.01}(\mathtt{df}=23)\) with the quantile function.
</p>



<pre class="src src-R">qt(0.01, df = 23, lower.tail = <span style="color: #228b22;">FALSE</span>)
</pre>


<pre class="example">
X11cairo 
       2
[1] 2.499867
</pre>


<p>
<div class="rem">
There are a few things to note about the \(\mathtt{t}(\mathtt{df}=r)\) distribution.
</p><ol>
<li>The \(\mathtt{t}(\mathtt{df}=1)\) distribution is the same as the \(\mathsf{cauchy}(\mathtt{location}=0,\,\mathtt{scale}=1)\) distribution. The Cauchy distribution is rather pathological and is a counterexample to many famous results. 
</li>
<li>The standard deviation of \(\mathsf{t}(\mathtt{df}=r)\) is undefined (that is, infinite) unless \(r&gt;2\). When \(r\) is more than 2, the standard deviation is always bigger than one, but decreases to 1 as \(r\to\infty\).
</li>
<li>As \(r\to\infty\), the \(\mathtt{t}(\mathtt{df}=r)\) distribution approaches the \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) distribution.
</li>
</ol>


<p>
</div>
</p>
</div>
</div>

</div>

<div id="outline-container-1-3" class="outline-3">
<h3 id="sec-1-3">The Central Limit Theorem</h3>
<div class="outline-text-3" id="text-1-3">


<p>
In this section we study the distribution of the sample mean when the underlying distribution is <i>not</i> normal. We saw in Section <a href="#sec-sampling-from-normal-dist">sampling-from-normal-dist</a> that when \(X_{1}\), \(X_{2}\), &hellip; , \(X_{n}\) is a \(SRS(n)\) from a \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) distribution then \(\overline{X}\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma/\sqrt{n})\). In other words, we may say (owing to Fact <a href="#fac-lin-trans-norm-is-norm">lin-trans-norm-is-norm</a>) when the underlying population is normal that the sampling distribution of \(Z\) defined by
\begin{equation}
Z=\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}
\end{equation}
is \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\). 
</p>
<p>
However, there are many populations that are <i>not</i> normal &hellip; and the statistician often finds herself sampling from such populations. What can be said in this case? The surprising answer is contained in the following theorem.
</p>
<p>
<div class="thm">
<b>The Central Limit Theorem.</b> Let \(X_{1}\), \(X_{2}\), &hellip;, \(X_{n}\) be a \(SRS(n)\) from a population distribution with mean \(\mu\) and finite standard deviation \(\sigma\). Then the sampling distribution of 
\begin{equation}
Z=\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}
\end{equation}
approaches a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) distribution as \(n\to\infty\).
</div>
</p>
<p>
<div class="rem">
We suppose that \(X_{1}\), \(X_{2}\), &hellip; , \(X_{n}\) are IID, and we learned in Section <a href="#sub-simple-random-samples">simple-random-samples</a> that \(\overline{X}\) has mean \(\mu\) and standard deviation \(\sigma/\sqrt{n}\), so we already knew that \(Z\) has mean zero and standard deviation one. The beauty of the CLT is that it addresses the <i>shape</i> of \(Z\)'s distribution when the sample size is large.
</div>
</p>
<p>
<div class="rem">
Notice that the shape of the underlying population's distribution is not mentioned in Theorem <a href="#thm-central-limit-thrm">central-limit-thrm</a>; indeed, the result is true for any population that is well-behaved enough to have a finite standard deviation. In particular, if the population is normally distributed then we know from Section <a href="#sub-samp-mean-dist-of">samp-mean-dist-of</a> that the distribution of \(\overline{X}\) (and \(Z\) by extension) is <i>exactly</i> normal, for <i>every</i> \(n\).
</div>
</p>
<p>
<div class="rem">
How large is ``sufficiently large''? It is here that the shape of the underlying population distribution plays a role. For populations with distributions that are approximately symmetric and mound-shaped, the samples may need to be only of size four or five, while for highly skewed or heavy-tailed populations the samples may need to be much larger for the distribution of the sample means to begin to show a bell-shape. Regardless, for a given population distribution (with finite standard deviation) the approximation tends to be better for larger sample sizes.
</div>
</p>

<p>
The <code>TeachingDemos</code> package \cite{Snowteachingdemos} has <code>clt.examp</code> and the <code>distrTeach</code> \cite{Ruckdescheldistr} package has <code>illustrateCLT</code>. Try the following at the command line (output omitted):
</p>


<pre class="src src-R"><span style="color: #008b8b;">library</span>(TeachingDemos)
example(clt.examp)
</pre>

<p>
and
</p>


<pre class="src src-R"><span style="color: #008b8b;">library</span>(distrTeach)
example(illustrateCLT)
</pre>


<p>
The <code>IPSUR</code>  package has the functions <code>clt1</code>, <code>clt2</code>, and <code>clt3</code> (see Exercise <a href="#xca-clt123">clt123</a> at the end of this chapter). Its purpose is to investigate what happens to the sampling distribution of \(\overline{X}\) when the population distribution is mound shaped, finite support, and skewed, namely \(\mathsf{t}(\mathtt{df}=3)\), \(\mathsf{unif}(\mathtt{a}=0,\,\mathtt{b}=10)\), and \(\mathsf{gamma}(\mathtt{shape}=1.21,\,\mathtt{scale}=1/2.37)\), respectively. 
</p>
<p>
For example, when the command <code>clt1()</code>  is issued a plot window opens to show a graph of the PDF of a \(\mathsf{t}(\mathtt{df}=3)\) distribution. On the display are shown numerical values of the population mean and variance. While the students examine the graph the computer is simulating random samples of size <code>sample.size = 2</code> from the population distribution <code>rt</code> a total of <code>N.iter = 100000</code> times, and sample means are calculated of each sample. Next follows a histogram of the simulated sample means, which closely approximates the sampling distribution of \(\overline{X}\), see Section <a href="#sec-Simulated-Sampling-Distributions">Simulated-Sampling-Distributions</a>. Also shown are the sample mean and sample variance of all of the simulated  \( \overline{X} \) values. As a final step, when the student clicks the second plot, a normal curve with the same mean and variance as the simulated \( \overline{X} \) values is superimposed over the histogram. Students should compare the population theoretical mean and variance to the simulated mean and variance of the sampling distribution. They should also compare the shape of the simulated sampling distribution to the shape of the normal distribution.
</p>
<p>
The three separate <code>clt1</code>, <code>clt2</code>, and <code>clt3</code> functions were written so that students could compare what happens overall when the shape of the population distribution changes. It would be possible to combine all three into one big function, <code>clt</code> which covers all three cases (and more). 
</p>
</div>

</div>

<div id="outline-container-1-4" class="outline-3">
<h3 id="sec-1-4">Sampling Distributions of Two-Sample Statistics</h3>
<div class="outline-text-3" id="text-1-4">


<p>
There are often two populations under consideration, and it sometimes of interest to compare properties between groups. To do so we take independent samples from each population and calculate respective sample statistics for comparison. In some simple cases the sampling distribution of the comparison is known and easy to derive; such cases are the subject of the present section.
</p>

</div>

<div id="outline-container-1-4-1" class="outline-4">
<h4 id="sec-1-4-1">Difference of Independent Sample Means</h4>
<div class="outline-text-4" id="text-1-4-1">


<p>
<div class="prop">
Let \(X_{1}\), \(X_{2}\), &hellip; , \(X_{n_{1}}\) be an \(SRS(n_{1})\) from a \(\mathsf{norm}(\mathtt{mean}=\mu_{X},\,\mathtt{sd}=\sigma_{X})\) distribution and let \(Y_{1}\), \(Y_{2}\), &hellip; , \(Y_{n_{2}}\) be an \(SRS(n_{2})\) from a \(\mathsf{norm}(\mathtt{mean}=\mu_{Y},\,\mathtt{sd}=\sigma_{Y})\) distribution. Suppose that \(X_{1}\), \(X_{2}\), &hellip; , \(X_{n_{1}}\) and \(Y_{1}\), \(Y_{2}\), &hellip; , \(Y_{n_{2}}\) are independent samples. Then the quantity
\begin{equation}
\frac{\overline{X}-\overline{Y}-(\mu_{X}-\mu_{Y})}{\sqrt{\left.\sigma_{X}^{2}\right\slash n_{1}+\left.\sigma_{Y}^{2}\right\slash n_{2}}}\label{eq:diff-indep-sample-means}\end{equation}
has a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) sampling distribution. Equivalently, \(\overline{X}-\overline{Y}\) has a \(\mathsf{norm}(\mathtt{mean}=\mu_{X}-\mu_{Y},\,\mathtt{sd}=\sqrt{\left.\sigma_{X}^{2}\right\slash n_{1}+\left.\sigma_{Y}^{2}\right\slash n_{2}})\) sampling distribution.
</div>
</p>
<p>
<div class="proof">
We know that \(\overline{X}\) is \(\mathsf{norm}(\mathtt{mean}=\mu_{X},\,\mathtt{sd}=\sigma_{X}/\sqrt{n_{1}})\) and we also know that \(\overline{Y}\) is \(\mathsf{norm}(\mathtt{mean}=\mu_{Y},\,\mathtt{sd}=\sigma_{Y}/\sqrt{n_{2}})\). And since the samples \(X_{1}\), \(X_{2}\), &hellip;, \(X_{n_{1}}\) and \(Y_{1}\), \(Y_{2}\), &hellip;, \(Y_{n_{2}}\) are independent, so too are \(\overline{X}\) and \(\overline{Y}\). The distribution of their difference is thus normal as well, and the mean and standard deviation are given by Proposition <a href="#pro-mean-sd-lin-comb-two">mean-sd-lin-comb-two</a>.
</div>
</p>
<p>
<div class="rem">
Even if the distribution of one or both of the samples is not normal, the quantity in Equation <a href="#eq-diff-indep-sample-means">diff-indep-sample-means</a> will be approximately normal provided both sample sizes are large.
</div>
</p>
<p>
<div class="rem">
For the special case of \(\mu_{X}=\mu_{Y}\) we have shown that 
\begin{equation} \frac{\overline{X}-\overline{Y}}{\sqrt{\sigma_{X}^{2}/n_{1}+\sigma_{Y}^{2}/n_{2}}}
\end{equation}
has a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) sampling distribution, or in other words, \(\overline{X}-\overline{Y}\) has a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=\sqrt{\sigma_{X}^{2}/n_{1}+\sigma_{Y}^{2}/n_{2}})\) sampling distribution. This will be important when it comes time to do hypothesis tests; see Section <a href="#sec-Conf-Interv-for-Diff-Means">Conf-Interv-for-Diff-Means</a>.
</div>
</p>
</div>

</div>

<div id="outline-container-1-4-2" class="outline-4">
<h4 id="sec-1-4-2">Difference of Independent Sample Proportions</h4>
<div class="outline-text-4" id="text-1-4-2">


<p>
<div class="prop">
Let \(X_{1}\), \(X_{2}\), &hellip;, \(X_{n_{1}}\) be an \(SRS(n_{1})\) from a \(\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p_{1})\) distribution and let \(Y_{1}\), \(Y_{2}\), &hellip;, \(Y_{n_{2}}\) be an \(SRS(n_{2})\) from a \(\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p_{2})\) distribution. Suppose that \(X_{1}\), \(X_{2}\), &hellip; , \(X_{n_{1}}\) and \(Y_{1}\), \(Y_{2}\), &hellip; , \(Y_{n_{2}}\) are independent samples. Define 
\begin{equation}
\hat{p}_{1}=\frac{1}{n_{1}}\sum_{i=1}^{n_{1}}X_{i}\quad\mbox{and}\quad\hat{p}_{2}=\frac{1}{n_{2}}\sum_{j=1}^{n_{2}}Y_{j}.
\end{equation}
Then the sampling distribution of
\begin{equation}
\frac{\hat{p}_{1}-\hat{p}_{2}-(p_{1}-p_{2})}{\sqrt{\frac{p_{1}(1-p_{1})}{n_{1}}+\frac{p_{2}(1-p_{2})}{n_{2}}}}
\end{equation}
approaches a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) distribution as both \(n_{1},\, n_{2}\to\infty\). In other words, the sampling distribution of \(\hat{p}_{1}-\hat{p}_{2}\) is approximately
\begin{equation}
\mathsf{norm}\left(\mathtt{mean}=p_{1}-p_{2},\,\mathtt{sd}=\sqrt{\frac{p_{1}(1-p_{1})}{n_{1}}+\frac{p_{2}(1-p_{2})}{n_{2}}}\right),
\end{equation}
provided both \(n_{1}\) and \(n_{2}\) are sufficiently large.
</div>
</p>
<p>
<div class="proof">
We know that \(\hat{p}_{1}\) is approximately normal for \(n_{1}\) sufficiently large by the CLT, and we know that \(\hat{p}_{2}\) is approximately normal for \(n_{2}\) sufficiently large, also by the CLT. Further, \(\hat{p}_{1}\) and \(\hat{p}_{2}\) are independent since they are derived from independent samples. And a difference of independent (approximately) normal distributions is (approximately) normal, by Exercise <a href="#xca-diff-indep-norm">diff-indep-norm</a>.
The expressions for the mean and standard deviation follow immediately from Proposition <a href="#pro-mean-sd-lin-comb-two">mean-sd-lin-comb-two</a> combined with the formulas for the \(\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p)\) distribution from Chapter <a href="#cha-Discrete-Distributions">Discrete-Distributions</a>.
</div>
</p>
</div>

</div>

<div id="outline-container-1-4-3" class="outline-4">
<h4 id="sec-1-4-3">Ratio of Independent Sample Variances</h4>
<div class="outline-text-4" id="text-1-4-3">


<p>
<div class="prop">
Let \(X_{1}\), \(X_{2}\), &hellip;, \(X_{n_{1}}\) be an \(SRS(n_{1})\) from a \(\mathsf{norm}(\mathtt{mean}=\mu_{X},\,\mathtt{sd}=\sigma_{X})\) distribution and let \(Y_{1}\), \(Y_{2}\), &hellip; , \(Y_{n_{2}}\) be an \(SRS(n_{2})\) from a \(\mathsf{norm}(\mathtt{mean}=\mu_{Y},\,\mathtt{sd}=\sigma_{Y})\) distribution. Suppose that \(X_{1}\), \(X_{2}\), &hellip; , \(X_{n_{1}}\) and \(Y_{1}\), \(Y_{2}\), &hellip; , \(Y_{n_{2}}\) are independent samples. Then the ratio
\begin{equation}
F=\frac{\sigma_{Y}^{2}S_{X}^{2}}{\sigma_{X}^{2}S_{Y}^{2}}
\end{equation}
has an \(\mathsf{f}(\mathtt{df1}=n_{1}-1,\,\mathtt{df2}=n_{2}-1)\) sampling distribution.
</div>
</p>
<p>
<div class="proof">
We know from Theorem <a href="#thm-Xbar-andS">Xbar-andS</a> that \((n_{1}-1)S_{X}^{2}/\sigma_{X}^{2}\) is distributed \(\mathsf{chisq}(\mathtt{df}=n_{1}-1)\) and \((n_{2}-1)S_{Y}^{2}/\sigma_{Y}^{2}\) is distributed \(\mathsf{chisq}(\mathtt{df}=n_{2}-1)\). Now write
\[
F=\frac{\sigma_{Y}^{2}S_{X}^{2}}{\sigma_{X}^{2}S_{Y}^{2}}=\frac{\left.(n_{1}-1)S_{Y}^{2}\right\slash (n_{1}-1)}{\left.(n_{2}-1)S_{Y}^{2}\right\slash (n_{2}-1)}\cdot\frac{\left.1\right\slash \sigma_{X}^{2}}{\left.1\right\slash \sigma_{Y}^{2}},
\]
by multiplying and dividing the numerator with \(n_{1}-1\) and doing likewise for the denominator with \(n_{2}-1\). Now we may regroup the terms into
\[
F=\frac{\left.\frac{(n_{1}-1)S_{X}^{2}}{\sigma_{X}^{2}}\right\slash (n_{1}-1)}{\left.\frac{(n_{2}-1)S_{Y}^{2}}{\sigma_{Y}^{2}}\right\slash (n_{2}-1)},
\]
and we recognize \(F\) to be the ratio of independent \(\mathsf{chisq}\) distributions, each divided by its respective numerator \(\mathtt{df}=n_{1}-1\) and denominator \(\mathtt{df}=n_{1}-1\) degrees of freedom. This is, indeed, the definition of Snedecor's \(F\) distribution. 
</div>
</p>
<p>
<div class="rem">
For the special case of \(\sigma_{X}=\sigma_{Y}\) we have shown that
\begin{equation}
F=\frac{S_{X}^{2}}{S_{Y}^{2}}
\end{equation}
has an \(\mathsf{f}(\mathtt{df1}=n_{1}-1,\,\mathtt{df2}=n_{2}-1)\) sampling distribution. This will be important in Chapters <a href="#cha-Estimation">Estimation</a> onward.
</div>
</p>
</div>
</div>

</div>

<div id="outline-container-1-5" class="outline-3">
<h3 id="sec-1-5">Simulated Sampling Distributions</h3>
<div class="outline-text-3" id="text-1-5">


<p>
Some comparisons are meaningful, but their sampling distribution is not quite so tidy to describe analytically. What do we do then?
</p>
<p>
As it turns out, we do not need to know the exact analytical form of the sampling distribution; sometimes it is enough to approximate it with a simulated distribution. In this section we will show you how. Note that \(\mathsf{R}\) is particularly well suited to compute simulated sampling distributions, much more so than, say, SPSS or SAS.
</p>

</div>

<div id="outline-container-1-5-1" class="outline-4">
<h4 id="sec-1-5-1">The Interquartile Range</h4>
<div class="outline-text-4" id="text-1-5-1">





<pre class="src src-R">iqrs <span style="color: #008b8b;">&lt;-</span> replicate(100, IQR(rnorm(100)))
</pre>


<p>
We can look at the mean of the simulated values
</p>


<pre class="src src-R">mean(iqrs)    <span style="color: #b22222;"># </span><span style="color: #b22222;">close to 1</span>
</pre>


<pre class="example">
[1] 1.344881
</pre>


<p>
and we can see the standard deviation
</p>


<pre class="src src-R">sd(iqrs)
</pre>


<pre class="example">
[1] 0.1605655
</pre>


<p>
Now let's take a look at a plot of the simulated values
</p>



<pre class="src src-R">hist(iqrs, breaks = 20)
</pre>







<div id="fig-simulated-IQR" class="figure">
  <p><img src="svg/simulated-IQR.svg" width=500 alt="svg/simulated-IQR.svg" /></p>
  <p>A plot of simulated IQRs.</p>
</div>

</div>

</div>

<div id="outline-container-1-5-2" class="outline-4">
<h4 id="sec-1-5-2">The Median Absolute Deviation</h4>
<div class="outline-text-4" id="text-1-5-2">





<pre class="src src-R">mads <span style="color: #008b8b;">&lt;-</span> replicate(100, mad(rnorm(100)))
</pre>


<p>
We can look at the mean of the simulated values
</p>



<pre class="src src-R">mean(mads)    <span style="color: #b22222;"># </span><span style="color: #b22222;">close to 1.349</span>
</pre>


<pre class="example">
X11cairo 
       2
[1] 0.9937769
</pre>


<p>
and we can see the standard deviation
</p>



<pre class="src src-R">sd(mads)
</pre>


<pre class="example">
[1] 0.1011322
</pre>


<p>
Now let's take a look at a plot of the simulated values
</p>



<pre class="src src-R">hist(mads, breaks = 20)
</pre>







<div id="fig-simulated-MAD" class="figure">
  <p><img src="svg/simulated-MAD.svg" width=500 alt="svg/simulated-MAD.svg" /></p>
  <p>A plot of simulated MADs.</p>
</div>


</div>
</div>

</div>

<div id="outline-container-1-6" class="outline-3">
<h3 id="sec-1-6">Exercises</h3>
<div class="outline-text-3" id="text-1-6">




<p>
<div class="xca">
Suppose that we observe a random sample \(X_{1}\), \(X_{2}\), &hellip; , \(X_{n}\) of size \( SRS( n =  SRC_R{n[k]} ) \) from a \( \mathsf{norm}(\mathtt{mean}= SRC_R{mu[k]}) \) distribution. 
</p><ol>
<li>What is the mean of \(\overline{X}\)?
</li>
<li>What is the standard deviation of \(\overline{X}\)?
</li>
<li>What is the distribution of \(\overline{X}\)? (approximately)
</li>
<li>Find \(\Pr(a&lt; \overline{X} \leq b)\)
</li>
<li>Find \(\Pr(\overline{X} &gt; c)\).
</li>
</ol>


<p>
</div>
</p>
<p>
<div class="xca">
In this exercise we will investigate how the shape of the population distribution affects the time until the distribution of \(\overline{X}\) is acceptably normal.
</div>
</p>
<p>
<div class="xca">
Let \(X_{1}\),&hellip;, \(X_{25}\) be a random sample from a \(\mathsf{norm}(\mathtt{mean}=37,\,\mathtt{sd}=45)\) distribution, and let \(\overline{X}\) be the sample mean of these \(n=25\) observations.
</p><ol>
<li>How is \(\overline{X}\) distributed? 
   \(\mathsf{norm}(\mathtt{mean}=37,\,\mathtt{sd}=45/\sqrt{25})\) 
</li>
<li>Find \(\Pr(\overline{X} &gt; 43.1)\).
</li>
</ol>

<p>And that's all she wrote.
</div>
</p></div>
</div>
</div>
</div>

<div id="postamble">
<p class="date">Date: 2011-09-21 18:51:16 EDT</p>
<p class="author">Author: G. Jay Kerns</p>
<p class="creator">Org version 7.7 with Emacs version 23</p>
<a href="http://validator.w3.org/check?uri=referer">Validate XHTML 1.0</a>

</div>
</body>
</html>
