#+STARTUP:   indent
#+TITLE:     Hypothesis Testing
#+AUTHOR:    G. Jay Kerns
#+EMAIL:     gkerns@ysu.edu
#+DESCRIPTION: This chapter discusses hypothesis testing in the Neyman-Pearson tradition.
#+KEYWORDS: hypothesis testing statistics one two sample
#+LANGUAGE:  en
#+OPTIONS:   H:3 num:nil toc:t \n:nil @:t ::t |:t ^:{} -:t f:nil *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:nil
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP: index.html   
#+LINK_HOME: http://ipsur.org/index.html
#+XSLT:
#+BABEL: :session *R* :exports results :results value raw :cache no :tangle yes
#+INCLUDE: "prelim.R" src R :exports none :results hide

#+BEGIN_HTML
---
layout: chapter
title: IPSUR - Hypothesis Testing
previous: estimation.html
next: simple-linear-regression.html
description: This chapter discusses hypothesis testing in the Neyman-Pearson tradition.
keywords: hypothesis testing statistics one two sample
---

<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">Hypothesis Testing</a>
<ul>
<li><a href="#sec-1-1">Introduction</a>
<ul>
<li><a href="#sec-1-1-1">Terminology</a></li>
</ul>
</li>
<li><a href="#sec-1-2">One Sample Tests for Means and Variances</a>
<ul>
<li><a href="#sec-1-2-1">For Means</a></li>
<li><a href="#sec-1-2-2">Tests for a Variance</a></li>
</ul>
</li>
<li><a href="#sec-1-3">Two-Sample Tests for Means and Variances</a>
<ul>
<li><a href="#sec-1-3-1">Independent Samples</a></li>
<li><a href="#sec-1-3-2">Paired Samples</a></li>
</ul>
</li>
<li><a href="#sec-1-4">Other Hypothesis Tests</a>
<ul>
<li><a href="#sec-1-4-1">Kolmogorov-Smirnov Goodness-of-Fit Test</a></li>
<li><a href="#sec-1-4-2">Shapiro-Wilk Normality Test</a></li>
</ul>
</li>
<li><a href="#sec-1-5">Analysis of Variance</a></li>
<li><a href="#sec-1-6">Sample Size and Power</a></li>
<li><a href="#sec-1-7">Exercises</a></li>
</ul>
</li>
</ul>
</div>
</div>

#+END_HTML

* Hypothesis Testing                                                 :hypoth:
:PROPERTIES:
:tangle: R/hypoth.R
:END:
#+latex: \label{cha:Hypothesis-Testing}

#+latex: \paragraph*{What do I want them to know?}

- basic terminology and philosophy of the Neyman-Pearson paradigm
- classical hypothesis tests for the standard one and two sample problems with means, variances, and proportions
- the notion of between versus within group variation and how it plays out with one-way ANOVA
- the concept of statistical power and its relation to sample size

** Introduction
#+latex: \label{sec:Introduction-Hypothesis}

I spent a week during the summer of 2005 at the University of Nebraska at Lincoln grading Advanced Placement Statistics exams, and while I was there I attended a presentation by Dr. Roxy Peck. At the end of her talk she described an activity she had used with students to introduce the basic concepts of hypothesis testing. I was impressed by the activity and have used it in my own classes several times since.

The instructor (with a box of cookies in hand) enters a class of fifteen or more students and produces a brand-new, sealed deck of ordinary playing cards. The instructor asks for a student volunteer to break the seal, and then the instructor prominently shuffles the deck
#+latex: \footnote{The jokers are removed before shuffling.}
several times in front of the class, after which time the students are asked to line up in a row. They are going to play a game. Each student will draw a card from the top of the deck, in turn. If the card is black, then the lucky student will get a cookie. If the card is red, then the unlucky student will sit down empty-handed. Let the game begin.

The first student draws a card: red. There are jeers and outbursts, and the student slinks off to his/her chair. (S)he is disappointed, of course, but not really. After all, (s)he had a 50-50 chance of getting black, and it did not happen. Oh well.

The second student draws a card: red, again. There are more jeers, and the second student slips away. This student is also disappointed, but again, not so much, because it is probably his/her unlucky day. On to the next student.

The student draws: red again! There are a few wiseguys who yell (happy to make noise, more than anything else), but there are a few other students who are not yelling any more -- they are thinking. This is the third red in a row, which is possible, of course, but what is going on, here? They are not quite sure. They are now concentrating on the next card... it is bound to be black, right?

The fourth student draws: red. Hmmm... now there are groans instead of outbursts. A few of the students at the end of the line shrug their shoulders and start to make their way back to their desk, complaining that the teacher does not want to give away any cookies. There are still some students in line though, salivating, waiting for the inevitable black to appear.

The fifth student draws red. Now it isn't funny any more. As the remaining students make their way back to their seats an uproar ensues, from an entire classroom demanding cookies.

#+latex: \bigskip{}

Keep the preceding experiment in the back of your mind as you read the following sections. When you have finished the entire chapter, come back and read this introduction again. All of the mathematical jargon that follows is connected to the above paragraphs. In the meantime, I will get you started:

- Null hypothesis: :: it is an ordinary deck of playing cards, shuffled thoroughly.
- Alternative hypothesis: :: either it is a trick deck of cards, or the instructor did some fancy shufflework.
- Observed data: :: a sequence of draws from the deck, five reds in a row.

If it were truly an ordinary, well-shuffled deck of cards, the probability of observing zero blacks out of a sample of size five (without replacement) from a deck with 26 black cards and 26 red cards would be

#+begin_src R :exports both :results output pp 
dhyper(0, m = 26, n = 26, k = 5)
#+end_src

There are two very important final thoughts. First, everybody gets a cookie in the end. Second, the students invariably (and aggressively) attempt to get me to open up the deck and reveal the true nature of the cards. I never do.


Tests for Proportions
#+latex: \label{sec:Tests-for-Proportions}

#+latex: \begin{example}
#+latex: \label{exa:widget-machine}

We have a machine that makes widgets. 

- Under normal operation, about 0.10 of the widgets produced are defective.
- Go out and purchase a torque converter.
- Install the torque converter, and observe \(n=100\) widgets from the machine.
- Let \(Y=\mbox{number of defective widgets observed}\).

If

- \(Y=0\), then the torque converter is great!
- \(Y=4\), then the torque converter seems to be helping. 
- \(Y=9\), then there is not much evidence that the torque converter helps.
- \(Y=17\), then throw away the torque converter.

Let \(p\) denote the proportion of defectives produced by the machine. Before the installation of the torque converter \(p\) was \(0.10\). Then we installed the torque converter. Did \(p\) change? Did it go up or down? We use statistics to decide. Our method is to observe data and construct a 95% confidence interval for \(p\),
\begin{equation}
\hat{p}\pm z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}.
\end{equation}
If the confidence interval is 
- \([0.01,\,0.05]\), then we are 95% confident that \(0.01\leq p\leq0.05\), so there is evidence that the torque converter is helping.
- \([0.15,\,0.19]\), then we are 95% confident that \(0.15\leq p\leq0.19\), so there is evidence that the torque converter is hurting.
- \([0.07,\,0.11]\), then there is not enough evidence to conclude that the torque converter is doing anything at all, positive or negative.

#+latex: \end{example}

*** Terminology

The /null hypothesis/ \(H_{0}\) is a ``nothing'' hypothesis, whose interpretation could be that nothing has changed, there is no difference, there is nothing special taking place, /etc/. In Example \ref{exa:widget-machine} the null hypothesis would be \(H_{0}:\, p=0.10.\) The /alternative hypothesis/ \(H_{1}\) is the hypothesis that something has changed, in this case, \(H_{1}:\, p\neq0.10\). Our goal is to statistically /test/ the hypothesis \(H_{0}:\, p=0.10\) versus the alternative \(H_{1}:\, p\neq0.10\). Our procedure will be:
1. Go out and collect some data, in particular, a simple random sample of observations from the machine.
2. Suppose that \(H_{0}\) is true and construct a \(100(1-\alpha)\%\) confidence interval for \(p\).
3. If the confidence interval does not cover \(p=0.10\), then we /reject/ \(H_{0}\). Otherwise, we /fail to reject/ \(H_{0}\).

#+begin_rem
Every time we make a decision it is possible to be wrong, and there are two possible mistakes that we could make. We have committed a 
- Type I Error :: if we reject \(H_{0}\) when in fact \(H_{0}\) is true. This would be akin to convicting an innocent person for a crime (s)he did not commit.
- Type II Error :: if we fail to reject \(H_{0}\) when in fact \(H_{1}\) is true. This is analogous to a guilty person escaping conviction.
#+end_rem

Type I Errors are usually considered worse
#+latex: \footnote{There is no mathematical difference between the errors, however. The bottom line is that we choose one type of error to control with an iron fist, and we try to minimize the probability of making the other type. That being said, null hypotheses are often by design to correspond to the ``simpler'' model, so it is often easier to analyze (and thereby control) the probabilities associated with Type I Errors.}, 
and we design our statistical procedures to control the probability of making such a mistake. We define the
\begin{equation}
\mbox{significance level of the test}=\Pr(\mbox{Type I Error})=\alpha.
\end{equation}
We want \(\alpha\) to be small which conventionally means, say, \(\alpha=0.05\), \(\alpha=0.01\), or \(\alpha=0.005\) (but could mean anything, in principle).
- The /rejection region/ (also known as the /critical region/) for the test is the set of sample values which would result in the rejection of \(H_{0}\). For Example \ref{exa:widget-machine}, the rejection region would be all possible samples that result in a 95% confidence interval that does not cover \(p=0.10\).
- The above example with \(H_{1}:p\neq0.10\) is called a /two-sided/ test. Many times we are interested in a /one-sided/ test, which would look like \(H_{1}:p<0.10\) or \(H_{1}:p>0.10\).

We are ready for tests of hypotheses for one proportion.
Table here.
Don't forget the assumptions.

#+latex: \begin{example}
Find
1. The null and alternative hypotheses.
2. Check your assumptions.
3. Define a critical region with an \(\alpha=0.05\) significance level.
4. Calculate the value of the test statistic and state your conclusion.
#+latex: \end{example}

#+latex: \begin{example}
#+latex: \label{exa:prop-test-pvalue-A}

Suppose \(p=\mbox{the proportion of students}\) who are admitted to the graduate school of the University of California at Berkeley, and suppose that a public relations officer boasts that UCB has historically had a 40% acceptance rate for its graduate school. Consider the data stored in the table =UCBAdmissions= from 1973. Assuming these observations constituted a simple random sample, are they consistent with the officer's claim, or do they provide evidence that the acceptance rate was significantly less than 40%? Use an \(\alpha=0.01\) significance level.

Our null hypothesis in this problem is \(H_{0}:\, p=0.4\) and the alternative hypothesis is \(H_{1}:\, p<0.4\). We reject the null hypothesis if \(\hat{p}\) is too small, that is, if
\begin{equation} 
\frac{\hat{p}-0.4}{\sqrt{0.4(1-0.4)/n}}<-z_{\alpha},
\end{equation}
where \(\alpha=0.01\) and \(-z_{0.01}\) is 
#+begin_src R :exports both :results output pp 
-qnorm(0.99)
#+end_src

Our only remaining task is to find the value of the test statistic and see where it falls relative to the critical value. We can find the number of people admitted and not admitted to the UCB graduate school with the following. 

#+begin_src R :exports both :results output pp 
A <- as.data.frame(UCBAdmissions)
head(A)
xtabs(Freq ~ Admit, data = A)
#+end_src

Now we calculate the value of the test statistic.

#+begin_src R :exports both :results output pp 
phat <- 1755/(1755 + 2771)
(phat - 0.4)/sqrt(0.4 * 0.6/(1755 + 2771)) 
#+end_src

Our test statistic is not less than \(-2.32\), so it does not fall into the critical region. Therefore, we /fail/ to reject the null hypothesis that the true proportion of students admitted to graduate school is less than 40% and say that the observed data are consistent with the officer's claim at the \(\alpha=0.01\) significance level. 

#+latex: \end{example}

#+latex: \begin{example}
#+latex: \label{exa:prop-test-pvalue-B}
We are going to do Example \ref{exa:prop-test-pvalue-A} all over again. Everything will be exactly the same except for one change. Suppose we choose significance level \(\alpha=0.05\) instead of \(\alpha=0.01\). Are the 1973 data consistent with the officer's claim?

Our null and alternative hypotheses are the same. Our observed test statistic is the same: it was approximately \(-1.68\). But notice that our critical value has changed: \(\alpha=0.05\) and \(-z_{0.05}\) is 
#+latex: \end{example}
#+begin_src R :exports both :results output pp 
-qnorm(0.95)
#+end_src

Our test statistic is less than \(-1.64\) so it now falls into the critical region! We now /reject/ the null hypothesis and conclude that the 1973 data provide evidence that the true proportion of students admitted to the graduate school of UCB in 1973 was significantly less than 40%. The data are /not/ consistent with the officer's claim at the \(\alpha=0.05\) significance level.

What is going on, here? If we choose \(\alpha=0.05\) then we reject the null hypothesis, but if we choose \(\alpha=0.01\) then we fail to reject the null hypothesis. Our final conclusion seems to depend on our selection of the significance level. This is bad; for a particular test, we never know whether our conclusion would have been different if we had chosen a different significance level. 

Or do we?

Clearly, for some significance levels we reject, and for some significance levels we do not. Where is the boundary? That is, what is the significance level for which we would /reject/ at any significance level /bigger/, and we would /fail to reject/ at any significance level /smaller/? This boundary value has a special name: it is called the /p-value/ of the test.

#+begin_defn
The /p-value/, or /observed significance level/, of a hypothesis test is the probability when the null hypothesis is true of obtaining the observed value of the test statistic (such as \(\hat{p}\)) or values more extreme -- meaning, in the direction of the alternative hypothesis
#+latex: \footnote{Bickel and Doksum \cite{Bickel2001} state the definition particularly well: the \(p\)-value is ``the smallest level of significance \(\alpha\) at which an experimenter using the test statistic \(T\) would reject \(H_{0}\) on the basis of the observed sample outcome \(x\)''.}. 
#+end_defn

#+latex: \begin{example}
Calculate the \(p\)-value for the test in Examples \ref{exa:prop-test-pvalue-A} and \ref{exa:prop-test-pvalue-B}.

The \(p\)-value for this test is the probability of obtaining a \(z\)-score equal to our observed test statistic (which had \(z\)-score \(\approx-1.680919\)) or more extreme, which in this example is less than the observed test statistic. In other words, we want to know the area under a standard normal curve on the interval \((-\infty,\,-1.680919]\). We can get this easily with
#+latex: \end{example}

#+begin_src R :exports both :results output pp 
pnorm(-1.680919)
#+end_src

We see that the \(p\)-value is strictly between the significance levels \(\alpha=0.01\) and \(\alpha=0.05\). This makes sense: it has to be bigger than \(\alpha=0.01\) (otherwise we would have rejected \(H_{0}\) in Example \ref{exa:prop-test-pvalue-A}) and it must also be smaller than \(\alpha=0.05\) (otherwise we would not have rejected \(H_{0}\) in Example \ref{exa:prop-test-pvalue-B}). Indeed, \(p\)-values are a characteristic indicator of whether or not we would have rejected at assorted significance levels, and for this reason a statistician will often skip the calculation of critical regions and critical values entirely. If (s)he knows the \(p\)-value, then (s)he knows immediately whether or not (s)he would have rejected at /any/ given significance level.

Thus, another way to phrase our significance test procedure is: we will reject \(H_{0}\) at the \(\alpha\)-level of significance if the \(p\)-value is less than \(\alpha\).

#+begin_rem
If we have two populations with proportions \(p_{1}\) and \(p_{2}\) then we can test the null hypothesis \(H_{0}:p_{1}=p_{2}\).
#+end_rem

Table Here.

#+latex: \begin{example}
Example.
#+latex: \end{example}


#+latex: \paragraph*{How to do it with \(\mathsf{R}\)}

The following does the test.

#+begin_src R :exports both :results output pp 
prop.test(1755, 1755 + 2771, p = 0.4, alternative = "less", 
          conf.level = 0.99, correct = FALSE)
#+end_src

Do the following to make the plot.


Use Yates' continuity correction when the expected frequency of successes is less than 10. You can use it all of the time, but you will have a decrease in power. For large samples the correction does not matter. 

#+latex: \paragraph*{With the \(\mathsf{R}\) Commander}

If you already know the number of successes and failures, then you can use the menu =Statistics= \(\triangleright\) =Proportions= \(\triangleright\) =IPSUR Enter table for single sample...=

Otherwise, your data -- the raw successes and failures -- should be in a column of the Active Data Set. Furthermore, the data must be stored as a ``factor'' internally. If the data are not a factor but are numeric then you can use the menu =Data= \(\triangleright\) =Manage variables in active data set= \(\triangleright\) =Convert numeric variables to factors...= to convert the variable to a factor. Or, you can always use the =factor= function.

Once your unsummarized data is a column, then you can use the menu =Statistics= \(\triangleright\) =Proportions= \(\triangleright\) =Single-sample proportion test...=

** One Sample Tests for Means and Variances
#+latex: \label{sec:One-Sample-Tests}

*** For Means

Here, \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) are a \(SRS(n)\) from a \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) distribution. We would like to test \(H_{0}:\mu=\mu_{0}\).

- Case A: :: Suppose \(\sigma\) is known. Then under \(H_{0}\),
   \[
   Z=\frac{\overline{X}-\mu_{0}}{\sigma/\sqrt{n}}\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1).
   \]
   Table here.
- Case B: :: When \(\sigma\) is unknown, under \(H_{0}\)
   \[
   T = \frac{\overline{X}-\mu_{0}}{S/\sqrt{n}}\sim\mathsf{t}(\mathtt{df}=n-1).
   \]
   Table here.


#+begin_rem
If \(\sigma\) is unknown but \(n\) is large then we can use the \(z\)-test.
#+end_rem

#+latex: \begin{example}
In this example we
1. Find the null and alternative hypotheses.
2. Choose a test and find the critical region.
3. Calculate the value of the test statistic and state the conclusion.
4. Find the \(p\)-value.

#+latex: \end{example}

#+begin_rem
Another name for a \(p\)-value is /tail end probability/. We reject \(H_{0}\) when the \(p\)-value is small.
The quantity \(\sigma/\sqrt{n}\), when \(\sigma\) is known, is called the /standard error of the sample mean/. In general, if we have an estimator \(\hat{\theta}\) then \(\sigma_{\hat{\theta}}\) is called the /standard error/ of \(\hat{\theta}\). We usually need to estimate \(\sigma_{\hat{\theta}}\) with \(\hat{\sigma_{\hat{\theta}}}\).
#+end_rem

#+latex: \paragraph*{How to do it with \(\mathsf{R}\)}

I am thinking =z.test=\index{z.test@\texttt{z.test}} in =TeachingDemos=, =t.test=\index{t.test@\texttt{t.test}} in base \(\mathsf{R}\).

#+begin_src R :exports both :results output pp 
x <- rnorm(37, mean = 2, sd = 3)
library(TeachingDemos)
z.test(x, mu = 1, sd = 3, conf.level = 0.90)
#+end_src

The =RcmdrPlugin.IPSUR= package does not have a menu for =z.test= yet. 

#+begin_src R :exports both :results output pp 
x <- rnorm(13, mean = 2, sd = 3)
t.test(x, mu = 0, conf.level = 0.90, alternative = "greater")
#+end_src

#+latex: \paragraph*{With the \(\mathsf{R}\) Commander}

Your data should be in a single numeric column (a variable) of the Active Data Set. Use the menu =Statistics= \(\triangleright\) =Means= \(\triangleright\) =Single-sample t-test...= 

*** Tests for a Variance

Here, \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) are a \(SRS(n)\) from a \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) distribution. We would like to test \(H_{0}:\sigma^{2}=\sigma_{0}\). We know that under \(H_{0}\),
\[
X^{2}=\frac{(n-1)S^{2}}{\sigma^{2}}\sim\mathsf{chisq}(\mathtt{df}=n-1).
\]
Table here.

#+latex: \begin{example}
Give some data and a hypothesis.
- Give an \(\alpha\)-level and test the critical region way.
- Find the \(p\)-value for the test.
#+latex: \end{example}


#+latex: \paragraph*{How to do it with \(\mathsf{R}\)}
I am thinking about =sigma.test=\index{sigma.test@\texttt{sigma.test}} in the =TeachingDemos= package.

#+begin_src R :exports both :results output pp 
library(TeachingDemos)
sigma.test(women$height, sigma = 8)
#+end_src

** Two-Sample Tests for Means and Variances
#+latex: \label{sec:Two-Sample-Tests-for-Means}

The basic idea for this section is the following. We have \(X\sim\mathsf{norm}(\mathtt{mean}=\mu_{X},\,\mathtt{sd}=\sigma_{X})\) and \(Y\sim\mathsf{norm}(\mathtt{mean}=\mu_{Y},\,\mathtt{sd}=\sigma_{Y})\). distributed independently. We would like to know whether \(X\) and \(Y\) come from the same population distribution, that is, we would like to know:
\begin{equation}
\mbox{Does }X\overset{\mathrm{d}}{=}Y?
\end{equation}
where the symbol \(\overset{\mathrm{d}}{=}\) means equality of probability distributions.
Since both \(X\) and \(Y\) are normal, we may rephrase the question:
\begin{equation}
\mbox{Does }\mu_{X}=\mu_{Y}\mbox{ and }\sigma_{X}=\sigma_{Y}?
\end{equation}
Suppose first that we do not know the values of \(\sigma_{X}\) and \(\sigma_{Y}\), but we know that they are equal, \(\sigma_{X}=\sigma_{Y}\). Our test would then simplify to \(H_{0}:\mu_{X}=\mu_{Y}\). We collect data \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) and \(Y_{1}\), \(Y_{2}\), ..., \(Y_{m}\), both simple random samples of size \(n\) and \(m\) from their respective normal distributions. Then under \(H_{0}\) (that is, assuming \(H_{0}\) is true) we have \(\mu_{X}=\mu_{Y}\) or rewriting, \(\mu_{X}-\mu_{Y}=0\), so 
\begin{equation}
T=\frac{\overline{X}-\overline{Y}}{S_{p}\sqrt{\frac{1}{n}+\frac{1}{m}}}=\frac{\overline{X}-\overline{Y}-(\mu_{X}-\mu_{Y})}{S_{p}\sqrt{\frac{1}{n}+\frac{1}{m}}}\sim\mathsf{t}(\mathtt{df}=n+m-2).
\end{equation}

*** Independent Samples

#+begin_rem
If the values of \(\sigma_{X}\) and \(\sigma_{Y}\) are known, then we can plug them in to our statistic:
\begin{equation} 
Z=\frac{\overline{X}-\overline{Y}}{\sqrt{\sigma_{X}^{2}/n+\sigma_{Y}^{2}/m}};
\end{equation}
the result will have a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) distribution when \(H_{0}:\mu_{X}=\mu_{Y}\) is true. 
#+end_rem

#+begin_rem
Even if the values of \(\sigma_{X}\) and \(\sigma_{Y}\) are not known, if both \(n\) and \(m\) are large then we can plug in the sample estimates and the result will have approximately a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) distribution when \(H_{0}:\mu_{X}=\mu_{Y}\) is true.
\begin{equation} 
Z=\frac{\overline{X}-\overline{Y}}{\sqrt{S_{X}^{2}/n+S_{Y}^{2}/m}}.
\end{equation}
#+end_rem

#+begin_rem
It is usually important to construct side-by-side boxplots and other visual displays in concert with the hypothesis test. This gives a visual comparison of the samples and helps to identify departures from the test's assumptions -- such as outliers.
#+end_rem

#+begin_rem
WATCH YOUR ASSUMPTIONS.
- The normality assumption can be relaxed as long as the population distributions are not highly skewed.
- The equal variance assumption can be relaxed as long as both sample sizes \(n\) and \(m\) are large. However, if one (or both) samples is small, then the test does not perform well; we should instead use the methods of Chapter \ref{cha:resampling-methods}.
#+end_rem

For a nonparametric alternative to the two-sample \(F\) test see Chapter \ref{cha:Nonparametric-Statistics}.

*** Paired Samples

#+latex: \paragraph*{How to do it with \(\mathsf{R}\)}

#+begin_src R :exports both :results output pp 
t.test(extra ~ group, data = sleep, paired = TRUE)
#+end_src

** Other Hypothesis Tests
#+latex: \label{sec:Other-Hypothesis-Tests}

*** Kolmogorov-Smirnov Goodness-of-Fit Test
#+latex: \label{sub:Kolmogorov-Smirnov-Goodness-of-Fit-Test}

#+latex: \paragraph*{How to do it with \(\mathsf{R}\)}

#+begin_src R :exports both :results output pp 
ks.test(randu$x, "punif")
#+end_src

*** Shapiro-Wilk Normality Test
#+latex: \label{sub:Shapiro-Wilk-Normality-Test}

#+latex: \paragraph*{How to do it with \(\mathsf{R}\)}

#+begin_src R :exports both :results output pp 
shapiro.test(women$height)
#+end_src

** Analysis of Variance
#+latex: \label{sec:Analysis-of-Variance}

#+latex: \paragraph*{How to do it with \(\mathsf{R}\)}

I am thinking 
#+begin_src R :exports both :results output pp 
with(chickwts, by(weight, feed, shapiro.test))
#+end_src
and
#+begin_src R :exports code :results silent 
temp <- lm(weight ~ feed, data = chickwts)
#+end_src
and 
#+begin_src R :exports both :results output pp 
anova(temp)
#+end_src

Plot for the intuition of between versus within group variation.

#+srcname: Between-versus-within
#+begin_src R :exports none :results silent
y1 <- rnorm(300, mean = c(2,8,22))
plot(y1, xlim = c(-1,25), ylim = c(0,0.45) , type = "n")
f <- function(x){dnorm(x, mean = 2)}
curve(f, from = -1, to = 5, add = TRUE, lwd = 2)
f <- function(x){dnorm(x, mean = 8)}
curve(f, from = 5, to = 11, add = TRUE, lwd = 2)
f <- function(x){dnorm(x, mean = 22)}
curve(f, from = 19, to = 25, add = TRUE, lwd = 2)
rug(y1)
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/Between-versus-within.ps
  <<Between-versus-within>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/Between-versus-within.svg
  <<Between-versus-within>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/Between-versus-within.ps}
  \caption[Between group versus within group variation]{A plot of between group versus within group variation.}
  \label{fig:Between-versus-within}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-Between-versus-within" class="figure">
  <p><img src="svg/Between-versus-within.svg" width=500 alt="svg/Between-versus-within.svg" /></p>
  <p>A plot of between group versus within group variation.</p>
</div>
#+end_html

#+srcname: Some-F-plots-HH
#+begin_src R :exports code :results silent
library(HH)
old.omd <- par(omd = c(.05,.88, .05,1))
F.setup(df1 = 5, df2 = 30)
F.curve(df1 = 5, df2 = 30, col='blue')
F.observed(3, df1 = 5, df2 = 30)
par(old.omd)
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/Some-F-plots-HH.ps
  <<Some-F-plots-HH>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/Some-F-plots-HH.svg
  <<Some-F-plots-HH>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/Some-F-plots-HH.ps}
  \caption[Some \(F\) plots from the \texttt{HH} package]{\small Some \(F\) plots from the \texttt{HH} package.}
  \label{fig:Some-F-plots-HH}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-Some-F-plots-HH" class="figure">
  <p><img src="svg/Some-F-plots-HH.svg" width=500 alt="svg/Some-F-plots-HH.svg" /></p>
  <p>Some \(F\) plots from the <code>HH</code> package.</p>
</div>
#+end_html

** Sample Size and Power
#+latex: \label{sec:Sample-Size-and-Power}

The power function of a test for a parameter \(\theta\) is
\[
\beta(\theta)=\Pr_{\theta}(\mbox{Reject }H_{0}),\quad-\infty<\theta<\infty.
\]
Here are some properties of power functions:
1. \(\beta(\theta)\leq\alpha\) for any \(\theta\in\Theta_{0}\), and \(\beta(\theta_{0})=\alpha\). We interpret this by saying that no matter what value \(\theta\) takes inside the null parameter space, there is never more than a chance of \(\alpha\) of rejecting the null hypothesis. We have controlled the Type I error rate to be no greater than \(\alpha\).
2. \(\lim_{n\to\infty}\beta(\theta)=1\) for any fixed \(\theta\in\Theta_{1}\). In other words, as the sample size grows without bound we are able to detect a nonnull value of \(\theta\) with increasing accuracy, no matter how close it lies to the null parameter space. This may appear to be a good thing at first glance, but it often turns out to be a curse, because this means that our Type II error rate grows as the sample size increases. 

#+latex: \paragraph*{How to do it with \(\mathsf{R}\)}

I am thinking about =replicate=\index{replicate@\texttt{replicate}} here, and also =power.examp=\index{power.examp@\texttt{power.examp}} from the =TeachingDemos= package. There is an even better plot in upcoming work from the =HH= package.

#+srcname: power-examp
#+begin_src R :exports code :results silent
library(TeachingDemos)
power.examp()
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/power-examp.ps
  <<power-examp>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/power-examp.svg
  <<power-examp>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/power-examp.ps}
  \caption[Plot of significance level and power]{\small This graph was generated by the \texttt{power.examp} function from the \texttt{TeachingDemos} package. The plot corresponds to the hypothesis test \(H_{0}:\,\mu=\mu_{0}\) versus \(H_{1}:\,\mu=\mu_{1}\) (where \(\mu_{0}=0\) and \(\mu_{1}=1\), by default) based on a single observation \(X\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\). The top graph is of the \(H_{0}\) density while the bottom is of the \(H_{1}\) density. The significance level is set at \(\alpha=0.05\), the sample size is \(n=1\), and the standard deviation is \(\sigma=1\). The pink area is the significance level, and the critical value \(z_{0.05}\approx1.645\) is marked at the left boundary -- this defines the rejection region. When \(H_{0}\) is true, the probability of falling in the rejection region is exactly \(\alpha=0.05\). The same rejection region is marked on the bottom graph, and the probability of falling in it (when \(H_{1}\)  is true) is the blue area shown at the top of the display to be approximately \(0.26\). This probability represents the \emph{power} to detect a non-null mean value of \(\mu=1\). With the command the \texttt{run.power.examp()} at the command line the same plot opens, but in addition, there are sliders available that allow the user to interactively change the sample size \(n\), the standard deviation \(\sigma\), the true difference between the means \(\mu_{1}-\mu_{0}\), and the significance level \(\alpha\). By playing around the student can investigate the effect each of the aforementioned parameters has on the statistical power. Note that you need the \texttt{tkrplot} package for \texttt{run.power.examp}.}
  \label{fig:power-examp}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-power-examp" class="figure">
  <p><img src="svg/power-examp.svg" width=500 alt="svg/power-examp.svg" /></p>
  <p>A plot of significance level and power.</p>
</div>
#+end_html

#+latex: \newpage{}

** Exercises

#+latex: \setcounter{thm}{0}
