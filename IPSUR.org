#+STARTUP:   indent
#+TITLE:     Introduction to Probability and Statistics Using R
#+AUTHOR:    G. Jay Kerns
#+EMAIL:     gkerns@ysu.edu
#+LANGUAGE:  en
#+OPTIONS:   H:5 toc:3 \n:nil @:t ::t |:t ^:{} -:t f:nil *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:nil
#+INFOJS_OPT: view:info toc:nil sdepth:2 ltoc:nil mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+LINK_UP: IPSUR.html
#+LINK_HOME: http://ipsur.org/index.html
#+STYLE: <link rel="stylesheet" type="text/css" href="css/stylesheet.css" />
#+EXPORT_SELECT_TAGS:
#+PROPERTY: session *R*
#+PROPERTY: exports results
#+PROPERTY: results value raw
#+PROPERTY: cache no
#+PROPERTY: tangle yes
#+LaTeX_CLASS: scrbook
#+LaTeX_CLASS_OPTIONS: [captions=tableheading]
#+LaTeX_CLASS_OPTIONS: [10pt,english]
#+LaTeX_HEADER: \input{tex/preamble}
#+LATEX: \input{tex/frontmatter}
#+LATEX: \input{tex/preface-second}
#+LATEX: \input{tex/preface-first}
#+INCLUDE: "R/prelim.R" src R :exports none :results hide

#+html: <link REL="SHORTCUT ICON" HREF="http://ipsur.org/img/favicon.ico">

* An Introduction to Probability and Statistics                     :introps:
:PROPERTIES:
:tangle: R/introps.R
:END:
#+latex: \pagenumbering{arabic} 

#+begin_html
<div class="outline-2">
  <h3>Getting Started</h3>
  <div class="outline-text-2">
    <p>
      If you are new here, the first thing you should do is type <code>?</code> (or click the <code>HELP</code> link in the upper left).  There you will find many keyboard shortcuts to make your life easier.  These include:
    </p>
    <table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
      <caption>Keyboard Shortcuts</caption>
      <colgroup><col class="left" /><col class="left" />
      </colgroup>
      <tbody>
	<tr><td class="left"><b>n</b> / <b>p</b></td><td class="left">goto the next / previous section</td></tr>
	<tr><td class="left"><b>i</b> / <b>C</b> </td><td class="left">show the table of contents / tags index</td></tr>
      <tr><td class="left"><b>m</b> / <b>x</b></td><td class="left">toggle view mode info / plain</td></tr>
	<tr><td class="left"><b>s</b> / <b>r</b></td><td class="left">search forward / backward</td></tr>
      </tbody>
    </table>
  </div>
</div>
#+end_html

#+latex: \noindent 
This chapter has proved to be the hardest to write, by far. The trouble is that there is so much to say -- and so many people have already said it so much better than I could. When I get something I like I will release it here.

In the meantime, there is a lot of information already available to a person with an Internet connection. I recommend to start at Wikipedia, which is not a flawless resource but it has the main ideas with links to reputable sources.

In my lectures I usually tell stories about Fisher, Galton, Gauss, Laplace, Quetelet, and the Chevalier de Mere.

** Probability

The common folklore is that probability has been around for millennia but did not gain the attention of mathematicians until approximately 1654 when the Chevalier de Mere had a question regarding the fair division of a game's payoff to the two players, supposing the game had to end prematurely.

** Statistics

Statistics concerns data; their collection, analysis, and interpretation. In this book we distinguish between two types of statistics: descriptive and inferential. 

Descriptive statistics concerns the summarization of data. We have a data set and we would like to describe the data set in multiple ways. Usually this entails calculating numbers from the data, called descriptive measures, such as percentages, sums, averages, and so forth.

Inferential statistics does more. There is an inference associated with the data set, a conclusion drawn about the population from which the data originated.

I would like to mention that there are two schools of thought of statistics: frequentist and bayesian. The difference between the schools is related to how the two groups interpret the underlying probability (see Section [[sec-Interpreting-Probabilities][Interpreting Probabilities]]). The frequentist school gained a lot of ground among statisticians due in large part to the work of Fisher, Neyman, and Pearson in the early twentieth century. That dominance lasted until inexpensive computing power became widely available; nowadays the bayesian school is garnering more attention and at an increasing rate.

This book is devoted mostly to the frequentist viewpoint because that is how I was trained, with the conspicuous exception of Sections [[sec-Bayes-Rule][Bayes' Rule]] and [[sec-Conditional-Distributions][Conditional Distributions]]. I plan to add more bayesian material in later editions of this book.

#+latex: \newpage{}

** Exercises
#+latex: \setcounter{thm}{0}

* An Introduction to R                                               :introR:
:PROPERTIES:
:tangle: R/introR.R
:CUSTOM_ID: cha-introduction-to-R
:END:

#+begin_src R :exports none :eval never
# Chapter: Introduction to R
# All code released under GPL Version 3
#+end_src

Every \(\mathsf{R}\) book I have ever seen has had a section/chapter that is an introduction to \(\mathsf{R}\), and so does this one.  The goal of this chapter is for a person to get up and running, ready for the material that follows.  See Section [[sec-External-Resources][External Resources]] for links to other material which the reader may find useful. 

*What do I want them to know?*
- Where to find \(\mathsf{R}\) to install on a home computer, and a few comments to help with the usual hiccups that occur when installing something.
- Abbreviated remarks about the available options to interact with \(\mathsf{R}\).
- Basic operations (arithmetic, entering data, vectors) at the command prompt.
- How and where to find help when they get in trouble.
- Other little shortcuts I am usually asked when introducing \(\mathsf{R}\).

** Downloading and Installing \(\mathsf{R}\)
:PROPERTIES:
:CUSTOM_ID: sec-download-install-R
:END:  

The instructions for obtaining \(\mathsf{R}\) largely depend on the user's hardware and operating system. The \(\mathsf{R}\)  Project has written an \(\mathsf{R}\)  Installation and Administration manual with complete, precise instructions about what to do, together with all sorts of additional information. The following is just a primer to get a person started.

*** Installing \(\mathsf{R}\) 

Visit one of the links below to download the latest version of \(\mathsf{R}\) 
for your operating system:

- Microsoft Windows: :: http://cran.r-project.org/bin/windows/base/
- MacOS: :: http://cran.r-project.org/bin/macosx/
- Linux: :: http://cran.r-project.org/bin/linux/

On Microsoft Windows, click the =R-x.y.z.exe= installer to start installation. When it asks for "Customized startup options", specify =Yes=. In the next window, be sure to select the SDI (single document interface) option; this is useful later when we discuss three dimensional plots with the =rgl= package \cite{rgl}.

**** Installing \(\mathsf{R}\) on a USB drive (Windows)

With this option you can use \(\mathsf{R}\) portably and without administrative privileges. There is an entry in the \(\mathsf{R}\) for Windows FAQ about this. Here is the procedure I use:  
1. Download the Windows installer above and start installation as usual. When it asks /where/ to install, navigate to the top-level directory of the USB drive instead of the default =C= drive.
2. When it asks whether to modify the Windows registry, uncheck the box; we do NOT want to tamper with the registry. 
3. After installation, change the name of the folder from =R-x.y.z= to just plain \(\mathsf{R}\). (Even quicker: do this in step 1.) 
4. [[http://ipsur.r-forge.r-project.org/book/download/R.exe][Download this shortcut]] and move it to the top-level directory of the USB drive, right beside the \(\mathsf{R}\) folder, not inside the folder. Use the downloaded shortcut to run \(\mathsf{R}\).

Steps 3 and 4 are not required but save you the trouble of navigating to the =R-x.y.z/bin= directory to double-click =Rgui.exe= every time you want to run the program. It is useless to create your own shortcut to =Rgui.exe=. Windows does not allow shortcuts to have relative paths; they always have a drive letter associated with them. So if you make your own shortcut and plug your USB drive into some /other/ machine that happens to assign your drive a different letter, then your shortcut will no longer be pointing to the right place. 

*** Installing and Loading Add-on Packages
:PROPERTIES:
:CUSTOM_ID: sub-installing-loading-packages
:END:

There are /base/ packages (which come with \(\mathsf{R}\) automatically), and /contributed/ packages (which must be downloaded for installation). For example, on the version of \(\mathsf{R}\) being used for this document the default base packages loaded at startup are 

#+begin_src R :exports both :results output pp
getOption("defaultPackages")
#+end_src

The base packages are maintained by a select group of volunteers, called \(\mathsf{R}\) Core. In addition to the base packages, there are literally thousands of additional contributed packages written by individuals all over the world. These are stored worldwide on mirrors of the Comprehensive \(\mathsf{R}\) Archive Network, or =CRAN= for short. Given an active Internet connection, anybody is free to download and install these packages and even inspect the source code.

To install a package named =foo=, open up \(\mathsf{R}\) and type =install.packages("foo")=\index{install.packages@\texttt{install.packages}}. To install =foo= and additionally install all of the other packages on which =foo= depends, instead type =install.packages("foo", depends = TRUE)=.

The general command =install.packages()= will (on most operating systems) open a window containing a huge list of available packages; simply choose one or more to install.

No matter how many packages are installed onto the system, each one must first be loaded for use with the =library=\index{library@\texttt{library}} function. For instance, the =foreign= package \cite{foreign} contains all sorts of functions needed to import data sets into \(\mathsf{R}\) from other software such as SPSS, SAS, /etc/. But none of those functions will be available until the command =library("foreign")= is issued. 

Type =library()= at the command prompt (described below) to see a list of all available packages in your library.

For complete, precise information regarding installation of \(\mathsf{R}\) and add-on packages, see the [[http://cran.r-project.org/manuals.html][\(\mathsf{R}\) Installation and Administration manual]].

** Communicating with \(\mathsf{R}\)
:PROPERTIES:
:CUSTOM_ID: sec-Communicating-with-R
:END:

*** One line at a time

This is the most basic method and is the first one that beginners will use.
- RGui (Microsoft \(\circledR\) Windows)
- Terminal
- Emacs/ESS, XEmacs
- JGR

*** Multiple lines at a time

For longer programs (called /scripts/) there is too much code to write all at once at the command prompt. Furthermore, for longer scripts it is convenient to be able to only modify a certain piece of the script and run it again in \(\mathsf{R}\). Programs called /script editors/ are specially designed to aid the communication and code writing process. They have all sorts of helpful features including \(\mathsf{R}\) syntax highlighting, automatic code completion, delimiter matching, and dynamic help on the \(\mathsf{R}\) functions as they are being written. Even more, they often have all of the text editing features of programs like Microsoft\(\circledR\)Word. Lastly, most script editors are fully customizable in the sense that the user can customize the appearance of the interface to choose what colors to display, when to display them, and how to display them.

- \(\mathsf{R}\) Editor (Windows):\index{R Editor@\textsf{R} Editor} :: In Microsoft\(\circledR\) Windows, \(\mathsf{R}\) Gui has its own built-in script editor, called \(\mathsf{R}\) Editor. From the console window, select =File= \(\triangleright\) =New Script=. A script window opens, and the lines of code can be written in the window. When satisfied with the code, the user highlights all of the commands and presses \textsf{Ctrl+R}. The commands are automatically run at once in \(\mathsf{R}\) and the output is shown. To save the script for later, click =File= \(\triangleright\) =Save as...= in \(\mathsf{R}\) Editor. The script can be reopened later with =File= \(\triangleright\)} =Open Script...= in =RGui=. Note that \(\mathsf{R}\) Editor does not have the fancy syntax highlighting that the others do.
- \(\mathsf{R}\) WinEdt:\index{RWinEdt@\textsf{R}WinEdt} :: This option is coordinated with WinEdt for \LaTeX{} and has additional features such as code highlighting, remote sourcing, and a ton of other things. However, one first needs to download and install a shareware version of another program, WinEdt, which is only free for a while -- pop-up windows will eventually appear that ask for a registration code. \(\mathsf{R}\) WinEdt is nevertheless a very fine choice if you already own WinEdt or are planning to purchase it in the near future.
- Tinn \(\mathsf{R}\) / Sciviews K:\index{Tinn R@Tinn \textsf{R}}\index{Sciviews K} :: This one is completely free and has all of the above mentioned options and more. It is simple enough to use that the user can virtually begin working with it immediately after installation. But Tinn \(\mathsf{R}\) proper is only available for Microsoft\(\circledR\) Windows operating systems. If you are on MacOS or Linux, a comparable alternative is Sci-Views - Komodo Edit.
- Emacs/ESS:\index{Emacs}\index{ESS} :: Emacs is an all purpose text editor. It can do absolutely anything with respect to modifying, searching, editing, and manipulating, text. And if Emacs can't do it, then you can write a program that extends Emacs to do it. Once such extension is called =ESS=, which stands for /E/-macs /S/-peaks /S/-tatistics. With ESS a person can speak to \(\mathsf{R}\), do all of the tricks that the other script editors offer, and much, much, more. Please see the following for installation details, documentation, reference cards, and a whole lot more: http://ess.r-project.org.
     /Fair warning/: if you want to try Emacs and if you grew up with Microsoft\(\circledR\) Windows or Macintosh, then you are going to need to relearn everything you thought you knew about computers your whole life. (Or, since Emacs is completely customizable, you can reconfigure Emacs to behave the way you want.) I have personally experienced this transformation and I will never go back.
- JGR (read ``Jaguar''):\index{JGR} :: This one has the bells and whistles of =RGui= plus it is based on Java, so it works on multiple operating systems. It has its own script editor like \(\mathsf{R}\) Editor but with additional features such as syntax highlighting and code-completion. If you do not use Microsoft\(\circledR\) Windows (or even if you do) you definitely want to check out this one. 
- Kate, Bluefish, /etc/ :: There are literally dozens of other text editors available, many of them free, and each has its own (dis)advantages. I only have mentioned the ones with which I have had substantial personal experience and have enjoyed at some point. Play around, and let me know what you find.

*** Graphical User Interfaces (GUIs)

By the word ``GUI'' I mean an interface in which the user communicates with \(\mathsf{R}\) by way of points-and-clicks in a menu of some sort. Again, there are many, many options and I only mention ones that I have used and enjoyed. Some of the other more popular script editors can be downloaded from the \(\mathsf{R}\)-Project website at http://www.sciviews.org/_rgui/. On the left side of the screen (under *Projects*) there are several choices available. 

- \(\mathsf{R}\) Commander :: provides\index{R Commander@\textsf{R} Commander} a point-and-click interface to many basic statistical tasks. It is called the ``Commander'' because every time one makes a selection from the menus, the code corresponding to the task is listed in the output window. One can take this code, copy-and-paste it to a text file, then re-run it again at a later time without the \(\mathsf{R}\) Commander's assistance. It is well suited for the introductory level. =Rcmdr= \cite{Rcmdr} also allows for user-contributed ``Plugins'' which are separate packages on =CRAN= that add extra functionality to the =Rcmdr= package. The plugins are typically named with the prefix =RcmdrPlugin= to make them easy to identify in the =CRAN= package list. One such plugin is the =RcmdrPlugin.IPSUR= package \cite{RcmdrPlugin.IPSUR} which accompanies this text.
- Poor Man's GUI\index{Poor Man's GUI} :: is an alternative to the =Rcmdr= which is based on GTk instead of Tcl/Tk. It has been a while since I used it but I remember liking it very much when I did. One thing that stood out was that the user could drag-and-drop data sets for plots. See here for more information: http://wiener.math.csi.cuny.edu/pmg/. 
- Rattle\index{Rattle} :: is a data mining toolkit which was designed to manage/analyze very large data sets, but it provides enough other general functionality to merit mention here. See \cite{rattle} for more information.
- Deducer\index{Deducer} :: is relatively new and shows promise from what I have seen, but I have not actually used it in the classroom yet.

** Basic \(\mathsf{R}\) Operations and Concepts
:PROPERTIES:
:CUSTOM_ID: sec-Basic-R-Operations
:END:

The \(\mathsf{R}\) developers have written an introductory document entitled ``An Introduction to \(\mathsf{R}\)''. There is a sample session included which shows what basic interaction with \(\mathsf{R}\) looks like. I recommend that all new users of \(\mathsf{R}\) read that document, but bear in mind that there are concepts mentioned which will be unfamiliar to the beginner.

Below are some of the most basic operations that can be done with \(\mathsf{R}\). Almost every book about \(\mathsf{R}\) begins with a section like the one below; look around to see all sorts of things that can be done at this most basic level.

*** Arithmetic
:PROPERTIES:
:CUSTOM_ID: sub-Arithmetic
:END:

#+begin_src R :exports both :results output pp  
2 + 3       # add
4 * 5 / 6   # multiply and divide
7^8         # 7 to the 8th power
#+end_src

Notice the comment character =#=\index{#@\texttt{\#}}. Anything typed after a =#= symbol is ignored by \(\mathsf{R}\). We know that \(20/6\) is a repeating decimal, but the above example shows only 7 digits. We can change the number of digits displayed with =options=\index{options@\texttt{options}}:

#+begin_src R :exports both :results output pp 
options(digits = 16)
10/3                 # see more digits
sqrt(2)              # square root
exp(1)               # Euler's constant, e
pi       
options(digits = 7)  # back to default
#+end_src

Note that it is possible to set =digits=\index{digits@\texttt{digits}} up to 22, but setting them over 16 is not recommended (the extra significant digits are not necessarily reliable). Above notice the =sqrt=\index{sqrt@\texttt{sqrt}} function for square roots and the =exp=\index{exp@\texttt{exp}} function for powers of \(\mathrm{e}\), Euler's number.

*** Assignment, Object names, and Data types
:PROPERTIES:
:CUSTOM_ID: sub-Assignment-Object-names
:END:

It is often convenient to assign numbers and values to variables (objects) to be used later. The proper way to assign values to a variable is with the =<-= operator (with a space on either side). The ~=~ symbol works too, but it is recommended by the \(\mathsf{R}\) masters to reserve ~=~ for specifying arguments to functions (discussed later). In this book we will follow their advice and use =<-= for assignment. Once a variable is assigned, its value can be printed by simply entering the variable name by itself.

#+begin_src R :exports both :results output pp 
x <- 7*41/pi   # don't see the calculated value
x              # take a look
#+end_src

When choosing a variable name you can use letters, numbers, dots ``\texttt{.}'', or underscore ``\texttt{\_}'' characters. You cannot use mathematical operators, and a leading dot may not be followed by a number. Examples of valid names are: =x=, =x1=, =y.value=, and =!y_hat=. (More precisely, the set of allowable characters in object names depends on one's particular system and locale; see An Introduction to \(\mathsf{R}\) for more discussion on this.)

Objects can be of many /types/, /modes/, and /classes/. At this level, it is not necessary to investigate all of the intricacies of the respective types, but there are some with which you need to become familiar:

- integer: :: the values \(0\), \(\pm1\), \(\pm2\), ...; these are represented exactly by \(\mathsf{R}\).
- double: :: real numbers (rational and irrational); these numbers are not represented exactly (save integers or fractions with a denominator that is a power of 2, see \cite{Venables2010}).
- character: :: elements that are wrapped with pairs of ="= or ';
- logical: :: includes =TRUE=, =FALSE=, and =NA= (which are reserved words); the =NA=\index{NA@\texttt{NA}} stands for ``not available'', /i.e./, a missing value.

You can determine an object's type with the =typeof=\index{typeof@\texttt{typeof}} function. In addition to the above, there is the =complex=\index{complex@\texttt{complex}}\index{as.complex@\texttt{as.complex}} data type:

#+begin_src R :exports both :results output pp 
sqrt(-1)              # isn't defined
sqrt(-1+0i)           # is defined
sqrt(as.complex(-1))  # same thing
(0 + 1i)^2            # should be -1
typeof((0 + 1i)^2)
#+end_src

Note that you can just type =(1i)^2= to get the same answer. The =NaN=\index{NaN@\texttt{NaN}} stands for ``not a number''; it is represented internally as =double=\index{double}. 

*** Vectors
:PROPERTIES:
:CUSTOM_ID: sub-Vectors
:END:

All of this time we have been manipulating vectors of length 1. Now let us move to vectors with multiple entries.

**** Entering data vectors

*The long way:*\index{c@\texttt{c}} If you would like to enter the data =74,31,95,61,76,34,23,54,96= into \(\mathsf{R}\), you may create a data vector with the =c= function (which is short for /concatenate/).

#+begin_src R :exports both :results output pp 
x <- c(74, 31, 95, 61, 76, 34, 23, 54, 96)
x
#+end_src

The elements of a vector are usually coerced by \(\mathsf{R}\) to the the most general type of any of the elements, so if you do =c(1, "2")= then the result will be =c("1", "2")=.

*A shorter way:* \index{scan@\texttt{scan}}: The =scan= method is useful when the data are stored somewhere else. For instance, you may type =x <- scan()= at the command prompt and \(\mathsf{R}\) will display =1:= to indicate that it is waiting for the first data value. Type a value and press =Enter=, at which point \(\mathsf{R}\) will display =2:=, and so forth. Note that entering an empty line stops the scan. This method is especially handy when you have a column of values, say, stored in a text file or spreadsheet. You may copy and paste them all at the =1:= prompt, and \(\mathsf{R}\) will store all of the values instantly in the vector =x=. 

*Repeated data; regular patterns:* the =seq=\index{seq@\texttt{seq}} function will generate all sorts of sequences of numbers. It has the arguments =from=, =to=, =by=, and =length.out= which can be set in concert with one another. We will do a couple of examples to show you how it works.

#+begin_src R :exports both :results output pp 
seq(from = 1, to = 5)
seq(from = 2, by = -0.1, length.out = 4)
#+end_src

Note that we can get the first line much quicker with the colon operator.

#+begin_src R :exports both :results output pp 
1:5
#+end_src

The vector =LETTERS=\index{LETTERS@\texttt{LETTERS}} has the 26 letters of the English alphabet in uppercase and =letters=\index{letters@\texttt{letters}} has all of them in lowercase.

**** Indexing data vectors

Sometimes we do not want the whole vector, but just a piece of it. We can access the intermediate parts with the =[]=\index{[]@\texttt{{[}{]}}} operator. Observe (with =x= defined above)

#+begin_src R :exports both :results output pp 
x[1]
x[2:4]
x[c(1,3,4,8)]
x[-c(1,3,4,8)]
#+end_src

Notice that we used the minus sign to specify those elements that we do /not/ want. 

#+begin_src R :exports both :results output pp 
LETTERS[1:5]
letters[-(6:24)]
#+end_src

*** Functions and Expressions
:PROPERTIES:
:CUSTOM_ID: sub-Functions-and-Expressions
:END:

A function takes arguments as input and returns an object as output. There are functions to do all sorts of things. We show some examples below.

#+begin_src R :exports both :results output pp 
x <- 1:5
sum(x)
length(x)
min(x)
mean(x)      # sample mean
sd(x)        # sample standard deviation
#+end_src

It will not be long before the user starts to wonder how a particular function is doing its job, and since \(\mathsf{R}\) is open-source, anybody is free to look under the hood of a function to see how things are calculated. For detailed instructions see the article ``Accessing the Sources'' by Uwe Ligges \cite{Ligges2006}. In short:

*Type the name of the function* without any parentheses or arguments. If you are lucky then the code for the entire function will be printed, right there looking at you. For instance, suppose that we would like to see how the =intersect=\index{intersect@\texttt{intersect}} function works:

#+begin_src R :exports both :results output pp 
intersect
#+end_src

*If instead* it shows =UseMethod(something)=\index{UseMethod@\texttt{UseMethod}} then you will need to choose the /class/ of the object to be inputted and next look at the /method/ that will be /dispatched/ to the object. For instance, typing =rev=\index{rev@\texttt{rev}} says 

#+begin_src R :exports both :results output pp 
rev
#+end_src

The output is telling us that there are multiple methods associated with the =rev= function. To see what these are, type

#+begin_src R :exports both :results output pp 
methods(rev)
#+end_src

Now we learn that there are two different =rev(x)= functions, only one of which being chosen at each call depending on what =x= is. There is one for =dendrogram= objects and a =default= method for everything else. Simply type the name to see what each method does. For example, the =default= method can be viewed with

#+begin_src R :exports both :results output pp 
rev.default
#+end_src

*Some functions are hidden* by a /namespace/ (see An Introduction to \(\mathsf{R}\) \cite{Venables2010}), and are not visible on the first try. For example, if we try to look at the code for =wilcox.test=\index{wilcox.test@\texttt{wilcox.test}} (see Chapter [[cha-Nonparametric-Statistics][Nonparametric Statistics]]) we get the following:

#+begin_src R :exports both :results output pp 
wilcox.test
methods(wilcox.test)
#+end_src

If we were to try =wilcox.test.default=  we would get a ``not found'' error, because it is hidden behind the namespace for the package =stats= \cite{stats} (shown in the last line when we tried =wilcox.test=). In cases like these we prefix the package name to the front of the function name with three colons; the command =stats:::wilcox.test.default= will show the source code, omitted here for brevity.

*If it shows* =.Internal(something)=\index{.Internal@\texttt{.Internal}} or =.Primitive(something)=\index{.Primitive@\texttt{.Primitive}}, then it will be necessary to download the source code of \(\mathsf{R}\) (which is /not/ a binary version with an =.exe= extension) and search inside the code there. See Ligges \cite{Ligges2006} for more discussion on this. An example is =exp=:

#+begin_src R :exports both :results output pp 
exp
#+end_src

Be warned that most of the =.Internal= functions are written in other computer languages which the beginner may not understand, at least initially.

** Getting Help
:PROPERTIES:
:CUSTOM_ID: sec-Getting-Help
:END:

When you are using \(\mathsf{R}\), it will not take long before you find yourself needing help. Fortunately, \(\mathsf{R}\) has extensive help resources and you should immediately become familiar with them. Begin by clicking =Help= on =RGui=. The following options are available. 
- Console: :: gives useful shortcuts, for instance, =Ctrl+L=, to clear the \(\mathsf{R}\) console screen. 
- FAQ on \(\mathsf{R}\): :: frequently asked questions concerning general \(\mathsf{R}\) operation.
- FAQ on \(\mathsf{R}\) for Windows: :: frequently asked questions about \(\mathsf{R}\), tailored to the Microsoft Windows operating system.
- Manuals: :: technical manuals about all features of the \(\mathsf{R}\) system including installation, the complete language definition, and add-on packages.
- \(\mathsf{R}\) functions (text)...: :: use this if you know the /exact/ name of the function you want to know more about, for example, =mean= or =plot=. Typing =mean= in the window is equivalent to typing =help("mean")=\index{help@\texttt{help}} at the command line, or more simply, =?mean=\index{?@\texttt{?}}. Note that this method only works if the function of interest is contained in a package that is already loaded into the search path with =library=. 
- HTML Help: :: use this to browse the manuals with point-and-click links. It also has a Search Engine \& Keywords for searching the help page titles, with point-and-click links for the search results. This is possibly the best help method for beginners. It can be started from the command line with the command =help.start()=\index{help.start@\texttt{help.start}}.
- Search help ...: :: use this if you do not know the exact name of the function of interest, or if the function is in a package that has not been loaded yet. For example, you may enter =plo= and a text window will return listing all the help files with an alias, concept, or title matching `=plo=' using regular expression matching; it is equivalent to typing =help.search("plo")=\index{help.search@\texttt{help.search}} at the command line. The advantage is that you do not need to know the exact name of the function; the disadvantage is that you cannot point-and-click the results. Therefore, one may wish to use the HTML Help search engine instead. An equivalent way is =??plo=\index{??@\texttt{??}} at the command line.
- search.r-project.org ...: :: this will search for words in help lists and email archives of the \(\mathsf{R}\) Project. It can be very useful for finding other questions that other users have asked. 
- Apropos ...: :: use this for more sophisticated partial name matching of functions. See =?apropos=\index{apropos@\texttt{apropos}} for details.

On the help pages for a function there are sometimes ``Examples'' listed at the bottom of the page, which will work if copy-pasted at the command line (unless marked otherwise). The =example=\index{example@\texttt{example}} function will run the code automatically, skipping the intermediate step. For instance, we may try =example(mean)= to see a few examples of how the =mean= function works.

*** \(\mathsf{R}\) Help Mailing Lists

There are several mailing lists associated with \(\mathsf{R}\), and there is a huge community of people that read and answer questions related to \(\mathsf{R}\). See [[http://www.r-project.org/mail.html][here]] for an idea of what is available. Particularly pay attention to the bottom of the page which lists several special interest groups (SIGs) related to \(\mathsf{R}\).

Bear in mind that \(\mathsf{R}\) is free software, which means that it was written by volunteers, and the people that frequent the mailing lists are also volunteers who are not paid by customer support fees. Consequently, if you want to use the mailing lists for free advice then you must adhere to some basic etiquette, or else you may not get a reply, or even worse, you may receive a reply which is a bit less cordial than you are used to. Below are a few considerations: 
1. Read the [[http://cran.r-project.org/faqs.html][FAQ]]. Note that there are different FAQs for different operating systems. You should read these now, even without a question at the moment, to learn a lot about the idiosyncrasies of \(\mathsf{R}\).
2. Search the archives. Even if your question is not a FAQ, there is a very high likelihood that your question has been asked before on the mailing list. If you want to know about topic =foo=, then you can do =RSiteSearch("foo")=\index{RSiteSearch@\texttt{RSiteSearch}} to search the mailing list archives (and the online help) for it. 
3. Do a Google search and an \texttt{RSeek.org} search.

If your question is not a FAQ, has not been asked on \(\mathsf{R}\)-help before, and does not yield to a Google (or alternative) search, then, and only then, should you even consider writing to \(\mathsf{R}\)-help. Below are a few additional considerations. 

- Read the [[http://www.r-project.org/posting-guide.html][posting guide]] before posting. This will save you a lot of trouble and pain. 
- Get rid of the command prompts (=>=) from output. Readers of your message will take the text from your mail and copy-paste into an \(\mathsf{R}\) session. If you make the readers' job easier then it will increase the likelihood of a response. 
- Questions are often related to a specific data set, and the best way to communicate the data is with a =dump=\index{dump@\texttt{dump}} command. For instance, if your question involves data stored in a vector =x=, you can type =dump("x","")= at the command prompt and copy-paste the output into the body of your email message. Then the reader may easily copy-paste the message from your email into \(\mathsf{R}\) and =x= will be available to him/her.
- Sometimes the answer the question is related to the operating system used, the attached packages, or the exact version of \(\mathsf{R}\) being used. The =sessionInfo()=\index{sessionInfo@\texttt{sessionInfo}} command collects all of this information to be copy-pasted into an email (and the Posting Guide requests this information). See Appendix [[cha-R-Session-Information][R Session Info]] for an example.

** External Resources
:PROPERTIES:
:CUSTOM_ID: sec-External-Resources
:END:

There is a mountain of information on the Internet about \(\mathsf{R}\). Below are a few of the important ones. 
- The \(\mathsf{R}\)- Project for Statistical Computing\index{The R-Project@The \textsf{R}-Project}: Go [[http://www.r-project.org/][there]] first.
- The Comprehensive \(\mathsf{R}\) Archive Network\index{CRAN}: [[http://cran.r-project.org/][That is where]] \(\mathsf{R}\) is stored along with thousands of contributed packages. There are also loads of contributed information (books, tutorials, /etc/.). There are mirrors all over the world with duplicate information.
- \(\mathsf{R}\)-Forge\index{R-Forge@\textsf{R}-Forge}: [[http://r-forge.r-project.org/][This is another location]] where \(\mathsf{R}\) packages are stored. Here you can find development code which has not yet been released to =CRAN=. 
- \(\mathsf{R}\)-Wiki\index{R-Wiki@\textsf{R}-Wiki}: There are many tips, tricks, and general advice [[http://wiki.r-project.org/rwiki/doku.php][listed here]]. If you find a trick of your own, login and share it with the world. 
- Other: the [[http://addictedtor.free.fr/graphiques/][\(\mathsf{R}\) Graph Gallery]]\index{R Graph Gallery@\textsf{R} Graph Gallery} and [[http://bm2.genes.nig.ac.jp/RGM2/index.php][\(\mathsf{R}\) Graphical Manual]]\index{R Graphical Manual@\textsf{R} Graphical Manual} have literally thousands of graphs to peruse. [[http://www.rseek.org][\(\mathsf{R}\) Seek]] is a search engine based on Google specifically tailored for \(\mathsf{R}\) queries. 

** Other Tips

It is unnecessary to retype commands repeatedly, since \(\mathsf{R}\) remembers what you have recently entered on the command line. On the Microsoft\(\circledR\) Windows \(\mathsf{R}\) Gui, to cycle through the previous commands just push the \(\uparrow\) (up arrow) key. On Emacs/ESS the command is =M-p= (which means hold down the =Alt= button and press ``p''). More generally, the command =history()=\index{history@\texttt{history}} will show a whole list of recently entered commands. 
- To find out what all variables are in the current work environment, use the commands =objects()=\index{objects@\texttt{objects}} or =ls()=\index{ls@\texttt{ls}}. These list all available objects in the workspace. If you wish to remove one or more variables, use =remove(var1, var2, var3)=\index{remove@\texttt{remove}}, or more simply use =rm(var1, var2, var3)=, and to remove all objects use =rm(list = ls())=.
- Another use of =scan= is when you have a long list of numbers (separated by spaces or on different lines) already typed somewhere else, say in a text file. To enter all the data in one fell swoop, first highlight and copy the list of numbers to the Clipboard with =Edit= \(\triangleright\) =Copy= (or by right-clicking and selecting =Copy=). Next type the =x <- scan()= command in the \(\mathsf{R}\) console, and paste the numbers at the =1:= prompt with =Edit= \(\triangleright\) =Paste=. All of the numbers will automatically be entered into the vector =x=.
- The command =Ctrl+l= clears the display in the Microsoft\(\circledR\) Windows \(\mathsf{R}\) Gui. In Emacs/ESS, press =Ctrl+l= repeatedly to cycle point (the place where the cursor is) to the bottom, middle, and top of the display.  
- Once you use \(\mathsf{R}\) for awhile there may be some commands that you wish to run automatically whenever \(\mathsf{R}\) starts. These commands may be saved in a file called =Rprofile.site=\index{Rprofile.site@\texttt{Rprofile.site}} which is usually in the =etc= folder, which lives in the \(\mathsf{R}\) home directory (which on Microsoft\(\circledR\) Windows usually is =C:\Program Files\R=). Alternatively, you can make a file =.Rprofile=\index{.Rprofile@\texttt{.Rprofile}} to be stored in the user's home directory, or anywhere \(\mathsf{R}\) is invoked. This allows for multiple configurations for different projects or users. See ``Customizing the Environment'' of /An Introduction to R/ for more details.
- When exiting \(\mathsf{R}\) the user is given the option to ``save the workspace''. I recommend that beginners DO NOT save the workspace when quitting. If =Yes= is selected, then all of the objects and data currently in \(\mathsf{R}\)'s memory is saved in a file located in the working directory called =.RData=\index{.RData@\texttt{.RData}}. This file is then automatically loaded the next time \(\mathsf{R}\) starts (in which case \(\mathsf{R}\) will say =[previously saved workspace restored]=). This is a valuable feature for experienced users of \(\mathsf{R}\), but I find that it causes more trouble than it saves with beginners. 

#+latex: \newpage{}

** Exercises
#+latex: \setcounter{thm}{0}

* Data Description                                                 :datadesc:
:PROPERTIES:
:tangle: R/datadesc.R
:CUSTOM_ID: cha-Describing-Data-Distributions
:END:

#+begin_src R :exports none :eval never
# Chapter: Data Description
# All code released under GPL Version 3
#+end_src

#+latex: \noindent 
In this chapter we introduce the different types of data that a statistician is likely to encounter, and in each subsection we give some examples of how to display the data of that particular type. Once we see how to display data distributions, we next introduce the basic properties of data distributions. We qualitatively explore several data sets. Once that we have intuitive properties of data sets, we next discuss how we may numerically measure and describe those properties with descriptive statistics.

 *What do I want them to know?*

- different data types, such as quantitative versus qualitative, nominal versus ordinal, and discrete versus continuous
- basic graphical displays for assorted data types, and some of their (dis)advantages 
- fundamental properties of data distributions, including center, spread, shape, and crazy observations
- methods to describe data (visually/numerically) with respect to the properties, and how the methods differ depending on the data type
- all of the above in the context of grouped data, and in particular, the concept of a factor

** Types of Data
:PROPERTIES:
:CUSTOM_ID: sec-Types-of-Data
:END: 

Loosely speaking, a datum is any piece of collected information, and a data set is a collection of data related to each other in some way. We will categorize data into five types and describe each in turn:

- Quantitative :: data associated with a measurement of some quantity on an observational unit,
- Qualitative :: data associated with some quality or property of an observational unit,
- Logical :: data which represent true or false and play an important role later,
- Missing :: data which should be there but are not, and
- Other types :: everything else under the sun.

In each subsection we look at some examples of the type in question and introduce methods to display them.

*** Quantitative data
:PROPERTIES:
:CUSTOM_ID: sub-Quantitative-Data
:END:

Quantitative data are any data that measure or are associated with a measurement of the quantity of something. They invariably assume numerical values. Quantitative data can be further subdivided into two categories. 

- /Discrete data/ take values in a finite or countably infinite set of numbers, that is, all possible values could (at least in principle) be written down in an ordered list. Examples include: counts, number of arrivals, or number of successes. They are often represented by integers, say, 0, 1, 2, /etc/.
- /Continuous data/ take values in an interval of numbers. These are also known as scale data, interval data, or measurement data. Examples include: height, weight, length, time, /etc/. Continuous data are often characterized by fractions or decimals: 3.82, 7.0001, 4 \(\frac{5}{8}\), /etc/.

Note that the distinction between discrete and continuous data is not always clear-cut. Sometimes it is convenient to treat data as if they were continuous, even though strictly speaking they are not continuous. See the examples.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
*Annual Precipitation in US Cities.* The vector =precip=\index{Data sets!precip@\texttt{precip}} contains average amount of rainfall (in inches) for each of 70 cities in the United States and Puerto Rico. Let us take a look at the data:

#+begin_src R :exports both :results output pp  
str(precip)
#+end_src

#+begin_src R :exports both :results output pp  
precip[1:4]
#+end_src

The output shows that =precip= is a numeric vector which has been /named/, that is, each value has a name associated with it (which can be set with the =names=\index{names@\texttt{names}} function). These are quantitative continuous data. 
#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
*Lengths of Major North American Rivers.* The U.S. Geological Survey recorded the lengths (in miles) of several rivers in North America. They are stored in the vector =rivers=\index{Data sets!rivers@\texttt{rivers}} in the =datasets= package \cite{datasets} (which ships with base \(\mathsf{R}\)). See =?rivers=. Let us take a look at the data with the =str=\index{str@\texttt{str}} function.

#+begin_src R :exports both :results output pp  
str(rivers)
#+end_src

The output says that =rivers= is a numeric vector of length 141, and the first few values are 735, 320, 325, /etc/. These data are definitely quantitative and it appears that the measurements have been rounded to the nearest mile. Thus, strictly speaking, these are discrete data. But we will find it convenient later to take data like these to be continuous for some of our statistical procedures. 
#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
*Yearly Numbers of Important Discoveries.* The vector =discoveries=\index{Data sets!discoveries@\texttt{discoveries}} contains numbers of “great” inventions/discoveries in each year from 1860 to 1959, as reported by the 1975 World Almanac. Let us take a look at the data:

#+begin_src R :exports both :results output pp  
str(discoveries)
#+end_src

#+latex: \end{exampletoo}
#+html: </div>

The output is telling us that =discoveries= is a /time series/ (see Section [[file:data-description::sub-other-data-types][Other Data Types]] for more) of length 100. The entries are integers, and since they represent counts this is a good example of discrete quantitative data. We will take a closer look in the following sections.

*** Displaying Quantitative Data
:PROPERTIES:
:CUSTOM_ID: sub-Displaying-Quantitative-Data
:END:

One of the first things to do when confronted by quantitative data (or any data, for that matter) is to make some sort of visual display to gain some insight into the data's structure. There are almost as many display types from which to choose as there are data sets to plot. We describe some of the more popular alternatives. 

**** Strip charts\index{strip chart} (also known as Dot plots)\index{dot plot| see\{strip chart\}}
:PROPERTIES:
:CUSTOM_ID: par-Strip-charts
:END:

These can be used for discrete or continuous data, and usually look best when the data set is not too large. Along the horizontal axis is a numerical scale above which the data values are plotted. We can do it in \(\mathsf{R}\) with a call to the =stripchart=\index{stripchart@\texttt{stripchart}} function. There are three available methods.

- overplot :: plots ties covering each other. This method is good to display only the distinct values assumed by the data set.

- jitter :: adds some noise to the data in the \(y\) direction in which case the data values are not covered up by ties.

- stack :: plots repeated values stacked on top of one another. This method is best used for discrete data with a lot of ties; if there are no repeats then this method is identical to overplot.

See Figure [[fig-stripcharts][stripcharts]], which was produced by the following code.

#+begin_src R :exports code :eval never
stripchart(precip, xlab="rainfall")
stripchart(rivers, method="jitter", xlab="length")
stripchart(discoveries, method="stack", xlab="number")
#+end_src

The leftmost graph is a strip chart of the =precip= data. The graph shows tightly clustered values in the middle with some others falling balanced on either side, with perhaps slightly more falling to the left. Later we will call this a symmetric distribution, see Section [[sub-Shape][Shape]]. The middle graph is of the =rivers= data, a vector of length 141. There are several repeated values in the rivers data, and if we were to use the overplot method we would lose some of them in the display. This plot shows a what we will later call a right-skewed shape with perhaps some extreme values on the far right of the display. The third graph strip charts =discoveries= data which are literally a textbook example of a right skewed distribution.

#+name: stripcharts
#+begin_src R :exports none :results silent
par(mfrow = c(3,1)) # 3 plots: 3 rows, 1 column
stripchart(precip, xlab="rainfall")
stripchart(rivers, method="jitter", xlab="length")
stripchart(discoveries, method="stack", xlab="number", ylim = c(0,3))
par(mfrow = c(1,1)) # back to normal
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/datadesc/stripcharts.ps
  <<stripcharts>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/datadesc/stripcharts.svg
  <<stripcharts>>
#+end_src

#+begin_latex
  \begin{figure}[th]
    \includegraphics[angle=270, totalheight=4in]{ps/datadesc/stripcharts.ps}
    \caption[Strip charts of \texttt{precip}, \texttt{rivers}, and \texttt{discoveries}]{\small Three stripcharts of three data sets.  The first graph uses the \texttt{overplot} method, the second the \texttt{jitter} method, and the third the \texttt{stack} method.}
    \label{fig-stripcharts}
  \end{figure}
#+end_latex

#+begin_html
  <div id="fig-stripcharts" class="figure">
    <p><img src="svg/datadesc/stripcharts.svg" width=500 alt="svg/datadesc/stripcharts.svg" /></p>
    <p>Three stripcharts of three data sets.  The first graph uses the <code>overplot</code> method, the second the <code>jitter</code> method, and the third the <code>stack</code> method.</p>
  </div>
#+end_html

The =DOTplot=\index{DOTplot@\texttt{DOTplot}} function in the =UsingR=\index{R packages!UsingR@\texttt{UsingR}} package \cite{UsingR} is another alternative.

**** Histogram\index{Histogram}

These are typically used for continuous data. A histogram is constructed by first deciding on a set of classes, or bins, which partition the real line into a set of boxes into which the data values fall. Then vertical bars are drawn over the bins with height proportional to the number of observations that fell into the bin. 

These are one of the most common summary displays, and they are often misidentified as ``Bar Graphs'' (see below.) The scale on the \(y\) axis can be frequency, percentage, or density (relative frequency). The term histogram was coined by Karl Pearson in 1891, see \cite{Miller}.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-annual>>
*Annual Precipitation in US Cities.* We are going to take another look at the =precip=\index{Data sets!precip@\texttt{precip}} data that we investigated earlier. The strip chart in Figure [[fig-stripcharts][Various-stripchart-methods]] suggested a loosely balanced distribution; let us now look to see what a histogram says. 

There are many ways to plot histograms in \(\mathsf{R}\), and one of the easiest is with the =hist=\index{hist@\texttt{hist}} function. The following code produces the plots in Figure [[fig-histograms][histograms]].

#+begin_src R :exports code :eval never
hist(precip, main = "")
hist(precip, freq = FALSE, main = "")
#+end_src

Notice the argument \(\mathtt{main = ""}\) which suppresses the main title from being displayed -- it would have said ``Histogram of =precip='' otherwise. The plot on the left is a frequency histogram (the default), and the plot on the right is a relative frequency histogram (=freq = FALSE=). 

#+begin_src R :eval never :exports code
m <- ggplot(as.data.frame(precip), aes(x = precip))
m + geom_histogram()
m + geom_histogram(aes(y = ..density..))
#+end_src

#+name: histograms
#+begin_src R :exports none :results silent
m <- ggplot(as.data.frame(precip), aes(x = precip))
a <- m + geom_histogram()
b <- m + geom_histogram(aes(y = ..density..))
grid.newpage()
pushViewport(viewport(layout = grid.layout(2, 1)))
vplayout <- function(x, y)
viewport(layout.pos.row = x, layout.pos.col = y)
print(a, vp = vplayout(1, 1))
print(b, vp = vplayout(2, 1))
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/datadesc/histograms.ps
  <<histograms>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/datadesc/histograms.svg
  <<histograms>>
#+end_src

#+begin_src R :eval never :exports code
par(mfrow = c(1,2)) # 2 plots: 1 row, 2 columns
hist(precip, main = "")
hist(precip, freq = FALSE, main = "")
par(mfrow = c(1,1)) # back to normal
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/datadesc/histograms.ps}
  \caption{(Relative) frequency histograms of the \texttt{precip} data}
  \label{fig-histograms}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-histograms" class="figure">
  <p><img src="svg/datadesc/histograms.svg" width=500 alt="svg/datadesc/histograms.svg" /></p>
  <p>(Relative) frequency histograms of the <code>precip</code> data.</p>
</div>
#+end_html

#+latex: \end{exampletoo}
#+html: </div>

Please mind the biggest weakness of histograms: the graph obtained strongly depends on the bins chosen. Choose another set of bins, and you will get a different histogram. Moreover, there are not any definitive criteria by which bins should be defined; the best choice for a given data set is the one which illuminates the data set's underlying structure (if any). Luckily for us there are algorithms to automatically choose bins that are likely to display well, and more often than not the default bins do a good job. This is not always the case, however, and a responsible statistician will investigate many bin choices to test the stability of the display.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Recall that the strip chart in Figure [[fig-stripcharts][Various-stripchart-methods]] suggested a relatively balanced shape to the =precip= data distribution. Watch what happens when we change the bins slightly (with the =breaks= argument to =hist=). See Figure [[fig-histograms-bins][histograms-bins]] which was produced by the following code.


#+begin_src R :exports code :eval never
qplot(precip, geom = "histogram", binwidth = 1)
qplot(precip, geom = "histogram", binwidth = 5)
qplot(precip, geom = "histogram", binwidth = 20)
m <- ggplot(as.data.frame(precip), aes(x = precip))
m + geom_histogram(binwidth = 1)
m + geom_histogram(binwidth = 5)
m + geom_histogram(binwidth = 20)
#+end_src

#+name: histograms-bins
#+begin_src R :exports none :results silent
m <- ggplot(as.data.frame(precip), aes(x = precip))
a1 <- m + geom_histogram(binwidth = 1)
a2 <- m + geom_histogram(binwidth = 5)
a3 <- m + geom_histogram(binwidth = 20)
grid.newpage()
pushViewport(viewport(layout = grid.layout(3, 1)))
vplayout <- function(x, y)
viewport(layout.pos.row = x, layout.pos.col = y)
print(a1, vp = vplayout(1, 1))
print(a2, vp = vplayout(2, 1))
print(a3, vp = vplayout(3, 1))
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/datadesc/histograms-bins.ps
  <<histograms-bins>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/datadesc/histograms-bins.svg
  <<histograms-bins>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/datadesc/histograms-bins.ps}
  \caption{More histograms of the \texttt{precip} data}
  \label{fig-histograms-bins}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-histograms-bins" class="figure">
  <p><img src="svg/datadesc/histograms-bins.svg" width=500 alt="svg/datadesc/histograms-bins.svg" /></p>
  <p>More histograms of the <code>precip</code> data.</p>
</div>
#+end_html

The leftmost graph (with =breaks = 10=) shows that the distribution is not balanced at all. There are two humps: a big one in the middle and a smaller one to the left. Graphs like this often indicate some underlying group structure to the data; we could now investigate whether the cities for which rainfall was measured were similar in some way, with respect to geographic region, for example.

The rightmost graph in Figure [[fig-histograms-bins][histograms-bins]] shows what happens when the number of bins is too large: the histogram is too grainy and hides the rounded appearance of the earlier histograms. If we were to continue increasing the number of bins we would eventually get all observed bins to have exactly one element, which is nothing more than a glorified strip chart.

#+latex: \end{exampletoo}
#+html: </div>

**** Stem-and-leaf displays (more to be said in Section [[sec-Exploratory-Data-Analysis][Exploratory Data Analysis]])

Stem-and-leaf displays (also known as stemplots) have two basic parts: /stems/ and /leaves/. The final digit of the data values is taken to be a /leaf/, and the leading digit(s) is (are) taken to be /stems/. We draw a vertical line, and to the left of the line we list the stems. To the right of the line, we list the leaves beside their corresponding stem. There will typically be several leaves for each stem, in which case the leaves accumulate to the right. It is sometimes necessary to round the data values, especially for larger data sets.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-ukdriverdeaths-first>>
=UKDriverDeaths=\index{Data sets!UKDriverDeaths@\texttt{UKDriverDeaths}} is a time series that contains the total car drivers killed or seriously injured in Great Britain monthly from Jan 1969 to Dec 1984. See =?UKDriverDeaths=. Compulsory seat belt use was introduced on January 31, 1983. We construct a stem and leaf diagram in \(\mathsf{R}\) with the =stem.leaf=\index{stem.leaf@\texttt{stem.leaf}} function from the =aplpack=\index{R packages@\textsf{R} packages!aplpack@\texttt{aplpack}} package\cite{aplpack}.
#+latex: \end{exampletoo}
#+html: </div>

#+begin_src R :exports both :results output pp  
stem.leaf(UKDriverDeaths, depth = FALSE)
#+end_src

The display shows a more or less balanced mound-shaped distribution, with one or maybe two humps, a big one and a smaller one just to its right. Note that the data have been rounded to the tens place so that each datum gets only one leaf to the right of the dividing line.

Notice that the \texttt{depth}s\index{depths} have been suppressed. To learn more about this option and many others, see Section [[sec-Exploratory-Data-Analysis][Exploratory Data Analysis]]. Unlike a histogram, the original data values may be recovered from the stem-and-leaf display -- modulo the rounding -- that is, starting from the top and working down we can read off the data values 1050, 1070, 1110, 1130, and so forth. 

**** Index plots

Done with the =plot=\index{plot@\texttt{plot}} function. These are good for plotting data which are ordered, for example, when the data are measured over time. That is, the first observation was measured at time 1, the second at time 2, /etc/. It is a two dimensional plot, in which the index (or time) is the \(x\) variable and the measured value is the \(y\) variable. There are several plotting methods for index plots, and we mention two of them:

- spikes :: draws a vertical line from the \(x\)-axis to the observation height.
- points :: plots a simple point at the observation height.


#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
*Level of Lake Huron 1875-1972.* Brockwell and Davis \cite{Brockwell1991} give the annual measurements of the level (in feet) of Lake Huron from 1875--1972. The data are stored in the time series =LakeHuron=\index{Data sets!LakeHuron@\texttt{LakeHuron}}. See =?LakeHuron=. Figure [[fig-indpl-lakehuron][indpl-lakehuron]] was produced with the following code:

Here is how to do it with base \(\mathsf{R}\).

#+begin_src R :exports code :eval never
plot(LakeHuron)
plot(LakeHuron, type = "p")
plot(LakeHuron, type = "h")
#+end_src

Here is how to do it with =ggplot2=.

#+begin_src R :exports code :eval never
huron <- data.frame(year = time(LakeHuron), level = as.vector(LakeHuron))
h <- ggplot(huron, aes(x=year)) 
h + geom_path(aes(y = level))
h + geom_point(aes(y = level))
h + geom_ribbon(aes(ymin = 576, ymax = level))
#+end_src

Here is how to do it with =lattice=.

#+begin_src R :exports code :eval never
library(lattice)
xyplot(level ~ year, data = huron, type = "l")
xyplot(level ~ year, data = huron, type = "p")
#+end_src


The plots show an overall decreasing trend to the observations, and there appears to be some seasonal variation that increases over time. 

#+name: indpl-lakehuron
#+begin_src R :exports none :results silent
par(mfrow = c(3,1))
plot(LakeHuron)
plot(LakeHuron, type = "p")
plot(LakeHuron, type = "h")
par(mfrow = c(1,1))
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/datadesc/indpl-lakehuron.ps
  <<indpl-lakehuron>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/datadesc/indpl-lakehuron.svg
  <<indpl-lakehuron>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/datadesc/indpl-lakehuron.ps}
  \caption{Index plots of the \texttt{LakeHuron} data}
  \label{fig-indpl-lakehuron}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-indpl-lakehuron" class="figure">
  <p><img src="svg/datadesc/indpl-lakehuron.svg" width=500 alt="svg/datadesc/indpl-lakehuron.svg" /></p>
  <p>Index plots of the <code>LakeHuron</code> data.</p>
</div>
#+end_html

#+latex: \end{exampletoo}
#+html: </div>

**** Density estimates

The default method uses a Gaussian kernel density estimate.

#+begin_src R :eval never
# The Old Faithful geyser data
     d <- density(faithful$eruptions, bw = "sj")
     d
     plot(d)
hist(precip, freq = FALSE)
lines(density(precip))
#+end_src

*** Qualitative Data, Categorical Data, and Factors
:PROPERTIES:
:CUSTOM_ID: sub-Qualitative-Data
:END:

Qualitative data are simply any type of data that are not numerical, or do not represent numerical quantities. Examples of qualitative variables include a subject's name, gender, race/ethnicity, political party, socioeconomic status, class rank, driver's license number, and social security number (SSN).

Please bear in mind that some data /look/ to be quantitative but are /not/, because they do not represent numerical quantities and do not obey mathematical rules. For example, a person's shoe size is typically written with numbers: 8, or 9, or 12, or \(12\,\frac{1}{2}\). Shoe size is not quantitative, however, because if we take a size 8 and combine with a size 9 we do not get a size 17.

Some qualitative data serve merely to /identify/ the observation (such a subject's name, driver's license number, or SSN). This type of data does not usually play much of a role in statistics. But other qualitative variables serve to /subdivide/ the data set into categories; we call these /factors/. In the above examples, gender, race, political party, and socioeconomic status would be considered factors (shoe size would be another one). The possible values of a factor are called its /levels/. For instance, the factor /gender/ would have two levels, namely, male and female. Socioeconomic status typically has three levels: high, middle, and low.

Factors may be of two types: /nominal/\index{nominal data} and /ordinal/\index{ordinal data}. Nominal factors have levels that correspond to names of the categories, with no implied ordering. Examples of nominal factors would be hair color, gender, race, or political party. There is no natural ordering to ``Democrat'' and ``Republican''; the categories are just names associated with different groups of people. 

In contrast, ordinal factors have some sort of ordered structure to the underlying factor levels. For instance, socioeconomic status would be an ordinal categorical variable because the levels correspond to ranks associated with income, education, and occupation. Another example of ordinal categorical data would be class rank.

Factors have special status in \(\mathsf{R}\). They are represented internally by numbers, but even when they are written numerically their values do not convey any numeric meaning or obey any mathematical rules (that is, Stage III cancer is not Stage I cancer + Stage II cancer).

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
The =state.abb=\index{Data sets!state.abb@\texttt{state.abb}}
vector gives the two letter postal abbreviations for all 50 states.

#+begin_src R :exports both :results output pp  
str(state.abb)
#+end_src

These would be ID data. The =state.name=\index{Data sets!state.name@\texttt{state.name}} vector lists all of the complete names and those data would also be ID.
#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
*U.S. State Facts and Features.* The U.S. Department of Commerce of the U.S. Census Bureau releases all sorts of information in the /Statistical Abstract of the United States/, and the =state.region=\index{Data sets!state.region@\texttt{state.region}} data lists each of the 50 states and the region to which it belongs, be it Northeast, South, North Central, or West. See =?state.region=.

#+begin_src R :exports both :results output pp  
str(state.region)
state.region[1:5]
#+end_src

The =str=\index{str@\texttt{str}} output shows that =state.region= is already stored internally as a factor and it lists a couple of the factor levels. To see all of the levels we printed the first five entries of the vector in the second line.
#+latex: \end{exampletoo}
#+html: </div>

*** Displaying Qualitative Data
:PROPERTIES:
:CUSTOM_ID: sub-Displaying-Qualitative-Data
:END:

**** Tables
:PROPERTIES:
:CUSTOM_ID: par-Tables
:END:

One of the best ways to summarize qualitative data is with a table of the data values. We may count frequencies with the =table= function or list proportions with the =prop.table=\index{prop.table@\texttt{prop.table}} function (whose input is a frequency table). In the \(\mathsf{R}\) Commander you can do it with =Statistics= \(\triangleright\) =Frequency Distribution...= Alternatively, to look at tables for all factors in the =Active data set=\index{Active data set@\texttt{Active data set}} you can do =Statistics= \(\triangleright\) =Summaries= \(\triangleright\) =Active Dataset=.

#+begin_src R :exports code :results silent 
Tbl <- table(state.division)
#+end_src

#+begin_src R :exports both :results output pp  
Tbl
#+end_src

#+begin_src R :exports both :results output pp  
Tbl/sum(Tbl)      # relative frequencies
#+end_src

#+begin_src R :exports both :results output pp  
prop.table(Tbl)   # same thing
#+end_src

**** Bar Graphs
:PROPERTIES:
:CUSTOM_ID: par-Bar-Graphs
:END:

A bar graph is the analogue of a histogram for categorical data. A bar is displayed for each level of a factor, with the heights of the bars proportional to the frequencies of observations falling in the respective categories. A disadvantage of bar graphs is that the levels are ordered alphabetically (by default), which may sometimes obscure patterns in the display. 

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
*U.S. State Facts and Features.* The =state.region= data lists each of the 50 states and the region to which it belongs, be it Northeast, South, North Central, or West. See =?state.region=. It is already stored internally as a factor. We make a bar graph with the =barplot=\index{barplot@\texttt{barplot}} function: 


#+begin_src R :exports code :eval never
barplot(table(state.region), cex.names = 0.50)
barplot(prop.table(table(state.region)), cex.names = 0.50)
#+end_src

See Figure [[fig-bar-gr-stateregion][bar-gr-stateregion]]. The display on the left is a frequency bar graph because the \(y\) axis shows counts, while the display on the left is a relative frequency bar graph. The only difference between the two is the scale. Looking at the graph we see that the majority of the fifty states are in the South, followed by West, North Central, and finally Northeast. Over 30% of the states are in the South.

Notice the =cex.names=\index{cex.names@\texttt{cex.names}} argument that we used, above. It shrinks the names on the \(x\) axis by 50% which makes them easier to read. See =?par=\index{par@\texttt{par}} for a detailed list of additional plot parameters.

#+name: bar-gr-stateregion
#+begin_src R :exports none :results silent
par(mfrow = c(1,2)) # 2 plots: 1 row, 2 columns
barplot(table(state.region), cex.names = 0.50)
barplot(prop.table(table(state.region)), cex.names = 0.50)
par(mfrow = c(1,1)) # back to normal
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/datadesc/bar-gr-stateregion.ps
  <<bar-gr-stateregion>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/datadesc/bar-gr-stateregion.svg
  <<bar-gr-stateregion>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/datadesc/bar-gr-stateregion.ps}
  \caption[Bar graphs of the \texttt{state.region} data]{\small The left graph is a frequency barplot made with \texttt{table} and the right is a relative frequency barplot made with \texttt{prop.table}.}
  \label{fig-bar-gr-stateregion}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-bar-gr-stateregion" class="figure">
  <p><img src="svg/datadesc/bar-gr-stateregion.svg" width=500 alt="svg/datadesc/bar-gr-stateregion.svg" /></p>
  <p>The left graph is a frequency barplot made with <code>table</code> and the right is a relative frequency barplot made with <code>prop.table</code>.</p>
</div>
#+end_html

#+latex: \end{exampletoo}
#+html: </div>

**** Pareto Diagrams
:PROPERTIES:
:CUSTOM_ID: par-Pareto-Diagrams
:END:

A pareto diagram is a lot like a bar graph except the bars are rearranged such that they decrease in height going from left to right. The rearrangement is handy because it can visually reveal structure (if any) in how fast the bars decrease -- this is much more difficult when the bars are jumbled. 

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
*U.S. State Facts and Features.* The =state.division=\index{Data sets!state.division@\texttt{state.division}} data record the division (New England, Middle Atlantic, South Atlantic, East South Central, West South Central, East North Central, West North Central, Mountain, and Pacific) of the fifty states. We can make a pareto diagram with either the =RcmdrPlugin.IPSUR=\index{R packages@\textsf{R} packages!RcmdrPlugin.IPSUR@\texttt{RcmdrPlugin.IPSUR}} package \cite{RcmdrPlugin.IPSUR} or with the =pareto.chart=\index{pareto.chart@\texttt{pareto.chart}} function from the =qcc=\index{R packages@\textsf{R} packages!qcc@\texttt{qcc}} package \cite{qcc}. See Figure [[fig-Pareto-chart][Pareto-chart]]. The code follows.

#+name: Pareto-chart
#+begin_src R :exports code :results silent
pareto.chart(table(state.division), ylab="Frequency")
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/datadesc/Pareto-chart.ps
  <<Pareto-chart>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/datadesc/Pareto-chart.svg
  <<Pareto-chart>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/datadesc/Pareto-chart.ps}
  \caption{Pareto chart of the \texttt{state.division} data}
  \label{fig-Pareto-chart}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-Pareto-chart" class="figure">
  <p><img src="svg/datadesc/Pareto-chart.svg" width=500 alt="svg/datadesc/Pareto-chart.svg" /></p>
  <p>Pareto chart of the <code>state.division</code> data.</p>
</div>
#+end_html

#+latex: \end{exampletoo}
#+html: </div>

**** Dot Charts
:PROPERTIES:
:CUSTOM_ID: par-Dotcharts
:END:

These are a lot like a bar graph that has been turned on its side with the bars replaced by dots on horizontal lines. They do not convey any more (or less) information than the associated bar graph, but the strength lies in the economy of the display. Dot charts are so compact that it is easy to graph very complicated multi-variable interactions together in one graph. See Section [[sec-Comparing-Data-Sets][Comparing Data Sets]]. We will give an example here using the same data as above for comparison. The graph was produced by the following code.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">

#+name: dot-charts
#+begin_src R :exports code :results silent
x <- table(state.region)
dotchart(as.vector(x), labels = names(x))
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/datadesc/dot-charts.ps
  <<dot-charts>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/datadesc/dot-charts.svg
  <<dot-charts>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/datadesc/dot-charts.ps}
  \caption{Dot chart of the \texttt{state.region} data}
  \label{fig-dot-charts}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-dot-charts" class="figure">
  <p><img src="svg/datadesc/dot-charts.svg" width=500 alt="svg/datadesc/dot-charts.svg" /></p>
  <p>Dot chart of the <code>state.region</code> data.</p>
</div>
#+end_html

See Figure [[fig-dot-charts][dot-charts]]. Compare it to Figure [[fig-bar-gr-stateregion][bar-gr-stateregion]].

#+latex: \end{exampletoo}
#+html: </div>

**** Pie Graphs
:PROPERTIES:
:CUSTOM_ID: par-Pie-Graphs
:END:

These can be done with \(\mathsf{R}\) and the \(\mathsf{R}\) Commander, but they fallen out of favor in recent years because researchers have determined that while the human eye is good at judging linear measures, it is notoriously bad at judging relative areas (such as those displayed by a pie graph). Pie charts are consequently a very bad way of displaying information. A bar chart or dot chart is a preferable way of displaying qualitative data. See =?pie=\index{pie@\texttt{pie}} for more information.

We are not going to do any examples of a pie graph and discourage their use elsewhere. 

*** Logical Data
:PROPERTIES:
:CUSTOM_ID: sub-Logical-Data
:END:

There is another type of information recognized by \(\mathsf{R}\) which does not fall into the above categories. The value is either =TRUE= or =FALSE= (note that equivalently you can use =1 = TRUE=, =0 = FALSE=). Here is an example of a logical vector:

#+begin_src R :exports both :results output pp  
x <- 5:9
y <- (x < 7.3)
y
#+end_src

Many functions in \(\mathsf{R}\) have options that the user may or may not want to activate in the function call. For example, the =stem.leaf= function has the =depths= argument which is =TRUE= by default. We saw in Section [[sub-Quantitative-Data][Quantitative Data]] how to turn the option off, simply enter =stem.leaf(x, depths = FALSE)= and they will not be shown on the display.

We can swap =TRUE= with =FALSE= with the exclamation point =!=.

#+begin_src R :exports both :results output pp  
!y
#+end_src

*** Missing Data
:PROPERTIES:
:CUSTOM_ID: sub-Missing-Data
:END:

Missing data are a persistent and prevalent problem in many statistical analyses, especially those associated with the social sciences. \(\mathsf{R}\) reserves the special symbol =NA= to representing missing data.

Ordinary arithmetic with =NA= values give =NA='s (addition, subtraction, /etc/.) and applying a function to a vector that has an =NA= in it will usually give an =NA=.

#+begin_src R :exports both :results output pp  
x <- c(3, 7, NA, 4, 7)
y <- c(5, NA, 1, 2, 2)
x + y
#+end_src

Some functions have a =na.rm= argument which when =TRUE= will ignore missing data as if they were not there (such as =mean=, =var=, =sd=, =IQR=, =mad=, ...). 

#+begin_src R :exports both :results output pp  
sum(x)
sum(x, na.rm = TRUE)
#+end_src

Other functions do not have a =na.rm= argument and will return =NA= or an error if the argument has \texttt{NA}s. In those cases we can find the locations of any \texttt{NA}s with the =is.na= function and remove those cases with the =[]= operator.

#+begin_src R :exports both :results output pp  
is.na(x)
z <- x[!is.na(x)]
sum(z)
#+end_src

The analogue of =is.na= for rectangular data sets (or data frames) is the =complete.cases= function. See Appendix [[sec-Editing-Data-Sets][Editing Data Sets]].

*** Other Data Types
:PROPERTIES:
:CUSTOM_ID: sub-other-data-types
:END:

** Features of Data Distributions
:PROPERTIES:
:CUSTOM_ID: sec-features-of-data
:END:

Given that the data have been appropriately displayed, the next step is to try to identify salient features represented in the graph. The acronym to remember is /C/-enter, /U/-nusual features, /S/-pread, and /S/-hape. (CUSS).

*** Center
:PROPERTIES:
:CUSTOM_ID: sub-Center
:END:

One of the most basic features of a data set is its center. Loosely speaking, the center of a data set is associated with a number that represents a middle or general tendency of the data. Of course, there are usually several values that would serve as a center, and our later tasks will be focused on choosing an appropriate one for the data at hand. Judging from the histogram that we saw in Figure [[fig-histograms-bins][histograms-bins]], a measure of center would be about \( SRC_R{round(mean(precip))} \). 

*** Spread
:PROPERTIES:
:CUSTOM_ID: sub-Spread
:END:

The spread of a data set is associated with its variability; data sets with a large spread tend to cover a large interval of values, while data sets with small spread tend to cluster tightly around a central value. 

*** Shape
:PROPERTIES:
:CUSTOM_ID: sub-Shape
:END:

When we speak of the /shape/ of a data set, we are usually referring to the shape exhibited by an associated graphical display, such as a histogram. The shape can tell us a lot about any underlying structure to the data, and can help us decide which statistical procedure we should use to analyze them.

**** Symmetry and Skewness

A distribution is said to be /right-skewed/ (or /positively skewed/) if the right tail seems to be stretched from the center. A /left-skewed/ (or /negatively skewed/) distribution is stretched to the left side. A symmetric distribution has a graph that is balanced about its center, in the sense that half of the graph may be reflected about a central line of symmetry to match the other
half.

We have already encountered skewed distributions: both the discoveries data in Figure [[fig-stripcharts][stripcharts]] and the =precip= data in Figure [[fig-histograms-bins][histograms-bins]] appear right-skewed. The =UKDriverDeaths= data in Example [[exa-ukdriverdeaths-first][UK Driver Deaths]] is relatively symmetric (but note the one extreme value 2654 identified at the bottom of the stem-and-leaf display).

**** Kurtosis

Another component to the shape of a distribution is how ``peaked'' it is. Some distributions tend to have a flat shape with thin tails. These are called /platykurtic/, and an example of a platykurtic distribution is the uniform distribution; see Section [[sec-The-Continuous-Uniform][Continuous Uniform]]. On the other end of the spectrum are distributions with a steep peak, or spike, accompanied by heavy tails; these are called /leptokurtic/. Examples of leptokurtic distributions are the Laplace distribution and the logistic distribution. See Section [[sec-Other-Continuous-Distributions][Other Continuous Distributions]]. In between are distributions (called /mesokurtic/) with a rounded peak and moderately sized tails. The standard example of a mesokurtic distribution is the famous bell-shaped curve, also known as the Gaussian, or normal, distribution, and the binomial distribution can be mesokurtic for specific choices of \(p\). See Sections [[sec-binom-dist][Binomial Distribution]] and [[sec-The-Normal-Distribution][The Normal Distribution]].

*** Clusters and Gaps
:PROPERTIES:
:CUSTOM_ID: sub-clusters-and-gaps
:END:

Clusters or gaps are sometimes observed in quantitative data distributions. They indicate clumping of the data about distinct values, and gaps may exist between clusters. Clusters often suggest an underlying grouping to the data. For example, take a look at the =faithful= data which contains the duration of =eruptions= and the =waiting= time between eruptions of the Old Faithful geyser in Yellowstone National Park. Do not be frightened by the complicated information at the left of the display for now; we will learn how to interpret it in Section [[sec-Exploratory-Data-Analysis][Exploratory Data Analysis]].

# <<exa-stemleaf-multiple-lines-stem>>
#+begin_src R :exports both :results output pp
with(faithful, stem.leaf(eruptions))
#+end_src

There are definitely two clusters of data here; an upper cluster and a lower cluster. 

*** Extreme Observations and other Unusual Features
:PROPERTIES:
:CUSTOM_ID: sub-Extreme-Observations-and
:END:

Extreme observations fall far from the rest of the data. Such observations are troublesome to many statistical procedures; they cause exaggerated estimates and instability. It is important to identify extreme observations and examine the source of the data more closely. There are many possible reasons underlying an extreme observation:

- *Maybe the value is a typographical error.* Especially with large data sets becoming more prevalent, many of which being recorded by hand, mistakes are a common problem. After closer scrutiny, these can often be fixed.

- *Maybe the observation was not meant for the study*, because it does not belong to the population of interest. For example, in medical research some subjects may have relevant complications in their genealogical history that would rule out their participation in the experiment. Or when a manufacturing company investigates the properties of one of its devices, perhaps a particular product is malfunctioning and is not representative of the majority of the items.

- *Maybe it indicates a deeper trend or phenomenon.* Many of the most influential scientific discoveries were made when the investigator noticed an unexpected result, a value that was not predicted by the classical theory. Albert Einstein, Louis Pasteur, and others built their careers on exactly this circumstance.

** Descriptive Statistics
:PROPERTIES:
:CUSTOM_ID: sec-Descriptive-Statistics
:END:

One of my favorite professors would repeatedly harp, ``You cannot do statistics without data.''  


*What do I want them to know?*
- The fundamental data types we encounter most often, how to classify given data into a likely type, and that sometimes the distinction is blurry.\

*** Frequencies and Relative Frequencies
:PROPERTIES:
:CUSTOM_ID: sub-Frequencies-and-Relative
:END:

These are used for categorical data. The idea is that there are a number of different categories, and we would like to get some idea about how the categories are represented in the population. 

*** Measures of Center
:PROPERTIES:
:CUSTOM_ID: sub-Measures-of-Center
:END:

The /sample mean/ is denoted \(\overline{x}\) (read ``\(x\)-bar'') and is simply the arithmetic average of the observations:
\begin{equation} 
\overline{x}=\frac{x_{1}+x_{2}+\cdots+x_{n}}{n}=\frac{1}{n}\sum_{i=1}^{n}x_{i}.
\end{equation}
- Good: natural, easy to compute, has nice mathematical properties
- Bad: sensitive to extreme values

It is appropriate for use with data sets that are not highly skewed without extreme observations.

The /sample median/ is another popular measure of center and is denoted \(\tilde{x}\). To calculate its value, first sort the data into an increasing sequence of numbers. If the data set has an odd number of observations then \(\tilde{x}\) is the value of the middle observation, which lies in position \((n+1)/2\); otherwise, there are two middle observations and \(\tilde{x}\) is the average of those middle values.

- Good: resistant to extreme values, easy to describe
- Bad: not as mathematically tractable, need to sort the data to calculate

One desirable property of the sample median is that it is /resistant/ to extreme observations, in the sense that the value of \(\tilde{x}\) depends only on those data values in the middle, and is quite unaffected by the actual values of the outer observations in the ordered list. The same cannot be said for the sample mean. Any significant changes in the magnitude of an observation \(x_{k}\) results in a corresponding change in the value of the mean. Consequently, the sample mean is said to be /sensitive/ to extreme observations.

The /trimmed mean/ is a measure designed to address the sensitivity of the sample mean to extreme observations. The idea is to ``trim'' a fraction (less than 1/2) of the observations off each end of the ordered list, and then calculate the sample mean of what remains. We will denote it by \(\overline{x}_{t=0.05}\).

- Good: resistant to extreme values, shares nice statistical properties
- Bad: need to sort the data

**** How to do it with \(\mathsf{R}\)

- You can calculate frequencies or relative frequencies with the =table= function, and relative frequencies with =prop.table(table())=. 
- You can calculate the sample mean of a data vector =x= with the command =mean(x)=. 
- You can calculate the sample median of =x= with the command =median(x)=.
- You can calculate the trimmed mean with the =trim= argument; =mean(x, trim = 0.05)=.

*** Order Statistics and the Sample Quantiles
:PROPERTIES:
:CUSTOM_ID: sub-Order-Statistics-and
:END:

A common first step in an analysis of a data set is to sort the values. Given a data set \(x_{1}\), \(x_{2}\), ..., \(x_{n}\), we may sort the values to obtain an increasing sequence
\begin{equation} 
x_{(1)}\leq x_{(2)}\leq x_{(3)}\leq\cdots\leq x_{(n)}
\end{equation}
and the resulting values are called the /order statistics/. The \(k^{\mathrm{th}}\) entry in the list, \(x_{(k)}\), is the \(k^{\mathrm{th}}\) order statistic, and approximately \(100(k/n)\)% of the observations fall below \(x_{(k)}\). The order statistics give an indication of the shape of the data distribution, in the sense that a person can look at the order statistics and have an idea about where the data are concentrated, and where they are sparse.

The /sample quantiles/ are related to the order statistics. Unfortunately, there is not a universally accepted definition of them. Indeed, \(\mathsf{R}\) is equipped to calculate quantiles using nine distinct definitions! We will describe the default method (=type = 7=), but the interested reader can see the details for the other methods with =?quantile=.

Suppose the data set has \(n\) observations. Find the sample quantile of order \(p\) (\(0<p<1\)), denoted \(\tilde{q}_{p}\) , as follows: 

- First step: :: sort the data to obtain the order statistics \(x_{(1)}\), \(x_{(2)}\), ...,\(x_{(n)}\). 
- Second step: :: calculate \((n-1)p+1\) and write it in the form \(k.d\), where \(k\) is an integer and \(d\) is a decimal.
- Third step: :: The sample quantile \(\tilde{q}_{p}\) is
   \begin{equation}
      \tilde{q}_{p}=x_{(k)}+d(x_{(k+1)}-x_{(k)}).
   \end{equation}

The interpretation of \(\tilde{q}_{p}\) is that approximately \(100p\) % of the data fall below the value \(\tilde{q}_{p}\) . 

Keep in mind that there is not a unique definition of percentiles, quartiles, /etc/. Open a different book, and you'll find a different definition. The difference is small and seldom plays a role except in small data sets with repeated values. In fact, most people do not even notice in common use.

Clearly, the most popular sample quantile is \(\tilde{q}_{0.50}\), also known as the sample median, \(\tilde{x}\). The closest runners-up are the /first quartile/ \(\tilde{q}_{0.25}\) and the /third quartile/ \(\tilde{q}_{0.75}\) (the /second quartile/ is the median). 

**** How to do it with \(\mathsf{R}\)

*At the command prompt*
We can find the order statistics of a data set stored in a vector =x= with the command =sort(x)=.

We can calculate the sample quantiles of any order \(p\) where \(0<p<1\) for a data set stored in a data vector =x= with the =quantile= function, for instance, the command =quantile(x, probs = c(0, 0.25, 0.37))= will return the smallest observation, the first quartile, \(\tilde{q}_{0.25}\), and the 37th sample quantile, \(\tilde{q}_{0.37}\). For \(\tilde{q}_{p}\) simply change the values in the =probs= argument to the value \(p\).


*With the R Commander*
we can find the order statistics of a variable in the =Active data set= by doing =Data= \(\triangleright\) =Manage variables in Active data set...= \(\triangleright\) =Compute new variable...= In the =Expression to compute= dialog simply type =sort(varname)=, where =varname= is the variable that it is desired to sort.

In =Rcmdr=, we can calculate the sample quantiles for a particular variable with the sequence =Statistics= \(\triangleright\) =Summaries= \(\triangleright\) =Numerical Summaries...= We can automatically calculate the quartiles for all variables in the =Active data set= with the sequence =Statistics= \(\triangleright\) =Summaries= \(\triangleright\) =Active Dataset=.

*** Measures of Spread
:PROPERTIES:
:CUSTOM_ID: sub-Measures-of-Spread
:END:

**** Sample Variance and Standard Deviation

The /sample variance/ is denoted \(s^{2}\) and is calculated with the formula
\begin{equation}
s^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}.
\end{equation}
The /sample standard deviation/ is \(s=\sqrt{s^{2}}\). Intuitively, the sample variance is approximately the average squared distance of the observations from the sample mean. The sample standard deviation is used to scale the estimate back to the measurement units of the original data.
- Good: tractable, has nice mathematical/statistical properties
- Bad: sensitive to extreme values
We will spend a lot of time with the variance and standard deviation in the coming chapters. In the meantime, the following two rules give some meaning to the standard deviation, in that there are bounds on how much of the data can fall past a certain distance from the mean.

#+begin_fact
Chebychev's Rule: The proportion of observations within \(k\) standard deviations of the mean is at least \(1-1/k^{2}\), /i.e./, at least 75%, 89%, and 94% of the data are within 2, 3, and 4 standard deviations of the mean, respectively.
#+end_fact

Note that Chebychev's Rule does not say anything about when \(k=1\), because \(1-1/1^{2}=0\), which states that at least 0% of the observations are within one standard deviation of the mean (which is not saying much).

Chebychev's Rule applies to any data distribution, /any/ list of numbers, no matter where it came from or what the histogram looks like. The price for such generality is that the bounds are not very tight; if we know more about how the data are shaped then we can say more about how much of the data can fall a given distance from the mean.

#+begin_fact
# <<fac-Empirical-Rule>>
Empirical Rule: If data follow a bell-shaped curve, then approximately 68%, 95%, and 99.7% of the data are within 1, 2, and 3 standard deviations of the mean, respectively. 
#+end_fact

**** Interquartile Range

Just as the sample mean is sensitive to extreme values, so the associated measure of spread is similarly sensitive to extremes. Further, the problem is exacerbated by the fact that the extreme distances are squared. We know that the sample quartiles are resistant to extremes, and a measure of spread associated with them is the /interquartile range/ (\(IQR\)) defined by \(IQR=q_{0.75}-q_{0.25}\).

- Good: stable, resistant to outliers, robust to nonnormality, easy to explain
- Bad: not as tractable, need to sort the data, only involves the middle 50% of the data.

**** Median Absolute Deviation

A measure even more robust than the \(IQR\) is the /median absolute deviation/ (\(MAD\)). To calculate it we first get the median \(\widetilde{x}\), next the /absolute deviations/ \(|x_{1}-\tilde{x}|\), \(|x_{2}-\tilde{x}|\), ..., \(|x_{n}-\tilde{x}|\), and the \(MAD\) is proportional to the median of those deviations:
\begin{equation}
MAD\propto\mbox{median}(|x_{1}-\tilde{x}|,\ |x_{2}-\tilde{x}|,\ldots,|x_{n}-\tilde{x}|).
\end{equation}
That is, the \(MAD=c\cdot\mbox{median}(|x_{1}-\tilde{x}|,\ |x_{2}-\tilde{x}|,\ldots,|x_{n}-\tilde{x}|)\), where \(c\) is a constant chosen so that the \(MAD\) has nice properties. The value of \(c\) in \(\mathsf{R}\) is by default \(c=1.4286\). This value is chosen to ensure that the estimator of \(\sigma\) is correct, on the average, under suitable sampling assumptions (see Section [[sec-Point-Estimation-1][Point Estimation 1]]).
- Good: stable, very robust, even more so than the \(IQR\).
- Bad: not tractable, not well known and less easy to explain.

**** Comparing Apples to Apples

We have seen three different measures of spread which, for a given data set, will give three different answers. Which one should we use? It depends on the data set. If the data are well behaved, with an approximate bell-shaped distribution, then the sample mean and sample standard deviation are natural choices with nice mathematical properties. However, if the data have an unusual or skewed shape with several extreme values, perhaps the more resistant choices among the \(IQR\) or \(MAD\) would be more appropriate.

However, once we are looking at the three numbers it is important to understand that the estimators are not all measuring the same quantity, on the average. In particular, it can be shown that when the data follow an approximately bell-shaped distribution, then on the average, the sample standard deviation \(s\) and the \(MAD\) will be the approximately the same value, namely, \(\sigma\), but the \(IQR\) will be on the average 1.349 times larger than \(s\) and the \(MAD\). See [[cha-Sampling-Distributions][Sampling Distributions]] for more details.

**** How to do it with \(\mathsf{R}\)

*At the command prompt*
we may compute the sample range with =range(x)= and the sample variance with =var(x)=, where =x= is a numeric vector. The sample standard deviation is =sqrt(var(x))= or just =sd(x)=. The \(IQR\) is =IQR(x)= and the median absolute deviation is =mad(x)=.

*With the R Commander*
we can calculate the sample standard deviation with the =Statistics= \(\triangleright\) =Summaries= \(\triangleright\) =Numerical Summaries...= combination. \(\mathsf{R}\) Commander does not calculate the \(IQR\) or \(MAD\) in any of the menu selections, by default.

*** Measures of Shape
:PROPERTIES:
:CUSTOM_ID: sub-Measures-of-Shape
:END: 

**** Sample Skewness

The /sample skewness/, denoted by \(g_{1}\), is defined by the formula
\begin{equation}
g_{1}=\frac{1}{n}\frac{\sum_{i=1}^{n}(x_{i}-\overline{x})^{3}}{s^{3}}.
\end{equation}
The sample skewness can be any value \(-\infty<g_{1}<\infty\). The sign of \(g_{1}\) indicates the direction of skewness of the distribution. Samples that have \(g_{1}>0\) indicate right-skewed distributions (or positively skewed), and samples with \(g_{1}<0\) indicate left-skewed distributions (or negatively skewed). Values of \(g_{1}\) near zero indicate a symmetric distribution. These are not hard and fast rules, however. The value of \(g_{1}\) is subject to sampling variability and thus only provides a suggestion to the skewness of the underlying distribution. 

We still need to know how big is ``big'', that is, how do we judge whether an observed value of \(g_{1}\) is far enough away from zero for the data set to be considered skewed to the right or left? A good rule of thumb is that data sets with skewness larger than \(2\sqrt{6/n}\) in magnitude are substantially skewed, in the direction of the sign of \(g_{1}\). See Tabachnick & Fidell \cite{Tabachnick2006} for details.

**** Sample Excess Kurtosis

The /sample excess kurtosis/, denoted by \(g_{2}\), is given by the formula
\begin{equation}
g_{2}=\frac{1}{n}\frac{\sum_{i=1}^{n}(x_{i}-\overline{x})^{4}}{s^{4}}-3.
\end{equation}
The sample excess kurtosis takes values \(-2\leq g_{2}<\infty\). The subtraction of 3 may seem mysterious but it is done so that mound shaped samples have values of \(g_{2}\) near zero. Samples with \(g_{2}>0\) are called /leptokurtic/, and samples with \(g_{2}<0\) are called /platykurtic/. Samples with \(g_{2}\approx0\) are called /mesokurtic/.

As a rule of thumb, if \(|g_{2}|>4\sqrt{6/n}\) then the sample excess kurtosis is substantially different from zero in the direction of the sign of \(g_{2}\). See Tabachnick & Fidell \cite{Tabachnick2006} for details.

Notice that both the sample skewness and the sample kurtosis are invariant with respect to location and scale, that is, the values of \(g_{1}\) and \(g_{2}\) do not depend on the measurement units of the data. 

**** How to do it with \(\mathsf{R}\)

The =e1071= package \cite{e1071} has the =skewness= function for the sample skewness and the =kurtosis= function for the sample excess kurtosis. Both functions have a =na.rm= argument which is =FALSE= by default.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
We said earlier that the =discoveries= data looked positively skewed; let's see what the statistics say:
#+latex: \end{exampletoo}
#+html: </div>

#+begin_src R :exports both :results output pp  
skewness(discoveries)
2*sqrt(6/length(discoveries))
#+end_src

The data are definitely skewed to the right. Let us check the sample excess kurtosis of the =UKDriverDeaths= data:

#+begin_src R :exports both :results output pp  
kurtosis(UKDriverDeaths)
4*sqrt(6/length(UKDriverDeaths))
#+end_src

so that the =UKDriverDeaths= data appear to be mesokurtic, or at least not substantially leptokurtic.

** Exploratory Data Analysis
:PROPERTIES:
:CUSTOM_ID: sec-Exploratory-Data-Analysis
:END:

This field was founded (mostly) by John Tukey (1915-2000). Its tools are useful when not much is known regarding the underlying causes associated with the data set, and are often used for checking assumptions. For example, suppose we perform an experiment and collect some data... now what? We look at the data using exploratory visual tools.

*** More About Stem-and-leaf Displays

There are many bells and whistles associated with stemplots, and the =stem.leaf= function can do many of them.

- Trim Outliers: :: Some data sets have observations that fall far from the bulk of the other data (in a sense made more precise in Section [[sub-Outliers][Outliers]]). These extreme observations often obscure the underlying structure to the data and are best left out of the data display. The =trim.outliers= argument (which is =TRUE= by default) will separate the extreme observations from the others and graph the stemplot without them; they are listed at the bottom (respectively, top) of the stemplot with the label =HI= (respectively =LO=). 

- Split Stems: :: The standard stemplot has only one line per stem, which means that all observations with first digit =3= are plotted on the same line, regardless of the value of the second digit. But this gives some stemplots a ``skyscraper'' appearance, with too many observations stacked onto the same stem. We can often fix the display by increasing the number of lines available for a given stem. For example, we could make two lines per stem, say, =3*= and =3.=. Observations with second digit 0 through 4 would go on the upper line, while observations with second digit 5 through 9 would go on the lower line. (We could do a similar thing with five lines per stem, or even ten lines per stem.) The end result is a more spread out stemplot which often looks better. A good example of this was shown on page \pageref{exa-stemleaf-multiple-lines-stem}. 

- Depths: :: these are used to give insight into the balance of the observations as they accumulate toward the median. In a column beside the standard stemplot, the frequency of the stem containing the sample median is shown in parentheses. Next, frequencies are accumulated from the outside inward, including the outliers. Distributions that are more symmetric will have better balanced depths on either side of the sample median.

**** How to do it with \(\mathsf{R}\)

The basic command is =stem(x)= or a more sophisticated version written by Peter Wolf called =stem.leaf(x)= in the \(\mathsf{R}\) Commander. We will describe =stem.leaf= since that is the one used by \(\mathsf{R}\) Commander.


WARNING: Sometimes when making a stem-and-leaf display the result will not be what you expected. There are several reasons for this: 
- Stemplots by default will trim extreme observations (defined in Section [[sub-Outliers][Outliers]]) from the display. This in some cases will result in stemplots that are not as wide as expected.
- The leafs digit is chosen automatically by =stem.leaf= according to an algorithm that the computer believes will represent the data well. Depending on the choice of the digit, =stem.leaf= may drop digits from the data or round the values in unexpected ways.

Let us take a look at the =rivers= data set
# <<ite-stemplot-rivers>>

#+begin_src R :exports both :results output pp  
stem.leaf(rivers)
#+end_src

The stem-and-leaf display shows a right-skewed shape to the =rivers= data distribution. Notice that the last digit of each of the data values were dropped from the display. Notice also that there were eight extreme observations identified by the computer, and their exact values are listed at the bottom of the stemplot. Look at the scale on the left of the stemplot and try to imagine how ridiculous the graph would have looked had we tried to include enough stems to include these other eight observations; the stemplot would have stretched over several pages. Notice finally that we can use the depths to approximate the sample median for these data. The median lies in the row identified by =(18)=, which means that the median is the average of the ninth and tenth observation on that row. Those two values correspond to =43= and =43=, so a good guess for the median would be 430. (For the record, the sample median is \(\widetilde{x}=425\). Recall that stemplots round the data to the nearest stem-leaf pair.) 

Next let us see what the =precip= data look like.

#+begin_src R :exports both :results output pp  
stem.leaf(precip)
#+end_src

Here is an example of split stems, with two lines per stem. The final digit of each datum has been dropped for the display. The data appear to be left skewed with four extreme values to the left and one extreme value to the right. The sample median is approximately 37 (it turns out to be 36.6).

*** Hinges and the Five Number Summary
:PROPERTIES:
:CUSTOM_ID: sub-hinges-and-5NS
:END:

Given a data set \(x_{1}\), \(x_{2}\), ..., \(x_{n}\), the hinges are found by the following method:  
- Find the order statistics \(x_{(1)}\), \(x_{(2)}\), ..., \(x_{(n)}\). 
- The /lower hinge/ \(h_{L}\) is in position \(L=\left\lfloor (n+3)/2\right\rfloor / 2\), where the symbol \( \left\lfloor x\right\rfloor \) denotes the largest integer less than or equal to \(x\). If the position \(L\) is not an integer, then the hinge \(h_{L}\) is the average of the adjacent order statistics. 
- The /upper hinge/ \(h_{U}\) is in position \(n+1-L\).
Given the hinges, the /five number summary/ (\(5NS\)) is 
\begin{equation} 
5NS=(x_{(1)},\ h_{L},\ \tilde{x},\ h_{U},\ x_{(n)}).
\end{equation}
An advantage of the \(5NS\) is that it reduces a potentially large data set to a shorter list of only five numbers, and further, these numbers give insight regarding the shape of the data distribution similar to the sample quantiles in Section [[sub-Order-Statistics-and][Order Statistics]].

**** How to do it with \(\mathsf{R}\)

If the data are stored in a vector =x=, then you can compute the \(5NS\) with the =fivenum= function.

*** Boxplots
:PROPERTIES:
:CUSTOM_ID: sub-boxplots
:END:

A boxplot is essentially a graphical representation of the \(5NS\). It can be a handy alternative to a stripchart when the sample size is large.

A boxplot is constructed by drawing a box alongside the data axis with sides located at the upper and lower hinges. A line is drawn parallel to the sides to denote the sample median. Lastly, whiskers are extended from the sides of the box to the maximum and minimum data values (more precisely, to the most extreme values that are not potential outliers, defined below).

Boxplots are good for quick visual summaries of data sets, and the relative positions of the values in the \(5NS\) are good at indicating the underlying shape of the data distribution, although perhaps not as effectively as a histogram. Perhaps the greatest advantage of a boxplot is that it can help to objectively identify extreme observations in the data set as described in the next section.

Boxplots are also good because one can visually assess multiple features of the data set simultaneously:

- Center :: can be estimated by the sample median, \(\tilde{x}\).

- Spread :: can be judged by the width of the box, \(h_{U}-h_{L}\). We know that this will be close to the \(IQR\), which can be compared to \(s\) and the \(MAD\), perhaps after rescaling if appropriate.

- Shape :: is indicated by the relative lengths of the whiskers, and the position of the median inside the box. Boxes with unbalanced whiskers indicate skewness in the direction of the long whisker. Skewed distributions often have the median tending in the opposite direction of skewness. Kurtosis can be assessed using the box and whiskers. A wide box with short whiskers will tend to be platykurtic, while a skinny box with wide whiskers indicates leptokurtic distributions.

- Extreme observations :: are identified with open circles (see below).

*** Outliers
:PROPERTIES:
:CUSTOM_ID: sub-Outliers
:END:

A /potential outlier/ is any observation that falls beyond 1.5 times the width of the box on either side, that is, any observation less than \(h_{L}-1.5(h_{U}-h_{L})\) or greater than \(h_{U}+1.5(h_{U}-h_{L})\). A /suspected outlier/ is any observation that falls beyond 3 times the width of the box on either side. In \(\mathsf{R}\), both potential and suspected outliers (if present) are denoted by open circles; there is no distinction between the two. 

When potential outliers are present, the whiskers of the boxplot are then shortened to extend to the most extreme observation that is not a potential outlier. If an outlier is displayed in a boxplot, the index of the observation may be identified in a subsequent plot in =Rcmdr= by clicking the =Identify outliers with mouse= option in the =Boxplot= dialog.

What do we do about outliers? They merit further investigation. The primary goal is to determine why the observation is outlying, if possible. If the observation is a typographical error, then it should be corrected before continuing. If the observation is from a subject that does not belong to the population of interest, then perhaps the datum should be removed. Otherwise, perhaps the value is hinting at some hidden structure to the data.

**** How to do it with \(\mathsf{R}\)

The quickest way to visually identify outliers is with a boxplot, described above. Another way is with the =boxplot.stats= function.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
The =rivers= data. We will look for potential outliers in the =rivers= data.

#+begin_src R :exports both :results output pp  
boxplot.stats(rivers)$out
#+end_src

We may change the =coef= argument to 3 (it is 1.5 by default) to identify suspected outliers.

#+begin_src R :exports both :results output pp  
boxplot.stats(rivers, coef = 3)$out
#+end_src

#+latex: \end{exampletoo}
#+html: </div>

*** Standardizing variables

It is sometimes useful to compare data sets with each other on a scale that is independent of the measurement units. Given a set of observed data \(x_{1}\), \(x_{2}\), ..., \(x_{n}\) we get \(z\) scores, denoted \(z_{1}\), \(z_{2}\), ..., \(z_{n}\), by means of the following formula
\[
z_{i}=\frac{x_{i}-\overline{x}}{s},\quad i=1,\,2,\,\ldots,\, n.
\]

**** How to do it with \(\mathsf{R}\)

The =scale= function will rescale a numeric vector (or data frame) by subtracting the sample mean from each value (column) and/or by dividing each observation by the sample standard deviation.

** Multivariate Data and Data Frames
:PROPERTIES:
:CUSTOM_ID: sec-multivariate-data
:END:

We have had experience with vectors of data, which are long lists of numbers. Typically, each entry in the vector is a single measurement on a subject or experimental unit in the study. We saw in Section [[sub-Vectors][Vectors]] how to form vectors with the =c= function or the =scan= function. 

However, statistical studies often involve experiments where there are two (or more) measurements associated with each subject. We display the measured information in a rectangular array in which each row corresponds to a subject, and the columns contain the measurements for each respective variable. For instance, if one were to measure the height and weight and hair color of each of 11 persons in a research study, the information could be represented with a rectangular array. There would be 11 rows. Each row would have the person's height in the first column and hair color in the second column.

The corresponding objects in \(\mathsf{R}\) are called /data frames/, and they can be constructed with the =data.frame= function. Each row is an observation, and each column is a variable.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Suppose we have two vectors =x= and =y= and we want to make a data frame out of them.

#+begin_src R :exports code :results silent
x <- 5:8
y <- letters[3:6]
A <- data.frame(v1 = x, v2 = y)
#+end_src

#+latex: \end{exampletoo}
#+html: </div>

Notice that =x= and =y= are the same length. This is /necessary/. Also notice that =x= is a numeric vector and =y= is a character vector. We may choose numeric and character vectors (or even factors) for the columns of the data frame, but each column must be of exactly one type. That is, we can have a column for =height= and a column for =gender=, but we will get an error if we try to mix function =height= (numeric) and =gender= (character or factor) information in the same column.

Indexing of data frames is similar to indexing of vectors. To get the entry in row \(i\) and column \(j\) do =A[i,j]=. We can get entire rows and columns by omitting the other index. 

#+begin_src R :exports both :results output pp
A[3, ]
A[ , 1]
A[ , 2]
#+end_src

There are several things happening above. Notice that =A[3, ]= gave a data frame (with the same entries as the third row of =A=) yet =A[ , 1]= is a numeric vector. =A[ ,2]= is a factor vector because the default setting for =data.frame= is =stringsAsFactors = TRUE=.

Data frames have a =names= attribute and the names may be extracted with the =names= function. Once we have the names we may extract given columns by way of the dollar sign.

#+begin_src R :exports both :results output pp
names(A)
A['v1']
#+end_src

The above is identical to =A[ ,1]=. 

*** Bivariate Data
:PROPERTIES:
:CUSTOM_ID: sub-Bivariate-Data
:END:

- Stacked bar charts
- odds ratio and relative risk
- Introduce the sample correlation coefficient.

The *sample Pearson product-moment correlation
coefficient*:

\[
r=\frac{\sum_{i=1}^{n}(x_{i}-\overline{x})(y_{i}-\overline{y})}{\sqrt{\sum_{i=1}^{n}(x_{i}-\overline{x})}\sqrt{\sum_{i=1}^{n}(y_{i}-\overline{y})}}
\]

- independent of scale
- \(-1< r <1\)
- measures /strength/ and /direction/ of linear association
- Two-Way Tables. Done with =table=, or in the \(\mathsf{R}\) Commander by following =Statistics= \(\triangleright\) =Contingency Tables= \(\triangleright\)} =Two-way Tables=. You can also enter and analyze a two-way table.
  - table
  - prop.table
  - addmargins
  - rowPercents (Rcmdr)
  - colPercents (Rcmdr)
  - totPercents(Rcmdr)
  - A <- xtabs(~ gender + race, data = RcmdrTestDrive)
  - xtabs( Freq ~ Class + Sex, data = Titanic) from built in table
  - barplot(A, legend.text=TRUE) 
  - barplot(t(A), legend.text=TRUE) 
  - barplot(A, legend.text=TRUE, beside = TRUE)
  - spineplot(gender ~ race, data = RcmdrTestDrive)
  - Spine plot: plots categorical versus categorical


- Scatterplot: look for linear association and correlation. 
  - carb ~ optden, data = Formaldehyde (boring)
  - conc ~ rate, data = Puromycin
  - xyplot(accel ~ dist, data = attenu) nonlinear association
  - xyplot(eruptions ~ waiting, data = faithful) (linear, two groups)
  - xyplot(Petal.Width ~ Petal.Length, data = iris)
  - xyplot(pressure ~ temperature, data = pressure) (exponential growth)
  - xyplot(weight ~ height, data = women) (strong positive linear)

*** Multivariate Data
:PROPERTIES:
:CUSTOM_ID: sub-Multivariate-Data
:END:

Multivariate Data Display

- Multi-Way Tables. You can do this with =table=,
or in \(\mathsf{R}\) Commander by following =Statistics= \(\triangleright\) =Contingency Tables= \(\triangleright\) =Multi-way Tables=.
- Scatterplot matrix. used for displaying pairwise scatterplots simultaneously. Again, look for linear association and correlation.
- 3D Scatterplot. See Figure [[fig-3D-scatterplot-trees][3D-scatterplot-trees]]
- =plot(state.region, state.division)= 
- =barplot(table(state.division,state.region), legend.text=TRUE)=

#+begin_src :eval never
require(graphics)
mosaicplot(HairEyeColor)
x <- apply(HairEyeColor, c(1, 2), sum)
x
mosaicplot(x, main = "Relation between hair and eye color")
y <- apply(HairEyeColor, c(1, 3), sum)
y
mosaicplot(y, main = "Relation between hair color and sex")
z <- apply(HairEyeColor, c(2, 3), sum)
z
mosaicplot(z, main = "Relation between eye color and sex")
#+end_src

** Comparing Populations
:PROPERTIES:
:CUSTOM_ID: sec-Comparing-Data-Sets
:END:

Sometimes we have data from two or more groups (or populations) and we would like to compare them and draw conclusions. Some issues that we would like to address:

- Comparing centers and spreads: variation within versus between groups
- Comparing clusters and gaps
- Comparing outliers and unusual features
- Comparing shapes.

*** Numerically

I am thinking here about the =Statistics= \(\triangleright\) =Numerical Summaries= \(\triangleright\) =Summarize by groups= option or the =Statistics= \(\triangleright\) =Summaries= \(\triangleright\) =Table of Statistics= option. 

*** Graphically

- Boxplots
  - Variable width: the width of the drawn boxplots are proportional to \(\sqrt{n_{i}}\), where \(n_{i}\) is the size of the \(i^{\mathrm{th}}\) group. Why? Because many statistics have variability proportional to the reciprocal of the square root of the sample size.
  - Notches: extend to \(1.58\cdot(h_{U}-h_{L})/\sqrt{n}\). The idea is to give roughly a 95% confidence interval for the difference in two medians. See Chapter [[cha-Hypothesis-Testing][Hypothesis Testing]].
- Stripcharts
  - stripchart(weight ~ feed, method= "stack", data=chickwts) 
- Bar Graphs
  - barplot(xtabs(Freq ~ Admit + Gender, data = UCBAdmissions)) stacked bar chart
  - barplot(xtabs(Freq ~ Admit, data = UCBAdmissions))
  - barplot(xtabs(Freq ~ Gender + Admit, data = UCBAdmissions, legend = TRUE, beside = TRUE)  oops, discrimination.
  - barplot(xtabs(Freq ~ Admit+Dept, data = UCBAdmissions), legend = TRUE, beside = TRUE) different departments have different standards
  - barplot(xtabs(Freq ~ Gender+Dept, data = UCBAdmissions), legend = TRUE, beside = TRUE) men mostly applied to easy departments, women mostly applied to difficult departments
  - barplot(xtabs(Freq ~ Gender+Dept, data = UCBAdmissions), legend = TRUE, beside = TRUE)
  - barchart(Admit ~ Freq, data = C)
  - barchart(Admit ~ Freq|Gender, data = C)
  - barchart(Admit ~ Freq | Dept, groups = Gender, data = C)
  - barchart(Admit ~ Freq | Dept, groups = Gender, data = C, auto.key = TRUE)

- Histograms
  - ~ breaks | wool{*}tension, data = warpbreaks
  - ~ weight | feed, data = chickwts
  - ~ weight | group, data = PlantGrowth 
  - ~ count | spray, data = InsectSprays
  - ~ len | dose, data = ToothGrowth
  - ~ decrease | treatment, data = OrchardSprays (or rowpos or colpos)

- Scatterplots

#+begin_src R :exports code :eval never
xyplot(Petal.Width ~ Petal.Length, data = iris, group = Species)
#+end_src

#+name: xyplot-group
#+begin_src R :exports none :results silent
print(xyplot(Petal.Width ~ Petal.Length, data = iris, group = Species))
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/datadesc/xyplot-group.ps
  <<xyplot-group>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/datadesc/xyplot-group.svg
  <<xyplot-group>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/datadesc/xyplot-group.ps}
  \caption[Scatterplot of Petal width versus length in the iris data.]{Scatterplot of Petal width versus length in the iris data.}
  \label{fig-xyplot-group}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-xyplot-group" class="figure">
  <p><img src="svg/datadesc/xyplot-group.svg" width=500 alt="svg/datadesc/xyplot-group.svg" /></p>
  <p>Thre  <code>overplot</code> method.</p>
</div>
#+end_html

- Scatterplot matrices
  - splom( ~ cbind(GNP.deflator,GNP,Unemployed,Armed.Forces,Population,Year,Employed),  data = longley) 
  - splom( ~ cbind(pop15,pop75,dpi), data = LifeCycleSavings)
  - splom( ~ cbind(Murder, Assault, Rape), data = USArrests)
  - splom( ~ cbind(CONT, INTG, DMNR), data = USJudgeRatings)
  - splom( ~ cbind(area,peri,shape,perm), data = rock)
  - splom( ~ cbind(Air.Flow, Water.Temp, Acid.Conc., stack.loss), data = stackloss)
  - splom( ~ cbind(Fertility,Agriculture,Examination,Education,Catholic,Infant.Mortality), data = swiss)
  - splom(~ cbind(Fertility,Agriculture,Examination), data = swiss) (positive and negative)

- Dot charts
  - dotchart(USPersonalExpenditure)
  - dotchart(t(USPersonalExpenditure))
  - dotchart(WorldPhones) (transpose is no good)
  - freeny.x is no good, neither is volcano
  - dotchart(UCBAdmissions{[},,1{]})
  - dotplot(Survived ~ Freq | Class, groups = Sex, data = B)
  - dotplot(Admit ~ Freq | Dept, groups = Gender, data = C)

- Mosaic plot
  - mosaic( ~ Survived + Class + Age + Sex, data = Titanic) (or just mosaic(Titanic))
  - mosaic( ~ Admit + Dept + Gender, data = UCBAdmissions)

- Spine plots
  - spineplot(xtabs(Freq ~ Admit + Gender, data = UCBAdmissions))
  - # rescaled barplot

- Quantile-quantile plots: There are two ways to do this. One way is to compare two independent samples (of the same size). qqplot(x,y). Another way is to compare the sample quantiles of one variable to the theoretical quantiles of another distribution. 


Given two samples \(x_{1}\), \(x_{2}\), ..., \(x_{n}\), and \(y_{1}\), \(y_{2}\), ..., \(y_{n}\), we may find the order statistics \(x_{(1)}\leq x_{(2)}\leq\cdots\leq x_{(n)}\) and \(y_{(1)}\leq y_{(2)}\leq\cdots\leq y_{(n)}\). Next, plot the \(n\) points \((x_{(1)},y_{(1)})\), \((x_{(2)},y_{(2)})\), ..., \((x_{(n)},y_{(n)})\).

It is clear that if \(x_{(k)}=y_{(k)}\) for all \(k=1,2,\ldots,n\), then we will have a straight line. It is also clear that in the real world, a straight line is NEVER observed, and instead we have a scatterplot that hopefully had a general linear trend. What do the rules tell us? 

- If the \(y\)-intercept of the line is greater (less) than zero, then the center of the \(Y\) data is greater (less) than the center of the \(X\) data.

- If the slope of the line is greater (less) than one, then the spread of the \(Y\) data is greater (less) than the spread of the \(X\) data.

*** Lattice Graphics
:PROPERTIES:
:CUSTOM_ID: sub-Lattice-Graphics
:END:

The following types of plots are useful when there is one variable of interest and there is a factor in the data set by which the variable is categorized. 

It is sometimes nice to set =lattice.options(default.theme = "col.whitebg")=

**** Side by side boxplots

#+begin_src R :exports code :eval never
bwplot(~weight | feed, data = chickwts)
#+end_src

#+name: bwplot
#+begin_src R :exports none :results silent
print(bwplot(~weight | feed, data = chickwts))
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/datadesc/bwplot.ps
  <<bwplot>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/datadesc/bwplot.svg
  <<bwplot>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/datadesc/bwplot.ps}
  \caption{Boxplots of \texttt{weight} by \texttt{feed} type in the \texttt{chickwts} data.}
  \label{fig-bwplot}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-bwplot" class="figure">
  <p><img src="svg/datadesc/bwplot.svg" width=500 alt="svg/datadesc/bwplot.svg" /></p>
  <p>Boxplots of <code>weight</code> by <code>feed</code> type in the <code>chickwts</code> data.</p>
</div>
#+end_html

**** Histograms

#+begin_src R :exports code :eval never
histogram(~age | education, data = infert)
#+end_src

#+name: histograms-lattice
#+begin_src R :exports none :results silent
print(histogram(~age | education, data = infert))
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/datadesc/histograms-lattice.ps
  <<histograms-lattice>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/datadesc/histograms-lattice.svg
  <<histograms-lattice>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/datadesc/histograms-lattice.ps}
  \caption[Histograms of \texttt{age} by \texttt{education} level]{Histograms of \texttt{age} by \texttt{education} level from the \texttt{infert} data}
  \label{fig-histograms-lattice}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-histograms-lattice" class="figure">
  <p><img src="svg/datadesc/histograms-lattice.svg" width=500 alt="svg/datadesc/histograms-lattice.svg" /></p>
  <p>Histograms of <code>age</code> by <code>education</code> level from the <code>infert</code> data.</p>
</div>
#+end_html

**** Scatterplots

#+begin_src R :exports code :eval never
xyplot(Petal.Length ~ Petal.Width | Species, data = iris)
#+end_src

#+name: xyplot
#+begin_src R :exports none :results silent
print(xyplot(Petal.Length ~ Petal.Width | Species, data = iris))
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/datadesc/xyplot.ps
  <<xyplot>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/datadesc/xyplot.svg
  <<xyplot>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/datadesc/xyplot.ps}
  \caption[An \texttt{xyplot} of \texttt{Petal.Length} versus \texttt{Petal.Width} by \texttt{Species}]{An \texttt{xyplot} of \texttt{Petal.Length} versus \texttt{Petal.Width} by \texttt{Species} in the \texttt{iris} data}
  \label{fig-xyplot}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-xyplot" class="figure">
  <p><img src="svg/datadesc/xyplot.svg" width=500 alt="svg/datadesc/xyplot.svg" /></p>
  <p>An <code>xyplot</code> of <code>Petal.Length</code> versus <code>Petal.Width</code> by <code>Species</code> in the <code>iris</code> data.</p>
</div>
#+end_html

**** Coplots

#+begin_src R :exports code :eval never
coplot(conc ~ uptake | Type * Treatment, data = CO2)
#+end_src

#+name: coplot
#+begin_src R :exports none :results silent
print(coplot(conc ~ uptake | Type * Treatment, data = CO2))
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/datadesc/coplot.ps
  <<coplot>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/datadesc/coplot.svg
  <<coplot>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/datadesc/coplot.ps}
  \caption[A \texttt{coplot} of \texttt{conc} versus \texttt{uptake} by \texttt{Type} and \texttt{Treatment}]{A \texttt{coplot} of \texttt{conc} versus \texttt{uptake} by \texttt{Type} and \texttt{Treatment} in the \texttt{CO2} data}
  \label{fig-coplot}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-coplot" class="figure">
  <p><img src="svg/datadesc/coplot.svg" width=500 alt="svg/datadesc/coplot.svg" /></p>
  <p>A <code>coplot</code> of <code>conc</code> versus <code>uptake</code> by <code>Type</code> and <code>Treatment</code> in the <code>CO2</code> data.</p>
</div>
#+end_html


#+latex: \newpage{}

** Some Remarks about Plotting

Getting your labels to look right
#+begin_src R :eval never
library(ggplot2)
a <- qplot(state.division, geom = "bar")
a + opts(axis.text.x = theme_text(angle = -90, hjust = 0))
#+end_src

#+begin_src R :eval never
hist(precip, freq = FALSE)
lines(density(precip))
qplot(precip, geom = "density")
m <- ggplot(as.data.frame(precip), aes(x = precip))
m + geom_histogram()
m + geom_histogram(aes(y = ..density..)) + geom_density()
#+end_src


** Exercises
#+latex: \setcounter{thm}{0}

Open \(\mathsf{R}\) and issue the following commands at the command line to get started. Note that you need to have the =RcmdrPlugin.IPSUR= package \cite{RcmdrPlugin.IPSUR} installed, and for some exercises you need the =e1071= package \cite{e1071}.

#+begin_src R :exports code :results silent
library("RcmdrPlugin.IPSUR")
data(RcmdrTestDrive)
attach(RcmdrTestDrive)
names(RcmdrTestDrive)
#+end_src

To load the data in the \(\mathsf{R}\) Commander (=Rcmdr=), click the =Data Set= button, and select =RcmdrTestDrive= as the active data set. To learn more about the data set and where it comes from, type =?RcmdrTestDrive= at the command line.

#+begin_xca
# <<xca-summary-RcmdrTestDrive>>

Perform a summary of all variables in =RcmdrTestDrive=. You can do this with the command =summary(RcmdrTestDrive)=.

Alternatively, you can do this in the =Rcmdr= with the sequence =Statistics= \(\triangleright\) =Summaries= \(\triangleright\) =Active Data Set=. Report the values of the summary statistics for each variable.
#+end_xca

#+begin_xca
Make a table of the =race= variable. Do this with =Statistics= \(\triangleright\) =Summaries= \(\triangleright\) =Frequency Distributions - IPSUR...=
1. Which ethnicity has the highest frequency?
1. Which ethnicity has the lowest frequency?
1. Include a bar graph of =race=. Do this with =Graphs= \(\triangleright\) =IPSUR - Bar Graph...=
#+end_xca

#+begin_xca
Calculate the average =salary= by the factor =gender=. Do this with =Statistics= \(\triangleright\) =Summaries= \(\triangleright\) =Table of Statistics...= 
1. Which =gender= has the highest mean =salary=? 
1. Report the highest mean =salary=.
1. Compare the spreads for the genders by calculating the standard deviation of =salary= by =gender=. Which =gender= has the biggest standard deviation?
1. Make boxplots of =salary= by =gender= with the following method:
   #+begin_quote
   On the =Rcmdr=, click =Graphs= \(\triangleright\) =IPSUR - Boxplot...=
   In the =Variable= box, select =salary=.
   Click the =Plot by groups...= box and select =gender=. Click =OK=.
   Click =OK= to graph the boxplot.
   #+end_quote
   How does the boxplot compare to your answers to (1) and (3)?
#+end_xca

#+begin_xca
For this problem we will study the variable =reduction=.
1. Find the order statistics and store them in a vector =x=. /Hint:/ =x <- sort(reduction)=
1. Find \(x_{(137)}\), the 137\(^{\mathrm{th}}\) order statistic.
1. Find the IQR.
1. Find the Five Number Summary (5NS).
1. Use the 5NS to calculate what the width of a boxplot of =reduction= would be.
1. Compare your answers (3) and (5). Are they the same? If not, are they close?
1. Make a boxplot of =reduction=, and include the boxplot in your report. You can do this with the =boxplot= function, or in =Rcmdr= with =Graphs= \(\triangleright\) =IPSUR - Boxplot...=
1. Are there any potential/suspected outliers? If so, list their values. /Hint:/ use your answer to (a).
1. Using the rules discussed in the text, classify answers to (8), if any, as /potential/ or /suspected/ outliers.
#+end_xca

#+begin_xca
In this problem we will compare the variables =before= and =after=. Don't forget =library("e1071")=.
1. Examine the two measures of center for both variables. Judging from these measures, which variable has a higher center?
1. Which measure of center is more appropriate for =before=? (You may want to look at a boxplot.) Which measure of center is more appropriate for =after=?
1. Based on your answer to (2), choose an appropriate measure of spread for each variable, calculate it, and report its value. Which variable has the biggest spread? (Note that you need to make sure that your measures are on the same scale.) 
1. Calculate and report the skewness and kurtosis for =before=. Based on these values, how would you describe the shape of =before=?
1. Calculate and report the skewness and kurtosis for =after=. Based on these values, how would you describe the shape of =after=?
1. Plot histograms of =before= and =after= and compare them to your answers to (4) and (5).
#+end_xca

#+begin_xca
Describe the following data sets just as if you were communicating with an alien, but one who has had a statistics class. Mention the salient features (data type, important properties, anything special). Support your answers with the appropriate visual displays and descriptive statistics.
1. Conversion rates of Euro currencies stored in =euro=.
1. State abbreviations stored in =state.abb=.
#+end_xca

* Probability                                                          :prob:
:PROPERTIES:
:tangle: R/prob.R
:CUSTOM_ID: cha-Probability
:END:

#+begin_src R :exports none :eval never
# Chapter: Probability
# All code released under GPL Version 3
#+end_src

#+latex: \noindent 
In this chapter we define the basic terminology associated with probability and derive some of its properties. We discuss three interpretations of probability. We discuss conditional probability and independent events, along with Bayes' Theorem. We finish the chapter with an introduction to random variables, which paves the way for the next two chapters.

In this book we distinguish between two types of experiments: /deterministic/ and /random/. A /deterministic/ experiment is one whose outcome may be predicted with certainty beforehand, such as combining Hydrogen and Oxygen, or adding two numbers such as \(2+3\). A /random/ experiment is one whose outcome is determined by chance. We posit that the outcome of a random experiment may not be predicted with certainty beforehand, even in principle. Examples of random experiments include tossing a coin, rolling a die, and throwing a dart on a board, how many red lights you encounter on the drive home, how many ants traverse a certain patch of sidewalk over a short period, /etc/.

 *What do I want them to know?*
- that there are multiple interpretations of probability, and the methods used depend somewhat on the philosophy chosen 
- nuts and bolts of basic probability jargon: sample spaces, events, probability functions, /etc/.
- how to count
- conditional probability and its relationship with independence
- Bayes' Rule and how it relates to the subjective view of probability
- what we mean by 'random variables', and where they come from

#+name: diagram
#+begin_src R :exports none :results silent
require(diagram)
par(mex = 0.2, cex = 0.5)
openplotmat(frame.plot=TRUE)
straightarrow(from = c(0.46,0.74), to = c(0.53,0.71), arr.pos = 1)
straightarrow(from = c(0.3,0.65), to = c(0.3,0.51), arr.pos = 1)
textellipse(mid = c(0.74,0.55), box.col = grey(0.95), 
  radx = 0.24, rady = 0.22, 
  lab = c(expression(bold(underline(DETERMINISTIC))), 
          expression(2*H[2]+O[2] %->% H[2]*O), "3 + 4 = 7"), cex = 2 )
textrect(mid = c(0.3, 0.75), radx = 0.15, rady = 0.1, 
  lab = c(expression(bold(Experiments))), cex = 2 )
textellipse(mid = c(0.29,0.25), box.col = grey(0.95), 
  radx = 0.27, rady = 0.22, lab = c(expression(bold(underline(RANDOM))), 
  "toss coin, roll die", "count ants on sidewalk", "measure rainfall" ), 
  cex = 2 )
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/prob/diagram.ps
  <<diagram>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/prob/diagram.svg
  <<diagram>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/prob/diagram.ps}
  \caption[Two types of experiments]{Two types of experiments}
  \label{fig-diagram}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-diagram" class="figure">
  <p><img src="svg/prob/diagram.svg" width=500 alt="svg/prob/diagram.svg" /></p>
  <p>Two types of experiments.</p>
</div>
#+end_html

** Sample Spaces
:PROPERTIES:
:CUSTOM_ID: sec-Sample-Spaces
:END:

For a random experiment \(E\), the set of all possible outcomes of \(E\) is called the /sample space/\index{sample space} and is denoted by the letter \(S\). For a coin-toss experiment, \(S\) would be the results ``Head'' and ``Tail'', which we may represent by \( S = \{H,T \} \). Formally, the performance of a random experiment is the unpredictable selection of an outcome in \(S\).

*** How to do it with \(\mathsf{R}\)

Most of the probability work in this book is done with the =prob= package \cite{prob}. A sample space is (usually) represented by a /data frame/, that is, a rectangular collection of variables (see Section [[sub-Multivariate-Data][Multivariate Data]]). Each row of the data frame corresponds to an outcome of the experiment. The data frame choice is convenient both for its simplicity and its compatibility with the \(\mathsf{R}\) Commander. Data frames alone are, however, not sufficient to describe some of the more interesting probabilistic applications we will study later; to handle those we will need to consider a more general /list/ data structure. See Section [[sub-howto-ps-objects][How to =ps=]] for details.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Consider the random experiment of dropping a Styrofoam cup onto the floor from a height of four feet. The cup hits the ground and eventually comes to rest. It could land upside down, right side up, or it could land on its side. We represent these possible outcomes of the random experiment by the following.

#+begin_src R :exports both :results output pp  
S <- data.frame(lands = c("down","up","side"))
S
#+end_src

The sample space =S= contains the column =lands= which stores the outcomes =down=, =up=, and =side=. 

#+latex: \end{exampletoo}
#+html: </div>

Some sample spaces are so common that convenience wrappers were written to set them up with minimal effort. The underlying machinery that does the work includes the =expand.grid= function in the =base= package \cite{base}, =combn= in the =combinat= package \cite{combinat}, and =permsn= in the =prob= package \cite{prob}
#+latex: \footnote{The seasoned \(\mathsf{R}\) user can get the job done without the convenience wrappers. I encourage the beginner to use them to get started, but I also recommend that introductory students wean themselves as soon as possible. The wrappers were designed for ease and intuitive use, not for speed or efficiency.}.
Consider the random experiment of tossing a coin. The outcomes are \(H\) and \(T\). We can set up the sample space quickly with the =tosscoin= function:

#+begin_src R :exports both :results output pp  
tosscoin(1)
#+end_src

The number =1= tells =tosscoin= that we only want to toss the coin once. We could toss it three times: 

#+begin_src R :exports both :results output pp  
tosscoin(3)
#+end_src

Alternatively we could roll a fair die: 

#+begin_src R :exports both :results output pp  
rolldie(1) 
#+end_src

The =rolldie= function defaults to a 6-sided die, but we can specify others with the =nsides= argument. The command =rolldie(3, nsides = 4)= would be used to roll a 4-sided die three times.

Perhaps we would like to draw one card from a standard set of playing cards (it is a long data frame):

#+begin_src R :exports both :results output pp
head(cards()) 
#+end_src

The =cards= function that we just used has optional arguments =jokers= (if you would like Jokers to be in the deck) and =makespace= which we will discuss later. There is also a =roulette= function which returns the sample space associated with one spin on a roulette wheel. There are EU and USA versions available. Interested readers may contribute any other game or sample spaces that may be of general interest.

*** Sampling from Urns
:PROPERTIES:
:CUSTOM_ID: sub-sampling-from-urns
:END:

This is perhaps the most fundamental type of random experiment. We have an urn that contains a bunch of distinguishable objects (balls) inside. We shake up the urn, reach inside, grab a ball, and take a look. That's all.

But there are all sorts of variations on this theme. Maybe we would like to grab more than one ball -- say, two balls. What are all of the possible outcomes of the experiment now? It depends on how we sample. We could select a ball, take a look, put it back, and sample again. Another way would be to select a ball, take a look -- but do not put it back -- and sample again (equivalently, just reach in and grab two balls). There are certainly more possible outcomes of the experiment in the former case than in the latter. In the first (second) case we say that sampling is done /with (without) replacement/.

There is more. Suppose we do not actually keep track of which ball came first. All we observe are the two balls, and we have no idea about the order in which they were selected. We call this /unordered sampling/ (in contrast to /ordered/) because the order of the selections does not matter with respect to what we observe. We might as well have selected the balls and put them in a bag before looking.

Note that this one general class of random experiments contains as a special case all of the common elementary random experiments. Tossing a coin twice is equivalent to selecting two balls labeled \(H\) and \(T\) from an urn, with replacement. The die-roll experiment is equivalent to selecting a ball from an urn with six elements, labeled 1 through 6.

**** How to do it with \(\mathsf{R}\) 

The =prob= package \cite{prob} accomplishes sampling from urns with the =urnsamples=\index{urnsamples@\texttt{urnsamples}} function, which has arguments =x=, =size=, =replace=, and =ordered=. The argument =x= represents the urn from which sampling is to be done. The =size= argument tells how large the sample will be. The =ordered= and =replace= arguments are logical and specify how sampling will be performed. We will discuss each in turn.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-sample-urn-two-from-three>>
Let our urn simply contain three balls, labeled 1, 2, and 3, respectively. We are going to take a sample of size 2 from the urn. 

**** Ordered, With Replacement

If sampling is with replacement, then we can get any outcome 1, 2, or 3 on any draw. Further, by ``ordered'' we mean that we shall keep track of the order of the draws that we observe. We can accomplish this in \(\mathsf{R}\) with

#+begin_src R :exports both :results output pp  
urnsamples(1:3, size = 2, replace = TRUE, ordered = TRUE)
#+end_src 

Notice that rows 2 and 4 are identical, save for the order in which the numbers are shown. Further, note that every possible pair of the numbers 1 through 3 are listed. This experiment is equivalent to rolling a 3-sided die twice, which we could have accomplished with =rolldie(2, nsides = 3)=.

**** Ordered, Without Replacement

Here sampling is without replacement, so we may not observe the same number twice in any row. Order is still important, however, so we expect to see the outcomes =1,2= and =2,1= somewhere in our data frame. 

#+begin_src R :exports both :results output pp   
urnsamples(1:3, size = 2, replace = FALSE, ordered = TRUE)
#+end_src

This is just as we expected. Notice that there are less rows in this answer due to the more restrictive sampling procedure. If the numbers 1, 2, and 3 represented ``Fred'', ``Mary'', and ``Sue'', respectively, then this experiment would be equivalent to selecting two people of the three to serve as president and vice-president of a company, respectively, and the sample space shown above lists all possible ways that this could be done.

**** Unordered, Without Replacement

Again, we may not observe the same outcome twice, but in this case, we will only retain those outcomes which (when jumbled) would not duplicate earlier ones. 

#+begin_src R :exports both :results output pp   
urnsamples(1:3, size = 2, replace = FALSE, ordered = FALSE) 
#+end_src 

This experiment is equivalent to reaching in the urn, picking a pair, and looking to see what they are. This is the default setting of =urnsamples=, so we would have received the same output by simply typing =urnsamples(1:3, 2)=.

**** Unordered, With Replacement

The last possibility is perhaps the most interesting. We replace the balls after every draw, but we do not remember the order in which the draws came. 

#+begin_src R :exports both :results output pp   
urnsamples(1:3, size = 2, replace = TRUE, ordered = FALSE) 
#+end_src

We may interpret this experiment in a number of alternative ways. One way is to consider this as simply putting two 3-sided dice in a cup, shaking the cup, and looking inside -- as in a game of /Liar's Dice/, for instance. Each row of the sample space is a potential pair we could observe. Another way is to view each outcome as a separate method to distribute two identical golf balls into three boxes labeled 1, 2, and 3. Regardless of the interpretation, =urnsamples= lists every possible way that the experiment can conclude.

#+latex: \end{exampletoo}
#+html: </div>
Note that the urn does not need to contain numbers; we could have just as easily taken our urn to be =x = c("Red","Blue","Green")=. But, there is an *important* point to mention before proceeding. Astute readers will notice that in our example, the balls in the urn were /distinguishable/ in the sense that each had a unique label to distinguish it from the others in the urn. A natural question would be, ``What happens if your urn has indistinguishable elements, for example, what if =x = c("Red","Red","Blue")=?'' The answer is that =urnsamples= behaves as if each ball in the urn is distinguishable, regardless of its actual contents. We may thus imagine that while there are two red balls in the urn, the balls are such that we can tell them apart (in principle) by looking closely enough at the imperfections on their
surface.

In this way, when the =x= argument of =urnsamples= has repeated elements, the resulting sample space may appear to be =ordered = TRUE= even when, in fact, the call to the function was =urnsamples(..., ordered = FALSE)=. Similar remarks apply for the =replace= argument. 

** Events
:PROPERTIES:
:CUSTOM_ID: sec-Events
:END:

An /event/\index{event} \(A\) is merely a collection of outcomes, or in other words, a subset of the sample space
#+latex: \footnote{This naive definition works for finite or countably infinite sample spaces, but is inadequate for sample spaces in general. In this book, we will not address the subtleties that arise, but will refer the interested reader to any text on advanced probability or measure theory.}.
After the performance of a random experiment \(E\) we say that the event \(A\) /occurred/ if the experiment's outcome belongs to \(A\). We say that a bunch of events \(A_{1}\), \(A_{2}\), \(A_{3}\), ... are /mutually exclusive/\index{mutually exclusive} or /disjoint/ if \(A_{i}\cap A_{j}=\emptyset\) for any distinct pair \(A_{i}\neq A_{j}\). For instance, in the coin-toss experiment the events \( A = \{ \mbox{Heads} \}\) and \( B = \{ \mbox{Tails} \} \) would be mutually exclusive. Now would be a good time to review the algebra of sets in Appendix [[sec-The-Algebra-of][Algebra of Sets]].

*** How to do it with \(\mathsf{R}\)

Given a data frame sample/probability space =S=, we may extract rows using the =[]= operator: 

#+begin_src R :exports both :results output pp   
S <- tosscoin(2, makespace = TRUE) 
S[1:3, ] 
#+end_src

#+begin_src R :exports both :results output pp   
S[c(2,4), ] 
#+end_src

and so forth. We may also extract rows that satisfy a logical expression using the =subset= function, for instance 

#+begin_src R :exports code :results silent
S <- cards() 
#+end_src 

#+begin_src R :exports both :results output pp  
subset(S, suit == "Heart") 
#+end_src

#+begin_src R :exports both :results output pp  
subset(S, rank %in% 7:9)
#+end_src

We could continue indefinitely. Also note that mathematical expressions are allowed: 

#+begin_src R :exports both :results output pp   
subset(rolldie(3), X1+X2+X3 > 16) 
#+end_src

*** Functions for Finding Subsets

It does not take long before the subsets of interest become complicated to specify. Yet the main idea remains: we have a particular logical condition to apply to each row. If the row satisfies the condition, then it should be in the subset. It should not be in the subset otherwise. The ease with which the condition may be coded depends of course on the question being asked. Here are a few functions to get started.

**** The =%in%= function

The function =%in%= helps to learn whether each value of one vector lies somewhere inside another vector. 

#+begin_src R :exports both :results output pp  
x <- 1:10 
y <- 8:12 
y %in% x
#+end_src

Notice that the returned value is a vector of length 5 which tests whether each element of =y= is in =x=, in turn.

**** The =isin= function

It is more common to want to know whether the /whole/ vector =y= is in =x=. We can do this with the =isin= function. 

#+begin_src R :exports both :results output pp   
isin(x,y) 
#+end_src

Of course, one may ask why we did not try something like =all(y %in% x)=, which would give a single result, =TRUE=. The reason is that the answers are different in the case that =y= has repeated values. Compare: 

#+begin_src R :exports code :results silent
x <- 1:10 
y <- c(3,3,7) 
#+end_src 

#+begin_src R :exports both :results output pp   
all(y %in% x)
isin(x,y) 
#+end_src 

The reason for the above is of course that =x= contains the value 3, but =x= does not have /two/ 3's. The difference is important when rolling multiple dice, playing cards, /etc/. Note that there is an optional argument =ordered= which tests whether the elements of =y= appear in =x= in the order in which they are appear in =y=. The consequences are 

#+begin_src R :exports both :results output pp   
isin(x, c(3,4,5), ordered = TRUE) 
isin(x, c(3,5,4), ordered = TRUE) 
#+end_src 

The connection to probability is that have a data frame sample space and we would like to find a subset of that space. A =data.frame= method was written for =isin= that simply applies the function to each row of the data frame. We can see the method in action with the following: 

#+begin_src R :exports both :results output pp  
S <- rolldie(4) 
subset(S, isin(S, c(2,2,6), ordered = TRUE)) 
#+end_src

There are a few other functions written to find useful subsets, namely, =countrep= and =isrep=. Essentially these were written to test for (or count) a specific number of designated values in outcomes. See the documentation for details.

*** Set Union, Intersection, and Difference

Given subsets \(A\) and \(B\), it is often useful to manipulate them in an algebraic fashion. To this end, we have three set operations at our disposal: union, intersection, and difference. Below is a table that summarizes the pertinent information about these operations.

#+CAPTION: [Set operations]{Basic set operations.  The first column lists the name, the second shows the typical notation, the third describes set membership, and the fourth shows how to accomplish it with R.}
|--------------+-------------------+---------------------------+------------------|
| Name         | Denoted           | Defined by elements       | Code             |
|--------------+-------------------+---------------------------+------------------|
| Union        | \(A\cup B\)       | in \(A\) or \(B\) or both | =union(A,B)=     |
| Intersection | \(A\cap B\)       | in both \(A\) and \(B\)   | =intersect(A,B)= |
| Difference   | \(A\backslash B\) | in \(A\) but not in \(B\) | =setdiff(A,B)=   |
|--------------+-------------------+---------------------------+------------------|


Some examples follow. 

#+begin_src R :exports code :results silent
S <- cards() 
A <- subset(S, suit == "Heart") 
B <- subset(S, rank %in% 7:9)
#+end_src 

We can now do some set algebra: 

#+begin_src R :exports both :results output pp  
union(A,B)
#+end_src

#+begin_src R :exports both :results output pp  
intersect(A,B)
#+end_src 

#+begin_src R :exports both :results output pp  
setdiff(A,B)
#+end_src

#+begin_src R :exports both :results output pp  
setdiff(B,A) 
#+end_src 

Notice that =setdiff= is not symmetric. Further, note that we can calculate the /complement/ of a set \(A\), denoted \(A^{c}\) and defined to be the elements of \(S\) that are not in \(A\) simply with =setdiff(S,A)=. There have been methods written for =intersect=, =setdiff=, =subset=, and =union= in the case that the input objects are of class =ps=. See Section [[sub-howto-ps-objects][How to =ps=]].

#+begin_note

When the =prob= package \cite{prob} loads you will notice a message: =The following object(s) are masked from package:base: intersect, setdiff=. The reason for this message is that there already exist methods for the functions =intersect=, =setdiff=, =subset=, and =union= in the =base= package which ships with \(\mathsf{R}\). However, these methods were designed for when the arguments are vectors of the same mode. Since we are manipulating sample spaces which are data frames and lists, it was necessary to write methods to handle those cases as well. When the =prob= package is loaded, \(\mathsf{R}\) recognizes that there are multiple versions of the same function in the search path and acts to shield the new definitions from the existing ones. But there is no cause for alarm, thankfully, because the =prob= functions have been carefully defined to match the usual =base= package definition in the case that the arguments are vectors. 

#+end_note

** Model Assignment
:PROPERTIES:
:CUSTOM_ID: sec-Interpreting-Probabilities
:END:

Let us take a look at the coin-toss experiment more closely. What do we mean when we say ``the probability of Heads'' or write \(\mathbb{P}(\mbox{Heads})\)? Given a coin and an itchy thumb, how do we go about finding what \(\mathbb{P}(\mbox{Heads})\) should be?

*** The Measure Theory Approach

This approach states that the way to handle \(\mathbb{P}(\mbox{Heads})\) is to define a mathematical function, called a /probability measure/, on the sample space. Probability measures satisfy certain axioms (to be introduced later) and have special mathematical properties, so not just any mathematical function will do. But in any given physical circumstance there are typically all sorts of probability measures from which to choose, and it is left to the experimenter to make a reasonable choice -- one usually based on considerations of objectivity. For the tossing coin example, a valid probability measure assigns probability \(p\) to the event \( \{ \mbox{Heads} \} \), where \(p\) is some number \(0\leq p\leq1\). An experimenter that wishes to incorporate the symmetry of the coin would choose \(p=1/2\) to balance the likelihood of \( \{\mbox{Heads} \} \) and \( \{ \mbox{Tails} \} \).

Once the probability measure is chosen (or determined), there is not much left to do. All assignments of probability are made by the probability function, and the experimenter needs only to plug the event \(\{ \mbox{Heads} \}\) into to the probability function to find \(\mathbb{P}(\mbox{Heads})\). In this way, the probability of an event is simply a calculated value, nothing more, nothing less. Of course this is not the whole story; there are many theorems and consequences associated with this approach that will keep us occupied for the remainder of this book. The approach is called /measure theory/ because the measure (probability) of a set (event) is associated with how big it is (how likely it is to occur).

The measure theory approach is well suited for situations where there is symmetry to the experiment, such as flipping a balanced coin or spinning an arrow around a circle with well-defined pie slices. It is also handy because of its mathematical simplicity, elegance, and flexibility. There are literally volumes of information that one can prove about probability measures, and the cold rules of mathematics allow us to analyze intricate probabilistic problems with vigor. 

The large degree of flexibility is also a disadvantage, however. When symmetry fails it is not always obvious what an ``objective'' choice of probability measure should be; for instance, what probability should we assign to \( \{ \mbox{Heads} \} \) if we spin the coin rather than flip it? (It is not \(1/2\).) Furthermore, the mathematical rules are restrictive when we wish to incorporate subjective knowledge into the model, knowledge which changes over time and depends on the experimenter, such as personal knowledge about the properties of the specific coin being flipped, or of the person doing the flipping.

The mathematician who revolutionized this way to do probability theory was Andrey Kolmogorov, who published a landmark monograph in 1933. See [[http://www-history.mcs.st-andrews.ac.uk/Mathematicians/Kolmogorov.html][here]] for more information.

*** Relative Frequency Approach

This approach states that the way to determine \(\mathbb{P}(\mbox{Heads})\) is to flip the coin repeatedly, in exactly the same way each time. Keep a tally of the number of flips and the number of Heads observed. Then a good approximation to \(\mathbb{P}(\mbox{Heads})\) will be
\begin{equation} 
\mathbb{P}(\mbox{Heads})\approx\frac{\mbox{number of observed Heads}}{\mbox{total number of flips}}.
\end{equation}
The mathematical underpinning of this approach is the celebrated *Law of Large Numbers* which may be loosely described as follows. Let \(E\) be a random experiment in which the event \(A\) either does or does not occur. Perform the experiment repeatedly, in an identical manner, in such a way that the successive experiments do not influence each other. After each experiment, keep a running tally of whether or not the event \(A\) occurred. Let \(S_{n}\) count the number of times that \(A\) occurred in the \(n\) experiments. Then the law of large numbers says that 
\begin{equation}
\frac{S_{n}}{n}\to\mathbb{P}(A)\mbox{ as }n\to\infty.
\end{equation}
As the reasoning goes, to learn about the probability of an event \(A\) we need only repeat the random experiment to get a reasonable estimate of the probability's value, and if we are not satisfied with our estimate then we may simply repeat the experiment more times all the while confident that with more and more experiments our estimate will stabilize to the true value. 

The frequentist approach is good because it is relatively light on assumptions and does not worry about symmetry or claims of objectivity like the measure-theoretic approach does. It is perfect for the spinning coin experiment. One drawback to the method is that one can never know the exact value of a probability, only a long-run approximation. It also does not work well with experiments that can not be repeated indefinitely, say, the probability that it will rain today, the chances that you get will get an A in your Statistics class, or the probability that the world is destroyed by nuclear war.

This approach was espoused by Richard von Mises in the early twentieth century, and some of his main ideas were incorporated into the measure theory approach. See [[http://www-history.mcs.st-andrews.ac.uk/Biographies/Mises.html][here]] for more.

*** The Subjective Approach

The subjective approach interprets probability as the experimenter's /degree of belief/ that the event will occur. The estimate of the probability of an event is based on the totality of the individual's knowledge at the time. As new information becomes available, the estimate is modified accordingly to best reflect his/her current knowledge. The method by which the probabilities are updated is commonly done with Bayes' Rule, discussed in Section [[sec-Bayes-Rule][Bayes' Rule]]. 

So for the coin toss example, a person may have \(\mathbb{P}(\mbox{Heads})=1/2\) in the absence of additional information. But perhaps the observer knows additional information about the coin or the thrower that would shift the probability in a certain direction. For instance, parlor magicians may be trained to be quite skilled at tossing coins, and some are so skilled that they may toss a fair coin and get nothing but Heads, indefinitely. I have /seen/ this. It was similarly claimed in /Bringing Down the House/ \cite{Mezrich2003} that MIT students were accomplished enough with cards to be able to cut a deck to the same location, every single time. In such cases, one clearly should use the additional information to assign \(\mathbb{P}(\mbox{Heads})\) away from the symmetry value of \(1/2\).

This approach works well in situations that cannot be repeated indefinitely, for example, to assign your probability that you will get an A in this class, the chances of a devastating nuclear war, or the likelihood that a cure for the common cold will be discovered.

The roots of subjective probability reach back a long time. See [[http://en.wikipedia.org/wiki/Subjective_probability][here]] for a short discussion and links to references about the subjective approach.

*** Equally Likely Model (ELM)

We have seen several approaches to the assignment of a probability model to a given random experiment and they are very different in their underlying interpretation. But they all cross paths when it comes to the equally likely model which assigns equal probability to all elementary outcomes of the experiment.

The ELM appears in the measure theory approach when the experiment boasts symmetry of some kind. If symmetry guarantees that all outcomes have equal ``size'', and if outcomes with equal ``size'' should get the same probability, then the ELM is a logical objective choice for the experimenter. Consider the balanced 6-sided die, the fair coin, or the dart board with equal-sized wedges.

The ELM appears in the subjective approach when the experimenter resorts to indifference or ignorance with respect to his/her knowledge of the outcome of the experiment. If the experimenter has no prior knowledge to suggest that (s)he prefer Heads over Tails, then it is reasonable for the him/her to assign equal subjective probability to both possible outcomes.

The ELM appears in the relative frequency approach as a fascinating fact of Nature: when we flip balanced coins over and over again, we observe that the proportion of times that the coin comes up Heads tends to \(1/2\). Of course if we assume that the measure theory applies then we can prove that the sample proportion must tend to 1/2 as expected, but that is putting the cart before the horse, in a manner of speaking.

The ELM is only available when there are finitely many elements in the sample space.

**** How to do it with \(\mathsf{R}\)

In the =prob= package \cite{prob}, a probability space is an object of outcomes =S= and a vector of probabilities (called =probs=) with entries that correspond to each outcome in =S=. When =S= is a data frame, we may simply add a column called =probs= to =S= and we will be finished; the probability space will simply be a data frame which we may call =S=. In the case that S is a list, we may combine the =outcomes= and =probs= into a larger list, =space=; it will have two components: =outcomes= and =probs=. The only requirements we need are for the entries of =probs= to be nonnegative and =sum(probs)= to be one.

To accomplish this in \(\mathsf{R}\), we may use the =probspace= function. The general syntax is =probspace(x, probs)=, where =x= is a sample space of outcomes and =probs= is a vector (of the same length as the number of outcomes in =x=). The specific choice of =probs= depends on the context of the problem, and some examples follow to demonstrate some of the more common choices. 

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
The Equally Likely Model asserts that every outcome of the sample space has the same probability, thus, if a sample space has \(n\) outcomes, then =probs= would be a vector of length \(n\) with identical entries \(1/n\). The quickest way to generate =probs= is with the =rep= function. We will start with the experiment of rolling a die, so that \(n=6\). We will construct the sample space, generate the =probs= vector, and put them together with =probspace=. 

#+begin_src R :exports both :results output pp   
outcomes <- rolldie(1) 
p <- rep(1/6, times = 6) 
probspace(outcomes, probs = p) 
#+end_src

The =probspace= function is designed to save us some time in the most common situations. For example, due to the especial simplicity of the sample space in this case, we could have achieved the same result with only (note the name change for the first column) 

#+begin_src R :exports both :results output pp   
probspace(1:6, probs = p) 
#+end_src 

Further, since the equally likely model plays such a fundamental role in the study of probability the =probspace= function will assume that the equally model is desired if no =probs= are specified. Thus, we get the same answer with only 

#+begin_src R :exports both :results output pp   
probspace(1:6) 
#+end_src 

And finally, since rolling dice is such a common experiment in probability classes, the =rolldie= function has an additional logical argument =makespace= that will add a column of equally likely =probs= to the generated sample space: 

#+begin_src R :exports both :results output pp   
rolldie(1, makespace = TRUE)
#+end_src

#+latex: \noindent 
or just =rolldie(1, TRUE)=. Many of the other sample space functions (=tosscoin=, =cards=, =roulette=, /etc/.) have similar =makespace= arguments. Check the documentation for details.

#+latex: \end{exampletoo}
#+html: </div>

One sample space function that does NOT have a =makespace= option is the =urnsamples= function. This was intentional. The reason is that under the varied sampling assumptions the outcomes in the respective sample spaces are NOT, in general, equally likely. It is important for the user to carefully consider the experiment to decide whether or not the outcomes are equally likely and then use =probspace= to assign the model.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-unbalanced-coin>>
*An unbalanced coin.* While the =makespace= argument to =tosscoin= is useful to represent the tossing of a /fair/ coin, it is not always appropriate. For example, suppose our coin is not perfectly balanced, for instance, maybe the \(H\) side is somewhat heavier such that the chances of a \(H\) appearing in a single toss is 0.70 instead of 0.5. We may set up the probability space with 

#+begin_src R :exports both :results output pp   
probspace(tosscoin(1), probs = c(0.70, 0.30)) 
#+end_src 

The same procedure can be used to represent an unbalanced die, roulette wheel, /etc/.

#+latex: \end{exampletoo}
#+html: </div>

*** Words of Warning

It should be mentioned that while the splendour of \(\mathsf{R}\) is uncontested,  it, like everything else, has limits both with respect to the sample/probability spaces it can manage and with respect to the finite accuracy of the representation of most numbers (see the \(\mathsf{R}\) FAQ 7.31). When playing around with probability, one may be tempted to set up a probability space for tossing 100 coins or rolling 50 dice in an attempt to answer some scintillating question. (Bear in mind: rolling a die just 9 times has a sample space with over /10 million/ outcomes.)

Alas, even if there were enough RAM to barely hold the sample space (and there were enough time to wait for it to be generated), the infinitesimal probabilities that are associated with /so many/ outcomes make it difficult for the underlying machinery to handle reliably. In some cases, special algorithms need to be called just to give something
that holds asymptotically. User beware.

** Properties of Probability
:PROPERTIES:
:CUSTOM_ID: sec-Properties-of-Probability
:END:

*** Probability Functions
:PROPERTIES:
:CUSTOM_ID: sub-Probability-Functions
:END:

A /probability function/ is a rule that associates with each event \(A\) of the sample space a single number \(\mathbb{P}(A)=p\), called the /probability of/ \(A\). Any probability function \(\mathbb{P}\) satisfies the following three Kolmogorov Axioms: 

#+begin_ax
# <<ax-prob-nonnegative>>
\(\mathbb{P}(A)\geq0\) for any event \(A\subset S\).
#+end_ax

#+begin_ax
# <<ax-total-mass-one>>
\(\mathbb{P}(S)=1\).
#+end_ax

#+begin_ax
# <<ax-countable-additivity>>
If the events \(A_{1}\), \(A_{2}\), \(A_{3}\)... are disjoint then
\begin{equation}
\mathbb{P}\left(\bigcup_{i=1}^{n}A_{i}\right)=\sum_{i=1}^{n}\mathbb{P}(A_{i})\mbox{ for every }n,
\end{equation}
and furthermore,
\begin{equation}
\mathbb{P}\left(\bigcup_{i=1}^{\infty}A_{i}\right)=\sum_{i=1}^{\infty}\mathbb{P}(A_{i}).
\end{equation}
#+end_ax

The intuition behind the axioms goes like this: first, the probability of an event should never be negative. Second, since the sample space contains all possible outcomes, its probability should be one, or 100%. The last axiom may look intimidating but it simply means that in a sequence of disjoint events (in other words, sets that do not overlap), the total probability (measure) should equal the sum of its parts. For example, the chance of rolling a 1 or a 2 on a die should be the chance of rolling a 1 plus the chance of rolling a 2.

*** Properties

For any events \(A\) and \(B\),
# <<enu-prop-prob-complement>>
1. \(\mathbb{P}(A^{c})=1-\mathbb{P}(A)\).
  #+begin_proof
  Since \(A\cup A^{c}=S\) and \(A\cap A^{c}=\emptyset\), we have
  \[
  1=\mathbb{P}(S)=\mathbb{P}(A\cup A^{c})=\mathbb{P}(A)+\mathbb{P}(A^{c}).
  \]
  #+end_proof
1. \(\mathbb{P}(\emptyset)=0\).
  #+begin_proof
  Note that \(\emptyset=S^{c}\), and use Property 1.
  #+end_proof
1. If \(A\subset B\) , then \(\mathbb{P}(A)\leq\mathbb{P}(B)\).
  #+begin_proof
  Write \(B=A\cup\left(B\cap A^{c}\right)\), and notice that \(A\cap\left(B\cap A^{c}\right)=\emptyset\); thus
  \[
  \mathbb{P}(B)=\mathbb{P}(A\cup\left(B\cap A^{c}\right))=\mathbb{P}(A)+\mathbb{P}\left(B\cap A^{c}\right)\geq\mathbb{P}(A),
  \]
  since \(\mathbb{P}\left(B\cap A^{c}\right)\ge0\). 
  #+end_proof
1. \(0\leq\mathbb{P}(A)\leq1\).
  #+begin_proof
  The left inequality is immediate from Axiom [[ax-prob-nonnegative][Probability is Nonnegative]], and the second inequality follows from Property 3 since \(A\subset S\).
  #+end_proof

1. *The General Addition Rule.*
  \begin{equation}
  \label{eq-general-addition-rule-1}
  \mathbb{P}(A\cup B)=\mathbb{P}(A)+\mathbb{P}(B)-\mathbb{P}(A\cap B).
  \end{equation}
  More generally, for events \(A_{1}\), \(A_{2}\), \(A_{3}\),..., \(A_{n}\),
  \begin{equation}
  \mathbb{P}\left(\bigcup_{i=1}^{n}A_{i}\right)=\sum_{i=1}^{n}\mathbb{P}(A_{i})-\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}\mathbb{P}(A_{i}\cap A_{j})+\cdots+(-1)^{n-1}\mathbb{P}\left(\bigcap_{i=1}^{n}A_{i}\right)
  \end{equation}
1. *The Theorem of Total Probability.* 
  Let \(B_{1}\), \(B_{2}\), ..., \(B_{n}\) be mutually exclusive and exhaustive. Then
  \begin{equation}
  \label{eq-theorem-total-probability}
  \mathbb{P}(A)=\mathbb{P}(A\cap B_{1})+\mathbb{P}(A\cap B_{2})+\cdots+\mathbb{P}(A\cap B_{n}).
  \end{equation}

*** Assigning Probabilities

A model of particular interest is the /equally likely model/. The idea is to divide the sample space \(S\) into a finite collection of elementary events \( \{ a_{1},\ a_{2}, \ldots, a_{N} \} \) that are equally likely in the sense that each \(a_{i}\) has equal chances of occurring. The probability function associated with this model must satisfy \(\mathbb{P}(S)=1\), by Axiom 2. On the other hand, it must also satisfy
\[
\mathbb{P}(S)=\mathbb{P}( \{ a_{1},\ a_{2},\ldots,a_{N} \} )=\mathbb{P}(a_{1}\cup a_{2}\cup\cdots\cup a_{N})=\sum_{i=1}^{N}\mathbb{P}(a_{i}),
\]
by Axiom 3. Since \(\mathbb{P}(a_{i})\) is the same for all \(i\), each one necessarily equals \(1/N\). 

For an event \(A\subset S\), we write \(A\) as a collection of elementary outcomes: if \( A = \{ a_{i_{1}}, a_{i_{2}}, \ldots, a_{i_{k}} \} \) then \(A\) has \(k\) elements and 
\begin{align*}
\mathbb{P}(A) & =\mathbb{P}(a_{i_{1}})+\mathbb{P}(a_{i_{2}})+\cdots+\mathbb{P}(a_{i_{k}}),\\
 & =\frac{1}{N}+\frac{1}{N}+\cdots+\frac{1}{N},\\
 & =\frac{k}{N}=\frac{\#(A)}{\#(S)}.
\end{align*}
In other words, under the equally likely model, the probability of an event \(A\) is determined by the number of elementary events that \(A\) contains. 

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Consider the random experiment \(E\) of tossing a coin. Then the sample space is \(S=\{H,T\}\), and under the equally likely model, these two outcomes have \(\mathbb{P}(H)=\mathbb{P}(T)=1/2\). This model is taken when it is reasonable to assume that the coin is fair.
#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Suppose the experiment \(E\) consists of tossing a fair coin twice. The sample space may be represented by \(S=\{HH,\, HT,\, TH,\, TT\}\). Given that the coin is fair and that the coin is tossed in an independent and identical manner, it is reasonable to apply the equally likely model. 

What is \(\mathbb{P}(\mbox{at least 1 Head})\)? Looking at the sample space we see the elements \(HH\), \(HT\), and \(TH\) have at least one Head; thus, \(\mathbb{P}(\mbox{at least 1 Head})=3/4\). 

What is \(\mathbb{P}(\mbox{no Heads})\)? Notice that the event \(\{ \mbox{no Heads} \} = \{ \mbox{at least one Head} \} ^{c}\), which by Property [[enu-prop-prob-complement][prob-comp]] means \(\mathbb{P}(\mbox{no Heads})=1-\mathbb{P}(\mbox{at least one head})=1-3/4=1/4\). It is obvious in this simple example that the only outcome with no Heads is \(TT\), however, this complementation trick can be handy in more complicated problems.
#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-three-child-family>>
Imagine a three child family, each child being either Boy (\(B\)) or Girl (\(G\)). An example sequence of siblings would be \(BGB\). The sample space may be written
\[
S=\left\{ 
\begin{array}{cccc}
BBB, & BGB, & GBB, & GGB,\\
BBG, & BGG, & GBG, & GGG
\end{array}
\right\}.
\]
Note that for many reasons (for instance, it turns out that girls are slightly more likely to be born than boys), this sample space is /not/ equally likely. For the sake of argument, however, we will assume that the elementary outcomes each have probability \(1/8\).

What is \(\mathbb{P}(\mbox{exactly 2 Boys})\)? Inspecting the sample space reveals three outcomes with exactly two boys: \( \{ BBG,\, BGB,\, GBB \} \).  Therefore \(\mathbb{P}(\mbox{exactly 2 Boys})=3/8\). 

What is \(\mathbb{P}(\mbox{at most 2 Boys})\)? One way to solve the problem would be to count the outcomes that have 2 or less Boys, but a quicker way would be to recognize that the only way that the event \(\{ \mbox{at most 2 Boys} \}\) does /not/ occur is the event \(\{ \mbox{all Girls} \}\).

Thus
\[
\mathbb{P}(\mbox{at most 2 Boys})=1-\mathbb{P}(GGG)=1-1/8=7/8.
\]

#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Consider the experiment of rolling a six-sided die, and let the outcome be the face showing up when the die comes to rest. Then \( S = \{ 1,\,2,\,3,\,4,\,5,\,6 \} \). It is usually reasonable to suppose that the die is fair, so that the six outcomes are equally likely.
#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Consider a standard deck of 52 cards. These are usually labeled with the four /suits/: Clubs, Diamonds, Hearts, and Spades, and the 13 /ranks/: 2, 3, 4, ..., 10, Jack (J), Queen (Q), King (K), and Ace (A). Depending on the game played, the Ace may be ranked below 2 or above King. 

Let the random experiment \(E\) consist of drawing exactly one card from a well-shuffled deck, and let the outcome be the face of the card. Define the events \( A = \{ \mbox{draw an Ace} \} \) and \( B = \{ \mbox{draw a Club} \} \). Bear in mind: we are only drawing one card.

Immediately we have \(\mathbb{P}(A)=4/52\) since there are four Aces in the deck; similarly, there are \(13\) Clubs which implies \(\mathbb{P}(B)=13/52\).

What is \(\mathbb{P}(A\cap B)\)? We realize that there is only one card of the 52 which is an Ace and a Club at the same time, namely, the Ace of Clubs. Therefore \(\mathbb{P}(A\cap B)=1/52\).

To find \(\mathbb{P}(A\cup B)\) we may use the above with the General Addition Rule to get
\begin{eqnarray*}
\mathbb{P}(A\cup B) & = & \mathbb{P}(A)+\mathbb{P}(B)-\mathbb{P}(A\cap B),\\
 & = & 4/52+13/52-1/52,\\
 & = & 16/52.
\end{eqnarray*}

#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Staying with the deck of cards, let another random experiment be the selection of a five card stud poker hand, where ``five card stud'' means that we draw exactly five cards from the deck without replacement, no more, and no less. It turns out that the sample space \(S\) is so large and complicated that we will be obliged to settle for the trivial description \( S = \{ \mbox{all possible 5 card hands} \} \) for the time being. We will have a more precise description later.

What is \(\mathbb{P}(\mbox{Royal Flush})\), or in other words, \(\mathbb{P}(\mbox{A, K, Q, J, 10 all in the same suit})\)? 

It should be clear that there are only four possible royal flushes. Thus, if we could only count the number of outcomes in \(S\) then we could simply divide four by that number and we would have our answer under the equally likely model. This is the subject of Section [[sec-Methods-of-Counting][Counting Methods]].

#+latex: \end{exampletoo}
#+html: </div>

**** How to do it with \(\mathsf{R}\)

Probabilities are calculated in the =prob= package \cite{prob} with the =prob= function.

Consider the experiment of drawing a card from a standard deck of playing cards. Let's denote the probability space associated with the experiment as =S=, and let the subsets =A= and =B= be defined by the following: 

#+begin_src R :exports code :results silent
S <- cards(makespace = TRUE) 
A <- subset(S, suit == "Heart") 
B <- subset(S, rank %in% 7:9)
#+end_src 

Now it is easy to calculate 

#+begin_src R :exports both :results output pp   
prob::prob(A) 
#+end_src 

Note that we can get the same answer with 

#+begin_src R :exports both :results output pp   
prob::prob(S, suit == "Heart") 
#+end_src 

We also find =prob::prob(B) = 0.23= (listed here approximately, but 12/52 actually) and =prob::prob(S) = 1=. Internally, the =prob= function operates by summing the =probs= column of its argument. It will find subsets on-the-fly if desired.

We have as yet glossed over the details. More specifically, =prob= has three arguments: =x=, which is a probability space (or a subset of one), =event=, which is a logical expression used to define a subset, and =given=, which is described in Section [[sec-Conditional-Probability][Conditional Probability]].

/WARNING/. The =event= argument is used to define a subset of =x=, that is, the only outcomes used in the probability calculation will be those that are elements of =x= and satisfy =event= simultaneously. In other words, =prob::prob(x, event)= calculates 

: prob::prob(intersect(x, subset(x, event)))

Consequently, =x= should be the entire probability space in the case that =event= is non-null.

** Counting Methods
:PROPERTIES:
:CUSTOM_ID: sec-Methods-of-Counting
:END:

The equally-likely model is a convenient and popular way to analyze random experiments. And when the equally likely model applies, finding the probability of an event \(A\) amounts to nothing more than counting the number of outcomes that \(A\) contains (together with the number of events in \(S\)). Hence, to be a master of probability one must be skilled at counting outcomes in events of all kinds.

#+begin_prop
The Multiplication Principle. Suppose that an experiment is composed of two successive steps. Further suppose that the first step may be performed in \(n_{1}\) distinct ways while the second step may be performed in \(n_{2}\) distinct ways. Then the experiment may be performed in \(n_{1}n_{2}\) distinct ways.

More generally, if the experiment is composed of \(k\) successive steps which may be performed in \(n_{1}\), \(n_{2}\), ..., \(n_{k}\) distinct ways, respectively, then the experiment may be performed in \(n_{1}n_{2}\cdots n_{k}\) distinct ways.
#+end_prop

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
We would like to order a pizza. It will be sure to have cheese (and marinara sauce), but we may elect to add one or more of the following five (5) available toppings:
\[
\mbox{pepperoni, sausage, anchovies, olives, and green peppers.}
\]
How many distinct pizzas are possible?

There are many ways to approach the problem, but the quickest avenue employs the Multiplication Principle directly. We will separate the action of ordering the pizza into a series of stages. At the first stage, we will decide whether or not to include pepperoni on the pizza (two possibilities). At the next stage, we will decide whether or not to include sausage on the pizza (again, two possibilities). We will continue in this fashion until at last we will decide whether or not to include green peppers on the pizza.

At each stage we will have had two options, or ways, to select a pizza to be made. The Multiplication Principle says that we should multiply the 2's to find the total number of possible pizzas: \(2\cdot2\cdot2\cdot2\cdot2=2^{5}=32\).

#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
We would like to buy a desktop computer to study statistics. We go to a website to build our computer our way. Given a line of products we have many options to customize our computer. In particular, there are 2 choices for a processor, 3 different operating systems, 4 levels of memory, 4 hard drives of differing sizes, and 10 choices for a monitor. How many possible types of computer must the company be prepared to build? *Answer:* \(2\cdot3\cdot4\cdot4\cdot10=960\)
#+latex: \end{exampletoo}
#+html: </div>

*** Ordered Samples

Imagine a bag with \(n\) distinguishable balls inside. Now shake up the bag and select \(k\) balls at random. How many possible sequences might we observe?

#+begin_prop
The number of ways in which one may select an ordered sample of \(k\) subjects from a population that has \(n\) distinguishable members is
- \(n^{k}\) if sampling is done with replacement,
- \(n(n-1)(n-2)\cdots(n-k+1)\) if sampling is done without replacement.
#+end_prop

Recall from calculus the notation for /factorials/: 
\begin{eqnarray*}
1! & = & 1,\\
2! & = & 2\cdot1=2,\\
3! & = & 3\cdot2\cdot1=6,\\
 & \vdots\\
n! & = & n(n-1)(n-2)\cdots3\cdot2\cdot1.
\end{eqnarray*}

#+begin_fact
The number of permutations of \(n\) elements is \(n!\).
#+end_fact

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Take a coin and flip it 7 times. How many sequences of Heads and Tails are possible? *Answer:* \(2^{7}=128\).
#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
In a class of 20 students, we randomly select a class president, a class vice-president, and a treasurer. How many ways can this be done? *Answer:* \(20\cdot19\cdot18=6840\).
#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
We rent five movies to watch over the span of two nights. We wish to watch 3 movies on the first night. How many distinct sequences of 3 movies could we possibly watch? *Answer:* \(5\cdot4\cdot3=60\).
#+latex: \end{exampletoo}
#+html: </div>

*** Unordered Samples

#+begin_prop
The number of ways in which one may select an unordered sample of \(k\) subjects from a population that has \(n\) distinguishable members is
- \((n-1+k)!/[(n-1)!k!]\) if sampling is done with replacement,
- \(n!/[k!(n-k)!]\) if sampling is done without replacement.
#+end_prop

The quantity \(n!/[k!(n-k)!]\) is called a /binomial coefficient/ and plays a special role in mathematics; it is denoted
\begin{equation}
\label{eq-binomial-coefficient}
{n \choose k}=\frac{n!}{k!(n-k)!}
\end{equation}
and is read ``\(n\) choose \(k\)''.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
You rent five movies to watch over the span of two nights, but only wish to watch 3 movies the first night. Your friend, Fred, wishes to borrow some movies to watch at his house on the first night. You owe Fred a favor, and allow him to select 2 movies from the set of 5. How many choices does Fred have? *Answer:* \({5 \choose 2}=10\).
#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Place 3 six-sided dice into a cup. Next, shake the cup well and pour out the dice. How many distinct rolls are possible? *Answer:* \((6-1+3)!/[(6-1)!3!]={8 \choose 5}=56\). 
#+latex: \end{exampletoo}
#+html: </div>

**** How to do it with \(\mathsf{R}\)

The factorial \(n!\) is computed with the command =factorial(n)= and the binomial coefficient \({n \choose k}\) with the command =choose(n,k)=.

The sample spaces we have computed so far have been relatively small, and we can visually study them without much trouble. However, it is /very/ easy to generate sample spaces that are prohibitively large. And while \(\mathsf{R}\) is wonderful and powerful and does almost everything except wash windows, even \(\mathsf{R}\) has limits of which we should be mindful.

But we often do not need to actually generate the sample space; it suffices to count the number of outcomes. The =nsamp= function will calculate the number of rows in a sample space made by =urnsamples= without actually devoting the memory resources necessary to generate the space. The arguments are =n=, the number of (distinguishable) objects in the urn, =k=, the sample size, and =replace=, =ordered=, as above.


#+CAPTION: Sampling \(k\) from \(n\) objects with \texttt{urnsamples}
#+LABEL: tab-Sampling-k-from-n
|                   | =ordered = TRUE=    | =ordered = FALSE=           |
|-------------------+---------------------+-----------------------------|
| =replace = TRUE=  | \(n^{k}\)           | \((n-1+k)! / [(n-1)!k!]\)   |
| =replace = FALSE= | \( n! / (n-k)! \)   | \( {n \choose k} \)         |
|-------------------+---------------------+-----------------------------|


#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
We will compute the number of outcomes for each of the four =urnsamples= examples that we saw in Example [[exa-sample-urn-two-from-three][Sample-urn-2-from-3]]. Recall that we took a sample of size two from an urn with three distinguishable elements.
#+latex: \end{exampletoo}
#+html: </div>

#+begin_src R :exports both :results output pp   
nsamp(n=3, k=2, replace = TRUE, ordered = TRUE) 
nsamp(n=3, k=2, replace = FALSE, ordered = TRUE) 
nsamp(n=3, k=2, replace = FALSE, ordered = FALSE) 
nsamp(n=3, k=2, replace = TRUE, ordered = FALSE) 
#+end_src 

Compare these answers with the length of the data frames generated above.

**** The Multiplication Principle

A benefit of =nsamp= is that it is /vectorized/ so that entering vectors instead of numbers for =n=, =k=, =replace=, and =ordered= results in a vector of corresponding answers. This becomes particularly convenient for combinatorics problems.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
There are 11 artists who each submit a portfolio containing 7 paintings for competition in an art exhibition. Unfortunately, the gallery director only has space in the winners' section to accommodate 12 paintings in a row equally spread over three consecutive walls. The director decides to give the first, second, and third place winners each a wall to display the work of their choice. The walls boast 31 separate lighting options apiece. How many displays are possible?

*Answer:* The judges will pick 3 (ranked) winners out of 11 (with =rep = FALSE=, =ord = TRUE=). Each artist will select 4 of his/her paintings from 7 for display in a row (=rep = FALSE=, =ord = TRUE=), and lastly, each of the 3 walls has 31 lighting possibilities (=rep = TRUE=, =ord = TRUE=). These three numbers can be calculated quickly with 

#+begin_src R :exports code :results silent
n <- c(11,7,31) 
k <- c(3,4,3) 
r <- c(FALSE,FALSE,TRUE) 
#+end_src 

#+begin_src R :exports code :results silent
x <- nsamp(n, k, rep = r, ord = TRUE) 
#+end_src 

(Notice that =ordered= is always =TRUE=; =nsamp= will recycle =ordered= and =replace= to the appropriate length.) By the Multiplication Principle, the number of ways to complete the experiment is the product of the entries of =x=: 

#+begin_src R :exports both :results output pp   
prod(x) 
#+end_src 

Compare this with the some other ways to compute the same thing: 

#+begin_src R :exports both :results output pp   
(11*10*9)*(7*6*5*4)*313 
#+end_src

or alternatively 

#+begin_src R :exports both :results output pp   
prod(9:11)*prod(4:7)*313 
#+end_src 

or even 

#+begin_src R :exports both :results output pp   
prod(factorial(c(11,7))/factorial(c(8,3)))*313 
#+end_src 

#+latex: \end{exampletoo}
#+html: </div>

As one can guess, in many of the standard counting problems there aren't substantial savings in the amount of typing; it is about the same using =nsamp= versus =factorial= and =choose=. But the virtue of =nsamp= lies in its collecting the relevant counting formulas in a one-stop shop. Ultimately, it is up to the user to choose the method that works best for him/herself. 

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
*The Birthday Problem.* Suppose that there are \(n\) people together in a room. Each person announces the date of his/her birthday in turn. The question is: what is the probability of at least one match? If we let the event \(A\) represent 
\[
\{ \mbox{there is at least one match}, \}
\]
then would like to know \(\mathbb{P}(A)\), but as we will see, it is more convenient to calculate \(\mathbb{P}(A^{c})\).

For starters we will ignore leap years and assume that there are only 365 days in a year. Second, we will assume that births are equally distributed over the course of a year (which is not true due to all sorts of complications such as hospital delivery schedules). See [[http://en.wikipedia.org/wiki/Birthday_problem][here]] for more.

Let us next think about the sample space. There are 365 possibilities for the first person's birthday, 365 possibilities for the second, and so forth. The total number of possible birthday sequences is therefore \(\#(S)=365^{n}\).

Now we will use the complementation trick we saw in Example [[exa-three-child-family][Three Child Family]]. We realize that the only situation in which \(A\) does /not/ occur is if there are /no/ matches among all people in the room, that is, only when everybody's birthday is different, so
\[
\mathbb{P}(A)=1-\mathbb{P}(A^{c})=1-\frac{\#(A^{c})}{\#(S)},
\]
since the outcomes are equally likely. Let us then suppose that there are no matches. The first person has one of 365 possible birthdays. The second person must not match the first, thus, the second person has only 364 available birthdays from which to choose. Similarly, the third person has only 363 possible birthdays, and so forth, until we reach the \(n^{\mathrm{th}}\) person, who has only \(365-n+1\) remaining possible days for a birthday. By the Multiplication Principle, we have \(\#(A^{c})=365\cdot364\cdots(365-n+1)\), and
\begin{equation}
\mathbb{P}(A)=1-\frac{365\cdot364\cdots(365-n+1)}{365^{n}}=1-\frac{364}{365}\cdot\frac{363}{365}\cdots\frac{(365-n+1)}{365}.
\end{equation}
As a surprising consequence, consider this: how many people does it take to be in the room so that the probability of at least one match is at least 0.50? Clearly, if there is only \(n=1\) person in the room then the probability of a match is zero, and when there are \(n=366\) people in the room there is a 100% chance of a match (recall that we are ignoring leap years). So how many people does it take so that there is an equal chance of a match and no match?

When I have asked this question to students, the usual response is ``somewhere around \(n=180\) people'' in the room. The reasoning seems to be that in order to get a 50% chance of a match, there should be 50% of the available days to be occupied. The number of students in a typical classroom is 25, so as a companion question I ask students to estimate the probability of a match when there are \(n=25\) students in the room. Common estimates are a 1%, or 0.5%, or even 0.1% chance of a match. After they have given their estimates, we go around the room and each student announces their birthday. More often than not, we observe a match in the class, to the students' disbelief.

Students are usually surprised to hear that, using the formula above, one needs only \(n=23\) students to have a greater than 50% chance of at least one match. Figure [[fig-birthday][birthday]] shows a graph of the birthday probabilities:
#+latex: \end{exampletoo}
#+html: </div>

#+name: birthday
#+begin_src R :exports none :results silent
g <- Vectorize(pbirthday.ipsur)
x <- 1:50; y <- g(1:50)
qplot(x, y) + geom_hline(yintercept=0.5) +
  geom_vline(xintercept = 23, linetype = 2) +
  xlab("number of people in room") +
  ylab("Prob(at least one match)")
# plot(1:50, g(1:50), xlab = "Number of people in room", ylab = "Prob(at least one match)" )
remove(g)
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/prob/birthday.ps
  <<birthday>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/prob/birthday.svg
  <<birthday>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/prob/birthday.ps}
  \caption[The birthday problem]{\small The birthday problem. The horizontal line is at \(p=0.50\) and the vertical line is at \(n=23\).}
  \label{fig-birthday}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-birthday" class="figure">
  <p><img src="svg/prob/birthday.svg" width=500 alt="svg/prob/birthday.svg" /></p>
  <p>The birthday problem. The horizontal line is at <code>p = 0.50</code> and the vertical line is at <code>n = 23</code>.</p>
</div>
#+end_html

**** How to do it with \(\mathsf{R}\)

We can make the plot in Figure [[fig-birthday][birthday]] with the following sequence of commands.

#+begin_src R :exports code :eval never
library(RcmdrPlugin.IPSUR)
g <- Vectorize(pbirthday.ipsur)
plot(1:50, g(1:50), xlab = "Number of people in room", 
  ylab = "Prob(at least one match)" )
abline(h = 0.5)
abline(v = 23, lty = 2)
remove(g)
#+end_src

There is a =Birthday problem= item in the =Probability= menu of =RcmdrPlugin.IPSUR=. In the base \(\mathsf{R}\) version, one can compute approximate probabilities for the more general case of probabilities other than 1/2, for differing total number of days in the year, and even for more than two matches.

** Conditional Probability
:PROPERTIES:
:CUSTOM_ID: sec-Conditional-Probability
:END:

Consider a full deck of 52 standard playing cards. Now select two cards from the deck, in succession. Let \( A = \{ \mbox{first card drawn is an Ace} \} \) and \( B = \{ \mbox{second card drawn is an Ace} \} \). Since there are four Aces in the deck, it is natural to assign \( \mathbb{P}(A) = 4/52 \). Suppose we look at the first card. What now is the probability of \(B\)? Of course, the answer depends on the value of the first card. If the first card is an Ace, then the probability that the second also is an Ace should be \( 3/51 \), but if the first card is not an Ace, then the probability that the second is an Ace should be \( 4/51 \). As notation for these two situations we write
\[
\mathbb{P}(B|A)=3/51,\quad \mathbb{P}(B|A^{c})=4/51.
\]

#+begin_defn
The conditional probability of \(B\) given \(A\), denoted \(\mathbb{P}(B|A)\), is defined by
\begin{equation}
\mathbb{P}(B|A)=\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(A)},\quad \mbox{if }\mathbb{P}(A)>0.
\end{equation}
We will not be discussing a conditional probability of \(B\) given \(A\) when \(\mathbb{P}(A)=0\), even though this theory exists, is well developed, and forms the foundation for the study of stochastic processes
#+latex: \footnote{Conditional probability in this case is defined by means of \emph{conditional expectation}, a topic that is well beyond the scope of this text. The interested reader should consult an advanced text on probability theory, such as Billingsley, Resnick, or Ash Dooleans-Dade.}.
#+end_defn

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Toss a coin twice. The sample space is given by \(S=\{ HH,\ HT,\ TH,\ TT \} \). Let \(A= \{ \mbox{a head occurs} \} \) and \(B= \{ \mbox{a head and tail occur} \} \). It should be clear that \(\mathbb{P}(A)=3/4\), \(\mathbb{P}(B)=2/4\), and \(\mathbb{P}(A\cap B)=2/4\). What now are the probabilities \(\mathbb{P}(A|B)\) and \(\mathbb{P}(B|A)\)?
\[
\mathbb{P}(A|B)=\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}=\frac{2/4}{2/4}=1,
\]
in other words, once we know that a Head and Tail occur, we may be certain that a Head occurs. Next
\[
\mathbb{P}(B|A)=\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(A)}=\frac{2/4}{3/4}=\frac{2}{3},
\]
which means that given the information that a Head has occurred, we no longer need to account for the outcome \(TT\), and the remaining three outcomes are equally likely with exactly two outcomes lying in the set \(B\). 
#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-Toss-a-six-sided-die-twice>>
Toss a six-sided die twice. The sample space consists of all ordered pairs \((i,j)\) of the numbers \(1,2,\ldots,6\), that is, \( S = \{ (1,1),\ (1,2),\ldots,(6,6) \} \). We know from Section [[sec-Methods-of-Counting][Counting Methods]] that \( \# (S) = 6^{2} = 36 \). Let \( A = \{ \mbox{outcomes match} \} \) and \( B = \{ \mbox{sum of outcomes at least 8} \} \). The sample space may be represented by a matrix:


#+name: twodiceAB
#+begin_src R :exports none :results silent
A <- rolldie(2)
B <- subset(A, X1==X2)
C <- subset(A, X1+X2 > 7)
B$lab <- rep("X", dim(B)[1])
C$lab <- rep("O", dim(C)[1])
p <- ggplot(rbind(B, C), aes(x=X1, y=X2, label=lab))
p + geom_text(size = 15) + xlab("First roll") + ylab("Second roll")
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/prob/twodiceAB.ps
  <<twodiceAB>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/prob/twodiceAB.svg
  <<twodiceAB>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/prob/twodiceAB.ps}
  \caption[Rolling two dice]{\small Rolling two dice. The outcomes in A are marked with X, the outcomes in B are marked with O.}
  \label{fig-twodiceAB}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-twodiceAB" class="figure">
  <p><img src="svg/prob/twodiceAB.svg" width=500 alt="svg/prob/twodiceAB.svg" /></p>
  <p>Rolling two dice. The outcomes in A are marked with X, the outcomes in B are marked with O.</p>
</div>
#+end_html

The outcomes lying in the event \(A\) are marked with the symbol ``X'', the outcomes falling in \(B\) are marked with ``O'', and the outcomes in \(A\cap B\) are those where the letters overlap. Now it is clear that \(\mathbb{P}(A)=6/36\), \(\mathbb{P}(B)=15/36\), and \(\mathbb{P}(A\cap B)=3/36\).  Finally, 
\[
\mathbb{P}(A|B)=\frac{3/36}{15/36}=\frac{1}{5},\quad \mathbb{P}(B|A)=\frac{3/36}{6/36}=\frac{1}{2}.
\]
Again, we see that given the knowledge that \(B\) occurred (the 15 outcomes in the upper right triangle), there are 3 of the 15 that fall into the set \(A\), thus the probability is \(3/15\). Similarly, given that \(A\) occurred (we are on the diagonal), there are 3 out of 6 outcomes that also fall in \(B\), thus, the probability of \(B\) given \(A\) is 1/2. 
#+latex: \end{exampletoo}
#+html: </div>

*** How to do it with \(\mathsf{R}\)

Continuing with Example [[exa-Toss-a-six-sided-die-twice][Toss die twice]], the first thing to do is set up the probability space with the =rolldie= function.

#+begin_src R :exports both :results output pp  
S <- rolldie(2, makespace = TRUE)  # assumes ELM
head(S)                            #  first few rows
#+end_src

Next we define the events

#+begin_src R :exports code :results silent
A <- subset(S, X1 == X2)
B <- subset(S, X1 + X2 >= 8)
#+end_src

And now we are ready to calculate probabilities. To do conditional probability, we use the =given= argument of the =prob= function:

#+begin_src R :exports both :results output pp  
prob::prob(A, given = B)
prob::prob(B, given = A)
#+end_src

Note that we do not actually need to define the events \(A\) and \(B\) separately as long as we reference the original probability space \(S\) as the first argument of the =prob= calculation:

#+begin_src R :exports both :results output pp  
prob::prob(S, X1==X2, given = (X1 + X2 >= 8) )
prob::prob(S, X1+X2 >= 8, given = (X1==X2) )
#+end_src

*** Properties and Rules

The following theorem establishes that conditional probabilities behave just like regular probabilities when the conditioned event is fixed. 

#+begin_thm
For any fixed event \(A\) with \(\mathbb{P}(A)>0\),
1. \( \mathbb{P} (B|A)\geq 0 \), for all events \( B \subset S\),
1. \( \mathbb{P} (S|A) = 1 \), and
1. If \(B_{1}\), \(B_{2}\), \(B_{3}\),... are disjoint events, then
  \begin{equation}
  \mathbb{P}\left(\left.\bigcup_{k=1}^{\infty}B_{k}\:\right|A\right)=\sum_{k=1}^{\infty}\mathbb{P}(B_{k}|A).
  \end{equation}
#+end_thm
In other words, \(\mathbb{P}(\cdot|A)\) is a legitimate probability function. With this fact in mind, the following properties are immediate:

#+begin_prop
For any events \(A\), \(B\), and \(C\) with \(\mathbb{P}(A)>0\),
1. \( \mathbb{P} ( B^{c} | A ) = 1 - \mathbb{P} (B|A).\)
1. If \(B\subset C\) then \(\mathbb{P}(B|A)\leq\mathbb{P}(C|A)\).
1. \( \mathbb{P} [ ( B\cup C ) | A ] = \mathbb{P} (B|A) + \mathbb{P}(C|A) - \mathbb{P} [ (B \cap C|A) ].\)
1. *The Multiplication Rule.* For any two events \(A\) and \(B\),
  \begin{equation}
  \label{eq-multiplication-rule-short}
  \mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B|A).
  \end{equation}
  And more generally, for events \(A_{1}\), \(A_{2}\), \(A_{3}\),..., \(A_{n}\),
  \begin{equation}
  \label{eq-multiplication-rule-long}
  \mathbb{P}(A_{1}\cap A_{2}\cap\cdots\cap A_{n})=\mathbb{P}(A_{1})\mathbb{P}(A_{2}|A_{1})\cdots\mathbb{P}(A_{n}|A_{1}\cap A_{2}\cap\cdots\cap A_{n-1}).
  \end{equation}
#+end_prop
The Multiplication Rule is very important because it allows us to find probabilities in random experiments that have a sequential structure, as the next example shows. 

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-two-cards-both-aces>>
At the beginning of the section we drew two cards from a standard playing deck. Now we may answer our original question, what is \(\mathbb{P}(\mbox{both Aces})\)?
\[
\mathbb{P}(\mbox{both Aces})=\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B|A)=\frac{4}{52}\cdot\frac{3}{51}\approx0.00452.
\]
#+latex: \end{exampletoo}
#+html: </div>

**** How to do it with \(\mathsf{R}\)
:PROPERTIES:
:CUSTOM_ID: sub-howto-ps-objects
:END:

Continuing Example [[exa-two-cards-both-aces][Both Aces]], we set up the probability space by way of a three step process. First we employ the =cards= function to get a data frame =L= with two columns: =rank= and =suit=. Both columns are stored internally as factors with 13 and 4 levels, respectively.

Next we sample two cards randomly from the =L= data frame by way of the =urnsamples= function. It returns a list =M= which contains all possible pairs of rows from =L= (there are =choose(52,2)= of them). The sample space for this experiment is exactly the list =M=.

At long last we associate a probability model with the sample space. This is right down the =probspace= function's alley. It assumes the equally likely model by default. We call this result =N= which is an object of class =ps= -- short for ``probability space''.

But do not be intimidated. The object =N= is nothing more than a list with two elements: =outcomes= and =probs=. The =outcomes= element is itself just another list, with =choose(52,2)= entries, each one a data frame with two rows which correspond to the pair of cards chosen. The =probs= element is just a vector with =choose(52,2)= entries all the same: =1/choose(52,2)=.

Putting all of this together we do 

#+begin_src R :exports code :results silent
L <- cards()
M <- urnsamples(L, size = 2)
N <- probspace(M)
#+end_src

Now that we have the probability space =N= we are ready to do some probability. We use the =prob= function, just like before. The only trick is to specify the event of interest correctly, and recall that we were interested in \(\mathbb{P}(\mbox{both Aces})\). But if the cards are both Aces then the =rank= of both cards should be =A=, which sounds like a job for the =all= function:

#+begin_src R :exports both :results output pp  
prob::prob(N, all(rank == "A"))
#+end_src

Note that this value matches what we found in Example [[exa-two-cards-both-aces][Both Aces]], above. We could calculate all sorts of probabilities at this point; we are limited only by the complexity of the event's computer representation. 


#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-urn-7-red-3-green>>
Consider an urn with 10 balls inside, 7 of which are red and 3 of which are green. Select 3 balls successively from the urn. Let \( A = \{ 1^{\mathrm{st}} \mbox{ ball is red} \} \), \( B = \{ 2^{\mathrm{nd}} \mbox{ ball is red} \} \), and \( C = \{ 3^{\mathrm{rd}} \mbox{ ball is red} \} \). Then
\[
\mathbb{P}(\mbox{all 3 balls are red})=\mathbb{P}(A\cap B\cap C)=\frac{7}{10}\cdot\frac{6}{9}\cdot\frac{5}{8}\approx 0.2917.
\]

#+latex: \end{exampletoo}
#+html: </div>

**** How to do it with \(\mathsf{R}\)

Example [[exa-urn-7-red-3-green][Urn 7 red 3 green]] is similar to Example [[exa-two-cards-both-aces][Both Aces]], but it is even easier. We need to set up an urn (vector =L=) to hold the balls, we sample from =L= to get the sample space (data frame =M=), and we associate a probability vector (column =probs=) with the outcomes (rows of =M=) of the sample space. The final result is a probability space (an ordinary data frame =N=).

It is easier for us this time because our urn is a vector instead of a =cards()= data frame. Before there were two dimensions of information associated with the outcomes (rank and suit) but presently we have only one dimension (color).

#+begin_src R :exports code :results silent
L <- rep(c("red","green"), times = c(7,3))
M <- urnsamples(L, size = 3, replace = FALSE, ordered = TRUE)
N <- probspace(M)
#+end_src

Now let us think about how to set up the event \(\{ \mbox{all 3 balls are red}\} \). Rows of =N= that satisfy this condition have \(\mathtt{X1=="red"\ \&\ X2=="red"\ \&\ X3=="red"}\), but there must be an easier way. Indeed, there is. The =isrep= function (short for ``is repeated'') in the =prob= package was written for this purpose. The command =isrep(N,"red",3)= will test each row of =N= to see whether the value =red= appears =3= times. The result is exactly what we need to define an event with the =prob= function. Observe

#+begin_src R :exports both :results output pp  
prob::prob(N, isrep(N, "red", 3))
#+end_src

Note that this answer matches what we found in Example [[exa-urn-7-red-3-green][Urn 7 red 3 green]]. Now let us try some other probability questions. What is the probability of getting two =red='s?

#+begin_src R :exports both :results output pp  
prob::prob(N, isrep(N, "red", 2))
#+end_src

Note that the exact value is \(21/40\); we will learn a quick way to compute this in Section [[sec-other-discrete-distributions][Other Discrete Distributions]]. What is the probability of observing =red=, then =green=, then =red=?

#+begin_src R :exports both :results output pp  
prob::prob(N, isin(N, c("red","green","red"), ordered = TRUE))
#+end_src

Note that the exact value is \(7/20\) (do it with the Multiplication Rule). What is the probability of observing =red=, =green=, and =red=, in no particular order?

#+begin_src R :exports both :results output pp  
prob::prob(N, isin(N, c("red","green","red")))
#+end_src

We already knew this. It is the probability of observing two =red='s, above.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Consider two urns, the first with 5 red balls and 3 green balls, and the second with 2 red balls and 6 green balls. Your friend randomly selects one ball from the first urn and transfers it to the second urn, without disclosing the color of the ball. You select one ball from the second urn. What is the probability that the selected ball is red? Let \( A = \{ \mbox{transferred ball is red} \} \) and \( B = \{ \mbox{selected ball is red} \} \). Write
\begin{align*}
B & =S\cap B\\
 & =(A\cup A^{c})\cap B\\
 & =(A\cap B)\cup(A^{c}\cap B)
\end{align*}
and notice that \(A\cap B\) and \(A^{c}\cap B\) are disjoint. Therefore
\begin{align*}
\mathbb{P}(B) & =\mathbb{P}(A\cap B)+\mathbb{P}(A^{c}\cap B)\\
 & =\mathbb{P}(A)\mathbb{P}(B|A)+\mathbb{P}(A^{c})\mathbb{P}(B|A^{c})\\
 & =\frac{5}{8}\cdot\frac{3}{9}+\frac{3}{8}\cdot\frac{2}{9}\\
 & =\frac{21}{72}\ 
\end{align*}
(which is 7/24 in lowest terms).

#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
We saw the =RcmdrTestDrive= data set in Chapter [[cha-introduction-to-R][Intro to R]] in which a two-way table of the smoking status versus the gender was 

#+begin_src R :exports both :results output pp
library(RcmdrPlugin.IPSUR)
data(RcmdrTestDrive)  
.Table <- xtabs( ~ smoking + gender, data = RcmdrTestDrive)
addmargins(.Table) # Table with marginal distributions
#+end_src

If one person were selected at random from the data set, then we see from the two-way table that \(\mathbb{P}(\mbox{Female})=70/168\) and \(\mathbb{P}(\mbox{Smoker})=32/168\). Now suppose that one of the subjects quits smoking, but we do not know the person's gender. If we now select one nonsmoker at random, what would be \(\mathbb{P}(\mbox{Female})\)? This example is just like the last example, but with different labels. Let \( A = \{ \mbox{the quitter is a female} \} \) and \( B = \{ \mbox{selected nonsmoker is a female} \} \). Write
\begin{align*}
B & =S\cap B\\
 & =(A\cup A^{c})\cap B\\
 & =(A\cap B)\cup(A^{c}\cap B)
\end{align*}
and notice that \(A\cap B\) and \(A^{c}\cap B\) are disjoint. Therefore
\begin{align*}
\mathbb{P}(B) & =\mathbb{P}(A\cap B)+\mathbb{P}(A^{c}\cap B),\\
 & =\mathbb{P}(A)\mathbb{P}(B|A)+\mathbb{P}(A^{c})\mathbb{P}(B|A^{c}),\\
 & =\frac{9}{32}\cdot\frac{62}{137}+\frac{23}{32}\cdot\frac{76}{137},\\
 & =\frac{2306}{4384},
\end{align*}
(which is 1153/2192 in lowest terms).

#+latex: \end{exampletoo}
#+html: </div>
Using the same reasoning, we can return to the example from the beginning of the section and show that
\[
\mathbb{P}(\{ \mbox{second card is an Ace} \} )=4/52.
\]
 
** Independent Events
:PROPERTIES:
:CUSTOM_ID: sec-Independent-Events
:END:

Toss a coin twice. The sample space is \(S= \{ HH,\ HT,\ TH,\ TT \} \). We know that \(\mathbb{P}(1^{\mathrm{st}}\mbox{ toss is }H)=2/4\), \(\mathbb{P}(2^{\mathrm{nd}}\mbox{ toss is }H)=2/4\), and \(\mathbb{P}(\mbox{both }H)=1/4\). Then
\begin{align*} \mathbb{P}(2^{\mathrm{nd}}\mbox{ toss is }H\ |\ 1^{\mathrm{st}}\mbox{ toss is }H) & =\frac{\mathbb{P}(\mbox{both }H)}{\mathbb{P}(1^{\mathrm{st}}\mbox{ toss is }H)},\\
 & =\frac{1/4}{2/4},\\
 & =\mathbb{P}(2^{\mathrm{nd}}\mbox{ toss is }H).
\end{align*}
Intuitively, this means that the information that the first toss is \(H\) has no bearing on the probability that the second toss is \(H\). The coin does not remember the result of the first toss. 

#+begin_defn
Events \(A\) and \(B\) are said to be /independent/ if 
\begin{equation}
\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B).
\end{equation}
Otherwise, the events are said to be /dependent/. 
#+end_defn

The connection with the above example stems from the following. We know from Section [[sec-Conditional-Probability][Conditional Probability]] that when \(\mathbb{P}(B)>0\) we may write
\begin{equation}
\mathbb{P}(A|B)=\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}.
\end{equation}
In the case that \(A\) and \(B\) are independent, the numerator of the fraction factors so that \(\mathbb{P}(B)\) cancels with the result:
\begin{equation}
\mathbb{P}(A|B)=\mathbb{P}(A)\mbox{ when \(A\), \(B\) are independent.}
\end{equation}
The interpretation in the case of independence is that the information that the event \(B\) occurred does not influence the probability of the event \(A\) occurring. Similarly, \(\mathbb{P}(B|A)=\mathbb{P}(B)\), and so the occurrence of the event \(A\) likewise does not affect the probability of event \(B\). It may seem more natural to define \(A\) and \(B\) to be independent when \(\mathbb{P}(A|B)=\mathbb{P}(A)\); however, the conditional probability \(\mathbb{P}(A|B)\) is only defined when \(\mathbb{P}(B)>0\). Our definition is not limited by this restriction. It can be shown that when \(\mathbb{P}(A),\ \mathbb{P}(B)>0\) the two notions of independence are equivalent.

#+begin_prop
If the events \(A\) and \(B\) are independent then
- \(A\) and \(B^{c}\) are independent,
- \(A^{c}\) and \(B\) are independent,
- \(A^{c}\) and \(B^{c}\) are independent.
#+end_prop

#+begin_proof
Suppose that \(A\) and \(B\) are independent. We will show the second one; the others are similar. We need to show that
\[
\mathbb{P}(A^{c}\cap B)=\mathbb{P}(A^{c})\mathbb{P}(B).
\]
To this end, note that the Multiplication Rule, Equation [[eq-multiplication-rule-short][MR short]] implies 
\begin{eqnarray*}
\mathbb{P}(A^{c}\cap B) & = & \mathbb{P}(B)\mathbb{P}(A^{c}|B),\\
 & = & \mathbb{P}(B)[1-\mathbb{P}(A|B)],\\
 & = & \mathbb{P}(B)\mathbb{P}(A^{c}).
\end{eqnarray*}
#+end_proof

#+begin_defn
The events \(A\), \(B\), and \(C\) are /mutually independent/ if the following four conditions are met: 
\begin{eqnarray*}
\mathbb{P}(A\cap B) & = & \mathbb{P}(A)\mathbb{P}(B),\\
\mathbb{P}(A\cap C) & = & \mathbb{P}(A)\mathbb{P}(C),\\
\mathbb{P}(B\cap C) & = & \mathbb{P}(B)\mathbb{P}(C),
\end{eqnarray*}
and
\[
\mathbb{P}(A\cap B\cap C)=\mathbb{P}(A)\mathbb{P}(B)\mathbb{P}(C).
\]
If only the first three conditions hold then \(A\), \(B\), and \(C\) are said to be independent /pairwise/. Note that pairwise independence is not the same as mutual independence when the number of events is larger than two.
#+end_defn

We can now deduce the pattern for \(n\) events, \(n>3\). The events will be mutually independent only if they satisfy the product equality pairwise, then in groups of three, in groups of four, and so forth, up to all \(n\) events at once. For \(n\) events, there will be \(2^{n}-n-1\) equations that must be satisfied (see Exercise [[xca-numb-cond-indep][Number conditions indep]]). Although these requirements for a set of events to be mutually independent may seem stringent, the good news is that for most of the situations considered in this book the conditions will all be met (or at least we will suppose that they are).

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-toss-ten-coins>>
Toss ten coins. What is the probability of observing at least one Head? Answer: Let \(A_{i}= \{ \mbox{the }i^{\mathrm{th}}\mbox{ coin shows }H \} ,\ i=1,2,\ldots,10\). Supposing that we toss the coins in such a way that they do not interfere with each other, this is one of the situations where all of the \(A_{i}\) may be considered mutually independent due to the nature of the tossing. Of course, the only way that there will not be at least one Head showing is if all tosses are Tails. Therefore,
\begin{align*}
\mathbb{P}(\mbox{at least one }H) & =1-\mathbb{P}(\mbox{all }T),\\
 & =1-\mathbb{P}(A_{1}^{c}\cap A_{2}^{c}\cap\cdots\cap A_{10}^{c}),\\
 & =1-\mathbb{P}(A_{1}^{c})\mathbb{P}(A_{2}^{c})\cdots\mathbb{P}(A_{10}^{c}),\\
 & =1-\left(\frac{1}{2}\right)^{10},
\end{align*}
which is approximately \(0.9990234\).

#+latex: \end{exampletoo}
#+html: </div>

*** How to do it with \(\mathsf{R}\)

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Toss ten coins. What is the probability of observing at least one Head?

#+begin_src R :exports both :results output pp  
S <- tosscoin(10, makespace = TRUE)
A <- subset(S, isrep(S, vals = "T", nrep = 10))
1 - prob::prob(A)
#+end_src

Compare this answer to what we got in Example [[exa-toss-ten-coins][Toss 10 coins]].

#+latex: \end{exampletoo}
#+html: </div>

*** Independent, Repeated Experiments

Generalizing from above it is common to repeat a certain experiment multiple times under identical conditions and in an independent manner. We have seen many examples of this already: tossing a coin repeatedly, rolling a die or dice, /etc/.

The =iidspace= function was designed specifically for this situation. It has three arguments: =x=, which is a vector of outcomes, =ntrials=, which is an integer telling how many times to repeat the experiment, and =probs= to specify the probabilities of the outcomes of =x= in a single trial. 

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
*An unbalanced coin* (continued, see Example [[exa-unbalanced-coin][Unbalanced Coin]]). It was easy enough to set up the probability space for one unbalanced toss, however, the situation becomes more complicated when there are many tosses involved. Clearly, the outcome \(HHH\) should not have the same probability as \(TTT\), which should again not have the same probability as \(HTH\). At the same time, there is symmetry in the experiment in that the coin does not remember the face it shows from toss to toss, and it is easy enough to toss the coin in a similar way repeatedly.

We may represent tossing our unbalanced coin three times with the following: 

#+begin_src R :exports both :results output pp  
iidspace(c("H","T"), ntrials = 3, probs = c(0.7, 0.3)) 
#+end_src 

As expected, the outcome \(HHH\) has the largest probability, while \(TTT\) has the smallest. (Since the trials are independent, \(\mathbb{P}(HHH)=0.7^{3}\) and \(\mathbb{P}(TTT)=0.3^{3}\), /etc/.) Note that the result of the function call is a probability space, not a sample space (which we could construct already with the =tosscoin= or =urnsamples= functions). The same procedure could be used to model an unbalanced die or any other experiment that may be represented with a vector of possible outcomes.

#+latex: \end{exampletoo}
#+html: </div>

Note that =iidspace= will assume =x= has equally likely outcomes if no =probs= argument is specified. Also note that the argument =x= is a /vector/, not a data frame. Something like =iidspace(tosscoin(1),...)= would give an error.

** Bayes' Rule
:PROPERTIES:
:CUSTOM_ID: sec-Bayes-Rule
:END:

We mentioned the subjective view of probability in Section [[sec-Interpreting-Probabilities][Interpreting Probabilities]]. In this section we introduce a rule that allows us to update our probabilities when new information becomes available. 

#+begin_thm
*Bayes' Rule*. Let \(B_{1}\), \(B_{2}\), ..., \(B_{n}\) be mutually exclusive and exhaustive and let \(A\) be an event with \(\mathbb{P}(A)>0\). Then 
\begin{equation}
\label{eq-bayes-rule}
\mathbb{P}(B_{k}|A)=\frac{\mathbb{P}(B_{k})\mathbb{P}(A|B_{k})}{\sum_{i=1}^{n}\mathbb{P}(B_{i})\mathbb{P}(A|B_{i})},\quad k=1,2,\ldots,n.
\end{equation}
#+end_thm

#+begin_proof
The proof follows from looking at \(\mathbb{P}(B_{k}\cap A)\) in two different ways. For simplicity, suppose that \(P(B_{k})>0\) for all \(k\). Then
\[
\mathbb{P}(A)\mathbb{P}(B_{k}|A)=\mathbb{P}(B_{k}\cap A)=\mathbb{P}(B_{k})\mathbb{P}(A|B_{k}).
\]
Since \(\mathbb{P}(A)>0\) we may divide through to obtain 
\[
\mathbb{P}(B_{k}|A)=\frac{\mathbb{P}(B_{k})\mathbb{P}(A|B_{k})}{\mathbb{P}(A)}.
\]
Now remembering that \(\{ B_{k} \}\) is a partition, the Theorem of Total Probability (Equation [[eq-theorem-total-probability][TTP]]) gives the denominator of the last expression to be
\[
\mathbb{P}(A)=\sum_{k=1}^{n}\mathbb{P}(B_{k}\cap A)=\sum_{k=1}^{n}\mathbb{P}(B_{k})\mathbb{P}(A|B_{k}).
\]
#+end_proof

What does it mean? Usually in applications we are given (or know) /a priori/ probabilities \(\mathbb{P}(B_{k})\). We go out and collect some data, which we represent by the event \(A\). We want to know: how do we *update* \(\mathbb{P}(B_{k})\) to \(\mathbb{P}(B_{k}|A)\)? The answer: Bayes' Rule.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-misfiling-assistants>>
*Misfiling Assistants.* In this problem, there are three assistants working at a company: Moe, Larry, and Curly. Their primary job duty is to file paperwork in the filing cabinet when papers become available. The three assistants have different work schedules:

#+CAPTION: [Misfiling assistants: workload]{}
|          | Moe      | Larry    | Curly    |
|----------+----------+----------+----------|
| Workload | \(60\%\) | \(30\%\) | \(10\%\) |

That is, Moe works 60% of the time, Larry works 30% of the time, and Curly does the remaining 10%, and they file documents at approximately the same speed. Suppose a person were to select one of the documents from the cabinet at random. Let \(M\) be the event
\[
M= \{ \mbox{Moe filed the document} \}
\]
and let \(L\) and \(C\) be the events that Larry and Curly, respectively, filed the document. What are these events' respective probabilities? In the absence of additional information, reasonable prior probabilities would just be

#+CAPTION: [Misfiling assistants: prior]{}
|                   | Moe | Larry | Curly |
|-------------------+-----+-------+-------|
| Prior Probability | 0.6 |   0.3 |   0.1 |

Now, the boss comes in one day, opens up the file cabinet, and selects a file at random. The boss discovers that the file has been misplaced. The boss is so angry at the mistake that (s)he threatens to fire the one who erred. The question is: who misplaced the file?

The boss decides to use probability to decide, and walks straight to the workload schedule. (S)he reasons that, since the three employees work at the same speed, the probability that a randomly selected file would have been filed by each one would be proportional to his workload. The boss notifies *Moe* that he has until the end of the day to empty his desk.

But Moe argues in his defense that the boss has ignored additional information. Moe's likelihood of having misfiled a document is smaller than Larry's and Curly's, since he is a diligent worker who pays close attention to his work. Moe admits that he works longer than the others, but he doesn't make as many mistakes as they do. Thus, Moe recommends that -- before making a decision -- the boss should update the probability (initially based on workload alone) to incorporate the likelihood of having observed a misfiled document.

And, as it turns out, the boss has information about Moe, Larry, and Curly's filing accuracy in the past (due to historical performance evaluations). The performance information may be represented by the following table:

#+CAPTION: [Misfiling assistants: misfile rate]{}
|              |   Moe | Larry | Curly |
|--------------+-------+-------+-------|
| Misfile Rate | 0.003 | 0.007 | 0.010 |

In other words, on the average, Moe misfiles 0.3% of the documents he is supposed to file. Notice that Moe was correct: he is the most accurate filer, followed by Larry, and lastly Curly. If the boss were to make a decision based only on the worker's overall accuracy, then *Curly* should get the axe. But Curly hears this and interjects that he only works a short period during the day, and consequently makes mistakes only very rarely; there is only the tiniest chance that he misfiled this particular document.

The boss would like to use this updated information to update the probabilities for the three assistants, that is, (s)he wants to use the additional likelihood that the document was misfiled to update his/her beliefs about the likely culprit. Let \(A\) be the event that a document is misfiled. What the boss would like to know are the three probabilities
\[
\mathbb{P}(M|A),\mbox{ }\mathbb{P}(L|A),\mbox{ and }\mathbb{P}(C|A).
\]
We will show the calculation for \(\mathbb{P}(M|A)\), the other two cases being similar. We use Bayes' Rule in the form
\[
\mathbb{P}(M|A)=\frac{\mathbb{P}(M\cap A)}{\mathbb{P}(A)}.
\]
Let's try to find \(\mathbb{P}(M\cap A)\), which is just \(\mathbb{P}(M)\cdot\mathbb{P}(A|M)\) by the Multiplication Rule. We already know \(\mathbb{P}(M)=0.6\) and \(\mathbb{P}(A|M)\) is nothing more than Moe's misfile rate, given above to be \(\mathbb{P}(A|M)=0.003\). Thus, we compute
\[
\mathbb{P}(M\cap A)=(0.6)(0.003)=0.0018.
\]
Using the same procedure we may calculate
\[
\mathbb{P}(L \cap A)=0.0021\mbox{ and }\mathbb{P}(C \cap A)=0.0010.
\]

Now let's find the denominator, \(\mathbb{P}(A)\). The key here is the notion that if a file is misplaced, then either Moe or Larry or Curly must have filed it; there is no one else around to do the misfiling. Further, these possibilities are mutually exclusive. We may use the Theorem of Total Probability [[eq-theorem-total-probability][TTP]] to write
\[ 
\mathbb{P}(A)=\mathbb{P}(A\cap M)+\mathbb{P}(A\cap L)+\mathbb{P}(A\cap C).
\]
Luckily, we have computed these above. Thus
\[
\mathbb{P}(A)=0.0018+0.0021+0.0010=0.0049.
\]
Therefore, Bayes' Rule yields
\[
\mathbb{P}(M|A)=\frac{0.0018}{0.0049}\approx0.37.
\]
This last quantity is called the posterior probability that Moe misfiled the document, since it incorporates the observed data that a randomly selected file was misplaced (which is governed by the misfile rate). We can use the same argument to calculate

#+CAPTION: [Misfiling assistants: posterior]{}
|                       | Moe           | Larry         | Curly         |
|-----------------------+---------------+---------------+---------------|
| Posterior Probability | \(\approx0.37\) | \(\approx0.43\) | \(\approx0.20\) |
The conclusion: *Larry* gets the axe. What is happening is an intricate interplay between the time on the job and the misfile rate. It is not obvious who the winner (or in this case, loser) will be, and the statistician needs to consult Bayes' Rule to determine the best course of action.
#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-misfiling-assistants-multiple>>
Suppose the boss gets a change of heart and does not fire anybody. But the next day (s)he randomly selects another file and again finds it to be misplaced. To decide whom to fire now, the boss would use the same procedure, with one small change. (S)he would not use the prior probabilities 60%, 30%, and 10%; those are old news. Instead, she would replace the prior probabilities with the posterior probabilities just calculated. After the math she will have new posterior probabilities, updated even more from the day before.

In this way, probabilities found by Bayes' rule are always on the cutting edge, always updated with respect to the best information available at the time.
#+latex: \end{exampletoo}
#+html: </div>

*** How to do it with \(\mathsf{R}\)

There are not any special functions for Bayes' Rule in the =prob= package \cite{prob}, but problems like the ones above are easy enough to do by hand.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
*Misfiling assistants* (continued from Example [[exa-misfiling-assistants][Misfiling Assistants]]). We store the prior probabilities and the likelihoods in vectors and go to town.

#+begin_src R :exports both :results output pp  
prior <- c(0.6, 0.3, 0.1)
like <- c(0.003, 0.007, 0.010)
post <- prior * like
post / sum(post)
#+end_src

#+latex: \end{exampletoo}
#+html: </div>


Compare these answers with what we got in Example [[exa-misfiling-assistants][Misfiling Assistants]]. We would replace =prior= with =post= in a future calculation. We could raise =like= to a power to see how the posterior is affected by future document mistakes. (Do you see why? Think back to Section [[sec-Independent-Events][Independent Events]].)


#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Let us incorporate the posterior probability (=post=) information from the last example and suppose that the assistants misfile seven more documents. Using Bayes' Rule, what would the new posterior probabilities be?

#+begin_src R :exports both :results output pp  
newprior <- post
post <- newprior * like^7
post / sum(post)
#+end_src

We see that the individual with the highest probability of having misfiled all eight documents given the observed data is no longer Larry, but Curly. 
#+latex: \end{exampletoo}
#+html: </div>

There are two important points. First, we did not divide =post= by the sum of its entries until the very last step; we do not need to calculate it, and it will save us computing time to postpone normalization until absolutely necessary, namely, until we finally want to interpret them as probabilities.

Second, the reader might be wondering what the boss would get if (s)he skipped the intermediate step of calculating the posterior after only one misfiled document. What if she started from the /original/ prior, then observed eight misfiled documents, and calculated the posterior? What would she get? It must be the same answer, of course.

#+begin_src R :exports both :results output pp  
fastpost <- prior * like^8
fastpost / sum(fastpost)
#+end_src

Compare this to what we got in Example [[exa-misfiling-assistants-multiple][Misfiling Assistants Multiple]].

** Random Variables
:PROPERTIES:
:CUSTOM_ID: sec-Random-Variables
:END:

We already know about experiments, sample spaces, and events. In this section, we are interested in a /number/ that is associated with the experiment. We conduct a random experiment \(E\) and after learning the outcome \(\omega\) in \(S\) we calculate a number \(X\). That is, to each outcome \(\omega\) in the sample space we associate a number \(X(\omega)=x\). 

#+begin_defn
A /random variable/ \(X\) is a function \(X:S\to\mathbb{R}\) that associates to each outcome \(\omega\in S\) exactly one number \(X(\omega)=x\). 
#+end_defn

We usually denote random variables by uppercase letters such as \(X\), \(Y\), and \(Z\), and we denote their observed values by lowercase letters \(x\), \(y\), and \(z\). Just as \(S\) is the set of all possible outcomes of \(E\), we call the set of all possible values of \(X\) the /support/ of \(X\) and denote it by \(S_{X}\).

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Let \(E\) be the experiment of flipping a coin twice. We have seen that the sample space is \( S = \{ HH,\ HT,\ TH,\ TT \} \). Now define the random variable \(X = \mbox{the number of heads}\). That is, for example, \(X(HH)=2\), while \(X(HT)=1\). We may make a table of the possibilities:

#+LABEL: tab-flip-coin-twice
#+CAPTION: [Flipping a coin twice]{Flipping a coin twice.}
| \(\omega\in S\) | \(HH\) | \(HT\) | \(TH\) | \(TT\) |
|-----------------+--------+--------+--------+--------|
| \(X(\omega)=x\) |      2 |      1 |      1 |      0 |

Taking a look at the second row of the table, we see that the support of \(X\) -- the set of all numbers that \(X\) assumes -- would be \( S_{X}= \{ 0,1,2 \} \).
#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Let \(E\) be the experiment of flipping a coin repeatedly until observing a Head. The sample space would be \(S= \{ H,\ TH,\ TTH,\ TTTH,\ \ldots \} \). Now define the random variable \(Y=\mbox{the number of Tails before the first head}\). Then the support of \(Y\) would be \( S_{Y}= \{ 0,1,2,\ldots \} \).
#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Let \(E\) be the experiment of tossing a coin in the air, and define the random variable \( Z = \mbox{the time (in seconds) until the coin hits the ground} \). In this case, the sample space is inconvenient to describe. Yet the support of \(Z\) would be \((0,\infty)\). Of course, it is reasonable to suppose that the coin will return to Earth in a short amount of time; in practice, the set \((0,\infty)\) is admittedly too large. However, we will find that in many circumstances it is mathematically convenient to study the extended set rather than a restricted one. 
#+latex: \end{exampletoo}
#+html: </div>

There are important differences between the supports of \(X\), \(Y\), and \(Z\). The support of \(X\) is a finite collection of elements that can be inspected all at once. And while the support of \(Y\) cannot be exhaustively written down, its elements can nevertheless be listed in a naturally ordered sequence. Random variables with supports similar to those of \(X\) and \(Y\) are called /discrete random variables/. We study these in Chapter [[cha-Discrete-Distributions][Discrete Distributions]].

In contrast, the support of \(Z\) is a continuous interval, containing all rational and irrational positive real numbers. For this reason
#+latex: \footnote{This isn't really the reason, but it serves as an effective litmus test at the introductory level. See Billingsley or Resnick.},
random variables with supports like \(Z\) are called /continuous random variables/, to be studied in Chapter [[cha-Continuous-Distributions][Continuous Distributions]].

*** How to do it with \(\mathsf{R}\)

The primary vessel for this task is the =addrv= function. There are two ways to use it, and we will describe both.

**** Supply a Defining Formula

The first method is based on the =transform= function. See =?transform=. The idea is to write a formula defining the random variable inside the function, and it will be added as a column to the data frame. As an example, let us roll a 4-sided die three times, and let us define the random variable \(U=X1-X2+X3\). 

#+begin_src R :exports code :results silent
S <- rolldie(3, nsides = 4, makespace = TRUE) 
S <- addrv(S, U = X1-X2+X3) 
#+end_src 

Now let's take a look at the values of \(U\). In the interest of space, we will only reproduce the first few rows of \(S\) (there are \(4^{3}=64\) rows in total). 

#+begin_src R :exports both :results output pp   
head(S)
#+end_src 

We see from the \(U\) column it is operating just like it should. We can now answer questions like

#+begin_src R :exports both :results output pp   
prob::prob(S, U > 6) 
#+end_src 

**** Supply a Function

Sometimes we have a function laying around that we would like to apply to some of the outcome variables, but it is unfortunately tedious to write out the formula defining what the new variable would be. The =addrv= function has an argument =FUN= specifically for this case. Its value should be a legitimate function from \(\mathsf{R}\), such as =sum=, =mean=, =median=, and so forth. Or, you can define your own function. Continuing the previous example, let's define \(V=\max(X1,X2,X3)\) and \(W=X1+X2+X3\). 

#+begin_src R :exports both :results output pp  
S <- addrv(S, FUN = max, invars = c("X1","X2","X3"), name = "V") 
S <- addrv(S, FUN = sum, invars = c("X1","X2","X3"), name = "W") 
head(S) 
#+end_src 

Notice that =addrv= has an =invars= argument to specify exactly to which columns one would like to apply the function =FUN=. If no input variables are specified, then =addrv= will apply =FUN= to all non-=probs= columns. Further, =addrv= has an optional argument =name= to give the new variable; this can be useful when adding several random variables to a probability space (as above). If not specified, the default name is =X=.

*** Marginal Distributions

As we can see above, often after adding a random variable \(V\) to a probability space one will find that \(V\) has values that are repeated, so that it becomes difficult to understand what the ultimate behavior of \(V\) actually is. We can use the =marginal= function to aggregate the rows of the sample space by values of \(V\), all the while accumulating the probability associated with \(V\)'s distinct values. Continuing our example from above, suppose we would like to focus entirely on the values and probabilities of \(V=\max(X1,X2,X3)\). 

#+begin_src R :exports both :results output pp   
marginal(S, vars = "V") 
#+end_src

We could save the probability space of \(V\) in a data frame and study it further, if we wish. As a final remark, we can calculate the marginal distributions of multiple variables desired using the =vars= argument. For example, suppose we would like to examine the joint distribution of \(V\) and \(W\). 

#+begin_src R :exports both :results output pp   
marginal(S, vars = c("V", "W")) 
#+end_src 

Note that the default value of =vars= is the names of all columns except =probs=. This can be useful if there are duplicated rows in the probability space.

#+latex: \newpage{}

** Exercises

#+latex: \setcounter{thm}{0}

#+begin_xca
# <<xca-numb-cond-indep>>
Prove the assertion given in the text: the number of conditions that the events \(A_{1}\), \(A_{2}\), ..., \(A_{n}\) must satisfy in order to be mutually independent is \(2^{n}-n-1\). (/Hint/: think about Pascal's triangle.)
#+end_xca

*Answer:*
The events must satisfy the product equalities two at a time, of which there are \({n \choose 2}\), then they must satisfy an additional \({n \choose 3}\) conditions three at a time, and so on, until they satisfy the \({n \choose n}=1\) condition including all \(n\) events. In total, there are 
\[
{n \choose 2}+{n \choose 3}+\cdots+{n \choose n}=\sum_{k=0}^{n}{n \choose k}-\left[{n \choose 0}+{n \choose 1}\right]
\]
conditions to be satisfied, but the binomial series in the expression on the right is the sum of the entries of the \(n^{\mathrm{th}}\) row of Pascal's triangle, which is \(2^{n}\).

* Discrete Distributions                                           :discdist:
:PROPERTIES:
:tangle: R/discdist.R
:CUSTOM_ID: cha-Discrete-Distributions
:END:

#+begin_src R :exports none :eval never
# Chapter: Discrete Distributions
# All code released under GPL Version 3
#+end_src

#+latex: \noindent 
In this chapter we introduce discrete random variables, those who take values in a finite or countably infinite support set. We discuss probability mass functions and some special expectations, namely, the mean, variance and standard deviation. Some of the more important discrete distributions are explored in detail, and the more general concept of expectation is defined, which paves the way for moment generating functions. 

We give special attention to the empirical distribution since it plays such a fundamental role with respect to resampling and Chapter [[cha-resampling-methods][Resampling Methods]]; it will also be needed in Section [[sub-Kolmogorov-Smirnov-Goodness-of-Fit-Test][Kolmogorov-Smirnov]] where we discuss the Kolmogorov-Smirnov test. Following this is a section in which we introduce a catalogue of discrete random variables that can be used to model experiments.

There are some comments on simulation, and we mention transformations of random variables in the discrete case. The interested reader who would like to learn more about any of the assorted discrete distributions mentioned here should take a look at /Univariate Discrete Distributions/ by Johnson /et al/\cite{Johnson1993}.

*What do I want them to know?*
- how to choose a reasonable discrete model under a variety of physical circumstances
- item the notion of mathematical expectation, how to calculate it, and basic properties- moment generating functions (yes, I want them to hear about those)
- the general tools of the trade for manipulation of continuous random variables, integration, /etc/.
- some details on a couple of discrete models, and exposure to a bunch of other ones
- how to make new discrete random variables from old ones

** Discrete Random Variables
:PROPERTIES:
:CUSTOM_ID: sec-discrete-random-variables
:END:

*** Probability Mass Functions
:PROPERTIES:
:CUSTOM_ID: sub-probability-mass-functions
:END:

Discrete random variables are characterized by their supports which take the form
\begin{equation}
S_{X}=\{u_{1},u_{2},\ldots,u_{k}\}\mbox{ or }S_{X}=\{u_{1},u_{2},u_{3}\ldots\}.
\end{equation}
Every discrete random variable \(X\) has associated with it a probability mass function (PMF) \(f_{X}:S_{X}\to[0,1]\) defined by
\begin{equation}
f_{X}(x)=\mathbb{P}(X=x),\quad x\in S_{X}.
\end{equation}
Since values of the PMF represent probabilities, we know from Chapter [[cha-Probability][Probability]] that PMFs enjoy certain properties. In particular, all PMFs satisfy
1. \(f_{X}(x)>0\) for \(x\in S\),
2. \(\sum_{x\in S}f_{X}(x)=1\), and
3. \(\mathbb{P}(X\in A)=\sum_{x\in A}f_{X}(x)\), for any event \(A\subset S\).

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-Toss-a-coin>>

Toss a coin 3 times. The sample space would be
\[
S=\{ HHH,\ HTH,\ THH,\ TTH,\ HHT,\ HTT,\ THT,\ TTT\}.
\]
Now let \(X\) be the number of Heads observed. Then \(X\) has support \(S_{X}=\{ 0,1,2,3\} \). Assuming that the coin is fair and was tossed in exactly the same way each time, it is not unreasonable to suppose that the outcomes in the sample space are all equally likely. 

What is the PMF of \(X\)? Notice that \(X\) is zero exactly when the outcome \(TTT\) occurs, and this event has probability \(1/8\). Therefore, \(f_{X}(0)=1/8\), and the same reasoning shows that \(f_{X}(3)=1/8\). Exactly three outcomes result in \(X=1\), thus, \(f_{X}(1)=3/8\) and \(f_{X}(3)\) holds the remaining \(3/8\) probability (the total is 1). We can represent the PMF with a table:

#+LABEL: tab-pmf-flip-coin-three
#+CAPTION: [Flipping a coin thrice: the PMF]{Flipping a coin three times: the PMF.}
| \(x\in S_{X}\)               |   0 |   1 |   2 |   3 | Total |
|------------------------------+-----+-----+-----+-----+-------|
| \(f_{X}(x)=\mathbb{P}(X=x)\) | 1/8 | 3/8 | 3/8 | 1/8 |     1 |

#+latex: \end{exampletoo}
#+html: </div>

*** Mean, Variance, and Standard Deviation
:PROPERTIES:
:CUSTOM_ID: sub-mean-variance-sd
:END:

There are numbers associated with PMFs. One important example is the mean \(\mu\), also known as \(\mathbb{E} X\) (which we will discuss later):
\begin{equation}
\mu=\mathbb{E} X=\sum_{x\in S}xf_{X}(x),
\end{equation}
provided the (potentially infinite) series \(\sum|x|f_{X}(x)\) is convergent. Another important number is the variance:
\begin{equation}
\sigma^{2}=\sum_{x\in S}(x-\mu)^{2}f_{X}(x),
\end{equation}
which can be computed (see Exercise [[xca-variance-shortcut][Variance Shortcut]]) with the alternate formula \(\sigma^{2}=\sum x{}^{2}f_{X}(x)-\mu^{2}\). Directly defined from the variance is the standard deviation \(\sigma=\sqrt{\sigma^{2}}\).
 
#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-disc-pmf-mean>>
We will calculate the mean of \(X\) in Example [[exa-Toss-a-coin][Toss a coin]].
\[
\mu=\sum_{x=0}^{3}xf_{X}(x)=0\cdot\frac{1}{8}+1\cdot\frac{3}{8}+2\cdot\frac{3}{8}+3\cdot\frac{1}{8}=1.5.
\]
We interpret \(\mu = 1.5\) by reasoning that if we were to repeat the random experiment many times, independently each time, observe many corresponding outcomes of the random variable \(X\), and take the sample mean of the observations, then the calculated value would fall close to 1.5. The approximation would get better as we observe more and more values of \(X\) (another form of the Law of Large Numbers; see Section [[sec-Interpreting-Probabilities][Interpreting Probabilities]]). Another way it is commonly stated is that \(X\) is 1.5 ``on the average'' or ``in the long run''.
#+latex: \end{exampletoo}
#+html: </div>

#+begin_rem
Note that although we say \(X\) is 3.5 on the average, we must keep in mind that our \(X\) never actually equals 3.5 (in fact, it is impossible for \(X\) to equal 3.5).
#+end_rem

Related to the probability mass function \(f_{X}(x)=\mathbb{P}(X=x)\) is another important function called the /cumulative distribution function/ (CDF), \(F_{X}\). It is defined by the formula
\begin{equation}
F_{X}(t)=\mathbb{P}(X\leq t),\quad -\infty < t < \infty.
\end{equation}
We know that all PMFs satisfy certain properties, and a similar statement may be made for CDFs. In particular, any CDF \(F_{X}\) satisfies
- \(F_{X}\) is nondecreasing (\(t_{1}\leq t_{2}\) implies \(F_{X}(t_{1})\leq F_{X}(t_{2})\)).
- \(F_{X}\) is right-continuous (\(\lim_{t\to a^{+}}F_{X}(t)=F_{X}(a)\) for all \(a\in\mathbb{R}\)).
- \(\lim_{t\to-\infty}F_{X}(t)=0\) and \(\lim_{t\to\infty}F_{X}(t)=1\).
We say that \(X\) has the distribution \(F_{X}\) and we write \(X\sim F_{X}\). In an abuse of notation we will also write \(X\sim f_{X}\) and for the named distributions the PMF or CDF will be identified by the family name instead of the defining formula.

**** How to do it with \(\mathsf{R}\)
:PROPERTIES:
:CUSTOM_ID: sub-disc-rv-how-r
:END:

The mean and variance of a discrete random variable is easy to compute at the console. Let's return to Example [[exa-disc-pmf-mean][Disc PMF mean]]. We will start by defining a vector =x= containing the support of \(X\), and a vector =f= to contain the values of \(f_{X}\) at the respective outcomes in =x=:

#+begin_src R :exports code :results silent
x <- c(0,1,2,3)
f <- c(1/8, 3/8, 3/8, 1/8)
#+end_src

To calculate the mean \(\mu\), we need to multiply the corresponding values of =x= and =f= and add them. This is easily accomplished in \(\mathsf{R}\) since operations on vectors are performed /element-wise/ (see Section [[sub-Functions-and-Expressions][Functions and Expressions]]): 

#+begin_src R :exports both :results output pp  
mu <- sum(x * f)
mu
#+end_src

To compute the variance \(\sigma^{2}\), we subtract the value of =mu= from each entry in =x=, square the answers, multiply by =f=,and =sum=. The standard deviation \(\sigma\) is simply the square root of \(\sigma^{2}\).

#+begin_src R :exports both :results output pp  
sigma2 <- sum((x-mu)^2 * f)
sigma2
#+end_src

#+begin_src R :exports both :results output pp  
sigma <- sqrt(sigma2)
sigma
#+end_src

Finally, we may find the values of the CDF \(F_{X}\) on the support by accumulating the probabilities in \(f_{X}\) with the =cumsum= function. 

#+begin_src R :exports both :results output pp  
F <- cumsum(f)
F
#+end_src

As easy as this is, it is even easier to do with the =distrEx= package \cite{distrEx}. We define a random variable =X= as an object, then compute things from the object such as mean, variance, and standard deviation with the functions =E=, =var=, and =sd=:

#+begin_src R :exports both :results output pp  
X <- DiscreteDistribution(supp = 0:3, prob = c(1,3,3,1)/8)
E(X); var(X); sd(X)
#+end_src

** The Discrete Uniform Distribution
:PROPERTIES:
:CUSTOM_ID: sec-disc-uniform-dist
:END:

We have seen the basic building blocks of discrete distributions and we now study particular models that statisticians often encounter in the field. Perhaps the most fundamental of all is the /discrete uniform/ distribution.

A random variable \(X\) with the discrete uniform distribution on the integers \(1,2,\ldots,m\) has PMF
\begin{equation}
f_{X}(x)=\frac{1}{m},\quad x=1,2,\ldots,m.
\end{equation}
We write \(X\sim\mathsf{disunif}(m)\). A random experiment where this distribution occurs is the choice of an integer at random between 1 and 100, inclusive. Let \(X\) be the number chosen. Then \(X\sim\mathsf{disunif}(m=100)\) and
\[
\mathbb{P}(X=x)=\frac{1}{100},\quad x=1,\ldots,100.
\]
We find a direct formula for the mean of \(X\sim\mathsf{disunif}(m)\):
\begin{equation}
\mu=\sum_{x=1}^{m}xf_{X}(x)=\sum_{x=1}^{m}x\cdot\frac{1}{m}=\frac{1}{m}(1+2+\cdots+m)=\frac{m+1}{2},
\end{equation}
where we have used the famous identity \(1+2+\cdots+m=m(m+1)/2\). That is, if we repeatedly choose integers at random from 1 to \(m\) then, on the average, we expect to get \((m+1)/2\). To get the variance we first calculate
\[
\sum_{x=1}^{m}x^{2}f_{X}(x)=\frac{1}{m}\sum_{x=1}^{m}x^{2}=\frac{1}{m}\frac{m(m+1)(2m+1)}{6}=\frac{(m+1)(2m+1)}{6},
\]
and finally,
\begin{equation}
\sigma^{2}=\sum_{x=1}^{m}x^{2}f_{X}(x)-\mu^{2}=\frac{(m+1)(2m+1)}{6}-\left(\frac{m+1}{2}\right)^{2}=\cdots=\frac{m^{2}-1}{12}.
\end{equation}

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Roll a die and let \(X\) be the upward face showing. Then \(m=6\), \(\mu=7/2=3.5\), and \(\sigma^{2}=(6^{2}-1)/12=35/12\).
#+latex: \end{exampletoo}
#+html: </div>

*** How to do it with \(\mathsf{R}\)

*** From the console:
One can choose an integer at random with the =sample= function. The general syntax to simulate a discrete uniform random variable is =sample(x, size, replace = TRUE)=.

The argument =x= identifies the numbers from which to randomly sample. If =x= is a number, then sampling is done from 1 to =x=. The argument =size= tells how big the sample size should be, and =replace= tells whether or not numbers should be replaced in the urn after having been sampled. The default option is =replace = FALSE= but for discrete uniforms the sampled values should be replaced. Some examples follow.

*** Examples
- To roll a fair die 3000 times, do =sample(6, size = 3000, replace = TRUE)=.
- To choose 27 random numbers from 30 to 70, do =sample(30:70, size = 27, replace = TRUE)=.
- To flip a fair coin 1000 times, do =sample(c("H","T"), size = 1000, replace = TRUE)=.

*** With the \(\mathsf{R}\) Commander:

Follow the sequence =Probability= \(\triangleright\) =Discrete Distributions= \(\triangleright\) =Discrete Uniform distribution= \(\triangleright\) =Simulate Discrete uniform variates...=.

Suppose we would like to roll a fair die 3000 times. In the =Number of samples= field we enter =1=. Next, we describe what interval of integers to be sampled. Since there are six faces numbered 1 through 6, we set =from = 1=, we set =to = 6=, and set =by = 1= (to indicate that we travel from 1 to 6 in increments of 1 unit). We will generate a list of 3000 numbers selected from among 1, 2, ..., 6, and we store the results of the simulation. For the time being, we select =New Data set=. Click =OK=.

Since we are defining a new data set, the \(\mathsf{R}\) Commander requests a name for the data set. The default name is =Simset1=, although in principle you could name it whatever you like (according to \(\mathsf{R}\)'s rules for object names). We wish to have a list that is 3000 long, so we set =Sample Size = 3000= and click =OK=.

In the \(\mathsf{R}\) Console window, the \(\mathsf{R}\) Commander should tell you that =Simset1= has been initialized, and it should also alert you that =There was 1 discrete uniform variate sample stored in Simset 1.=. To take a look at the rolls of the die, we click =View data set= and a window opens.  

The default name for the variable is =disunif.sim1=.

** The Binomial Distribution
:PROPERTIES:
:CUSTOM_ID: sec-binom-dist
:END:

The binomial distribution is based on a /Bernoulli trial/, which is a random experiment in which there are only two possible outcomes: success (\(S\)) and failure (\(F\)). We conduct the Bernoulli trial and let 
\begin{equation}
X=
\begin{cases}
1 & \mbox{if the outcome is $S$},\\
0 & \mbox{if the outcome is $F$}.
\end{cases}
\end{equation}
If the probability of success is \(p\) then the probability of failure must be \(1-p=q\) and the PMF of \(X\) is
\begin{equation}
f_{X}(x)=p^{x}(1-p)^{1-x},\quad x=0,1.
\end{equation}
It is easy to calculate \(\mu=\mathbb{E} X=p\) and \(\mathbb{E} X^{2}=p\) so that \(\sigma^{2}=p-p^{2}=p(1-p)\).

*** The Binomial Model
:PROPERTIES:
:CUSTOM_ID: sub-The-Binomial-Model
:END:

The Binomial model has three defining properties:
- Bernoulli trials are conducted \(n\) times,
- the trials are independent,
- the probability of success \(p\) does not change between trials.
If \(X\) counts the number of successes in the \(n\) independent trials, then the PMF of \(X\) is 
\begin{equation}
f_{X}(x)={n \choose x}p^{x}(1-p)^{n-x},\quad x=0,1,2,\ldots,n.
\end{equation}
We say that \(X\) has a /binomial distribution/ and we write \(X\sim\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p)\). It is clear that \(f_{X}(x)\geq0\) for all \(x\) in the support because the value is the product of nonnegative numbers. We next check that \(\sum f(x)=1\):
\[
\sum_{x=0}^{n}{n \choose x}p^{x}(1-p)^{n-x}=[p+(1-p)]^{n}=1^{n}=1.
\]
We next find the mean:
\begin{alignat*}{1}
\mu= & \sum_{x=0}^{n}x\,{n \choose x}p^{x}(1-p)^{n-x},\\
= & \sum_{x=1}^{n}x\,\frac{n!}{x!(n-x)!}p^{x}q^{n-x},\\
= & n\cdot p\sum_{x=1}^{n}\frac{(n-1)!}{(x-1)!(n-x)!}p^{x-1}q^{n-x},\\
= & np\,\sum_{x-1=0}^{n-1}{n-1 \choose x-1}p^{(x-1)}(1-p)^{(n-1)-(x-1)},\\
= & np.
\end{alignat*}
A similar argument shows that \(\mathbb{E} X(X-1)=n(n-1)p^{2}\) (see Exercise [[xca-binom-factorial-expectation][Binom factorial exp]]). Therefore
\begin{alignat*}{1}
\sigma^{2}= & \mathbb{E} X(X-1)+\mathbb{E} X-[\mathbb{E} X]^{2},\\
= & n(n-1)p^{2}+np-(np)^{2},\\
= & n^{2}p^{2}-np^{2}+np-n^{2}p^{2},\\
= & np-np^{2}=np(1-p).
\end{alignat*}

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
A four-child family. Each child may be either a boy (\(B\)) or a girl (\(G\)). For simplicity we suppose that \(\mathbb{P}(B)=\mathbb{P}(G)=1/2\) and that the genders of the children are determined independently. If we let \(X\) count the number of \(B\)'s, then \(X\sim\mathsf{binom}(\mathtt{size}=4,\,\mathtt{prob}=1/2)\). Further, \(\mathbb{P}(X=2)\) is
\[
f_{X}(2)={4 \choose 2}(1/2)^{2}(1/2)^{2}=\frac{6}{2^{4}}.
\]
The mean number of boys is \(4(1/2)=2\) and the variance of \(X\) is \(4(1/2)(1/2)=1\).
#+latex: \end{exampletoo}
#+html: </div>

**** How to do it with \(\mathsf{R}\)

The corresponding \(\mathsf{R}\) function for the PMF and CDF are =dbinom= and =pbinom=, respectively. We demonstrate their use in the following examples.  

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
We can calculate it in \(\mathsf{R}\) Commander under the =Binomial Distribution= menu with the =Binomial probabilities= menu item.
  #+begin_src R :exports results :results output pp
  A <- data.frame(Pr=dbinom(0:4, size = 4, prob = 0.5))
  rownames(A) <- 0:4 
  A
  #+end_src

#+latex: \end{exampletoo}
#+html: </div>

We know that the \(\mathsf{binom}(\mathtt{size}=4,\,\mathtt{prob}=1/2)\) distribution is supported on the integers 0, 1, 2, 3, and 4; thus the table is complete. We can read off the answer to be \(\mathbb{P}(X=2)=0.3750\).

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Roll 12 dice simultaneously, and let \(X\) denote the number of 6's that appear. We wish to find the probability of getting seven, eight, or nine 6's. If we let \(S=\{ \mbox{get a 6 on one roll} \} \), then \(\mathbb{P}(S)=1/6\) and the rolls constitute Bernoulli trials; thus \(X\sim\mathsf{binom}(\mathtt{size}=12,\ \mathtt{prob}=1/6)\) and our task is to find \(\mathbb{P}(7\leq X\leq9)\). This is just
\[ 
\mathbb{P}(7\leq X\leq9)=\sum_{x=7}^{9}{12 \choose x}(1/6)^{x}(5/6)^{12-x}.
\]
Again, one method to solve this problem would be to generate a probability mass table and add up the relevant rows. However, an alternative method is to notice that \(\mathbb{P}(7\leq X\leq9)=\mathbb{P}(X\leq9)-\mathbb{P}(X\leq6)=F_{X}(9)-F_{X}(6)\), so we could get the same answer by using the =Binomial tail probabilities...= menu in the \(\mathsf{R}\) Commander or the following from the command line: 

#+begin_src R :exports both :results output pp  
pbinom(9, size=12, prob=1/6) - pbinom(6, size=12, prob=1/6)
diff(pbinom(c(6,9), size = 12, prob = 1/6))  # same thing
#+end_src

#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-toss-coin-3-withR>>
Toss a coin three times and let \(X\) be the number of Heads observed. We know from before that \(X\sim\mathsf{binom}(\mathtt{size}=3,\,\mathtt{prob}=1/2)\) which implies the following PMF:

#+LABEL: tab-flip-coin-thrice
#+CAPTION: [Flipping a coin thrice: PMF]{Flipping a coin three times: the PMF.}
| \(x=\mbox{num. of Heads}\)   |   0 |   1 |   2 |   3 | Total |
|------------------------------+-----+-----+-----+-----+-------|
| \(f(x) = \mathbb{P}(X = x)\) | 1/8 | 3/8 | 3/8 | 1/8 |     1 |

Our next goal is to write down the CDF of \(X\) explicitly. The first case is easy: it is impossible for \(X\) to be negative, so if \(x<0\) then we should have \(\mathbb{P}(X\leq x)=0\). Now choose a value \(x\) satisfying \(0\leq x<1\), say, \(x=0.3\). The only way that \(X\leq x\) could happen would be if \(X=0\), therefore, \(\mathbb{P}(X\leq x)\) should equal \(\mathbb{P}(X=0)\), and the same is true for any \(0\leq x<1\). Similarly, for any \(1\leq x<2\), say, \(x=1.73\), the event \(\{ X\leq x \}\) is exactly the event \(\{ X=0\mbox{ or }X=1 \}\). Consequently, \(\mathbb{P}(X\leq x)\) should equal \(\mathbb{P}(X=0\mbox{ or }X=1)=\mathbb{P}(X=0)+\mathbb{P}(X=1)\). Continuing in this fashion, we may figure out the values of \(F_{X}(x)\) for all possible inputs \(-\infty<x<\infty\), and we may summarize our observations with the following piecewise defined function:
\[
F_{X}(x)=\mathbb{P}(X\leq x)=
\begin{cases}
0, & x<0,\\
\frac{1}{8}, & 0\leq x<1,\\
\frac{1}{8}+\frac{3}{8}=\frac{4}{8}, & 1\leq x<2,\\
\frac{4}{8}+\frac{3}{8}=\frac{7}{8}, & 2\leq x<3,\\
1, & x\geq3.
\end{cases}
\]
In particular, the CDF of \(X\) is defined for the entire real line, \(\mathbb{R}\). The CDF is right continuous and nondecreasing. A graph of the \(\mathsf{binom}(\mathtt{size}=3,\,\mathtt{prob}=1/2)\) CDF is shown in Figure [[fig-binom-cdf-base][binom-cdf-base]].
#+latex: \end{exampletoo}
#+html: </div>

#+name: binom-cdf-base
#+begin_src R :exports none :results silent
plot(0, xlim = c(-1.2, 4.2), ylim = c(-0.04, 1.04), type = "n", xlab = "number of successes", ylab = "cumulative probability")
abline(h = c(0,1), lty = 2, col = "grey")
lines(stepfun(0:3, pbinom(-1:3, size = 3, prob = 0.5)), verticals = FALSE, do.p = FALSE)
points(0:3, pbinom(0:3, size = 3, prob = 0.5), pch = 16, cex = 1.2)
points(0:3, pbinom(-1:2, size = 3, prob = 0.5), pch = 1, cex = 1.2)
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/discdist/binom-cdf-base.ps
  <<binom-cdf-base>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/discdist/binom-cdf-base.svg
  <<binom-cdf-base>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/discdist/binom-cdf-base.ps}
  \caption[Graph of the \(\mathsf{binom}(\mathtt{size}=3,\,\mathtt{prob}=1/2)\) CDF]{\small A graph of the \(\mathsf{binom}(\mathtt{size}=3,\,\mathtt{prob}=1/2)\) CDF.}
  \label{fig-binom-cdf-base}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-binom-cdf-base" class="figure">
  <p><img src="svg/discdist/binom-cdf-base.svg" width=500 alt="svg/discdist/binom-cdf-base.svg" /></p>
  <p> A graph of the <code>binom(size = 3, prob = 1/2)</code> CDF.</p>
</div>
#+end_html

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Another way to do Example [[exa-toss-coin-3-withR][Toss coin 3 with R]] is with the =distr= family of packages \cite{distr}. They use an object oriented approach to random variables, that is, a random variable is stored in an object =X=, and then questions about the random variable translate to functions on and involving =X=. Random variables with distributions from the =base= package\cite{base} are specified by capitalizing the name of the distribution.

#+begin_src R :exports both :results output pp  
X <- Binom(size = 3, prob = 1/2)
X
#+end_src

The analogue of the =dbinom= function for =X= is the =d(X)= function, and the analogue of the =pbinom= function is the =p(X)= function. Compare the following:

#+begin_src R :exports both :results output pp  
d(X)(1)   # pmf of X evaluated at x = 1
p(X)(2)   # cdf of X evaluated at x = 2
#+end_src

#+latex: \end{exampletoo}
#+html: </div>

Random variables defined via the =distr= package \cite{distr} may be /plotted/, which will return graphs of the PMF, CDF, and quantile function (introduced in Section [[sub-Normal-Quantiles-QF][Normal Quantiles]]). See Figure [[fig-binom-plot-distr][binom-plot-distr]] for an example.

#+name: binom-plot-distr
#+begin_src R :exports code :results silent
plot(X, cex = 0.2)
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/discdist/binom-plot-distr.ps
  <<binom-plot-distr>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/discdist/binom-plot-distr.svg
  <<binom-plot-distr>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/discdist/binom-plot-distr.ps}
  \caption[The \textsf{binom}(\texttt{size} = 3, \texttt{prob} = 0.5) distribution from the \texttt{distr} package]{\small The \textsf{binom}(\texttt{size} = 3, \texttt{prob} = 0.5) distribution from the \texttt{distr} package.}
  \label{fig-binom-plot-distr}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-binom-plot-distr" class="figure">
  <p><img src="svg/discdist/binom-plot-distr.svg" width=500 alt="svg/discdist/binom-plot-distr.svg" /></p>
  <p>The <code>binom(size = 3, prob = 0.5)</code> distribution from the <code>distr</code> package.</p>
</div>
#+end_html

#+CAPTION: [Correspondence between \texttt{stats} and \texttt{distr}.]{Correspondence between =stats= and =distr=. We are given \(X\sim\mathsf{dbinom}(\mathtt{size}=n,\,\mathtt{prob}=p)\).  For the =distr= package we must first set \(\mathtt{X\ <-\ Binom(size=}n\mathtt{,\ prob=}p\mathtt{)}\).}
| How to do:              | with =stats= (default)               | with =distr=         |
|-------------------------+--------------------------------------+----------------------|
| PMF: \(\mathbb{P}(X=x)\)       | \(\mathtt{dbinom(x,size=n,prob=p)}\) | \(\mathtt{d(X)(x)}\) |
| CDF:  \(\mathbb{P}(X\leq x)\)  | \(\mathtt{pbinom(x,size=n,prob=p)}\) | \(\mathtt{p(X)(x)}\) |
| Simulate \(k\) variates | \(\mathtt{rbinom(k,size=n,prob=p)}\) | \(\mathtt{r(X)(k)}\) |
|-------------------------+--------------------------------------+----------------------|

** Expectation and Moment Generating Functions
:PROPERTIES:
:CUSTOM_ID: sec-expectation-and-mgfs
:END:

*** The Expectation Operator
:PROPERTIES:
:CUSTOM_ID: sub-expectation-operator
:END:

We next generalize some of the concepts from Section [[sub-mean-variance-sd][Mean-variance-sd]]. There we saw that every
#+latex: \footnote{Not every, only those PMFs for which the (potentially infinite) series converges.}
PMF has two important numbers associated with it:
\begin{equation}
\mu=\sum_{x\in S}xf_{X}(x),\quad \sigma^{2}=\sum_{x\in S}(x-\mu)^{2}f_{X}(x).
\end{equation}
Intuitively, for repeated observations of \(X\) we would expect the sample mean to closely approximate \(\mu\) as the sample size increases without bound. For this reason we call \(\mu\) the /expected value/ of \(X\) and we write \(\mu=\mathbb{E} X\), where \(\mathbb{E}\) is an /expectation operator/.

#+begin_defn
More generally, given a function \(g\) we define the /expected value of/ \(g(X)\) by
\begin{equation}
\mathbb{E}\, g(X)=\sum_{x\in S}g(x)f_{X}(x),
\end{equation}
provided the (potentially infinite) series \(\sum_{x}|g(x)|f(x)\) is convergent. We say that \(\mathbb{E} g(X)\) /exists/.
#+end_defn

In this notation the variance is \(\sigma^{2}=\mathbb{E}(X-\mu)^{2}\) and we prove the identity
\begin{equation}
\mathbb{E}(X-\mu)^{2}=\mathbb{E} X^{2}-(\mathbb{E} X)^{2}
\end{equation}
in Exercise [[xca-variance-shortcut][Variance Shortcut]]. Intuitively, for repeated observations of \(X\) we would expect the sample mean of the \(g(X)\) values to closely approximate \(\mathbb{E}\, g(X)\) as the sample size increases without bound.

Let us take the analogy further. If we expect \(g(X)\) to be close to \(\mathbb{E} g(X)\) on the average, where would we expect \(3g(X)\) to be on the average? It could only be \(3\mathbb{E} g(X)\). The following theorem makes this idea precise.

#+begin_prop
# <<pro-expectation-properties>>
For any functions \(g\) and \(h\), any random variable \(X\), and any constant \(c\): 
1. \(\mathbb{E}\: c=c\),
2. \(\mathbb{E}[c\cdot g(X)]=c\mathbb{E} g(X)\)
3. \(\mathbb{E}[g(X)+h(X)]=\mathbb{E} g(X)+\mathbb{E} h(X)\),
provided \(\mathbb{E} g(X)\) and \(\mathbb{E} h(X)\) exist.
#+end_prop

#+begin_proof
Go directly from the definition. For example,
\[
\mathbb{E}[c\cdot g(X)]=\sum_{x\in S}c\cdot g(x)f_{X}(x)=c\cdot\sum_{x\in S}g(x)f_{X}(x)=c\mathbb{E} g(X).
\]
#+end_proof

*** Moment Generating Functions
:PROPERTIES:
:CUSTOM_ID: sub-MGFs
:END:

#+begin_defn
Given a random variable \(X\), its /moment generating function/ (abbreviated MGF) is defined by the formula
\begin{equation}
M_{X}(t)=\mathbb{E}\mathrm{e}^{tX}=\sum_{x\in S}\mathrm{e}^{tx}f_{X}(x),
\end{equation}
provided the (potentially infinite) series is convergent for all \(t\) in a neighborhood of zero (that is, for all \(-\epsilon<t<\epsilon\), for some \(\epsilon>0\)).
#+end_defn

Note that for any MGF \(M_{X}\),
\begin{equation}
M_{X}(0)=\mathbb{E}\mathrm{e}^{0\cdot X}=\mathbb{E}1=1.
\end{equation}
We will calculate the MGF for the two distributions introduced above.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Find the MGF for \(X\sim\mathsf{disunif}(m)\). 
Since \(f(x)=1/m\), the MGF takes the form
\[
M(t)=\sum_{x=1}^{m}\mathrm{e}^{tx}\frac{1}{m}=\frac{1}{m}(\mathrm{e}^{t}+\mathrm{e}^{2t}+\cdots+\mathrm{e}^{mt}),\quad \mbox{for any $t$.}
\]

#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Find the MGF for \(X\sim\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p)\).
#+latex: \end{exampletoo}
#+html: </div>

\begin{alignat*}{1}
M_{X}(t)= & \sum_{x=0}^{n}\mathrm{e}^{tx}\,{n \choose x}\, p^{x}(1-p)^{n-x},\\
= & \sum_{x=0}^{n}{n \choose x}\,(p\mathrm{e}^{t})^{x}q^{n-x},\\
= & (p\mathrm{e}^{t}+q)^{n},\quad \mbox{for any $t$.}
\end{alignat*}

**** Applications

We will discuss three applications of moment generating functions in this book. The first is the fact that an MGF may be used to accurately identify the probability distribution that generated it, which rests on the following:

#+begin_thm
# <<thm-mgf-unique>>
The moment generating function, if it exists in a neighborhood of zero, determines a probability distribution /uniquely/. 
#+end_thm

#+begin_proof
Unfortunately, the proof of such a theorem is beyond the scope of a text like this one. Interested readers could consult Billingsley \cite{Billingsley1995}.
#+end_proof


We will see an example of Theorem [[thm-mgf-unique][MGF Unique]] in action.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Suppose we encounter a random variable which has MGF
\[
M_{X}(t)=(0.3+0.7\mathrm{e}^{t})^{13}.
\]
Then \(X\sim\mathsf{binom}(\mathtt{size}=13,\,\mathtt{prob}=0.7)\).
#+latex: \end{exampletoo}
#+html: </div>

An MGF is also known as a ``Laplace Transform'' and is manipulated in that context in many branches of science and engineering.

**** Why is it called a Moment Generating Function?

This brings us to the second powerful application of MGFs. Many of the models we study have a simple MGF, indeed, which permits us to determine the mean, variance, and even higher moments very quickly. Let us see why. We already know that 
\begin{alignat*}{1}
M(t)= & \sum_{x\in S}\mathrm{e}^{tx}f(x).
\end{alignat*}
Take the derivative with respect to \(t\) to get
\begin{equation}
M'(t)=\frac{\mathrm{d}}{\mathrm{d} t}\left(\sum_{x\in S}\mathrm{e}^{tx}f(x)\right)=\sum_{x\in S}\ \frac{\mathrm{d}}{\mathrm{d} t}\left(\mathrm{e}^{tx}f(x)\right)=\sum_{x\in S}x\mathrm{e}^{tx}f(x),
\end{equation}
and so if we plug in zero for \(t\) we see
\begin{equation}
M'(0)=\sum_{x\in S}x\mathrm{e}^{0}f(x)=\sum_{x\in S}xf(x)=\mu=\mathbb{E} X.
\end{equation}
Similarly, \(M''(t)=\sum x^{2}\mathrm{e}^{tx}f(x)\) so that \(M''(0)=\mathbb{E} X^{2}\). And in general, we can see
#+latex: \footnote{We are glossing over some significant mathematical details in our derivation. Suffice it to say that when the MGF exists in a neighborhood of \(t=0\), the exchange of differentiation and summation is valid in that neighborhood, and our remarks hold true.}
that
\begin{equation}
M_{X}^{(r)}(0)=\mathbb{E} X^{r}=\mbox{\(r^{\mathrm{th}}\) moment of \(X\) about the origin.}
\end{equation}

These are also known as /raw moments/ and are sometimes denoted \(\mu_{r}'\). In addition to these are the so called /central moments/ \(\mu_{r}\) defined by
\begin{equation}
\mu_{r}=\mathbb{E}(X-\mu)^{r},\quad r=1,2,\ldots
\end{equation}

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Let \(X\sim\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p)\mbox{ with $M(t)=(q+p\mathrm{e}^{t})^{n}$}\).

We calculated the mean and variance of a binomial random variable in Section [[sec-binom-dist][Binomial Distribution]] by means of the binomial series. But look how quickly we find the mean and variance with the moment generating function.
\begin{alignat*}{1}
M'(t)= & n(q+p\mathrm{e}^{t})^{n-1}p\mathrm{e}^{t}\left|_{t=0}\right.,\\
= & n\cdot1^{n-1}p,\\
= & np.
\end{alignat*}
And
\begin{alignat*}{1}
M''(0)= & n(n-1)[q+p\mathrm{e}^{t}]^{n-2}(p\mathrm{e}^{t})^{2}+n[q+p\mathrm{e}^{t}]^{n-1}p\mathrm{e}^{t}\left|_{t=0}\right.,\\
\mathbb{E} X^{2}= & n(n-1)p^{2}+np.
\end{alignat*}
Therefore
\begin{alignat*}{1}
\sigma^{2}= & \mathbb{E} X^{2}-(\mathbb{E} X)^{2},\\
= & n(n-1)p^{2}+np-n^{2}p^{2},\\
= & np-np^{2}=npq.
\end{alignat*}
See how much easier that was?
#+latex: \end{exampletoo}
#+html: </div>

#+begin_rem
We learned in this section that \(M^{(r)}(0)=\mathbb{E} X^{r}\). We remember from Calculus II that certain functions \(f\) can be represented by a Taylor series expansion about a point \(a\), which takes the form
\begin{equation}
f(x)=\sum_{r=0}^{\infty}\frac{f^{(r)}(a)}{r!}(x-a)^{r},\quad \mbox{for all \(|x-a| < R\),}
\end{equation}
where \(R\) is called the /radius of convergence/ of the series (see Appendix [[sec-Sequences-and-Series][Sequences and Series]]). We combine the two to say that if an MGF exists for all \(t\) in the interval \((-\epsilon,\epsilon)\), then we can write
\begin{equation}
M_{X}(t)=\sum_{r=0}^{\infty}\frac{\mathbb{E} X^{r}}{r!}t^{r},\quad \mbox{for all $|t|<\epsilon$.}
\end{equation}
#+end_rem

**** How to do it with \(\mathsf{R}\)

The =distrEx= package \cite{distrEx} provides an expectation operator =E= which can be used on random variables that have been defined in the ordinary =distr= sense:

#+begin_src R :exports both :results output pp  
X <- Binom(size = 3, prob = 0.45)
E(X)
E(3*X + 4)
#+end_src

For discrete random variables with finite support, the expectation is simply computed with direct summation. In the case that the random variable has infinite support and the function is crazy, then the expectation is not computed directly, rather, it is estimated by first generating a random sample from the underlying model and next computing a sample mean of the function of interest. 

There are methods for other population parameters:

#+begin_src R :exports both :results output pp  
var(X)
sd(X)
#+end_src

There are even methods for =IQR=, =mad=, =skewness=, and =kurtosis=.

** The Empirical Distribution
:PROPERTIES:
:CUSTOM_ID: sec-empirical-distribution
:END:

Do an experiment \(n\) times and observe \(n\) values \(x_{1}\), \(x_{2}\), ..., \(x_{n}\) of a random variable \(X\). For simplicity in most of the discussion that follows it will be convenient to imagine that the observed values are distinct, but the remarks are valid even when the observed values are repeated. 

#+begin_defn
The /empirical cumulative distribution function/ \(F_{n}\) (written ECDF)\index{Empirical distribution} is the probability distribution that places probability mass \(1/n\) on each of the values \(x_{1}\), \(x_{2}\), ..., \(x_{n}\). The empirical PMF takes the form
\begin{equation} 
f_{X}(x)=\frac{1}{n},\quad x\in \{ x_{1},x_{2},...,x_{n} \}.
\end{equation}
If the value \(x_{i}\) is repeated \(k\) times, the mass at \(x_{i}\) is accumulated to \(k/n\).
#+end_defn

The mean of the empirical distribution is
\begin{equation}
\mu=\sum_{x\in S}xf_{X}(x)=\sum_{i=1}^{n}x_{i}\cdot\frac{1}{n}
\end{equation}
and we recognize this last quantity to be the sample mean, \(\overline{x}\). The variance of the empirical distribution is
\begin{equation}
\sigma^{2}=\sum_{x\in S}(x-\mu)^{2}f_{X}(x)=\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}\cdot\frac{1}{n}
\end{equation}
and this last quantity looks very close to what we already know to be the sample variance.
\begin{equation}
s^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}.
\end{equation}
The /empirical quantile function/ is the inverse of the ECDF. See Section [[sub-Normal-Quantiles-QF][Normal Quantiles]].

*** How to do it with \(\mathsf{R}\)

The empirical distribution is not directly available as a distribution in the same way that the other base probability distributions are, but there are plenty of resources available for the determined investigator.  Given a data vector of observed values =x=, we can see the empirical CDF with the =ecdf=\index{ecdf@\texttt{ecdf}} function:

#+begin_src R :exports both :results output pp  
x <- c(4, 7, 9, 11, 12)
ecdf(x)
#+end_src

The above shows that the returned value of =ecdf(x)= is not a /number/ but rather a /function/. The ECDF is not usually used by itself in this form. More commonly it is used as an intermediate step in a more complicated calculation, for instance, in hypothesis testing (see Chapter [[cha-Hypothesis-Testing][Hypothesis Testing]]) or resampling (see Chapter [[cha-resampling-methods][Resampling Methods]]). It is nevertheless instructive to see what the =ecdf= looks like, and there is a special plot method for =ecdf= objects.

#+name: empirical-CDF
#+begin_src R :exports code :results silent
plot(ecdf(x))
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/discdist/empirical-CDF.ps
  <<empirical-CDF>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/discdist/empirical-CDF.svg
  <<empirical-CDF>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/discdist/empirical-CDF.ps}
  \caption[The empirical CDF]{\small The empirical CDF.}
  \label{fig-empirical-CDF}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-empirical-CDF" class="figure">
  <p><img src="svg/discdist/empirical-CDF.svg" width=500 alt="svg/discdist/empirical-CDF.svg" /></p>
  <p>The empirical CDF.</p>
</div>
#+end_html

See Figure [[fig-empirical-CDF][empirical-CDF]]. The graph is of a right-continuous function with jumps exactly at the locations stored in =x=. There are no repeated values in =x= so all of the jumps are equal to \(1/5=0.2\).

The empirical PDF is not usually of particular interest in itself, but if we really wanted we could define a function to serve as the empirical PDF:

#+begin_src R :exports both :results output pp  
epdf <- function(x) function(t){sum(x %in% t)/length(x)}
x <- c(0,0,1)
epdf(x)(0)       # should be 2/3
#+end_src

To simulate from the empirical distribution supported on the vector =x=, we use the =sample=\index{sample@\texttt{sample}} function.

#+begin_src R :exports both :results output pp  
x <- c(0,0,1)
sample(x, size = 7, replace = TRUE)
#+end_src

We can get the empirical quantile function in \(\mathsf{R}\) with =quantile(x, probs = p, type = 1)=; see Section [[sub-Normal-Quantiles-QF][Normal Quantiles]].

As we hinted above, the empirical distribution is significant more because of how and where it appears in more sophisticated applications. We will explore some of these in later chapters -- see, for instance, Chapter [[cha-resampling-methods][Resampling Methods]].

** Other Discrete Distributions
:PROPERTIES:
:CUSTOM_ID: sec-other-discrete-distributions
:END:

The binomial and discrete uniform distributions are popular, and rightly so; they are simple and form the foundation for many other more complicated distributions. But the particular uniform and binomial models only apply to a limited range of problems. In this section we introduce situations for which we need more than what the uniform and binomial offer.

*** Dependent Bernoulli Trials
:PROPERTIES:
:CUSTOM_ID: sec-non-bernoulli-trials
:END:

**** The Hypergeometric Distribution
:PROPERTIES:
:CUSTOM_ID: sub-hypergeometric-dist
:END:

Consider an urn with 7 white balls and 5 black balls. Let our random experiment be to randomly select 4 balls, without replacement, from the urn. Then the probability of observing 3 white balls (and thus 1 black ball) would be
\begin{equation}
\mathbb{P}(3W,1B)=\frac{{7 \choose 3}{5 \choose 1}}{{12 \choose 4}}.
\end{equation}
More generally, we sample without replacement \(K\) times from an urn with \(M\) white balls and \(N\) black balls. Let \(X\) be the number of white balls in the sample. The PMF of \(X\) is
\begin{equation}
f_{X}(x)=\frac{{M \choose x}{N \choose K-x}}{{M+N \choose K}}.
\end{equation}
We say that \(X\) has a /hypergeometric distribution/ and write \(X\sim\mathsf{hyper}(\mathtt{m}=M,\,\mathtt{n}=N,\,\mathtt{k}=K)\).

The support set for the hypergeometric distribution is a little bit tricky. It is tempting to say that \(x\) should go from 0 (no white balls in the sample) to \(K\) (no black balls in the sample), but that does not work if \(K>M\), because it is impossible to have more white balls in the sample than there were white balls originally in the urn. We have the same trouble if \(K>N\). The good news is that the majority of examples we study have \(K\leq M\) and \(K\leq N\) and we will happily take the support to be \(x=0,\ 1,\ \ldots,\ K\). 

It is shown in Exercise [[xca-hyper-mean-variance][Hyper-MV]] that
\begin{equation}
\mu=K\frac{M}{M+N},\quad \sigma^{2}=K\frac{MN}{(M+N)^{2}}\frac{M+N-K}{M+N-1}.
\end{equation}

The associated \(\mathsf{R}\) functions for the PMF and CDF are =dhyper(x, m, n, k)= and =phyper=, respectively. There are two more functions: =qhyper=, which we will discuss in Section [[sub-Normal-Quantiles-QF][Normal Quantiles]], and =rhyper=, discussed below.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Suppose in a certain shipment of 250 Pentium processors there are 17 defective processors. A quality control consultant randomly collects 5 processors for inspection to determine whether or not they are defective. Let \(X\) denote the number of defectives in the sample.

Find the probability of exactly 3 defectives in the sample, that is, find \(\mathbb{P}(X=3)\). 
   /Solution:/ We know that \(X\sim\mathsf{hyper}(\mathtt{m}=17,\,\mathtt{n}=233,\,\mathtt{k}=5)\). So the required probability is just
   \[
   f_{X}(3)=\frac{{17 \choose 3}{233 \choose 2}}{{250 \choose 5}}.
   \]
   To calculate it in \(\mathsf{R}\) we just type 

   #+begin_src R :exports both :results output pp  
   dhyper(3, m = 17, n = 233, k = 5)
   #+end_src

   To find it with the \(\mathsf{R}\) Commander we go =Probability= \(\triangleright\) =Discrete Distributions= \(\triangleright\) =Hypergeometric distribution= \(\triangleright\) =Hypergeometric probabilities...=. We fill in the parameters \(m=17\), \(n=233\), and \(k=5\). Click =OK=, and the following table is shown in the window.

   #+begin_src R :exports both :results output pp  
   A <- data.frame(Pr=dhyper(0:4, m = 17, n = 233, k = 5))
   rownames(A) <- 0:4 
   A
   #+end_src

   We wanted \(\mathbb{P}(X=3)\), and this is found from the table to be approximately 0.0024. The value is rounded to the fourth decimal place.
   We know from our above discussion that the sample space should be \(x=0,1,2,3,4,5\), yet, in the table the probabilities are only displayed for \(x = 1,2,3,\) and 4. What is happening? As it turns out, the \(\mathsf{R}\) Commander will only display probabilities that are 0.00005 or greater. Since \(x=5\) is not shown, it suggests that the outcome has a tiny probability. To find its exact value we use the =dhyper= function:
   #+begin_src R :exports both :results output pp  
   dhyper(5, m = 17, n = 233, k = 5)
   #+end_src
   In other words, \(\mathbb{P}(X=5)\approx0.0000007916049\), a small number indeed.
Find the probability that there are at most 2 defectives in the sample, that is, compute \(\mathbb{P}(X\leq2)\).
   /Solution:/ Since \(\mathbb{P}(X\leq2)=\mathbb{P}(X=0,1,2)\), one way to do this would be to add the 0, 1, and 2 entries in the above table. this gives \(0.7011+0.2602+0.0362=0.9975\). Our answer should be correct up to the accuracy of 4 decimal places. However, a more precise method is provided by the \(\mathsf{R}\) Commander. Under the =Hypergeometric distribution= menu we select =Hypergeometric tail probabilities...=. We fill in the parameters \(m\), \(n\), and \(k\) as before, but in the =Variable value(s)= dialog box we enter the value 2. We notice that the =Lower tail= option is checked, and we leave that alone. Click =OK=.

   #+begin_src R :exports both :results output pp  
   phyper(2, m = 17, n = 233, k = 5)
   #+end_src

   And thus \(\mathbb{P}(X\leq2)\approx 0.9975771\). We have confirmed that the above answer was correct up to four decimal places.
Find \(\mathbb{P}(X>1)\). 
   The table did not give us the explicit probability \(\mathbb{P}(X=5)\), so we can not use the table to give us this probability. We need to use another method. Since \(\mathbb{P}(X>1)=1-\mathbb{P}(X\leq1)=1-F_{X}(1)\), we can find the probability with =Hypergeometric tail probabilities...=. We enter 1 for =Variable Value(s)=, we enter the parameters as before, and in this case we choose the =Upper tail= option. This results in the following output.

   #+begin_src R :exports both :results output pp  
   phyper(1, m = 17, n = 233, k = 5, lower.tail = FALSE)
   #+end_src

   In general, the =Upper tail= option of a tail probabilities dialog computes \(\mathbb{P}(X > x)\) for all given =Variable Value(s)= \(x\).
Generate \(100,000\) observations of the random variable \(X\).
   We can randomly simulate as many observations of \(X\) as we want in \(\mathsf{R}\) Commander. Simply choose =Simulate hypergeometric variates...= in the =Hypergeometric distribution= dialog. 
   In the =Number of samples= dialog, type 1. Enter the parameters as above. Under the =Store Values= section, make sure =New Data set= is selected. Click =OK=. 
   A new dialog should open, with the default name =Simset1=.  We could change this if we like, according to the rules for \(\mathsf{R}\) object names. In the sample size box, enter 100000. Click =OK=. 
   In the Console Window, \(\mathsf{R}\) Commander should issue an alert that =Simset1= has been initialized, and in a few seconds, it should also state that 100,000 hypergeometric variates were stored in =hyper.sim1=. We can view the sample by clicking the =View Data Set= button on the \(\mathsf{R}\) Commander interface.
   We know from our formulas that \(\mu=K\cdot M/(M+N)=5*17/250=0.34\). We can check our formulas using the fact that with repeated observations of \(X\) we would expect about 0.34 defectives on the average. To see how our sample reflects the true mean, we can compute the sample mean
   :  Rcmdr> mean(Simset2$hyper.sim1, na.rm=TRUE)
   :  [1] 0.340344
   
   :  Rcmdr> sd(Simset2$hyper.sim1, na.rm=TRUE)
   :  [1] 0.5584982
   :  ...
   We see that when given many independent observations of \(X\), the sample mean is very close to the true mean \(\mu\). We can repeat the same idea and use the sample standard deviation to estimate the true standard deviation of \(X\). From the output above our estimate is 0.5584982, and from our formulas we get
   \[
   \sigma^{2}=K\frac{MN}{(M+N)^{2}}\frac{M+N-K}{M+N-1}\approx0.3117896,
   \]
   with \(\sigma=\sqrt{\sigma^{2}}\approx0.5583811944\). Our estimate was pretty close.
   From the console we can generate random hypergeometric variates with the =rhyper= function, as demonstrated below.

   #+begin_src R :exports both :results output pp  
   rhyper(10, m = 17, n = 233, k = 5)
   #+end_src

#+latex: \end{exampletoo}
#+html: </div>

**** Sampling With and Without Replacement
:PROPERTIES:
:CUSTOM_ID: sub-Sampling-With-and
:END:

Suppose that we have a large urn with, say, \(M\) white balls and \(N\) black balls. We take a sample of size \(n\) from the urn, and let \(X\) count the number of white balls in the sample. If we sample
- without replacement, :: then \(X\sim\mathsf{hyper}(\mathtt{m=}M,\,\mathtt{n}=N,\,\mathtt{k}=n)\) and has mean and variance
     \begin{alignat*}{1}
     \mu= & n\frac{M}{M+N},\\
     \sigma^{2}= & n\frac{MN}{(M+N)^{2}}\frac{M+N-n}{M+N-1},\\
     = & n\frac{M}{M+N}\left(1-\frac{M}{M+N}\right)\frac{M+N-n}{M+N-1}.
     \end{alignat*}
On the other hand, if we sample
- with replacement, :: then \(X\sim\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=M/(M+N))\) with mean and variance
     \begin{alignat*}{1}
     \mu= & n\frac{M}{M+N},\\
     \sigma^{2}= & n\frac{M}{M+N}\left(1-\frac{M}{M+N}\right).
     \end{alignat*}
We see that both sampling procedures have the same mean, and the method with the larger variance is the ``with replacement'' scheme. The factor by which the variances differ,
\begin{equation}
\frac{M+N-n}{M+N-1},
\end{equation}
is called a /finite population correction/. For a fixed sample size \(n\), as \(M,N\to\infty\) it is clear that the correction goes to 1, that is, for infinite populations the sampling schemes are essentially the same with respect to mean and variance.

*** Waiting Time Distributions
:PROPERTIES:
:CUSTOM_ID: sec-Waiting-Time-Distributions
:END:

Another important class of problems is associated with the amount of time it takes for a specified event of interest to occur. For example, we could flip a coin repeatedly until we observe Heads. We could toss a piece of paper repeatedly until we make it in the trash can.

**** The Geometric Distribution
:PROPERTIES:
:CUSTOM_ID: sub-The-Geometric-Distribution
:END:

Suppose that we conduct Bernoulli trials repeatedly, noting the successes and failures. Let \(X\) be the number of failures before a success. If \(\mathbb{P}(S)=p\) then \(X\) has PMF
\begin{equation}
f_{X}(x)=p(1-p)^{x},\quad x=0,1,2,\ldots
\end{equation}
(Why?) We say that \(X\) has a /Geometric distribution/ and we write \(X\sim\mathsf{geom}(\mathtt{prob}=p)\). The associated \(\mathsf{R}\) functions are =dgeom(x, prob)=, =pgeom=, =qgeom=, and =rhyper=, which give the PMF, CDF, quantile function, and simulate random variates, respectively.

Again it is clear that \(f(x)\geq0\) and we check that \(\sum f(x)=1\) (see Equation [[eq-geom-series][Geometric Series]] in Appendix [[sec-Sequences-and-Series][Sequences and Series]]):
\begin{alignat*}{1}
\sum_{x=0}^{\infty}p(1-p)^{x}= & p\sum_{x=0}^{\infty}q^{x}=p\,\frac{1}{1-q}=1.
\end{alignat*}
We will find in the next section that the mean and variance are
\begin{equation}
\mu=\frac{1-p}{p}=\frac{q}{p}\mbox{ and }\sigma^{2}=\frac{q}{p^{2}}.
\end{equation}


#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
The Pittsburgh Steelers place kicker, Jeff Reed, made 81.2% of his attempted field goals in his career up to 2006. Assuming that his successive field goal attempts are approximately Bernoulli trials, find the probability that Jeff misses at least 5 field goals before his first successful goal.

/Solution/: If \(X=\) the number of missed goals until Jeff's first success, then \(X\sim\mathsf{geom}(\mathtt{prob}=0.812)\) and we want \(\mathbb{P}(X\geq5)=\mathbb{P}(X>4)\). We can find this in \(\mathsf{R}\) with

#+begin_src R :exports both :results output pp  
pgeom(4, prob = 0.812, lower.tail = FALSE)
#+end_src

#+latex: \end{exampletoo}
#+html: </div>

#+begin_note
Some books use a slightly different definition of the geometric distribution. They consider Bernoulli trials and let \(Y\) count instead the number of trials until a success, so that \(Y\) has PMF
\begin{equation}
f_{Y}(y)=p(1-p)^{y-1},\quad y=1,2,3,\ldots
\end{equation}
When they say ``geometric distribution'', this is what they mean. It is not hard to see that the two definitions are related. In fact, if \(X\) denotes our geometric and \(Y\) theirs, then \(Y=X+1\). Consequently, they have \(\mu_{Y}=\mu_{X}+1\) and \(\sigma_{Y}^{2}=\sigma_{X}^{2}\).
#+end_note

**** The Negative Binomial Distribution
:PROPERTIES:
:CUSTOM_ID: sub-The-Negative-Binomial
:END:

We may generalize the problem and consider the case where we wait for /more/ than one success. Suppose that we conduct Bernoulli trials repeatedly, noting the respective successes and failures. Let \(X\) count the number of failures before \(r\) successes. If \(\mathbb{P}(S)=p\) then \(X\) has PMF
\begin{equation}
f_{X}(x)={r+x-1 \choose r-1}\, p^{r}(1-p)^{x},\quad x=0,1,2,\ldots
\end{equation}

We say that \(X\) has a /Negative Binomial distribution/ and write \(X\sim\mathsf{nbinom}(\mathtt{size}=r,\,\mathtt{prob}=p)\). The associated \(\mathsf{R}\) functions are =dnbinom(x, size, prob)=, =pnbinom=, =qnbinom=, and =rnbinom=, which give the PMF, CDF, quantile function, and simulate random variates, respectively.

As usual it should be clear that \(f_{X}(x)\geq 0\) and the fact that \(\sum f_{X}(x)=1\) follows from a generalization of the geometric series by means of a Maclaurin's series expansion:
\begin{alignat}{1}
\frac{1}{1-t}= & \sum_{k=0}^{\infty}t^{k},\quad \mbox{for \(-1 < t < 1\)},\mbox{ and}\\
\frac{1}{(1-t)^{r}}= & \sum_{k=0}^{\infty}{r+k-1 \choose r-1}\, t^{k},\quad \mbox{for \(-1 < t < 1\)}.
\end{alignat}
Therefore
\begin{equation}
\sum_{x=0}^{\infty}f_{X}(x)=p^{r}\sum_{x=0}^{\infty}{r+x-1 \choose r-1}\, q^{x}=p^{r}(1-q)^{-r}=1,
\end{equation}
since \(|q|=|1-p|<1\). 

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
We flip a coin repeatedly and let \(X\) count the number of Tails until we get seven Heads. What is \(\mathbb{P}(X=5)?\)
/Solution/: We know that \(X\sim\mathsf{nbinom}(\mathtt{size}=7,\,\mathtt{prob}=1/2)\).
\[
\mathbb{P}(X=5)=f_{X}(5)={7+5-1 \choose 7-1}(1/2)^{7}(1/2)^{5}={11 \choose 6}2^{-12}
\]
and we can get this in \(\mathsf{R}\) with

#+begin_src R :exports both :results output pp  
dnbinom(5, size = 7, prob = 0.5)
#+end_src

Let us next compute the MGF of \(X\sim\mathsf{nbinom}(\mathtt{size}=r,\,\mathtt{prob}=p)\).
\begin{alignat*}{1}
M_{X}(t)= & \sum_{x=0}^{\infty}\mathrm{e}^{tx}\ {r+x-1 \choose r-1}p^{r}q^{x}\\
= & p^{r}\sum_{x=0}^{\infty}{r+x-1 \choose r-1}[q\mathrm{e}^{t}]^{x}\\
= & p^{r}(1-qe^{t})^{-r},\quad \mbox{provided $|q\mathrm{e}^{t}|<1$,}
\end{alignat*}
and so
\begin{equation}
M_{X}(t)=\left(\frac{p}{1-q\mathrm{e}^{t}}\right)^{r},\quad \mbox{for $q\mathrm{e}^{t}<1$}.
\end{equation}
We see that \(q\mathrm{e}^{t}<1\) when \(t<-\ln(1-p)\).

Let \(X\sim\mathsf{nbinom}(\mathtt{size}=r,\mathtt{prob}=p)\mbox{ with $M(t)=p^{r}(1-q\mathrm{e}^{t})^{-r}$}\). We proclaimed above the values of the mean and variance. Now we are equipped with the tools to find these directly.
\begin{alignat*}{1}
M'(t)= & p^{r}(-r)(1-q\mathrm{e}^{t})^{-r-1}(-q\mathrm{e}^{t}),\\
= & rq\mathrm{e}^{t}p^{r}(1-q\mathrm{e}^{t})^{-r-1},\\
= & \frac{rq\mathrm{e}^{t}}{1-q\mathrm{e}^{t}}M(t),\mbox{ and so }\\
M'(0)= & \frac{rq}{1-q}\cdot1=\frac{rq}{p}.
\end{alignat*}
Thus \(\mu=rq/p\). We next find \(\mathbb{E} X^{2}\).
\begin{alignat*}{1}
M''(0)= & \left.\frac{rq\mathrm{e}^{t}(1-q\mathrm{e}^{t})-rq\mathrm{e}^{t}(-q\mathrm{e}^{t})}{(1-q\mathrm{e}^{t})^{2}}M(t)+\frac{rq\mathrm{e}^{t}}{1-q\mathrm{e}^{t}}M'(t)\right|_{t=0},\\
= & \frac{rqp+rq^{2}}{p^{2}}\cdot1+\frac{rq}{p}\left(\frac{rq}{p}\right),\\
= & \frac{rq}{p^{2}}+\left(\frac{rq}{p}\right)^{2}.
\end{alignat*}
Finally we may say \( \sigma^{2} = M''(0) - [M'(0)]^{2} = rq/p^{2}. \)
#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
A random variable has MGF
\[
M_{X}(t)=\left(\frac{0.19}{1-0.81\mathrm{e}^{t}}\right)^{31}.
\]
Then \(X\sim\mathsf{nbinom}(\mathtt{size}=31,\,\mathtt{prob}=0.19)\).
#+latex: \end{exampletoo}
#+html: </div>

#+begin_note
As with the Geometric distribution, some books use a slightly different definition of the Negative Binomial distribution. They consider Bernoulli trials and let \(Y\) be the number of trials until \(r\) successes, so that \(Y\) has PMF
\begin{equation}
f_{Y}(y)={y-1 \choose r-1}p^{r}(1-p)^{y-r},\quad y=r,r+1,r+2,\ldots
\end{equation}
It is again not hard to see that if \(X\) denotes our Negative Binomial and \(Y\) theirs, then \(Y=X+r\). Consequently, they have \(\mu_{Y}=\mu_{X}+r\) and \(\sigma_{Y}^{2}=\sigma_{X}^{2}\).
#+end_note

*** Arrival Processes
:PROPERTIES:
:CUSTOM_ID: sec-Arrival-Processes
:END:

**** The Poisson Distribution
:PROPERTIES:
:CUSTOM_ID: sub-The-Poisson-Distribution
:END:

This is a distribution associated with ``rare events'', for reasons which will become clear in a moment. The events might be:
- traffic accidents,
- typing errors, or
- customers arriving in a bank.


Let \(\lambda\) be the average number of events in the time interval \([0,1]\). Let the random variable \(X\) count the number of events occurring in the interval. Then under certain reasonable conditions it can be shown that
\begin{equation}
f_{X}(x)=\mathbb{P}(X=x)=\mathrm{e}^{-\lambda}\frac{\lambda^{x}}{x!},\quad x=0,1,2,\ldots
\end{equation}
We use the notation \(X\sim\mathsf{pois}(\mathtt{lambda}=\lambda)\). The associated \(\mathsf{R}\) functions are =dpois(x, lambda)=, =ppois=, =qpois=, and =rpois=, which give the PMF, CDF, quantile function, and simulate random variates, respectively.

**** What are the reasonable conditions?

Divide \([0,1]\) into subintervals of length \(1/n\). A /Poisson process/\index{Poisson process} satisfies the following conditions:
- the probability of an event occurring in a particular subinterval is \(\approx\lambda/n\).
- the probability of two or more events occurring in any subinterval is \(\approx 0\).
- occurrences in disjoint subintervals are independent.

#+begin_rem
# <<rem-poisson-process>>

If \(X\) counts the number of events in the interval \([0,t]\) and \(\lambda\) is the average number that occur in unit time, then \(X\sim\mathsf{pois}(\mathtt{lambda}=\lambda t)\), that is,
\begin{equation}
\mathbb{P}(X=x)=\mathrm{e}^{-\lambda t}\frac{(\lambda t)^{x}}{x!},\quad x=0,1,2,3\ldots
\end{equation}
#+end_rem

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
On the average, five cars arrive at a particular car wash every hour. Let \(X\) count the number of cars that arrive from 10AM to 11AM. Then \(X\sim\mathsf{pois}(\mathtt{lambda}=5)\). Also, \(\mu=\sigma^{2}=5\). What is the probability that no car arrives during this period? 
/Solution/: The probability that no car arrives is
\[
\mathbb{P}(X=0)=\mathrm{e}^{-5}\frac{5^{0}}{0!}=\mathrm{e}^{-5}\approx0.0067.
\]
#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Suppose the car wash above is in operation from 8AM to 6PM, and we let \(Y\) be the number of customers that appear in this period. Since this period covers a total of 10 hours, from Remark [[rem-poisson-process][Poisson Process]] we get that \(Y\sim\mathsf{pois}(\mathtt{lambda}=5\ast10=50)\). What is the probability that there are between 48 and 50 customers, inclusive? 
/Solution/: We want \(\mathbb{P}(48\leq Y\leq50)=\mathbb{P}(X\leq50)-\mathbb{P}(X\leq47)\). 

#+begin_src R :exports both :results output pp  
diff(ppois(c(47, 50), lambda = 50))
#+end_src

#+latex: \end{exampletoo}
#+html: </div>

** Functions of Discrete Random Variables
:PROPERTIES:
:CUSTOM_ID: sec-functions-discrete-rvs
:END:

We have built a large catalogue of discrete distributions, but the tools of this section will give us the ability to consider infinitely many more. Given a random variable \(X\) and a given function \(h\), we may consider \(Y=h(X)\). Since the values of \(X\) are determined by chance, so are the values of \(Y\). The question is, what is the PMF of the random variable \(Y\)? The answer, of course, depends on \(h\). In the case that \(h\) is one-to-one (see Appendix [[sec-Differential-and-Integral][Differential-Integral Calculus]]), the solution can be found by simple substitution.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Let \(X\sim\mathsf{nbinom}(\mathtt{size}=r,\,\mathtt{prob}=p)\). We saw in [[sec-other-discrete-distributions][Other Discrete Distributions]] that \(X\) represents the number of failures until \(r\) successes in a sequence of Bernoulli trials. Suppose now that instead we were interested in counting the number of trials (successes and failures) until the \(r^{\mathrm{th}}\) success occurs, which we will denote by \(Y\). In a given performance of the experiment, the number of failures (\(X\)) and the number of successes (\(r\)) together will comprise the total number of trials (\(Y\)), or in other words, \(X+r=Y\). We may let \(h\) be defined by \(h(x)=x+r\) so that \(Y=h(X)\), and we notice that \(h\) is linear and hence one-to-one. Finally, \(X\) takes values \(0,\ 1,\ 2,\ldots\) implying that the support of \(Y\) would be \(\{ r,\ r+1,\ r+2,\ldots \}\). Solving for \(X\) we get \(X=Y-r\). Examining the PMF of \(X\)
\begin{equation}
f_{X}(x)={r+x-1 \choose r-1}\, p^{r}(1-p)^{x},
\end{equation}
we can substitute \( x = y - r \) to get
\begin{eqnarray*}
f_{Y}(y) & = & f_{X}(y-r),\\
 & = & {r+(y-r)-1 \choose r-1}\, p^{r}(1-p)^{y-r},\\
 & = & {y-1 \choose r-1}\, p^{r}(1-p)^{y-r},\quad y=r,\, r+1,\ldots
\end{eqnarray*}
#+latex: \end{exampletoo}
#+html: </div>


Even when the function \(h\) is not one-to-one, we may still find the PMF of \(Y\) simply by accumulating, for each \(y\), the probability of all the \(x\)'s that are mapped to that \(y\).
#+begin_prop
Let \(X\) be a discrete random variable with PMF \(f_{X}\) supported on the set \(S_{X}\). Let \(Y=h(X)\) for some function \(h\). Then \(Y\) has PMF \(f_{Y}\) defined by
\begin{equation}
f_{Y}(y)=\sum_{\{x\in S_{X}|\, h(x)=y\}}f_{X}(x)
\end{equation}
#+end_prop

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Let \(X\sim\mathsf{binom}(\mathtt{size}=4,\,\mathtt{prob}=1/2)\), and let \(Y=(X-1)^{2}\). Consider the following table:

#+LABEL: tab-disc-transf
#+CAPTION: [Transform discrete random variable]{Transforming a discrete random variable.}
| x               |    0 |   1 |    2 |   3 |    4 |
|-----------------+------+-----+------+-----+------|
| \(f_{X}(x)\)    | 1/16 | 1/4 | 6/16 | 1/4 | 1/16 |
|-----------------+------+-----+------+-----+------|
| \(y=(x-1)^{2}\) |    1 |   0 |    1 |   4 |    9 |

From this we see that \(Y\) has support \(S_{Y}=\{0,1,4,9\}\). We also see that \(h(x)=(x-1)^{2}\) is not one-to-one on the support of \(X\), because both \(x=0\) and \(x=2\) are mapped by \(h\) to \(y=1\). Nevertheless, we see that \(Y=0\) only when \(X=1\), which has probability \(1/4\); therefore, \(f_{Y}(0)\) should equal \(1/4\). A similar approach works for \(y=4\) and \(y=9\). And \(Y=1\) exactly when \(X=0\) or \(X=2\), which has total probability \(7/16\). In summary, the PMF of \(Y\) may be written:

#+LABEL: tab-disc-transf-pmf
#+CAPTION: [Transforming discrete random variable: PMF]{Transforming a discrete random variable, its PMF.}
| y            |   0 |    1 |   4 |    9 |
|--------------+-----+------+-----+------|
| \(f_{Y}(y)\) | 1/4 | 7/16 | 1/4 | 1/16 |

There is not a special name for the distribution of \(Y\), it is just an example of what to do when the transformation of a random variable is not one-to-one. The method is the same for more complicated problems.
#+latex: \end{exampletoo}
#+html: </div>

#+begin_prop
If \(X\) is a random variable with \(\mathbb{E} X=\mu\) and \(\mbox{Var}(X)=\sigma^{2}\), then the mean and variance of \(Y=mX+b\) is
\begin{equation}
\mu_{Y}=m\mu+b,\quad \sigma_{Y}^{2}=m^{2}\sigma^{2},\quad \sigma_{Y}=|m|\sigma.
\end{equation}
#+end_prop

#+latex: \newpage{}

** Exercises
#+latex: \setcounter{thm}{0}

#+begin_xca
A recent national study showed that approximately 44.7% of college students have used Wikipedia as a source in at least one of their term papers. Let \(X\) equal the number of students in a random sample of size \(n=31\) who have used Wikipedia as a source. 
- How is \(X\) distributed? 
- Sketch the probability mass function (roughly).
- Sketch the cumulative distribution function (roughly).
- Find the probability that \(X\) is equal to 17.
- Find the probability that \(X\) is at most 13.
- Find the probability that \(X\) is bigger than 11.
- Find the probability that \(X\) is at least 15.
- Find the probability that \(X\) is between 16 and 19, inclusive.
- Give the mean of \(X\), denoted \(\mathbb{E} X\).
- Give the variance of \(X\).
- Give the standard deviation of \(X\).
- Find \(\mathbb{E}(4X+51.324)\).
#+end_xca

#+begin_xca
For the following situations, decide what the distribution of \(X\) should be. In nearly every case, there are additional assumptions that should be made for the distribution to apply; identify those assumptions (which may or may not hold in practice.)
- We shoot basketballs at a basketball hoop, and count the number of shots until we make a goal. Let \(X\) denote the number of missed shots. On a normal day we would typically make about 37% of the shots.
- In a local lottery in which a three digit number is selected randomly, let \(X\) be the number selected.
- We drop a Styrofoam cup to the floor twenty times, each time recording whether the cup comes to rest perfectly right side up, or not. Let \(X\) be the number of times the cup lands perfectly right side up.
- We toss a piece of trash at the garbage can from across the room. If we miss the trash can, we retrieve the trash and try again, continuing to toss until we make the shot. Let \(X\) denote the number of missed shots.
- Working for the border patrol, we inspect shipping cargo as when it enters the harbor looking for contraband. A certain ship comes to port with 557 cargo containers. Standard practice is to select 10 containers randomly and inspect each one very carefully, classifying it as either having contraband or not. Let \(X\) count the number of containers that illegally contain contraband.
- At the same time every year, some migratory birds land in a bush outside for a short rest. On a certain day, we look outside and let \(X\) denote the number of birds in the bush. 
- We count the number of rain drops that fall in a circular area on a sidewalk during a ten minute period of a thunder storm.
- We count the number of moth eggs on our window screen.
- We count the number of blades of grass in a one square foot patch of land.
- We count the number of pats on a baby's back until (s)he burps.
#+end_xca

#+begin_xca
# <<xca-variance-shortcut>>
Show that \(\mathbb{E}(X-\mu)^{2}=\mathbb{E} X^{2}-\mu^{2}\). /Hint/: expand the quantity \((X-\mu)^{2}\) and distribute the expectation over the resulting terms.
#+end_xca

#+begin_xca
# <<xca-binom-factorial-expectation>>
If \(X\sim\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p)\) show that \(\mathbb{E} X(X-1)=n(n-1)p^{2}\).
#+end_xca

#+begin_xca
# <<xca-hyper-mean-variance>>
Calculate the mean and variance of the hypergeometric distribution. Show that 
\begin{equation}
\mu=K\frac{M}{M+N},\quad \sigma^{2}=K\frac{MN}{(M+N)^{2}}\frac{M+N-K}{M+N-1}.
\end{equation}
#+end_xca

* Continuous Distributions                                         :contdist:
:PROPERTIES:
:tangle: R/contdist.R
:CUSTOM_ID: cha-Continuous-Distributions
:END:

#+begin_src R :exports none :eval never
# Chapter: Continuous Distributions
# All code released under GPL Version 3
#+end_src

#+latex: \noindent 
The focus of the last chapter was on random variables whose support can be written down in a list of values (finite or countably infinite), such as the number of successes in a sequence of Bernoulli trials. Now we move to random variables whose support is a whole range of values, say, an interval \((a,b)\). It is shown in later classes that it is impossible to write all of the numbers down in a list; there are simply too many of them.

This chapter begins with continuous random variables and the associated PDFs and CDFs The continuous uniform distribution is highlighted, along with the Gaussian, or normal, distribution. Some mathematical details pave the way for a catalogue of models.

The interested reader who would like to learn more about any of the assorted discrete distributions mentioned below should take a look at /Continuous Univariate Distributions, Volumes 1/ and /2/ by Johnson /et al/ \cite{Johnson1994,Johnson1995}.

*What do I want them to know?*
- how to choose a reasonable continuous model under a variety of physical circumstances
- basic correspondence between continuous versus discrete random variables
- the general tools of the trade for manipulation of continuous random variables, integration, /etc/.
- some details on a couple of continuous models, and exposure to a bunch of other ones
- how to make new continuous random variables from old ones

** Continuous Random Variables
:PROPERTIES:
:CUSTOM_ID: sec-continuous-random-variables
:END:

*** Probability Density Functions
:PROPERTIES:
:CUSTOM_ID: sub-probability-density-functions
:END:

Continuous random variables have supports that look like
\begin{equation}
S_{X}=[a,b]\mbox{ or }(a,b),
\end{equation}
or unions of intervals of the above form. Examples of random variables that are often taken to be continuous are:

- the height or weight of an individual,
- other physical measurements such as the length or size of an object, and
- durations of time (usually).

Every continuous random variable \(X\) has a /probability density function/ (PDF) denoted \(f_{X}\) associated with it
#+latex: \footnote{Not true. There are pathological random variables with no density function. (This is one of the crazy things that can happen in the world of measure theory). But in this book we will not get even close to these anomalous beasts, and regardless it can be proved that the CDF always exists.}
that satisfies three basic properties:
1. \(f_{X}(x)>0\) for \(x\in S_{X}\),
2. \(\int_{x\in S_{X}}f_{X}(x)\,\mathrm{d} x=1\), and
3. \(\mathbb{P}(X\in A)=\int_{x\in A}f_{X}(x)\:\mathrm{d} x\), for an event \(A\subset S_{X}\).
# <<enu-contrvcond3>>

#+begin_rem
We can say the following about continuous random variables:
- Usually, the set \(A\) in [[enu-contrvcond3]] takes the form of an interval, for example, \(A=[c,d]\), in which case
  \begin{equation}
  \mathbb{P}(X\in A)=\int_{c}^{d}f_{X}(x)\:\mathrm{d} x.
  \end{equation}
- It follows that the probability that \(X\) falls in a given interval is simply the /area under the curve/ of \(f_{X}\) over the interval.
- Since the area of a line \(x=c\) in the plane is zero, \(\mathbb{P}(X=c)=0\)  for any value \(c\). In other words, the chance that \(X\) equals a particular value \(c\) is zero, and this is true for any number \(c\). Moreover, when \(a<b\) all of the following probabilities are the same:
  \begin{equation}
  \mathbb{P}(a\leq X\leq b)=\mathbb{P}(a<X\leq b)=\mathbb{P}(a\leq X<b)=\mathbb{P}(a<X<b).
  \end{equation}
- The PDF \(f_{X}\) can sometimes be greater than 1. This is in contrast to the discrete case; every nonzero value of a PMF is a probability which is restricted to lie in the interval \([0,1]\).
#+end_rem

We met the cumulative distribution function, \(F_{X}\), in Chapter [[cha-Discrete-Distributions][Discrete Distributions]]. Recall that it is defined by \(F_{X}(t)=\mathbb{P}(X\leq t)\), for \(-\infty<t<\infty\). While in the discrete case the CDF is unwieldy, in the continuous case the CDF has a relatively convenient form:
\begin{equation}
F_{X}(t)=\mathbb{P}(X\leq t)=\int_{-\infty}^{t}f_{X}(x)\:\mathrm{d} x,\quad -\infty < t < \infty.
\end{equation}

#+begin_rem
For any continuous CDF \(F_{X}\) the following are true.
- \(F_{X}\) is nondecreasing , that is, \(t_{1}\leq t_{2}\) implies \(F_{X}(t_{1})\leq F_{X}(t_{2})\).
- \(F_{X}\) is continuous (see Appendix [[sec-Differential-and-Integral][Differential-Integral Calculus]]). Note the distinction from the discrete case: CDFs of discrete random variables are not continuous, they are only right continuous.
- \(\lim_{t\to-\infty}F_{X}(t)=0\) and \(\lim_{t\to\infty}F_{X}(t)=1\).
#+end_rem

There is a handy relationship between the CDF and PDF in the continuous case. Consider the derivative of \(F_{X}\):
\begin{equation}
F'_{X}(t)=\frac{\mathrm{d}}{\mathrm{d} t}F_{X}(t)=\frac{\mathrm{d}}{\mathrm{d} t}\,\int_{-\infty}^{t}f_{X}(x)\,\mathrm{d} x=f_{X}(t),
\end{equation}
the last equality being true by the Fundamental Theorem of Calculus, part (2) (see Appendix [[sec-Differential-and-Integral][Differential-Integral Calculus]]). In short, \((F_{X})'=f_{X}\) in the continuous case
#+latex: \footnote{In the discrete case, \(f_{X}(x)=F_{X}(x)-\lim_{t\to x^{-}}F_{X}(t)\).}. 

*** Expectation of Continuous Random Variables
:PROPERTIES:
:CUSTOM_ID: sub-Expectation-of-Continuous
:END:

For a continuous random variable \(X\) the expected value of \(g(X)\) is
\begin{equation}
\mathbb{E} g(X)=\int_{x\in S}g(x)f_{X}(x)\:\mathrm{d} x,
\end{equation}
provided the (potentially improper) integral \(\int_{S}|g(x)|\, f(x)\mathrm{d} x\) is convergent. One important example is the mean \(\mu\), also known as \(\mathbb{E} X\):
\begin{equation}
\mu=\mathbb{E} X=\int_{x\in S}xf_{X}(x)\:\mathrm{d} x,
\end{equation}
provided \(\int_{S}|x|f(x)\mathrm{d} x\) is finite. Also there is the variance
\begin{equation}
\sigma^{2}=\mathbb{E}(X-\mu)^{2}=\int_{x\in S}(x-\mu)^{2}f_{X}(x)\,\mathrm{d} x,
\end{equation}
which can be computed with the alternate formula \(\sigma^{2}=\mathbb{E} X^{2}-(\mathbb{E} X)^{2}\). In addition, there is the standard deviation \(\sigma=\sqrt{\sigma^{2}}\). The moment generating function is given by
\begin{equation}
M_{X}(t)=\mathbb{E}\:\mathrm{e}^{tX}=\int_{-\infty}^{\infty}\mathrm{e}^{tx}f_{X}(x)\:\mathrm{d} x,
\end{equation}
provided the integral exists (is finite) for all \(t\) in a neighborhood of \(t=0\).

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-cont-pdf3x2>>

Let the continuous random variable \(X\) have PDF
\[
f_{X}(x)=3x^{2},\quad 0\leq x\leq 1.
\]
We will see later that \(f_{X}\) belongs to the /Beta/ family of distributions. It is easy to see that \(\int_{-\infty}^{\infty}f(x)\mathrm{d} x=1\).
\begin{align*}
\int_{-\infty}^{\infty}f_{X}(x)\mathrm{d} x & =\int_{0}^{1}3x^{2}\:\mathrm{d} x\\
 & =\left.x^{3}\right|_{x=0}^{1}\\
 & =1^{3}-0^{3}\\
 & =1.
\end{align*}
This being said, we may find \(\mathbb{P}(0.14\leq X<0.71)\).
\begin{align*}
\mathbb{P}(0.14\leq X<0.71) & =\int_{0.14}^{0.71}3x^{2}\mathrm{d} x,\\
 & =\left.x^{3}\right|_{x=0.14}^{0.71}\\
 & =0.71^{3}-0.14^{3}\\
 & \approx0.355167.
\end{align*}
We can find the mean and variance in an identical manner.
\begin{align*}
\mu=\int_{-\infty}^{\infty}xf_{X}(x)\mathrm{d} x & =\int_{0}^{1}x\cdot3x^{2}\:\mathrm{d} x,\\
 & =\frac{3}{4}x^{4}|_{x=0}^{1},\\
 & =\frac{3}{4}.
\end{align*}
It would perhaps be best to calculate the variance with the shortcut formula \(\sigma^{2}=\mathbb{E} X^{2}-\mu^{2}\):
\begin{align*}
\mathbb{E} X^{2}=\int_{-\infty}^{\infty}x^{2}f_{X}(x)\mathrm{d} x & =\int_{0}^{1}x^{2}\cdot3x^{2}\:\mathrm{d} x\\
 & =\left.\frac{3}{5}x^{5}\right|_{x=0}^{1}\\
 & =3/5.
\end{align*}
which gives \(\sigma^{2}=3/5-(3/4)^{2}=3/80\).
#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-cont-pdf-3x4>>

We will try one with unbounded support to brush up on improper integration. Let the random variable \(X\) have PDF
\[
f_{X}(x)=\frac{3}{x^{4}},\quad x>1.
\]
We can show that \(\int_{-\infty}^{\infty}f(x)\mathrm{d} x=1\):
\begin{align*}
\int_{-\infty}^{\infty}f_{X}(x)\mathrm{d} x & =\int_{1}^{\infty}\frac{3}{x^{4}}\:\mathrm{d} x,\\
 & =\lim_{t\to\infty}\int_{1}^{t}\frac{3}{x^{4}}\:\mathrm{d} x,\\
 & =\lim_{t\to\infty}\ \left.3\,\frac{1}{-3}x^{-3}\right|_{x=1}^{t},\\
 & =-\left(\lim_{t\to\infty}\frac{1}{t^{3}}-1\right),\\
 & =1.
\end{align*}
We calculate \(\mathbb{P}(3.4\leq X<7.1)\):
\begin{align*}
\mathbb{P}(3.4\leq X<7.1) & =\int_{3.4}^{7.1}3x^{-4}\mathrm{d} x,\\
 & =\left.3\,\frac{1}{-3}x^{-3}\right|_{x=3.4}^{7.1},\\
 & =-1(7.1^{-3}-3.4^{-3}),\\
 & \approx0.0226487123.
\end{align*}
We locate the mean and variance just like before.
\begin{align*}
\mu=\int_{-\infty}^{\infty}xf_{X}(x)\mathrm{d} x & =\int_{1}^{\infty}x\cdot\frac{3}{x^{4}}\:\mathrm{d} x,\\
 & =\left.3\,\frac{1}{-2}x^{-2}\right|_{x=1}^{\infty},\\
 & =-\frac{3}{2}\left(\lim_{t\to\infty}\frac{1}{t^{2}}-1\right),\\
 & =\frac{3}{2}.
\end{align*}
Again we use the shortcut \(\sigma^{2}=\mathbb{E} X^{2}-\mu^{2}\):
\begin{align*}
\mathbb{E} X^{2}=\int_{-\infty}^{\infty}x^{2}f_{X}(x)\mathrm{d} x & =\int_{1}^{\infty}x^{2}\cdot\frac{3}{x^{4}}\:\mathrm{d} x,\\
 & =\left.3\:\frac{1}{-1}x^{-1}\right|_{x=1}^{\infty},\\
 & =-3\left(\lim_{t\to\infty}\frac{1}{t^{2}}-1\right),\\
 & =3,
\end{align*}
which closes the example with \(\sigma^{2}=3-(3/2)^{2}=3/4\).
#+latex: \end{exampletoo}
#+html: </div>

**** How to do it with \(\mathsf{R}\)

There exist utilities to calculate probabilities and expectations for general continuous random variables, but it is better to find a built-in model, if possible. Sometimes it is not possible. We show how to do it the long way, and the =distr=\index{R packages@\textsf{R} packages!distr@\texttt{distr}} package \cite{distr} way.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Let \(X\) have PDF \(f(x)=3x^{2}\), \(0<x<1\) and find \(\mathbb{P}(0.14\leq X\leq0.71)\). (We will ignore that \(X\) is a beta random variable for the sake of argument.)

#+begin_src R :exports both :results output pp 
f <- function(x) 3*x^2
integrate(f, lower = 0.14, upper = 0.71)
#+end_src

Compare this to the answer we found in Example [[exa-cont-pdf3x2][Cont-pdf3x2]]. We could integrate the function \(x \cdot f(x)=\) =3*x^3= from zero to one to get the mean, and use the shortcut \(\sigma^{2}=\mathbb{E} X^{2}-\left(\mathbb{E} X\right)^{2}\) for the variance. 

#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Let \(X\) have PDF \(f(x)=3/x^{4}\), \(x>1\). We may integrate the function \(g(x) = x \cdot f(x)=\) =3/x^3= from zero to infinity to get the mean of \(X\).

#+begin_src R :exports both :results output pp 
g <- function(x) 3/x^3
integrate(g, lower = 1, upper = Inf)
#+end_src

Compare this to the answer we got in Example [[exa-cont-pdf-3x4][Cont-pdf3x4]]. Use =-Inf= for \(-\infty\).

#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Let us redo Example [[exa-cont-pdf3x2][Cont-pdf3x2]] with the =distr= package \cite{distr}. The method is similar to that encountered in Section [[sub-disc-rv-how-r][Discrete RV How R]] in Chapter [[cha-Discrete-Distributions][Discrete Distributions]]. We define an absolutely continuous random variable:

#+begin_src R :exports both :results output pp
f <- function(x) 3*x^2
X <- AbscontDistribution(d = f, low1 = 0, up1 = 1)
p(X)(0.71) - p(X)(0.14)
#+end_src

Compare this to the answer we found earlier. Now let us try expectation with the =distrEx= package \cite{distrEx}:
#+begin_src R :exports both :results output pp 
E(X); var(X); 3/80
#+end_src

Compare these answers to the ones we found in Example [[exa-cont-pdf3x2][Cont-pdf3x2]]. Why are they different? Because the =distrEx= package resorts to numerical methods when it encounters a model it does not recognize. This means that the answers we get for calculations may not exactly match the theoretical values. Be careful.
#+latex: \end{exampletoo}
#+html: </div>

** The Continuous Uniform Distribution
:PROPERTIES:
:CUSTOM_ID: sec-The-Continuous-Uniform
:END:

A random variable \(X\) with the continuous uniform distribution on the interval \((a,b)\) has PDF
\begin{equation}
f_{X}(x)=\frac{1}{b-a}, \quad a < x < b.
\end{equation}
The associated \(\mathsf{R}\) function is \(\mathsf{dunif}(\mathtt{min}=a,\,\mathtt{max}=b)\). We write \(X\sim\mathsf{unif}(\mathtt{min}=a,\,\mathtt{max}=b)\). Due to the particularly simple form of this PDF we can also write down explicitly a formula for the CDF \(F_{X}\):
\begin{equation}
\label{eq-unif-cdf}
F_{X}(t)=
\begin{cases}
0, & t < 0,\\
\frac{t-a}{b-a}, & a\leq t < b,\\
1, & t \geq b.
\end{cases}
\end{equation}

The continuous uniform distribution is the continuous analogue of the discrete uniform distribution; it is used to model experiments whose outcome is an interval of numbers that are ``equally likely'' in the sense that any two intervals of equal length in the support have the same probability associated with them.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Choose a number in \( [0,1] \) at random, and let \(X\) be the number chosen. Then \(X\sim\mathsf{unif}(\mathtt{min}=0,\,\mathtt{max}=1)\).
The mean of \(X\sim\mathsf{unif}(\mathtt{min}=a,\,\mathtt{max}=b)\) is relatively simple to calculate:
\begin{align*}
\mu=\mathbb{E} X & =\int_{-\infty}^{\infty}x\, f_{X}(x)\,\mathrm{d} x,\\
 & =\int_{a}^{b}x\ \frac{1}{b-a}\ \mathrm{d} x,\\
 & =\left.\frac{1}{b-a}\ \frac{x^{2}}{2}\ \right|_{x=a}^{b},\\
 & =\frac{1}{b-a}\ \frac{b^{2}-a^{2}}{2},\\
 & =\frac{b+a}{2},
\end{align*}
using the popular formula for the difference of squares. The variance is left to Exercise [[xca-variance-dunif][Variance Uniform]].
#+latex: \end{exampletoo}
#+html: </div>

** The Normal Distribution
:PROPERTIES:
:CUSTOM_ID: sec-The-Normal-Distribution
:END:

We say that \(X\) has a /normal distribution/ if it has PDF
\begin{equation}
f_{X}(x)=\frac{1}{\sigma\sqrt{2\pi}}\exp \left\{ \frac{-(x-\mu)^{2}}{2\sigma^{2}} \right\},\quad -\infty < x < \infty.
\end{equation}
We write \(X\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\), and the associated \(\mathsf{R}\) function is =dnorm(x, mean = 0, sd = 1)=.

The familiar bell-shaped curve, the normal distribution is also known as the /Gaussian distribution/ because the German mathematician C. F. Gauss largely contributed to its mathematical development. This distribution is by far the most important distribution, continuous or discrete. The normal model appears in the theory of all sorts of natural phenomena, from to the way particles of smoke dissipate in a closed room, to the journey of a bottle in the ocean to the white noise of cosmic background radiation.

When \(\mu=0\) and \(\sigma=1\) we say that the random variable has a /standard normal/ distribution and we typically write \(Z\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\). The lowercase Greek letter phi (\(\phi\)) is used to denote the standard normal PDF and the capital Greek letter phi \(\Phi\) is used to denote the standard normal CDF: for \(-\infty<z<\infty\),
\begin{equation}
\phi(z)=\frac{1}{\sqrt{2\pi}}\,\mathrm{e}^{-z^{2}/2}\mbox{ and }\Phi(t)=\int_{-\infty}^{t}\phi(z)\,\mathrm{d} z.
\end{equation}

#+begin_prop
If \(X\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) then
\begin{equation}
Z=\frac{X-\mu}{\sigma}\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1).
\end{equation}
#+end_prop

The MGF of \(Z\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) is relatively easy to derive:
\begin{eqnarray*}
M_{Z}(t) & = & \int_{-\infty}^{\infty}\mathrm{e}^{tz}\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-z^{2}/2}\mathrm{d} z,\\
 & = & \int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}\exp \{ -\frac{1}{2}\left(z^{2}+2tz+t^{2}\right)+\frac{t^{2}}{2} \} \mathrm{d} z,\\
 & = & \mathrm{e}^{t^{2}/2}\left(\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-[z-(-t)]^{2}/2}\mathrm{d} z\right),
\end{eqnarray*}
and the quantity in the parentheses is the total area under a \(\mathsf{norm}(\mathtt{mean}=-t,\,\mathtt{sd}=1)\) density, which is one. Therefore,
\begin{equation}
M_{Z}(t)=\mathrm{e}^{t^{2}/2},\quad -\infty < t < \infty.
\end{equation}

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
The MGF of \(X\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) is then not difficult either because 
\[
Z=\frac{X-\mu}{\sigma},\mbox{ or rewriting, }X=\sigma Z+\mu.
\]
Therefore
\[
M_{X}(t)=\mathbb{E}\mathrm{e}^{tX}=\mathbb{E}\mathrm{e}^{t(\sigma Z+\mu)}=\mathbb{E}\mathrm{e}^{\sigma tZ}\mathrm{e}^{t\mu}=\mathrm{e}^{t\mu}M_{Z}(\sigma t),
\]
and we know that \(M_{Z}(t)=\mathrm{e}^{t^{2}/2}\), thus substituting we get
\[
M_{X}(t)=\mathrm{e}^{t\mu}\mathrm{e}^{(\sigma t)^{2}/2}=\exp\left\{ \mu t+\sigma^{2}t^{2}/2\right\} ,
\]
for \(-\infty<t<\infty\).
#+latex: \end{exampletoo}
#+html: </div>

#+begin_fact
The same argument above shows that if \(X\) has MGF \(M_{X}(t)\) then the MGF of \(Y=a+bX\) is
\begin{equation}
M_{Y}(t)=\mathrm{e}^{ta}M_{X}(bt).
\end{equation}
#+end_fact

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
The 68-95-99.7 Rule. We saw in Section [[sub-Measures-of-Spread][Measures of Spread]] that when an empirical distribution is approximately bell shaped there are specific proportions of the observations which fall at varying distances from the (sample) mean. We can see where these come from -- and obtain more precise proportions -- with the following:
#+latex: \end{exampletoo}
#+html: </div>

#+begin_src R :exports both :results output pp 
pnorm(1:3) - pnorm(-(1:3))
#+end_src

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-iq-model>>
Let the random experiment consist of a person taking an IQ test, and let \(X\) be the score on the test. The scores on such a test are typically standardized to have a mean of 100 and a standard deviation of 15, and IQ tests have (approximately and notoriously) a bell-shaped distribution. What is \(\mathbb{P}(85\leq X\leq115)\)?

/Solution/: this one is easy because the limits 85 and 115 fall exactly one standard deviation (below and above, respectively) from the mean of 100. The answer is therefore approximately 68%.
#+latex: \end{exampletoo}
#+html: </div>

*** Normal Quantiles and the Quantile Function
:PROPERTIES:
:CUSTOM_ID: sub-Normal-Quantiles-QF
:END:

Until now we have been given two values and our task has been to find the area under the PDF between those values. In this section, we go in reverse: we are given an area, and we would like to find the value(s) that correspond to that area. 

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-iq-quantile-state-problem>>
Assuming the IQ model of Example [[exa-iq-model][IQ Model]], what is the lowest possible IQ score that a person can have and still be in the top 1% of all IQ scores?
/Solution/: If a person is in the top 1%, then that means that 99% of the people have lower IQ scores. So, in other words, we are looking for a value \(x\) such that \(F(x)=\mathbb{P}(X\leq x)\) satisfies \(F(x)=0.99\), or yet another way to say it is that we would like to solve the equation \(F(x)-0.99=0\). For the sake of argument, let us see how to do this the long way. We define the function \(g(x)=F(x)-0.99\), and then look for the root of \(g\) with the =uniroot= function. It uses numerical procedures to find the root so we need to give it an interval of \(x\) values in which to search for the root. We can get an educated guess from the Empirical Rule [[fac-Empirical-Rule][Empirical Rule]]; the root should be somewhere between two and three standard deviations (15 each) above the mean (which is 100).
#+begin_src R :exports both :results output pp 
g <- function(x) pnorm(x, mean = 100, sd = 15) - 0.99
uniroot(g, interval = c(130, 145))
#+end_src

#+begin_src R :exports none :results silent
temp <- round(uniroot(g, interval = c(130, 145))$root, 4)
#+end_src

The answer is shown in =$root= which is approximately \( SRC_R{temp} \), that is, a person with this IQ score or higher falls in the top 1% of all IQ scores.
#+latex: \end{exampletoo}
#+html: </div>

The discussion in Example [[exa-iq-quantile-state-problem][IQ Quantile]] was centered on the search for a value \(x\) that solved an equation \(F(x)=p\), for some given probability \(p\), or in mathematical parlance, the search for \(F^{-1}\), the inverse of the CDF of \(X\), evaluated at \(p\). This is so important that it merits a definition all its own.

#+begin_defn
The /quantile function/
#+latex: \footnote{The precise definition of the quantile function is \(Q_{X}(p)=\inf \{ x:\ F_{X}(x)\geq p \}\), so at least it is well defined (though perhaps infinite) for the values \(p=0\) and \(p=1\).}
of a random variable \(X\) is the inverse of its cumulative distribution function:
\begin{equation}
Q_{X}(p)=\min\left\{ x:\ F_{X}(x)\geq p\right\} ,\quad 0 < p <1.
\end{equation}
#+end_defn

#+begin_rem
Here are some properties of quantile functions:
1. The quantile function is defined and finite for all \(0<p<1\).
1. \(Q_{X}\) is left-continuous (see Appendix [[sec-Differential-and-Integral][Differential-Integral Calculus]]). For discrete random variables it is a step function, and for continuous random variables it is a continuous function.
1. In the continuous case the graph of \(Q_{X}\) may be obtained by reflecting the graph of \(F_{X}\) about the line \(y=x\). In the discrete case, before reflecting one should: 1) connect the dots to get rid of the jumps -- this will make the graph look like a set of stairs, 2) erase the horizontal lines so that only vertical lines remain, and finally 3) swap the open circles with the solid dots. Please see Figure [[fig-binom-plot-distr][binom-plot-distr]] for a comparison. 
1. The two limits
   \[
   \lim_{p\to0^{+}}Q_{X}(p)\quad \mbox{and}\quad \lim_{p\to1^{-}}Q_{X}(p)
   \]
   always exist, but may be infinite (that is, sometimes \(\lim_{p\to0}Q(p)=-\infty\) and/or \(\lim_{p\to1}Q(p)=\infty\)).

#+end_rem

As the reader might expect, the standard normal distribution is a very special case and has its own special notation.

#+begin_defn
For \(0<\alpha<1\), the symbol \(z_{\alpha}\) denotes the unique solution of the equation \( \mathbb{P} ( Z > z_{\alpha}) = \alpha\), where \(Z \sim \mathsf{norm}(\mathtt{mean} = 0,\,\mathtt{sd} = 1)\). It can be calculated in one of two equivalent ways: \(\mathtt{qnorm(} 1 - \alpha \mathtt{)} \) and \(\mathtt{qnorm(} \alpha \mathtt{, lower.tail = FALSE)} \). 
#+end_defn

There are a few other very important special cases which we will encounter in later chapters. 

**** How to do it with \(\mathsf{R}\)

Quantile functions are defined for all of the base distributions with the =q= prefix to the distribution name, except for the ECDF whose quantile function is exactly the \( Q_{x}(p) = \mathsf{quantile}(x, \mathtt{probs} = p, \mathtt{type} = 1) \) function. 

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Back to Example [[exa-iq-quantile-state-problem][IQ Quantile]], we are looking for \(Q_{X}(0.99)\), where \(X\sim\mathsf{norm}(\mathtt{mean}=100,\,\mathtt{sd}=15)\). It could not be easier to do with \(\mathsf{R}\). 

#+begin_src R :exports both :results output pp 
qnorm(0.99, mean = 100, sd = 15)
#+end_src

Compare this answer to the one obtained earlier with =uniroot=.
#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Find the values \(z_{0.025}\), \(z_{0.01}\), and \(z_{0.005}\) (these will play an important role from Chapter [[cha-Estimation][Estimation]] onward).
#+latex: \end{exampletoo}
#+html: </div>

#+begin_src R :exports both :results output pp 
qnorm(c(0.025, 0.01, 0.005), lower.tail = FALSE)
#+end_src

Note the =lower.tail= argument. We would get the same answer with
: qnorm(c(0.975, 0.99, 0.995))

** Functions of Continuous Random Variables
:PROPERTIES:
:CUSTOM_ID: sec-Functions-of-Continuous
:END:

The goal of this section is to determine the distribution of \(U=g(X)\) based on the distribution of \(X\). In the discrete case all we needed to do was back substitute for \(x=g^{-1}(u)\) in the PMF of \(X\) (sometimes accumulating probability mass along the way). In the continuous case, however, we need more sophisticated tools. Now would be a good time to review Appendix [[sec-Differential-and-Integral][Differential-Integral Calculus]].

*** The PDF Method

#+begin_prop
# <<pro-func-cont-rvs-pdf-formula>>

Let \(X\) have PDF \(f_{X}\) and let \(g\) be a function which is one-to-one with a differentiable inverse \(g^{-1}\). Then the PDF of \(U=g(X)\) is given by
\begin{equation}
\label{eq-univ-trans-pdf-long}
f_{U}(u)=f_{X}\left[g^{-1}(u)\right]\ \left|\frac{\mathrm{d}}{\mathrm{d} u}g^{-1}(u)\right|.
\end{equation}
#+end_prop

#+begin_rem
The formula in Equation [[eq-univ-trans-pdf-long]] is nice, but does not really make any sense. It is better to write in the intuitive form
\begin{equation}
\label{eq-univ-trans-pdf-short}
f_{U}(u)=f_{X}(x)\left|\frac{\mathrm{d} x}{\mathrm{d} u}\right|.
\end{equation}
#+end_rem

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-lnorm-transformation>>
Let \(X\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\), and let \(Y=\mathrm{e}^{X}\). What is the PDF of \(Y\)? 
*Solution:* Notice first that \(\mathrm{e}^{x}>0\) for any \(x\), so the support of \(Y\) is \((0,\infty)\). Since the transformation is monotone, we can solve \(y=\mathrm{e}^{x}\) for \(x\) to get \(x=\ln\, y\), giving \(\mathrm{d} x/\mathrm{d} y=1/y\). Therefore, for any \(y>0\),
\[
f_{Y}(y)=f_{X}(\ln y)\cdot\left|\frac{1}{y}\right|=\frac{1}{\sigma\sqrt{2\pi}}\exp\left\{ \frac{(\ln y-\mu)^{2}}{2\sigma^{2}}\right\} \cdot\frac{1}{y},
\]
where we have dropped the absolute value bars since \(y>0\). The random variable \(Y\) is said to have a /lognormal distribution/; see Section [[sec-Other-Continuous-Distributions][Other Continuous Distributions]].
#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-lin-trans-norm>>
Suppose \(X\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) and let \(Y=4-3X\). What is the PDF of \(Y\)?
#+latex: \end{exampletoo}
#+html: </div>

The support of \(X\) is \((-\infty,\infty)\), and as \(x\) goes from \(-\infty\) to \(\infty\), the quantity \(y=4-3x\) also traverses \((-\infty,\infty)\). Solving for \(x\) in the equation \(y=4-3x\) yields \(x=-(y-4)/3\) giving \(\mathrm{d} x/\mathrm{d} y=-1/3\). And since
\[
f_{X}(x)=\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-x^{2}/2}, \quad -\infty < x < \infty ,
\]
we have
\begin{eqnarray*}
f_{Y}(y) & = & f_{X}\left(\frac{y-4}{3}\right)\cdot\left|-\frac{1}{3}\right|,\quad -\infty < y < \infty,\\
 & = & \frac{1}{3\sqrt{2\pi}}\mathrm{e}^{-(y-4)^{2}/2\cdot3^{2}},\quad -\infty < y < \infty.
\end{eqnarray*}
We recognize the PDF of \(Y\) to be that of a \(\mathsf{norm}(\mathtt{mean}=4,\,\mathtt{sd}=3)\) distribution. Indeed, we may use an identical argument as the above to prove the following fact:

#+begin_fact
# <<fac-lin-trans-norm-is-norm>>
If \(X\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) and if \(Y=a+bX\) for constants \(a\) and \(b\), with \(b\neq0\), then \(Y\sim\mathsf{norm}(\mathtt{mean}=a+b\mu,\,\mathtt{sd}=|b|\sigma)\). 
#+end_fact

Note that it is sometimes easier to /postpone/ solving for the inverse transformation \(x=x(u)\). Instead, leave the transformation in the form \(u=u(x)\) and calculate the derivative of the /original/ transformation
\begin{equation}
\mathrm{d} u/\mathrm{d} x=g'(x).
\end{equation}
Once this is known, we can get the PDF of \(U\) with
\begin{equation}
f_{U}(u)=f_{X}(x)\left|\frac{1}{\mathrm{d} u/\mathrm{d} x}\right|.
\end{equation}
In many cases there are cancellations and the work is shorter. Of course, it is not always true that
\begin{equation}
\label{eq-univ-jacob-recip}
\frac{\mathrm{d} x}{\mathrm{d} u}=\frac{1}{\mathrm{d} u/\mathrm{d} x},
\end{equation}
but for the well-behaved examples in this book the trick works just fine.

#+begin_rem
In the case that \(g\) is not monotone we cannot apply Proposition [[pro-func-cont-rvs-pdf-formula][func-cont-rvs-pdf]] directly. However, hope is not lost. Rather, we break the support of \(X\) into pieces such that \(g\) is monotone on each one. We apply Proposition [[pro-func-cont-rvs-pdf-formula][func-cont-rvs-pdf]] on each piece, and finish up by adding the results together.
#+end_rem

*** The CDF method

We know from Section [[sec-continuous-random-variables][Continuous Random Variables]] that \(f_{X}=F_{X}'\) in the continuous case. Starting from the equation \(F_{Y}(y)=\mathbb{P}(Y\leq y)\), we may substitute \(g(X)\) for \(Y\), then solve for \(X\) to obtain \(\mathbb{P}[X\leq g^{-1}(y)]\), which is just another way to write \(F_{X}[g^{-1}(y)]\). Differentiating this last quantity with respect to \(y\) will yield the PDF of \(Y\).

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Suppose \(X\sim\mathsf{unif}(\mathtt{min}=0,\,\mathtt{max}=1)\) and
suppose that we let \(Y=-\ln\, X\). What is the PDF of \(Y\)?

The support set of \(X\) is \((0,1),\) and \(y\) traverses \((0,\infty)\) as \(x\) ranges from \(0\) to \(1\), so the support set of \(Y\) is \(S_{Y}=(0,\infty)\). For any \(y>0\), we consider
\[
F_{Y}(y)=\mathbb{P}(Y\leq y)=\mathbb{P}(-\ln\, X\leq y)=\mathbb{P}(X\geq\mathrm{e}^{-y})=1-\mathbb{P}(X<\mathrm{e}^{-y}),
\]
where the next to last equality follows because the exponential function is /monotone/ (this point will be revisited later). Now since \(X\) is continuous the two probabilities \(\mathbb{P}(X<\mathrm{e}^{-y})\) and \(\mathbb{P}(X\leq\mathrm{e}^{-y})\) are equal; thus
\[
1-\mathbb{P}(X < \mathrm{e}^{-y})=1-\mathbb{P}(X\leq\mathrm{e}^{-y})=1-F_{X}(\mathrm{e}^{-y}).
\]
Now recalling that the CDF of a \(\mathsf{unif}(\mathtt{min}=0,\,\mathtt{max}=1)\) random variable satisfies \(F(u)=u\) (see Equation [[eq-unif-cdf][Uniform CDF]]), we can say
\[
F_{Y}(y)=1-F_{X}(\mathrm{e}^{-y})=1-\mathrm{e}^{-y},\quad \mbox{for }y>0.
\]
We have consequently found the formula for the CDF of \(Y\); to obtain the PDF \(f_{Y}\) we need only differentiate \(F_{Y}\):
\[
f_{Y}(y)=\frac{\mathrm{d}}{\mathrm{d} y}\left(1-\mathrm{e}^{-y}\right)=0-\mathrm{e}^{-y}(-1),
\]
or \(f_{Y}(y)=\mathrm{e}^{-y}\) for \(y>0\). This turns out to be a member of the exponential family of distributions, see Section [[sec-Other-Continuous-Distributions][Other Continuous Distributions]]. 
#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
*The Probability Integral Transform*. Given a continuous random variable \(X\) with strictly increasing CDF \(F_{X}\), let the random variable \(Y\) be defined by \(Y=F_{X}(X)\). Then the distribution of \(Y\) is \(\mathsf{unif}(\mathtt{min}=0,\,\mathtt{max}=1)\).
#+latex: \end{exampletoo}
#+html: </div>

#+begin_proof
We employ the CDF method. First note that the support of \(Y\) is \((0,1)\). Then for any \(0<y<1\),
\[
F_{Y}(y)=\mathbb{P}(Y\leq y)=\mathbb{P}(F_{X}(X)\leq y).
\]
Now since \(F_{X}\) is strictly increasing, it has a well defined inverse function \(F_{X}^{-1}\). Therefore,
\[
\mathbb{P}(F_{X}(X)\leq y)=\mathbb{P}(X\leq F_{X}^{-1}(y))=F_{X}[F_{X}^{-1}(y)]=y.
\]
Summarizing, we have seen that \(F_{Y}(y)=y\), \(0<y<1\). But this is exactly the CDF of a \(\mathsf{unif}(\mathtt{min}=0,\,\mathtt{max}=1)\) random variable. 
#+end_proof

#+begin_fact
The Probability Integral Transform is true for all continuous random variables with continuous CDFs, not just for those with strictly increasing CDFs (but the proof is more complicated). The transform is *not* true for discrete random variables, or for continuous random variables having a discrete component (that is, with jumps in their CDF).
#+end_fact

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-distn-of-z-squared>>
Let \(Z\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) and let \(U=Z^{2}\). What is the PDF of \(U\)? 
Notice first that \(Z^{2}\geq0\), and thus the support of \(U\) is \([0,\infty)\). And for any \(u\geq0\), 
\[
F_{U}(u)=\mathbb{P}(U\leq u)=\mathbb{P}(Z^{2}\leq u).
\]
But \(Z^{2}\leq u\) occurs if and only if \(-\sqrt{u}\leq Z\leq\sqrt{u}\). The last probability above is simply the area under the standard normal PDF from \(-\sqrt{u}\) to \(\sqrt{u}\), and since \(\phi\) is symmetric about 0, we have
\[
\mathbb{P}(Z^{2}\leq u)=2\mathbb{P}(0\leq Z\leq\sqrt{u})=2\left[F_{Z}(\sqrt{u})-F_{Z}(0)\right]=2\Phi(\sqrt{u})-1,
\]
because \(\Phi(0)=1/2\). To find the PDF of \(U\) we differentiate the CDF recalling that \(\Phi'=\phi\).
\[
f_{U}(u)=\left(2\Phi(\sqrt{u})-1\right)'=2\phi(\sqrt{u})\cdot\frac{1}{2\sqrt{u}}=u^{-1/2}\phi(\sqrt{u}).
\]
Substituting,
\[
f_{U}(u)=u^{-1/2}\frac{1}{\sqrt{2\pi}}\,\mathrm{e}^{-(\sqrt{u})^{2}/2}=(2\pi u)^{-1/2}\mathrm{e}^{-u/2},\quad u > 0.
\]
This is what we will later call a /chi-square distribution with 1 degree of freedom/. See Section [[sec-Other-Continuous-Distributions][Other Continuous Distributions]].
#+latex: \end{exampletoo}
#+html: </div>

**** How to do it with \(\mathsf{R}\)

The =distr= package \cite{distr} has functionality to investigate transformations of univariate distributions. There are exact results for ordinary transformations of the standard distributions, and =distr= takes advantage of these in many cases. For instance, the =distr= package can handle the transformation in Example [[exa-lin-trans-norm][Linear Trans-Norm]] quite nicely:

#+begin_src R :exports both :results output pp 
X <- Norm(mean = 0, sd = 1)
Y <- 4 - 3*X
Y
#+end_src

So =distr= ``knows'' that a linear transformation of a normal random variable is again normal, and it even knows what the correct =mean= and =sd= should be. But it is impossible for =distr= to know everything, and it is not long before we venture outside of the transformations that =distr= recognizes. Let us try Example [[exa-lnorm-transformation][Lnorm Trans]]:

#+begin_src R :exports both :results output pp 
Y <- exp(X)
Y
#+end_src

The result is an object of class =AbscontDistribution=, which is one of the classes that =distr= uses to denote general distributions that it does not recognize (it turns out that \(Z\) has a /lognormal/ distribution; see Section [[sec-Other-Continuous-Distributions][Other Continuous Distributions]]). A simplified description of the process that =distr= undergoes when it encounters a transformation \(Y=g(X)\) that it does not recognize is
1. Randomly generate many, many copies \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) from the distribution of \(X\),
1. Compute \(Y_{1}=g(X_{1})\), \(Y_{2}=g(X_{2})\), ..., \(Y_{n}=g(X_{n})\) and store them for use.
1. Calculate the PDF, CDF, quantiles, and random variates using the simulated values of \(Y\).
As long as the transformation is sufficiently nice, such as a linear transformation, the exponential, absolute value, /etc./, the =d-p-q= functions are calculated analytically based on the =d-p-q= functions associated with \(X\). But if we try a crazy transformation then we are greeted by a warning:

#+begin_src R :exports both :results output pp 
W <- sin(exp(X) + 27)
W
#+end_src

The warning confirms that the =d-p-q= functions are not calculated analytically, but are instead based on the randomly simulated values of \(Y\). /We must be careful to remember this./ The nature of random simulation means that we can get different answers to the same question: watch what happens when we compute \(\mathbb{P}(W\leq0.5)\) using the \(W\) above, then define \(W\) again, and compute the (supposedly) same \(\mathbb{P}(W\leq0.5)\) a few moments later.

#+begin_src R :exports both :results output pp 
p(W)(0.5)
W <- sin(exp(X) + 27)
p(W)(0.5)
#+end_src

The answers are not the same! Furthermore, if we were to repeat the process we would get yet another answer for \(\mathbb{P}(W\leq0.5)\).  

The answers were close, though. And the underlying randomly generated \(X\)'s were not the same so it should hardly be a surprise that the calculated \(W\)'s were not the same, either. This serves as a warning (in concert with the one that =distr= provides) that we should be careful to remember that complicated transformations computed by \(\mathsf{R}\) are only approximate and may fluctuate slightly due to the nature of the way the estimates are calculated.

** Other Continuous Distributions
:PROPERTIES:
:CUSTOM_ID: sec-Other-Continuous-Distributions
:END:

*** Waiting Time Distributions
:PROPERTIES:
:CUSTOM_ID: sub-Waiting-Time-Distributions
:END:

In some experiments, the random variable being measured is the time until a certain event occurs. For example, a quality control specialist may be testing a manufactured product to see how long it takes until it fails. An efficiency expert may be recording the customer traffic at a retail store to streamline scheduling of staff. 

**** The Exponential Distribution
:PROPERTIES:
:CUSTOM_ID: sub-The-Exponential-Distribution
:END:

We say that \(X\) has an /exponential distribution/ and write \(X\sim\mathsf{exp}(\mathtt{rate}=\lambda)\). 
\begin{equation}
f_{X}(x)=\lambda\mathrm{e}^{-\lambda x},\quad x>0
\end{equation}
The associated \(\mathsf{R}\) functions are =dexp(x, rate = 1)=, =pexp=, =qexp=, and =rexp=, which give the PDF, CDF, quantile function, and simulate random variates, respectively.

The parameter \(\lambda\) measures the rate of arrivals (to be described later) and must be positive. The CDF is given by the formula
\begin{equation}
F_{X}(t)=1-\mathrm{e}^{-\lambda t},\quad t>0.
\end{equation}
The mean is \(\mu=1/\lambda\) and the variance is \(\sigma^{2}=1/\lambda^{2}\). 

The exponential distribution is closely related to the Poisson distribution. If customers arrive at a store according to a Poisson process with rate \(\lambda\) and if \(Y\) counts the number of customers that arrive in the time interval \([0,t)\), then we saw in Section [[sec-other-discrete-distributions][Other Discrete Distributions]] that \( Y \sim \mathsf{pois}(\mathtt{lambda}=\lambda t). \) Now consider a different question: let us start our clock at time 0 and stop the clock when the first customer arrives. Let \(X\) be the length of this random time interval. Then \(X\sim\mathsf{exp}(\mathtt{rate}=\lambda)\). Observe the following string of equalities:
\begin{align*}
\mathbb{P}(X>t) & =\mathbb{P}(\mbox{first arrival after time \emph{t}}),\\
 & =\mathbb{P}(\mbox{no events in [0,\emph{t})}),\\
 & =\mathbb{P}(Y=0),\\
 & =\mathrm{e}^{-\lambda t},
\end{align*}
where the last line is the PMF of \(Y\) evaluated at \(y=0\). In other words, \(\mathbb{P}(X\leq t)=1-\mathrm{e}^{-\lambda t}\), which is exactly the CDF of an \(\mathsf{exp}(\mathtt{rate}=\lambda)\) distribution. 

The exponential distribution is said to be /memoryless/ because exponential random variables "forget" how old they are at every instant. That is, the probability that we must wait an additional five hours for a customer to arrive, given that we have already waited seven hours, is exactly the probability that we needed to wait five hours for a customer in the first place. In mathematical symbols, for any \(s,\, t>0\),
\begin{equation}
\mathbb{P}(X>s+t\,|\, X>t)=\mathbb{P}(X>s).
\end{equation}
See Exercise [[xca-prove-the-memoryless][Prove Memoryless]].

**** The Gamma Distribution
:PROPERTIES:
:CUSTOM_ID: sub-The-Gamma-Distribution
:END:

This is a generalization of the exponential distribution. We say that \(X\) has a gamma distribution and write \(X\sim\mathsf{gamma}(\mathtt{shape}=\alpha,\,\mathtt{rate}=\lambda)\). It has PDF
\begin{equation}
f_{X}(x)=\frac{\lambda^{\alpha}}{\Gamma(\alpha)}\: x^{\alpha-1}\mathrm{e}^{-\lambda x},\quad x>0.
\end{equation}

The associated \(\mathsf{R}\) functions are =dgamma(x, shape, rate = 1)=, =pgamma=, =qgamma=, and =rgamma=, which give the PDF, CDF, quantile function, and simulate random variates, respectively. If \(\alpha=1\) then \(X\sim\mathsf{exp}(\mathtt{rate}=\lambda)\). The mean is \(\mu=\alpha/\lambda\) and the variance is \(\sigma^{2}=\alpha/\lambda^{2}\).

To motivate the gamma distribution recall that if \(X\) measures the length of time until the first event occurs in a Poisson process with rate \(\lambda\) then \(X\sim\mathsf{exp}(\mathtt{rate}=\lambda)\). If we let \(Y\) measure the length of time until the \(\alpha^{\mathrm{th}}\) event occurs then \(Y\sim\mathsf{gamma}(\mathtt{shape}=\alpha,\,\mathtt{rate}=\lambda)\). When \(\alpha\) is an integer this distribution is also known as the /Erlang/ distribution.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
At a car wash, two customers arrive per hour on the average. We decide to measure how long it takes until the third customer arrives. If \(Y\) denotes this random time then \(Y\sim\mathsf{gamma}(\mathtt{shape}=3,\,\mathtt{rate}=1/2)\).
#+latex: \end{exampletoo}
#+html: </div>

*** The Chi square, Student's \(t\), and Snedecor's \(F\) Distributions
:PROPERTIES:
:CUSTOM_ID: sub-The-Chi-Square-t-F
:END:

**** The Chi square Distribution
:PROPERTIES:
:CUSTOM_ID: sub-The-Chi-Square
:END:

A random variable \(X\) with PDF
\begin{equation}
f_{X}(x)=\frac{1}{\Gamma(p/2)2^{p/2}}x^{p/2-1}\mathrm{e}^{-x/2},\quad x>0,
\end{equation}
is said to have a /chi-square distribution/ with \(p\) /degrees of freedom/. We write \(X\sim\mathsf{chisq}(\mathtt{df}=p)\). The associated \(\mathsf{R}\) functions are =dchisq(x, df)=, =pchisq=, =qchisq=, and =rchisq=, which give the PDF, CDF, quantile function, and simulate random variates, respectively. See Figure [[fig-chisq-dist-vary-df][chisq-dist-vary-df]]. In an obvious notation we may define \(\chi_{\alpha}^{2}(p)\) as the number on the \(x\)-axis such that there is exactly \(\alpha\) area under the \(\mathsf{chisq}(\mathtt{df}=p)\) curve to its right.

The code to produce Figure [[fig-chisq-dist-vary-df][chisq-dist-vary-df]] is

#+name: chisq-dist-vary-df
#+begin_src R :exports code :results silent
curve(dchisq(x, df = 3), from = 0, to = 20, ylab = "y")
ind <- c(4, 5, 10, 15)
for (i in ind) curve(dchisq(x, df = i), 0, 20, add = TRUE)
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/contdist/chisq-dist-vary-df.ps
  <<chisq-dist-vary-df>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/contdist/chisq-dist-vary-df.svg
  <<chisq-dist-vary-df>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/contdist/chisq-dist-vary-df.ps}
  \caption[Chi square distribution for various degrees of freedom]{\small The chi square distribution for various degrees of freedom.}
  \label{fig-chisq-dist-vary-df}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-chisq-dist-vary-df" class="figure">
  <p><img src="svg/contdist/chisq-dist-vary-df.svg" width=500 alt="svg/contdist/chisq-dist-vary-df.svg" /></p>
  <p>The chi square distribution for various degrees of freedom.</p>
</div>
#+end_html

#+begin_rem
Here are some useful things to know about the chi-square distribution.
1. If \(Z\sim\mathtt{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\), then \(Z^{2}\sim\mathsf{chisq}(\mathtt{df}=1)\). We saw this in Example [[exa-distn-of-z-squared][Distn Z2]], and the fact is important when it comes time to find the distribution of the sample variance, \(S^{2}\). See Theorem [[thm-Xbar-andS][Xbar and S]] in Section [[sub-Samp-Var-Dist][Sample Variance Distribution]].
1. The chi-square distribution is supported on the positive \(x\)-axis, with a right-skewed distribution.
1. The \(\mathsf{chisq}(\mathtt{df}=p)\) distribution is the same as a \(\mathsf{gamma}(\mathtt{shape}=p/2,\,\mathtt{rate}=1/2)\) distribution. 
1. The MGF of \(X\sim\mathsf{chisq}(\mathtt{df}=p)\) is
   \begin{equation}
   \label{eq-mgf-chisq}
   M_{X}(t)=\left(1-2t\right)^{-p},\quad t < 1/2.
   \end{equation}
#+end_rem

**** Student's \(t\) distribution
:PROPERTIES:
:CUSTOM_ID: sub-Student's-t-distribution
:END:

A random variable \(X\) with PDF
\begin{equation}
f_{X}(x) = \frac{\Gamma\left[ (r+1)/2\right] }{\sqrt{r\pi}\,\Gamma(r/2)}\left( 1 + \frac{x^{2}}{r} \right)^{-(r+1)/2},\quad -\infty < x < \infty
\end{equation}
is said to have /Student's/ \(t\) distribution with \(r\) /degrees of freedom/, and we write \(X\sim\mathsf{t}(\mathtt{df}=r)\). The associated \(\mathsf{R}\) functions are =dt=,=pt=, =qt=, and =rt=, which give the PDF, CDF, quantile function, and simulate random variates, respectively. See Section [[sec-sampling-from-normal-dist][Sampling from Normal]].

**** Snedecor's \(F\) distribution
:PROPERTIES:
:CUSTOM_ID: sub-snedecor-F-distribution
:END:

A random variable \(X\) with PDF
\begin{equation}
f_{X}(x)=\frac{\Gamma[(m+n)/2]}{\Gamma(m/2)\Gamma(n/2)}\left(\frac{m}{n}\right)^{m/2}x^{m/2-1}\left(1+\frac{m}{n}x\right)^{-(m+n)/2},\quad x>0.
\end{equation}
is said to have an \(F\) distribution with \((m,n)\) degrees of freedom. We write \(X\sim\mathsf{f}(\mathtt{df1}=m,\,\mathtt{df2}=n)\). The associated \(\mathsf{R}\) functions are =df(x, df1, df2)=, =pf=, =qf=, and =rf=, which give the PDF, CDF, quantile function, and simulate random variates, respectively. We define \(F_{\alpha}(m,n)\) as the number on the \(x\)-axis such that there is exactly \(\alpha\) area under the \(\mathsf{f}(\mathtt{df1}=m,\,\mathtt{df2}=n)\) curve to its right. 

#+begin_rem
Here are some notes about the \(F\) distribution.
1. If \(X\sim\mathsf{f}(\mathtt{df1}=m,\,\mathtt{df2}=n)\) and \(Y=1/X\), then \(Y\sim\mathsf{f}(\mathtt{df1}=n,\,\mathtt{df2}=m)\). Historically, this fact was especially convenient. In the old days, statisticians used printed tables for their statistical calculations. Since the \(F\) tables were symmetric in \(m\) and \(n\), it meant that publishers could cut the size of their printed tables in half. It plays less of a role today now that personal computers are widespread.
1. If \(X\sim\mathsf{t}(\mathtt{df}=r)\), then \(X^{2}\sim\mathsf{f}(\mathtt{df1}=1,\,\mathtt{df2}=r)\). We will see this again in Section [[sub-slr-overall-F-statistic][SLR Overall F]].
#+end_rem

*** Other Popular Distributions
:PROPERTIES:
:CUSTOM_ID: sub-Other-Popular-Distributions
:END:

**** The Cauchy Distribution
:PROPERTIES:
:CUSTOM_ID: sub-The-Cauchy-Distribution
:END:

This is a special case of the Student's \(t\) distribution. It has PDF
\begin{equation}
f_{X}(x) = \frac{1}{\beta\pi} \left[ 1+\left( \frac{x-m}{\beta} \right)^{2} \right]^{-1},\quad -\infty < x < \infty.
\end{equation}
We write \(X\sim\mathsf{cauchy}(\mathtt{location}=m,\,\mathtt{scale}=\beta)\). The associated \(\mathsf{R}\) function is =dcauchy(x, location = 0, scale = 1)=.

It is easy to see that a \(\mathsf{cauchy}(\mathtt{location}=0,\,\mathtt{scale}=1)\) distribution is the same as a \(\mathsf{t}(\mathtt{df}=1)\) distribution. The \(\mathsf{cauchy}\) distribution looks like a \(\mathsf{norm}\) distribution but with very heavy tails. The mean (and variance) do not exist, that is, they are infinite. The median is represented by the \(\mathtt{location}\) parameter, and the \(\mathtt{scale}\) parameter influences the spread of the distribution about its median.

**** The Beta Distribution
:PROPERTIES:
:CUSTOM_ID: sub-The-Beta-Distribution
:END:

This is a generalization of the continuous uniform distribution.
\begin{equation}
f_{X}(x)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\, x^{\alpha-1}(1-x)^{\beta-1},\quad 0 < x < 1.
\end{equation}
We write \(X\sim\mathsf{beta}(\mathtt{shape1}=\alpha,\,\mathtt{shape2}=\beta)\). The associated \(\mathsf{R}\) function is =dbeta(x, shape1, shape2)=. The mean and variance are
\begin{equation} 
\mu=\frac{\alpha}{\alpha+\beta}\mbox{ and }\sigma^{2}=\frac{\alpha\beta}{\left(\alpha+\beta\right)^{2}\left(\alpha+\beta+1\right)}.
\end{equation}
See Example [[exa-cont-pdf3x2][Cont-pdf3x2]]. This distribution comes up a lot in Bayesian statistics because it is a good model for one's prior beliefs about a population proportion \(p\), \(0\leq p\leq1\).

**** The Logistic Distribution
:PROPERTIES:
:CUSTOM_ID: sub-The-Logistic-Distribution
:END:

\begin{equation}
f_{X}(x)=\frac{1}{\sigma}\exp\left(-\frac{x-\mu}{\sigma}\right)\left[1+\exp\left(-\frac{x-\mu}{\sigma}\right)\right]^{-2},\quad -\infty < x < \infty.
\end{equation}
We write \(X\sim\mathsf{logis}(\mathtt{location}=\mu,\,\mathtt{scale}=\sigma)\). The associated \(\mathsf{R}\) function is =dlogis(x, location = 0, scale = 1)=. The logistic distribution comes up in differential equations as a model for population growth under certain assumptions. The mean is \(\mu\) and the variance is \(\pi^{2}\sigma^{2}/3\).

**** The Lognormal Distribution
:PROPERTIES:
:CUSTOM_ID: sub-The-Lognormal-Distribution
:END:

This is a distribution derived from the normal distribution (hence the name). If \(U\sim\mathtt{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\), then \( X = \mathrm{e}^{U} \) has PDF
\begin{equation}
f_{X}(x)=\frac{1}{\sigma x\sqrt{2\pi}}\exp\left[\frac{-(\ln x-\mu)^{2}}{2\sigma^{2}}\right], \quad 0 < x < \infty.
\end{equation}
We write \(X\sim\mathsf{lnorm}(\mathtt{meanlog}=\mu,\,\mathtt{sdlog}=\sigma)\). The associated \(\mathsf{R}\) function is =dlnorm(x, meanlog = 0, sdlog = 1)=. Notice that the support is concentrated on the positive \(x\) axis; the distribution is right-skewed with a heavy tail. See Example [[exa-lnorm-transformation][Lnorm-trans]].

**** The Weibull Distribution
:PROPERTIES:
:CUSTOM_ID: sub-The-Weibull-Distribution
:END:

This has PDF
\begin{equation}
f_{X}(x)=\frac{\alpha}{\beta}\left(\frac{x}{\beta}\right)^{\alpha-1}\exp\left(\frac{x}{\beta}\right)^{\alpha},\quad x>0.
\end{equation}
We write \(X\sim\mathsf{weibull}(\mathtt{shape}=\alpha,\,\mathtt{scale}=\beta)\). The associated \(\mathsf{R}\) function is =dweibull(x, shape, scale = 1)=. 

**** How to do it with \(\mathsf{R}\)

There is some support of moments and moment generating functions for some continuous probability distributions included in the =actuar= package \cite{actuar}. The convention is =m= in front of the distribution name for raw moments, and =mgf= in front of the distribution name for the moment generating function. At the time of this writing, the following distributions are supported: gamma, inverse Gaussian, (non-central) chi-squared, exponential, and uniform.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Calculate the first four raw moments for \(X\sim\mathsf{gamma}(\mathtt{shape}=13,\,\mathtt{rate}=1)\) and plot the moment generating function.

We load the =actuar= package and use the functions =mgamma= and =mgfgamma=:
#+begin_src R :exports both :results output pp 
mgamma(1:4, shape = 13, rate = 1)
#+end_src

For the plot we can use the function in the following form:

#+name: gamma-mgf
#+begin_src R :exports code :results silent
plot(function(x){mgfgamma(x, shape = 13, rate = 1)}, from=-0.1, to=0.1, ylab = "gamma mgf")
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/contdist/gamma-mgf.ps
  <<gamma-mgf>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/contdist/gamma-mgf.svg
  <<gamma-mgf>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/contdist/gamma-mgf.ps}
  \caption[Plot of the \textsf{gamma}(\texttt{shape} = 13, \texttt{rate} = 1) MGF]{\small A plot of the \textsf{gamma}(\texttt{shape} = 13, \texttt{rate} = 1) MGF.}
  \label{fig-gamma-mgf}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-gamma-mgf" class="figure">
  <p><img src="svg/contdist/gamma-mgf.svg" width=500 alt="svg/contdist/gamma-mgf.svg" /></p>
  <p>A plot of the <code>gamma(shape = 13, rate = 1)</code> MGF.</p>
</div>
#+end_html

#+latex: \end{exampletoo}
#+html: </div>

#+latex: \newpage{}

** Exercises
#+latex: \setcounter{thm}{0}

#+begin_xca
Find the constant \(C\) so that the given function is a valid PDF of a random variable \(X\).
1. \( f(x) = Cx^{n},\quad 0 < x <1 \).
1. \( f(x) = Cx\mathrm{e}^{-x},\quad 0 < x < \infty\).
1. \( f(x) = \mathrm{e}^{-(x - C)}, \quad 7 < x < \infty.\)
1. \( f(x) = Cx^{3}(1 - x)^{2},\quad 0 < x < 1.\)
1. \( f(x) = C(1 + x^{2}/4)^{-1}, \quad -\infty < x < \infty.\)
#+end_xca

#+begin_xca
For the following random experiments, decide what the distribution of \(X\) should be. In nearly every case, there are additional assumptions that should be made for the distribution to apply; identify those assumptions (which may or may not strictly hold in practice).
1. We throw a dart at a dart board. Let \(X\) denote the squared linear distance from the bulls-eye to the where the dart landed.
1. We randomly choose a textbook from the shelf at the bookstore and let \(P\) denote the proportion of the total pages of the book devoted to exercises. 
1. We measure the time it takes for the water to completely drain out of the kitchen sink.
1. We randomly sample strangers at the grocery store and ask them how long it will take them to drive home. 
#+end_xca

#+begin_xca
If \(Z\) is \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\), find 
1. \(\mathbb{P}(Z>2.64)\)
1. \(\mathbb{P}(0\leq Z<0.87)\)
1. \(\mathbb{P}(|Z|>1.39)\) (Hint: draw a picture!)
#+end_xca

#+begin_xca
# <<xca-variance-dunif>>
Calculate the variance of \(X\sim\mathsf{unif}(\mathtt{min}=a,\,\mathtt{max}=b)\). /Hint:/ First calculate \(\mathbb{E} X^{2}\).
#+end_xca

#+begin_xca
# <<xca-prove-the-memoryless>>
Prove the memoryless property for exponential random variables. That is, for \(X\sim\mathsf{exp}(\mathtt{rate}=\lambda)\) show that for any \(s,t>0\),
\[
\mathbb{P}(X>s+t\,|\, X>t)=\mathbb{P}(X>s).
\]
#+end_xca

* Multivariate Distributions                                       :multdist:
:PROPERTIES:
:tangle: R/multdist.R
:CUSTOM_ID: cha-Multivariable-Distributions
:END:

#+begin_src R :exports none :eval never
# Chapter: Multivariate Distributions
# All code released under GPL Version 3
#+end_src

#+latex: \noindent 
We have built up quite a catalogue of distributions, discrete and continuous. They were all univariate, however, meaning that we only considered one random variable at a time. We can imagine nevertheless many random variables associated with a single person: their height, their weight, their wrist circumference (all continuous), or their eye/hair color, shoe size, whether they are right handed, left handed, or ambidextrous (all categorical), and we can even surmise reasonable probability distributions to associate with each of these variables.

But there is a difference: for a single person, these variables are related. For instance, a person's height betrays a lot of information about that person's weight.

The concept we are hinting at is the notion of /dependence/ between random variables. It is the focus of this chapter to study this concept in some detail. Along the way, we will pick up additional models to add to our catalogue. Moreover, we will study certain classes of dependence, and clarify the special case when there is no dependence, namely, independence.

The interested reader who would like to learn more about any of the below mentioned multivariate distributions should take a look at /Discrete Multivariate Distributions/ by Johnson /et al/\cite{Johnson1997} or /Continuous Multivariate Distributions/ \cite{Kotz2000} by Kotz /et al/.

*What do I want them to know?*
- the basic notion of dependence and how it is manifested with multiple variables (two, in particular)
- joint versus marginal distributions/expectation (discrete and continuous)
- some numeric measures of dependence
- conditional distributions, in the context of independence and exchangeability
- some details of at least one multivariate model (discrete and continuous)
- what it looks like when there are more than two random variables present

** Joint and Marginal Probability Distributions
:PROPERTIES:
:CUSTOM_ID: sec-Joint-Probability-Distributions
:END:

Consider two discrete random variables \(X\) and \(Y\) with PMFs \(f_{X}\) and \(f_{Y}\) that are supported on the sample spaces \(S_{X}\) and \(S_{Y}\), respectively. Let \(S_{X,Y}\) denote the set of all possible observed /pairs/ \((x,y)\), called the /joint support set/ of \(X\) and \(Y\). Then the /joint probability mass function/ of \(X\) and \(Y\) is the function \(f_{X,Y}\) defined by
\begin{equation}
\label{eq-joint-pmf}
f_{X,Y}(x,y)=\mathbb{P}(X=x,\, Y=y),\quad \mbox{for }(x,y)\in S_{X,Y}.
\end{equation}
Every joint PMF satisfies
\begin{equation}
f_{X,Y}(x,y)>0\mbox{ for all }(x,y)\in S_{X,Y},
\end{equation}
and
\begin{equation}
\sum_{(x,y)\in S_{X,Y}}f_{X,Y}(x,y)=1.
\end{equation}
It is customary to extend the function \(f_{X,Y}\) to be defined on all of \(\mathbb{R}^{2}\) by setting \(f_{X,Y}(x,y)=0\) for \((x,y)\not\in S_{X,Y}\). 

In the context of this chapter, the PMFs \(f_{X}\) and \(f_{Y}\) are called the /marginal PMFs/ of \(X\) and \(Y\), respectively. If we are given only the joint PMF then we may recover each of the marginal PMFs by using the Theorem of Total Probability (see Equation [[eq-theorem-total-probability][TTP]]): observe
\begin{eqnarray}
f_{X}(x) & = & \mathbb{P}(X=x),\\
 & = & \sum_{y\in S_{Y}}\mathbb{P}(X=x,\, Y=y),\\
 & = & \sum_{y\in S_{Y}}f_{X,Y}(x,y).
\end{eqnarray}
By interchanging the roles of \(X\) and \(Y\) it is clear that 
\begin{equation}
\label{eq-marginal-pmf}
f_{Y}(y)=\sum_{x\in S_{Y}}f_{X,Y}(x,y).
\end{equation}
Given the joint PMF we may recover the marginal PMFs, but the converse is not true. Even if we have /both/ marginal distributions they are not sufficient to determine the joint PMF; more information is needed
#+latex: \footnote{We are not at a total loss, however. There are Frechet bounds which pose limits on how large (and small) the joint distribution must be at each point.}.

Associated with the joint PMF is the /joint cumulative distribution function/ \(F_{X,Y}\) defined by
\[
F_{X,Y}(x,y)=\mathbb{P}(X\leq x,\, Y\leq y),\quad \mbox{for }(x,y)\in\mathbb{R}^{2}.
\]
The bivariate joint CDF is not quite as tractable as the univariate CDFs, but in principle we could calculate it by adding up quantities of the form in Equation [[eq-joint-pmf][JPMF]]. The joint CDF is typically not used in practice due to its inconvenient form; one can usually get by with the joint PMF alone.

We now introduce some examples of bivariate discrete distributions. The first we have seen before, and the second is based on the first.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-toss-two-dice-joint-pmf>>

Roll a fair die twice. Let \(X\) be the face shown on the first roll, and let \(Y\) be the face shown on the second roll. We have already seen this example in Chapter [[cha-Probability][Probability]], Example [[exa-Toss-a-six-sided-die-twice][Toss die twice]]. For this example, it suffices to define
\[
f_{X,Y}(x,y)=\frac{1}{36},\quad x=1,\ldots,6,\ y=1,\ldots,6.
\]
The marginal PMFs are given by \(f_{X}(x)=1/6\), \(x=1,2,\ldots,6\), and \(f_{Y}(y)=1/6\), \(y=1,2,\ldots,6\), since
\[
f_{X}(x)=\sum_{y=1}^{6}\frac{1}{36}=\frac{1}{6},\quad x=1,\ldots,6,
\]
and the same computation with the letters switched works for \(Y\). 
#+latex: \end{exampletoo}
#+html: </div>

In the previous example, and in many other ones, the joint support can be written as a product set of the support of \(X\) ``times'' the support of \(Y\), that is, it may be represented as a cartesian product set, or rectangle, \(S_{X,Y}=S_{X}\times S_{Y}\), where \(S_{X} \times S_{Y}= \{ (x,y):\ x\in S_{X},\, y\in S_{Y} \} \). As we shall see presently in Section [[sec-Independent-Random-Variables][Independent Random Variables]], this form is a necessary condition for \(X\) and \(Y\) to be /independent/ (or alternatively /exchangeable/ when \(S_{X}=S_{Y}\)). But please note that in general it is not required for \(S_{X,Y}\) to be of rectangle form. We next investigate just such an example.


#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-max-sum-two-dice>>

Let the random experiment again be to roll a fair die twice, except now let us define the random variables \(U\) and \(V\) by
\begin{eqnarray*}
U & = & \mbox{the maximum of the two rolls, and }\\
V & = & \mbox{the sum of the two rolls.}
\end{eqnarray*}
We see that the support of \(U\) is \(S_{U}= \{ 1,2,\ldots,6 \} \) and the support of \(V\) is \(S_{V}= \{ 2,3,\ldots,12 \} \). We may represent the sample space with a matrix, and for each entry in the matrix we may calculate the value that \(U\) assumes. The result is in the left half of Figure [[fig-max-and-sum-two-dice][Max-sum two dice]]. 

We can use the table to calculate the marginal PMF of \(U\), because from Example [[exa-Toss-a-six-sided-die-twice][Toss die twice]] we know that each entry in the matrix has probability \(1/36\) associated with it. For instance, there is only one outcome in the matrix with \(U=1\), namely, the bottom left corner. This single entry has probability \(1/36\), therefore, it must be that \(f_{U}(1)=\mathbb{P}(U=1)=1/36\). Similarly we see that there are three entries in the matrix with \(U=2\), thus \(f_{U}(2)=3/36\). Continuing in this fashion we will find the marginal distribution of \(U\) may be written
\begin{equation}
f_{U}(u)=\frac{2u-1}{36},\quad u=1,\,2,\ldots,6.
\end{equation}
We may do a similar thing for \(V\); see the right half of Figure [[fig-max-and-sum-two-dice][Max-sum two dice]]. Collecting all of the probability we will find that the marginal PMF of \(V\) is
\begin{equation}
f_{V}(v)=\frac{6-|v-7|}{36},\quad v=2,\,3,\ldots,12.
\end{equation}


#+name: max-and-sum-two-dice
#+begin_src R :exports none :results silent
A <- rolldie(2, makespace = TRUE)
A <- addrv(A, max, name = "U")
A <- addrv(A, sum, name = "V", invars = c("X1", "X2"))
p1 <- ggplot(A, aes(x=X1, y=X2, label=U))
p1 <- p1 + geom_text(size = 6) + xlab("X1 = First roll") + 
      ylab("X2 = Second roll") + opts(title = "U = max(X1,X2)")
p2 <- ggplot(A, aes(x=X1, y=X2, label=V))
p2 <- p2 + geom_text(size = 6) + xlab("X1 = First roll") + 
      ylab("") + opts(title = "V = X1 + X2")
grid.newpage()
pushViewport(viewport(layout = grid.layout(1, 2)))
vplayout <- function(x, y) viewport(layout.pos.row = x, layout.pos.col = y)
print(p1, vp = vplayout(1, 1))
print(p2, vp = vplayout(1, 2))
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/multdist/max-and-sum-two-dice.ps
  <<max-and-sum-two-dice>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/multdist/max-and-sum-two-dice.svg
  <<max-and-sum-two-dice>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/multdist/max-and-sum-two-dice.ps}
  \caption[Max and Sum of two dice]{\small Rolling two dice. The value of U is the maximum of the two rolls, while the value of V is the sum of the two rolls.}
  \label{fig-max-and-sum-two-dice}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-max-and-sum-two-dice" class="figure">
  <p><img src="svg/multdist/max-and-sum-two-dice.svg" width=500 alt="svg/multdist/max-and-sum-two-dice.svg" /></p>
  <p>Rolling two dice. The value of U is the maximum of the two rolls, while the value of V is the sum of the two rolls.</p>
</div>
#+end_html

We may collapse the two matrices from Figure [[fig-max-and-sum-two-dice][max-and-sum-two-dice]] into one, big matrix of pairs of values \((u,v)\). The result is shown in Table [[fig-max-sum-two-dice-joint][max-sum-two-dice-joint]]. 


#+name: max-sum-two-dice-joint
#+begin_src R :exports none :results silent
labs <- with(A, paste("(", U, ",", V, ")", sep = ""))
p3 <- ggplot(A, aes(x = X1, y = X2, label = labs))
p3 + geom_text(size = 6) + xlab("First roll") + ylab("Second roll") + 
  opts(title = "Joint distribution of (U,V) pairs")
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/multdist/max-sum-two-dice-joint.ps
  <<max-sum-two-dice-joint>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/multdist/max-sum-two-dice-joint.svg
  <<max-sum-two-dice-joint>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/multdist/max-sum-two-dice-joint.ps}
  \caption[Joint outcomes of Max and Sum]{\small Rolling two dice. Joint values of U and V are shown as pairs, for each outcome in the sample space.}
  \label{fig-max-sum-two-dice-joint}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-max-sum-two-dice-joint" class="figure">
  <p><img src="svg/multdist/max-sum-two-dice-joint.svg" width=500 alt="svg/multdist/max-sum-two-dice-joint.svg" /></p>
  <p>Rolling two dice. Joint values of U and V are shown as pairs, for each outcome in the sample space.</p>
</div>
#+end_html

Again, each of these pairs has probability \(1/36\) associated with it and we are looking at the joint PDF of \((U,V)\) albeit in an unusual form. Many of the pairs are repeated, but some of them are not: \((1,2)\) appears only once, but \((2,3)\) appears twice. We can make more sense out of this by writing a new table with \(U\) on one side and \(V\) along the top. We will accumulate the probability just like we did in Example [[exa-toss-two-dice-joint-pmf][Two dice joint]]. See Table [[tab-max-sum-joint-pmf][Max-sum joint]].

#+LABEL: tab-max-sum-joint-pmf
#+CAPTION: [The joint PMF of \((U,V)\)]{The outcomes of \(U\) are along the left and the outcomes of \(V\) are along the top. Empty entries in the table have zero probability. The row totals (on the right) and column totals (on the bottom) correspond to the marginal distribution of \(U\) and \(V\), respectively.}
|       | 2                | 3                | 4                | 5                | 6                | 7                | 8                | 9                | 10               | 11               | 12               | Total            |
|-------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------|
|     / | <                |                  |                  |                  |                  |                  |                  |                  |                  |                  |                  | <                |
|     1 | \(\frac{1}{36}\) |                  |                  |                  |                  |                  |                  |                  |                  |                  |                  | \(\frac{1}{36}\) |
|     2 |                  | \(\frac{2}{36}\) | \(\frac{1}{36}\) |                  |                  |                  |                  |                  |                  |                  |                  | \(\frac{3}{36}\)  |
|     3 |                  |                  | \(\frac{2}{36}\) | \(\frac{2}{36}\) | \(\frac{1}{36}\) |                  |                  |                  |                  |                  |                  | \(\frac{5}{36}\)  |
|     4 |                  |                  |                  | \(\frac{2}{36}\) | \(\frac{2}{36}\) | \(\frac{2}{36}\) | \(\frac{1}{36}\) |                  |                  |                  |                  | \(\frac{7}{36}\)  |
|     5 |                  |                  |                  |                  | \(\frac{2}{36}\) | \(\frac{2}{36}\) | \(\frac{2}{36}\) | \(\frac{2}{36}\) | \(\frac{1}{36}\) |                  |                  | \(\frac{9}{36}\)  |
|     6 |                  |                  |                  |                  |                  | \(\frac{2}{36}\) | \(\frac{2}{36}\) | \(\frac{2}{36}\) | \(\frac{2}{36}\) | \(\frac{2}{36}\) | \(\frac{1}{36}\) | \(\frac{11}{36}\) |
|-------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------|
| Total | \(\frac{1}{36}\) | \(\frac{2}{36}\) | \(\frac{3}{36}\)  | \(\frac{4}{36}\)  | \(\frac{5}{36}\)  | \(\frac{6}{36}\)  | \(\frac{5}{36}\)  | \(\frac{4}{36}\)  | \(\frac{3}{36}\)  | \(\frac{2}{36}\) | \(\frac{1}{36}\) | 1                |

The joint support of \((U,V)\) is concentrated along the main diagonal;  note that the nonzero entries do not form a rectangle. Also notice that if we form row and column totals we are doing exactly the same thing as Equation [[eq-marginal-pmf]], so that the marginal distribution of \(U\) is the list of totals in the right ``margin'' of the Table [[tab-max-sum-joint-pmf][Max-sum joint]], and the marginal distribution of \(V\)  is the list of totals in the bottom ``margin''. 
#+latex: \end{exampletoo}
#+html: </div>

Continuing the reasoning for the discrete case, given two continuous random variables \(X\) and \(Y\) there similarly exists
#+latex: \footnote{Strictly speaking, the joint density function does not necessarily exist. But the joint CDF always exists.}
a function \(f_{X,Y}(x,y)\) associated with \(X\) and \(Y\) called the /joint probability density function/ of \(X\) and \(Y\). Every joint PDF satisfies
\begin{equation}
f_{X,Y}(x,y)\geq0\mbox{ for all }(x,y)\in S_{X,Y},
\end{equation}
and
\begin{equation}
\iintop_{S_{X,Y}}f_{X,Y}(x,y)\,\mathrm{d} x\,\mathrm{d} y=1.
\end{equation}
In the continuous case there is not such a simple interpretation for the joint PDF; however, we do have one for the joint CDF, namely,
\[
F_{X,Y}(x,y)=\mathbb{P}(X\leq x,\, Y\leq y)=\int_{-\infty}^{x}\int_{-\infty}^{y}f_{X,Y}(u,v)\,\mathrm{d} v\,\mathrm{d} u,
\]
for \((x,y)\in\mathbb{R}^{2}\). If \(X\) and \(Y\) have the joint PDF \(f_{X,Y}\), then the marginal density of \(X\) may be recovered by
\begin{equation}
f_{X}(x)=\int_{S_{Y}}f_{X,Y}(x,y)\,\mathrm{d} y,\quad x \in S_{X}
\end{equation}
and the marginal PDF of \(Y\) may be found with
\begin{equation}
f_{Y}(y)=\int_{S_{X}}f_{X,Y}(x,y)\,\mathrm{d} x, \quad y \in S_{Y}.
\end{equation}


#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-joint-pdf>>
Let the joint PDF of \((X,Y)\) be given by
\[
f_{X,Y}(x,y)=\frac{6}{5}\left(x+y^{2}\right),\quad 0 < x < 1,\ 0 < y < 1.
\]
The marginal PDF of \(X\) is
\begin{eqnarray*}
f_{X}(x) & = & \int_{0}^{1}\frac{6}{5}\left(x+y^{2}\right)\,\mathrm{d} y,\\
 & = & \left.\frac{6}{5}\left(xy+\frac{y^{3}}{3}\right)\right|_{y=0}^{1},\\
 & = & \frac{6}{5}\left(x+\frac{1}{3}\right),
\end{eqnarray*}
for \(0 < x < 1\), and the marginal PDF of \(Y\) is
\begin{eqnarray*}
f_{Y}(y) & = & \int_{0}^{1}\frac{6}{5}\left(x+y^{2}\right)\,\mathrm{d} x,\\
 & = & \left.\frac{6}{5}\left(\frac{x^{2}}{2}+xy^{2}\right)\right|_{x=0}^{1},\\
 & = & \frac{6}{5}\left(\frac{1}{2}+y^{2}\right),
\end{eqnarray*}
for \(0 < y < 1\). In this example the joint support set was a rectangle \([0,1]\times[0,1]\), but it turns out that \(X\) and \(Y\) are not independent. See Section [[sec-Independent-Random-Variables][Independent Random Variables]].
#+latex: \end{exampletoo}
#+html: </div>

*** How to do it with \(\mathsf{R}\)

We will show how to do Example [[exa-max-sum-two-dice][Max-sum two dice]] using \(\mathsf{R}\); it is much simpler to do it with \(\mathsf{R}\) than without. First we set up the sample space with the =rolldie= function. Next, we add random variables \(U\) and \(V\) with the =addrv= function. We take a look at the very top of the data frame (probability space) to make sure that everything is operating according to plan.

#+begin_src R :exports both :results output pp  
S <- rolldie(2, makespace = TRUE)
S <- addrv(S, FUN = max, invars = c("X1","X2"), name = "U")
S <- addrv(S, FUN = sum, invars = c("X1","X2"), name = "V")
head(S)
#+end_src

Yes, the \(U\) and \(V\) columns have been added to the data frame and have been computed correctly. This result would be fine as it is, but the data frame has too many rows: there are repeated pairs \((u,v)\) which show up as repeated rows in the data frame. The goal is to aggregate the rows of \(S\) such that the result has exactly one row for each unique pair \((u,v)\) with positive probability. This sort of thing is exactly the task for which the =marginal= function was designed. We may take a look at the joint distribution of \(U\) and \(V\) (we only show the first few rows of the data frame, but the complete one has 11 rows).

#+begin_src R :exports both :results output pp 
UV <- marginal(S, vars = c("U", "V"))
head(UV)
#+end_src

The data frame is difficult to understand. It would be better to have a tabular display like Table [[tab-max-sum-joint-pmf][max-sum-joint-pmf]]. We can do that with the =xtabs= function. 

#+begin_src R :exports both :results output pp 
xtabs(round(probs,3) ~ U + V, data = UV)
#+end_src

Compare these values to the ones shown in Table [[tab-max-sum-joint-pmf][max-sum-joint-pmf]]. We can repeat the process with =marginal= to get the univariate marginal distributions of \(U\) and \(V\) separately.

#+begin_src R :exports both :results output pp 
marginal(UV, vars = "U")
head(marginal(UV, vars = "V"))
#+end_src

Another way to do the same thing is with the =rowSums= and =colSums= of the =xtabs= object. Compare

#+begin_src R :exports both :results output pp 
temp <- xtabs(probs ~ U + V, data = UV)
rowSums(temp)
colSums(temp)
#+end_src

You should check that the answers that we have obtained exactly match the same (somewhat laborious) calculations that we completed in Example [[exa-max-sum-two-dice][Max-sum two dice]].

** Joint and Marginal Expectation
:PROPERTIES:
:CUSTOM_ID: sec-Joint-and-Marginal-Expectation
:END:

Given a function \(g\) with arguments \((x,y)\) we would like to know the long-run average behavior of \(g(X,Y)\) and how to mathematically calculate it. Expectation in this context is computed in the pedestrian way. We simply integrate (sum) with respect to the joint probability density (mass) function.
\begin{equation}
\mathbb{E}\, g(X,Y)=\iintop_{S_{X,Y}}g(x,y)\, f_{X,Y}(x,y)\,\mathrm{d} x\,\mathrm{d} y,
\end{equation}
or in the discrete case,
\begin{equation}
\mathbb{E}\, g(X,Y)=\mathop{\sum\sum}\limits _{(x,y)\in S_{X,Y}}g(x,y)\, f_{X,Y}(x,y).
\end{equation}

*** Covariance and Correlation

There are two very special cases of joint expectation: the /covariance/ and the /correlation/. These are measures which help us quantify the dependence between \(X\) and \(Y\). 

#+begin_defn
The /covariance/ of \(X\) and \(Y\) is
\begin{equation}
\mbox{Cov}(X,Y)=\mathbb{E}(X-\mathbb{E} X)(Y-\mathbb{E} Y).
\end{equation}
#+end_defn

By the way, there is a shortcut formula for covariance which is almost as handy as the shortcut for the variance:
\begin{equation}
\mbox{Cov}(X,Y)=\mathbb{E}(XY)-(\mathbb{E} X)(\mathbb{E} Y).
\end{equation}
The proof is left to Exercise [[xca-Prove-cov-shortcut][Prove Covariance Shortcut]].

The Pearson product moment correlation between \(X\) and \(Y\) is the covariance between \(X\) and \(Y\) rescaled to fall in the interval \([-1,1]\). It is formally defined by 
\begin{equation}
\mbox{Corr}(X,Y)=\frac{\mbox{Cov}(X,Y)}{\sigma_{X}\sigma_{Y}}.
\end{equation}

The correlation is usually denoted by \(\rho_{X,Y}\) or simply \(\rho\) if the random variables are clear from context. There are some important facts about the correlation coefficient: 
1. The range of correlation is \(-1\leq\rho_{X,Y}\leq1\).
1. Equality holds above (\(\rho_{X,Y}=\pm1\)) if and only if \(Y\) is a linear function of \(X\) with probability one.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-max-sum-dice-covariance>>

We will compute the covariance for the discrete distribution in Example [[exa-max-sum-two-dice][Max-sum two dice]]. The expected value of \(U\) is
\[
\mathbb{E} U=\sum_{u=1}^{6}u\, f_{U}(u)=\sum_{u=1}^{6}u\,\frac{2u-1}{36}=1\left(\frac{1}{36}\right)+2\left(\frac{3}{36}\right)+\cdots+6\left(\frac{11}{36}\right)=\frac{161}{36},
\]
and the expected value of \(V\) is
\[
\mathbb{E} V=\sum_{v=2}^{12}v\,\frac{6-|7-v|}{36}=2\left(\frac{1}{36}\right)+3\left(\frac{2}{36}\right)+\cdots+12\left(\frac{1}{36}\right)=7,
\]
and the expected value of \(UV\) is
\[
\mathbb{E} UV=\sum_{u=1}^{6}\sum_{v=2}^{12}uv\, f_{U,V}(u,v)=1\cdot2\left(\frac{1}{36}\right)+2\cdot3\left(\frac{2}{36}\right)+\cdots+6\cdot12\left(\frac{1}{36}\right)=\frac{308}{9}.
\]
Therefore the covariance of \((U,V)\) is
\[
\mbox{Cov}(U,V)=\mathbb{E} UV-\left(\mathbb{E} U\right)\left(\mathbb{E} V\right)=\frac{308}{9}-\frac{161}{36}\cdot7=\frac{35}{12}.
\]
All we need now are the standard deviations of \(U\) and \(V\) to calculate the correlation coefficient (omitted).
#+latex: \end{exampletoo}
#+html: </div>

We will do a continuous example so that you can see how it works.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">

Let us find the covariance of the variables \((X,Y)\) from Example [[exa-joint-pdf]]. The expected value of \(X\) is
\[
\mathbb{E} X=\int_{0}^{1}x\cdot\frac{6}{5}\left(x+\frac{1}{3}\right)\mathrm{d} x=\left.\frac{2}{5}x^{3}+\frac{1}{5}x^{2}\right|_{x=0}^{1}=\frac{3}{5},
\]
and the expected value of \(Y\) is
\[
\mathbb{E} Y=\int_{0}^{1}y\cdot\frac{6}{5}\left(\frac{1}{2}+y^{2}\right)\mathrm{d} x=\left.\frac{3}{10}y^{2}+\frac{3}{20}y^{4}\right|_{y=0}^{1}=\frac{9}{20}.
\]
Finally, the expected value of \(XY\) is
\begin{eqnarray*}
\mathbb{E} XY & = & \int_{0}^{1}\int_{0}^{1}xy\,\frac{6}{5}\left(x+y^{2}\right)\mathrm{d} x\,\mathrm{d} y,\\
 & = & \int_{0}^{1}\left.\left(\frac{2}{5}x^{3}y+\frac{3}{5}x^{2}y^{3}\right)\right|_{x=0}^{1}\mathrm{d} y,\\
 & = & \int_{0}^{1}\left(\frac{2}{5}y+\frac{3}{5}y^{3}\right)\mathrm{d} y,\\
 & = & \frac{1}{5}+\frac{3}{20},
\end{eqnarray*}
which is 7/20. Therefore the covariance of \((X,Y)\) is
\[
\mbox{Cov}(X,Y)=\frac{7}{20}-\left(\frac{3}{5}\right)\left(\frac{9}{20}\right)=\frac{2}{25}.
\]
#+latex: \end{exampletoo}
#+html: </div>

**** How to do it with \(\mathsf{R}\)

There are not any specific functions in the =prob= package \cite{prob} designed for multivariate expectation. This is not a problem, though, because it is easy enough to do expectation the long way -- with column operations. We just need to keep the definition in mind. For instance, we may compute the covariance of \((U,V)\) from Example [[exa-max-sum-dice-covariance][Max-sum covariance]].

#+begin_src R :exports both :results output pp 
Eu <- sum(S$U*S$probs)
Ev <- sum(S$V*S$probs)
Euv <- sum(S$U*S$V*S$probs)
Euv - Eu * Ev
#+end_src

Compare this answer to what we got in Example [[exa-max-sum-dice-covariance][Max-sum covariance]].

To do the continuous case we could use the computer algebra utilities of =Yacas= and the associated \(\mathsf{R}\) package =Ryacas= \cite{Ryacas}. See Section [[sub-bivariate-transf-R][Bivariate Trans with R]] for another example where the =Ryacas= package appears.

** Conditional Distributions
:PROPERTIES:
:CUSTOM_ID: sec-Conditional-Distributions
:END:

If \(x\in S_{X}\) is such that \(f_{X}(x)>0\), then we define the /conditional density/ of \(Y|\, X=x\), denoted \(f_{Y|x}\), by 
\begin{equation}
f_{Y|x}(y|x)=\frac{f_{X,Y}(x,y)}{f_{X}(x)},\quad y\in S_{Y}.
\end{equation}
We define \(f_{X|y}\) in a similar fashion.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Let the joint PMF of \(X\) and \(Y\) be given by
\[
f_{X,Y}(x,y) =
\]
#+latex: \end{exampletoo}
#+html: </div>

*** Bayesian Connection

Conditional distributions play a fundamental role in Bayesian probability and statistics. There is a parameter \(\theta\) which is of primary interest, and about which we would like to learn. But rather than observing \(\theta\) directly, we instead observe a random variable \(X\) whose probability distribution depends on \(\theta\). Using the information we provided by \(X,\) we would like to update the information that we have about \(\theta\).

Our initial beliefs about \(\theta\) are represented by a probability distribution, called the /prior distribution/, denoted by \(\pi\). The PDF \(f_{X|\theta}\) is called the /likelihood function/, also called the /likelihood of/ \(X\) /conditional on/ \(\theta\). Given an observation \(X=x\), we would like to update our beliefs \(\pi\) to a new distribution, called the /posterior distribution of/ \(\theta\) /given the observation/ \(X=x\), denoted \(\pi_{\theta|x}\). It may seem a mystery how to obtain \(\pi_{\theta|x}\) based only on the information provided by \(\pi\) and \(f_{X|\theta}\), but it should not be. We have already studied this in Section [[sec-Bayes-Rule][Bayes' Rule]] where it was called Bayes' Rule:
\begin{equation} 
\pi(\theta|x)=\frac{\pi(\theta)\, f(x|\theta)}{\int\pi(u)\, f(x|u)\mathrm{d} u}.
\end{equation} 
Compare the above expression to Equation [[eq-bayes-rule][BR]].

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Suppose the parameter \(\theta\) is the \(\mathbb{P}(\mbox{Heads})\) for a biased coin. It could be any value from 0 to 1. Perhaps we have some prior information about this coin, for example, maybe we have seen this coin before and we have reason to believe that it shows Heads less than half of the time. Suppose that we represent our beliefs about \(\theta\) with a \(\mathsf{beta}(\mathtt{shape1}=1,\,\mathtt{shape2}=3)\) prior distribution, that is, we assume 
\[
\theta\sim\pi(\theta)=3(1-\theta)^{2},\quad 0 < \theta < 1.
\]
To learn more about \(\theta\), we will do what is natural: flip the coin. We will observe a random variable \(X\) which takes the value \(1\) if the coin shows Heads, and 0 if the coin shows Tails. Under these circumstances, \(X\) will have a Bernoulli distribution, and in particular, \(X|\theta\sim\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=\theta)\):
\[ 
f_{X|\theta}(x|\theta)=\theta^{x}(1-\theta)^{1-x},\quad x=0,1.
\]
Based on the observation \(X=x\), we will update the prior distribution to the posterior distribution, and we will do so with Bayes' Rule: it says
\begin{eqnarray*}
\pi(\theta|x) & \propto & f(x|\theta) \, \pi(\theta),\\
 & = & \theta^{x}(1-\theta)^{1-x}\cdot3(1-\theta)^{2},\\
 & = & 3\,\theta^{x}(1-\theta)^{3-x},\quad 0 < \theta < 1,
\end{eqnarray*}
where the constant of proportionality is given by
\[
\int3\, u^{x}(1-u)^{3-x}\mathrm{d} u=\int3\, u^{(1+x)-1}(1-u)^{(4-x)-1}\mathrm{d} u=3\,\frac{\Gamma(1+x)\Gamma(4-x)}{\Gamma[(1+x)+(4-x)]},
\]
the integral being calculated by inspection of the formula for a \(\mathsf{beta}(\mathtt{shape1}=1+x,\,\mathtt{shape2}=4-x)\) distribution. That is to say, our posterior distribution is precisely
\[
\theta|x\sim\mathsf{beta}(\mathtt{shape1}=1+x,\,\mathtt{shape2}=4-x).
\]
The Bayesian statistician uses the posterior distribution for all matters concerning inference about \(\theta\).

#+latex: \end{exampletoo}
#+html: </div>

#+begin_rem
We usually do not restrict ourselves to the observation of only one \(X\) conditional on \(\theta\). In fact, it is common to observe an entire sample \(X_{1}\), \(X_{2}\),...,\(X_{n}\) conditional on \(\theta\) (which itself is often multidimensional). Do not be frightened, however, because the intuition is the same. There is a prior distribution \(\pi(\theta)\), a likelihood \(f(x_{1},x_{2},\ldots,x_{n}|\theta)\), and a posterior distribution \(\pi(\theta|x_{1},x_{2},\ldots,x_{n})\). Bayes' Rule states that the relationship between the three is
\[
\pi(\theta|x_{1},x_{2},\ldots,x_{n})\propto\pi(\theta)\, f(x_{1},x_{2},\ldots,x_{n}|\theta),
\]
where the constant of proportionality is \(\int\pi(u)\, f(x_{1},x_{2},\ldots,x_{n}|u)\,\mathrm{d} u\). Any good textbook on Bayesian Statistics will explain these notions in detail; to the interested reader I recommend Gelman \cite{Gelman2004} or Lee \cite{Lee1997}.
#+end_rem

** Independent Random Variables
:PROPERTIES:
:CUSTOM_ID: sec-Independent-Random-Variables
:END:

*** Independent Random Variables
:PROPERTIES:
:CUSTOM_ID: sub-Independent-Random-Variables
:END:

We recall from Chapter [[cha-Probability][Probability]] that the events \(A\) and \(B\) are said to be independent when
\begin{equation}
\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B).
\end{equation}
If it happens that
\begin{equation}
\mathbb{P}(X=x,Y=y)=\mathbb{P}(X=x)\mathbb{P}(Y=y),\quad \mbox{for every }x\in S_{X},\ y\in S_{Y},
\end{equation}
then we say that \(X\) and \(Y\) are /independent random variables/. Otherwise, we say that \(X\) and \(Y\) are /dependent/. Using the PMF notation from above, we see that independent discrete random variables satisfy
\begin{equation}
f_{X,Y}(x,y)=f_{X}(x)f_{Y}(y)\quad \mbox{for every }x\in S_{X},\ y\in S_{Y}.
\end{equation}
Continuing the reasoning, given two continuous random variables \(X\) and \(Y\) with joint PDF \(f_{X,Y}\) and respective marginal PDFs \(f_{X}\) and \(f_{Y}\) that are supported on the sets \(S_{X}\) and \(S_{Y}\), if it happens that
\begin{equation}
f_{X,Y}(x,y)=f_{X}(x)f_{Y}(y)\quad \mbox{for every }x\in S_{X},\ y\in S_{Y},
\end{equation}
then we say that \(X\) and \(Y\) are independent.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
In Example [[exa-toss-two-dice-joint-pmf][Toss two dice joint]] we considered the random experiment of rolling a fair die twice. There we found the joint PMF to be
\[
f_{X,Y}(x,y)=\frac{1}{36},\quad x=1,\ldots,6,\ y=1,\ldots,6,
\]
and we found the marginal PMFs \(f_{X}(x)=1/6\), \(x=1,2,\ldots,6\), and \(f_{Y}(y)=1/6\), \(y=1,2,\ldots,6\). Therefore in this experiment \(X\) and \(Y\) are independent since for every \(x\) and \(y\) in the joint support the joint PMF satisfies
\[
f_{X,Y}(x,y)=\frac{1}{36}=\left(\frac{1}{6}\right)\left(\frac{1}{6}\right)=f_{X}(x)\, f_{Y}(y).
\]
#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
In Example [[exa-max-sum-two-dice][Max-sum two dice]] we considered the same experiment but different random variables \(U\) and \(V\). We can prove that \(U\) and \(V\) are not independent if we can find a single pair \((u,v)\) where the independence equality does not hold. There are many such pairs. One of them is \((6,12)\):
\[
f_{U,V}(6,12)=\frac{1}{36}\neq\left(\frac{11}{36}\right)\left(\frac{1}{36}\right)=f_{U}(6)\, f_{V}(12).
\]
#+latex: \end{exampletoo}
#+html: </div>

Independent random variables are very useful to the mathematician. They have many, many, tractable properties. We mention some of the more important ones.

#+begin_prop
# <<pro-indep-implies-prodexpect>>
If \(X\) and \(Y\) are independent, then for any functions \(u\) and \(v\), 
\begin{equation}
\mathbb{E}\left(u(X)v(Y)\right)=\left(\mathbb{E} u(X)\right)\left(\mathbb{E} v(Y)\right).
\end{equation}
#+end_prop

#+begin_proof
This is straightforward from the definition.
\begin{eqnarray*}
\mathbb{E}\left(u(X)v(Y)\right) & = & \iint\, u(x)v(y)\, f_{X,Y}(x,y)\,\mathrm{d} x\mathrm{d} y\\
 & = & \iint\, u(x)v(y)\, f_{X}(x)\, f_{Y}(y)\,\mathrm{d} x\mathrm{d} y\\
 & = & \int u(x)\, f_{X}(x)\,\mathrm{d} x\ \int v(y)\, f_{Y}(y)\,\mathrm{d} y
\end{eqnarray*}
and this last quantity is exactly \(\left(\mathbb{E} u(X)\right)\left(\mathbb{E} v(Y)\right)\). 
#+end_proof

Now that we have Proposition [[pro-indep-implies-prodexpect]] we mention a corollary that will help us later to quickly identify those random variables which are /not/ independent.

#+begin_cor
# <<cor-indep-implies-uncorr>>
If \(X\) and \(Y\) are independent, then \(\mbox{Cov}(X,Y)=0\), and consequently,
\(\mbox{Corr}(X,Y)=0\).
#+end_cor

#+begin_proof
When \(X\) and \(Y\) are independent then \(\mathbb{E} XY=\mathbb{E} X\,\mathbb{E} Y\). And when the covariance is zero the numerator of the correlation is 0.
#+end_proof

#+begin_rem
# <<rem-cov0-not-imply-indep>>
Unfortunately, the converse of Corollary [[cor-indep-implies-uncorr]] is not true. That is, there are many random variables which are dependent yet their covariance and correlation is zero. For more details, see Casella and Berger \cite{Casella2002}.
#+end_rem

Proposition [[pro-indep-implies-prodexpect]] is useful to us and we will receive mileage out of it, but there is another fact which will play an even more important role. Unfortunately, the proof is beyond the techniques presented here. The inquisitive reader should consult Casella and Berger \cite{Casella2002}, Resnick \cite{Resnick1999}, /etc/.

#+begin_fact
# <<fac-indep-then-function-indep>>
If \(X\) and \(Y\) are independent, then \(u(X)\) and \(v(Y)\) are independent for any functions \(u\) and \(v\).
#+end_fact

*** Combining Independent Random Variables
:PROPERTIES:
:CUSTOM_ID: sub-Combining-Independent-Random
:END:

Another important corollary of Proposition [[pro-indep-implies-prodexpect]] will allow us to find the distribution of sums of random variables. 

#+begin_cor
If \(X\) and \(Y\) are independent, then the moment generating function of \(X+Y\) is 
\begin{equation}
M_{X+Y}(t)=M_{X}(t)\cdot M_{Y}(t).
\end{equation}
#+end_cor

#+begin_proof
Choose \(u(x)=\mathrm{e}^{x}\) and \(v(y)=\mathrm{e}^{y}\) in Proposition [[pro-indep-implies-prodexpect]], and remember the identity \(\mathrm{e}^{t(x+y)}=\mathrm{e}^{tx}\,\mathrm{e}^{ty}\).
#+end_proof

Let us take a look at some examples of the corollary in action.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Let \(X\sim\mathsf{binom}(\mathtt{size}=n_{1},\,\mathtt{prob}=p)\) and \(Y\sim\mathsf{binom}(\mathtt{size}=n_{2},\,\mathtt{prob}=p)\) be independent. Then \(X+Y\) has MGF
\[
M_{X+Y}(t)=M_{X}(t)\, M_{Y}(t)=\left(q+p\mathrm{e}^{t}\right)^{n_{1}}\left(q+p\mathrm{e}^{t}\right)^{n_{2}}=\left(q+p\mathrm{e}^{t}\right)^{n_{1}+n_{2}},
\]
which is the MGF of a \(\mathsf{binom}(\mathtt{size}=n_{1}+n_{2},\,\mathtt{prob}=p)\) distribution. Therefore, \(X+Y\sim\mathsf{binom}(\mathtt{size}=n_{1}+n_{2},\,\mathtt{prob}=p)\).
#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Let \(X\sim\mathsf{norm}(\mathtt{mean}=\mu_{1},\,\mathtt{sd}=\sigma_{1})\) and \(Y\sim\mathsf{norm}(\mathtt{mean}=\mu_{2},\,\mathtt{sd}=\sigma_{2})\) be independent. Then \(X+Y\) has MGF
\[
M_{X}(t)\, M_{Y}(t)=\exp\left\{ \mu_{1}t+t^{2}\sigma_{1}^{2}/2\right\} \exp\left\{ \mu_{2}t+t^{2}\sigma_{2}^{2}/2\right\} =\exp\left\{ \left(\mu_{1}+\mu_{2}\right)t+t^{2}\left(\sigma_{1}^{2}+\sigma_{2}^{2}\right)/2\right\} ,
\]
which is the MGF of a \(\mathsf{norm}\left(\mathtt{mean}=\mu_{1}+\mu_{2},\,\mathtt{sd}=\sqrt{\sigma_{1}^{2}+\sigma_{2}^{2}}\right)\) distribution. 
#+latex: \end{exampletoo}
#+html: </div>

Even when we cannot use the MGF trick to identify the exact distribution of a linear combination of random variables, we can still say something about its mean and variance.

#+begin_prop
# <<pro-mean-sd-lin-comb-two>>
Let \(X_{1}\) and \(X_{2}\) be independent with respective population means \(\mu_{1}\) and \(\mu_{2}\) and population standard deviations \(\sigma_{1}\) and \(\sigma_{2}\). For given constants \(a_{1}\) and \(a_{2}\), define \(Y=a_{1}X_{1}+a_{2}X_{2}\). Then the mean and standard deviation of \(Y\) are given by the formulas
\begin{equation}
\mu_{Y}=a_{1}\mu_{1}+a_{2}\mu_{2},\quad \sigma_{Y}=\left(a_{1}^{2}\sigma_{1}^{2}+a_{2}^{2}\sigma_{2}^{2}\right)^{1/2}.
\end{equation}
#+end_prop

#+begin_proof
We use Proposition [[pro-expectation-properties][Expectation Properties]]:
\[
\mathbb{E} Y=\mathbb{E}\left(a_{1}X_{1}+a_{2}X_{2}\right)=a_{1}\mathbb{E} X_{1}+a_{2}\mathbb{E} X_{2}=a_{1}\mu_{1}+a_{2}\mu_{2}.
\]
For the standard deviation, we will find the variance and take the square root at the end. And to calculate the variance we will first compute \(\mathbb{E} Y^{2}\) with an eye toward using the identity \(\sigma_{Y}^{2}=\mathbb{E} Y^{2}-\left(\mathbb{E} Y\right)^{2}\) as a final step. 
\[
\mathbb{E} Y^{2}=\mathbb{E}\left(a_{1}X_{1}+a_{2}X_{2}\right)^{2}=\mathbb{E}\left(a_{1}^{2}X_{1}^{2}+a_{2}^{2}X_{2}^{2}+2a_{1}a_{2}X_{1}X_{2}\right).
\]
Using linearity of expectation the \(\mathbb{E}\) distributes through the sum. Now \(\mathbb{E} X_{i}^{2}=\sigma_{i}^{2}+\mu_{i}^{2}\), for \(i=1\) and 2 and \(\mathbb{E} X_{1}X_{2}=\mathbb{E} X_{1}\mathbb{E} X_{2}=\mu_{1}\mu_{2}\) because of independence. Thus
\begin{eqnarray*}
\mathbb{E} Y^{2} & = & a_{1}^{2}(\sigma_{1}^{2}+\mu_{1}^{2})+a_{2}^{2}(\sigma_{2}^{2}+\mu_{2}^{2})+2a_{1}a_{2}\mu_{1}\mu_{2},\\
 & = & a_{1}^{2}\sigma_{1}^{2}+a_{2}^{2}\sigma_{2}^{2}+\left(a_{1}^{2}\mu_{1}^{2}+a_{2}^{2}\mu_{2}^{2}+2a_{1}a_{2}\mu_{1}\mu_{2}\right).
\end{eqnarray*}
But notice that the expression in the parentheses is exactly \(\left(a_{1}\mu_{1}+a_{2}\mu_{2}\right)^{2}=\left(\mathbb{E} Y\right)^{2}\), so the proof is complete.
#+end_proof

** Exchangeable Random Variables
:PROPERTIES:
:CUSTOM_ID: sec-Exchangeable-Random-Variables
:END:

Two random variables \(X\) and \(Y\) are said to be /exchangeable/ if their joint CDF is a symmetric function of its arguments:
\begin{equation}
F_{X,Y}(x,y)=F_{X,Y}(y,x),\quad \mbox{for all }(x,y)\in\mathbb{R}^{2}.
\end{equation} 
When the joint density \(f\) exists, we may equivalently say that \(X\) and \(Y\) are exchangeable if \(f(x,y)=f(y,x)\) for all \((x,y)\).

Exchangeable random variables exhibit symmetry in the sense that a person may exchange one variable for the other with no substantive changes to their joint random behavior. While independence speaks to a /lack of influence/ between the two variables, exchangeability aims to capture the /symmetry/ between them.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Let \(X\) and \(Y\) have joint PDF
\begin{multline}
f_{X,Y}(x,y)=(1+\alpha)\lambda^{2}\mathrm{e}^{-\lambda(x+y)}+\alpha(2\lambda)^{2}\mathrm{e}^{-2\lambda(x+y)}-2\alpha\lambda^{2}\left(\mathrm{e}^{-\lambda(2x+y)}+\mathrm{e}^{-\lambda(x+2y)}\right).
\end{multline}
It is straightforward and tedious to check that \(\iint f=1\). We may see immediately that \(f_{X,Y}(x,y)=f_{X,Y}(y,x)\) for all \((x,y)\), which confirms that \(X\) and \(Y\) are exchangeable. Here, \(\alpha\) is said to be an association parameter. This particular example is one from the Farlie-Gumbel-Morgenstern family of distributions; see \cite{Kotz2000}.
#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-binom-exchangeable>>
Suppose \(X\) and \(Y\) are IID \(\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p)\). Then their joint PMF is
\begin{eqnarray*}
f_{X,Y}(x,y) & = & f_{X}(x)f_{Y}(y)\\
 & = & {n \choose x}\, p^{x}(1-p)^{n-x}\,{n \choose y}\, p^{y}(1-p)^{n-y},\\
 & = & {n \choose x}{n \choose y}\, p^{x+y}(1-p)^{2n-(x+y)},
\end{eqnarray*}
and the value is the same if we exchange \(x\) and \(y\). Therefore \((X,Y)\) are exchangeable.
#+latex: \end{exampletoo}
#+html: </div>

Looking at Example [[exa-binom-exchangeable][Binomial Exchangeable]] more closely we see that the fact that \((X,Y)\) are exchangeable has nothing to do with the \(\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p)\) distribution; it only matters that they are independent (so that the joint PDF factors) and they are identically distributed (in which case we may swap letters to no effect). We could just have easily used any other marginal distribution. We will take this as a proof of the following proposition.

#+begin_prop
If \(X\) and \(Y\) are IID (with common marginal distribution \(F\)) then \(X\) and \(Y\) are exchangeable. 
#+end_prop

Exchangeability thus contains IID as a special case. 

** The Bivariate Normal Distribution
:PROPERTIES:
:CUSTOM_ID: sec-The-Bivariate-Normal
:END:

The bivariate normal PDF is given by the unwieldy formula
\begin{multline}
f_{X,Y}(x,y)=\frac{1}{2\pi\,\sigma_{X}\sigma_{Y}\sqrt{1-\rho^{2}}}\exp\left\{ -\frac{1}{2(1-\rho^{2})}\left[\left(\frac{x-\mu_{X}}{\sigma_{X}}\right)^{2}+\cdots\right.\right.\\
\left.\left.\cdots+2\rho\left(\frac{x-\mu_{X}}{\sigma_{X}}\right)\left(\frac{y-\mu_{Y}}{\sigma_{Y}}\right)+\left(\frac{y-\mu_{Y}}{\sigma_{Y}}\right)^{2}\right]\right\} ,
\end{multline}
for \((x,y)\in\mathbb{R}^{2}\). We write \((X,Y)\sim\mathsf{mvnorm}(\mathtt{mean}=\upmu,\,\mathtt{sigma}=\Sigma)\), where
\begin{equation}
\upmu=(\mu_{X},\,\mu_{Y})^{T},\quad \sum=\left(
\begin{array}{cc}
\sigma_{X}^{2} & \rho\sigma_{X}\sigma_{Y}\\
\rho\sigma_{X}\sigma_{Y} & \sigma_{Y}^{2}
\end{array}
\right).
\end{equation}
See Appendix [[cha-Mathematical-Machinery][Mathematical Machinery]]. The vector notation allows for a more compact rendering of the joint PDF:
\begin{equation}
f_{X,Y}(\mathbf{x})=\frac{1}{2\pi\left|\Sigma\right|^{1/2}}\exp\left\{ -\frac{1}{2}\left(\mathbf{x}-\upmu\right)^{\top}\Sigma^{-1}\left(\mathbf{x}-\upmu\right)\right\} ,
\end{equation}
where in an abuse of notation we have written \(\mathbf{x}\) for \((x,y)\). Note that the formula only holds when \(\rho\neq\pm1\).

#+begin_rem
In Remark [[rem-cov0-not-imply-indep]] we noted that just because random variables are uncorrelated it does not necessarily mean that they are independent. However, there is an important exception to this rule: the bivariate normal distribution. Indeed, \((X,Y)\sim\mathsf{mvnorm}(\mathtt{mean}=\upmu,\,\mathtt{sigma}=\Sigma)\) are independent if and only if \(\rho=0\). 
#+end_rem

#+begin_rem
Inspection of the joint PDF shows that if \(\mu_{X}=\mu_{Y}\) and \(\sigma_{X}=\sigma_{Y}\) then \(X\) and \(Y\) are exchangeable.
#+end_rem

The bivariate normal MGF is
\begin{equation}
M_{X,Y}(\mathbf{t})=\exp\left(\upmu^{\top}\mathbf{t}+\frac{1}{2}\mathbf{t}^{\top}\Sigma\mathbf{t}\right),
\end{equation}
where \(\mathbf{t}=(t_{1},t_{2})\).

The bivariate normal distribution may be intimidating at first but it turns out to be very tractable compared to other multivariate distributions. An example of this is the following fact about the marginals. 

#+begin_fact
If \((X,Y)\sim\mathsf{mvnorm}(\mathtt{mean}=\upmu,\,\mathtt{sigma}=\Sigma)\) then
\begin{equation}
X\sim\mathsf{norm}(\mathtt{mean}=\mu_{X},\,\mathtt{sd}=\sigma_{X})\mbox{ and }Y\sim\mathsf{norm}(\mathtt{mean}=\mu_{Y},\,\mathtt{sd}=\sigma_{Y}).
\end{equation}
#+end_fact

From this we immediately get that \(\mathbb{E} X=\mu_{X}\) and \(\mbox{Var}(X)=\sigma_{X}^{2}\) (and the same is true for \(Y\) with the letters switched). And it should be no surprise that the correlation between \(X\) and \(Y\) is exactly \(\mbox{Corr}(X,Y)=\rho\).

#+begin_prop
# <<pro-mvnorm-cond-dist>>
The conditional distribution of \(Y|\, X=x\) is \(\mathsf{norm}(\mathtt{mean}=\mu_{Y|x},\,\mathtt{sd}=\sigma_{Y|x})\), where
\begin{equation}
\mu_{Y|x}=\mu_{Y}+\rho\frac{\sigma_{Y}}{\sigma_{X}}\left(x-\mu_{X}\right),\mbox{ and }\sigma_{Y|x}=\sigma_{Y}\sqrt{1-\rho^{2}}.
\end{equation}
#+end_prop

There are a few things to note about Proposition [[pro-mvnorm-cond-dist]] which will be important in Chapter [[cha-simple-linear-regression][Simple Linear Regression]]. First, the conditional mean of \(Y|x\) is linear in \(x\), with slope
\begin{equation}
\label{eq-population-slope-slr}
\rho\,\frac{\sigma_{Y}}{\sigma_{X}}.
\end{equation}
Second, the conditional variance of \(Y|x\) is independent of \(x\). 

*** How to do it with \(\mathsf{R}\)

The multivariate normal distribution is implemented in both the =mvtnorm= package \cite{mvtnorm:1} and the =mnormt= package \cite{mnormt}. We use the =mvtnorm= package in this book simply because it is a dependency of another package used in the book. 

The =mvtnorm= package has functions =dmvnorm= and =rmvnorm= for the PDF and to generate random vectors, respectively. Let us get started with a graph of the bivariate normal PDF. We can make the plot with the following code, where the workhorse is the =persp= function in base \(\mathsf{R}\).

Another way to do this is with the =curve3d= function in the =emdbook= package \cite{emdbook}. It looks like this:
: library("emdbook"); library("mvtnorm") # note: the order matters
: mu <- c(0,0); sigma <- diag(2)
: f <- function(x,y) dmvnorm(c(x,y), mean = mu, sigma = sigma)
: curve3d(f(x,y), from = c(-3,-3), to = c(3,3), theta = -30, phi = 30)
The code above is slightly shorter than that using =persp= and is easier to understand. One must be careful, however. If the =library= calls are swapped then the code will not work because both packages =emdbook= and =mvtnorm= have a function called =dmvnorm=; one must load them to the search path in the correct order or \(\mathsf{R}\) will use the wrong one (the arguments are named differently and the underlying algorithms are different).

#+begin_src R :exports code :eval never
x <- y <- seq(from = -3, to = 3, length.out = 30)
f <- function(x,y) dmvnorm(cbind(x,y), mean = c(0,0), sigma = diag(2))
z <- outer(x, y, FUN = f)
persp(x, y, z, theta = -30, phi = 30, ticktype = "detailed")
#+end_src

We chose the standard bivariate normal, \(\mathsf{mvnorm}(\mathtt{mean}=\mathbf{0},\,\mathtt{sigma}=\mathbf{I})\), to display.

#+name: mvnorm-pdf
#+begin_src R :exports code :results silent
x <- y <- seq(from = -3, to = 3, length.out = 30)
f <- function(x,y) dmvnorm(cbind(x,y), mean = c(0,0), sigma = diag(2))
z <- outer(x, y, FUN = f)
persp(x, y, z, theta = -30, phi = 30, ticktype = "detailed")
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/multdist/mvnorm-pdf.ps
  <<mvnorm-pdf>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/multdist/mvnorm-pdf.svg
  <<mvnorm-pdf>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/multdist/mvnorm-pdf.ps}
  \caption[Graph of a bivariate normal PDF]{\small A graph of a bivariate normal PDF.}
  \label{fig-mvnorm-pdf}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-mvnorm-pdf" class="figure">
  <p><img src="svg/multdist/mvnorm-pdf.svg" width=500 alt="svg/multdist/mvnorm-pdf.svg" /></p>
  <p>A graph of a bivariate normal PDF.</p>
</div>
#+end_html

** Bivariate Transformations of Random Variables
:PROPERTIES:
:CUSTOM_ID: sec-Transformations-Multivariate
:END:

We studied in Section [[sec-Functions-of-Continuous][Functions of Continuous RVs]] how to find the PDF of \(Y=g(X)\) given the PDF of \(X\). But now we have two random variables \(X\) and Y, with joint PDF \(f_{X,Y}\), and we would like to consider the joint PDF of two new random variables
\begin{equation}
U=g(X,Y)\quad \mbox{and}\quad V=h(X,Y),
\end{equation}
where \(g\) and \(h\) are two given functions, typically ``nice'' in the sense of Appendix [[sec-Multivariable-Calculus][Multivariable Calculus]]. 

Suppose that the transformation \((x,y)\longmapsto(u,v)\) is one-to-one. Then an inverse transformation \(x=x(u,v)\) and \(y=y(u,v)\) exists, so let \(\partial(x,y)/\partial(u,v)\) denote the Jacobian of the inverse transformation. Then the joint PDF of \((U,V)\) is given by 
\begin{equation}
f_{U,V}(u,v)=f_{X,Y}\left[x(u,v),\, y(u,v)\right]\left|\frac{\partial(x,y)}{\partial(u,v)}\right|,
\end{equation}
or we can rewrite more shortly as
\begin{equation}
\label{eq-biv-trans-pdf-short}
f_{U,V}(u,v)=f_{X,Y}(x,y)\left|\frac{\partial(x,y)}{\partial(u,v)}\right|.
\end{equation}
Take a moment and compare Equation [[eq-biv-trans-pdf-short]] to Equation [[eq-univ-trans-pdf-short]]. Do you see the connection? 

#+begin_rem
It is sometimes easier to /postpone/ solving for the inverse transformation \(x=x(u,v)\) and \(y=y(u,v)\). Instead, leave the transformation in the form \(u=u(x,y)\) and \(v=v(x,y)\) and calculate the Jacobian of the /original/ transformation
\begin{equation}
\frac{\partial(u,v)}{\partial(x,y)}=\left|\begin{array}{cc}
\frac{\partial u}{\partial x} & \frac{\partial u}{\partial y}\\
\frac{\partial v}{\partial x} & \frac{\partial v}{\partial y}\end{array}\right|=\frac{\partial u}{\partial x}\frac{\partial v}{\partial y}-\frac{\partial u}{\partial y}\frac{\partial v}{\partial x}.
\end{equation}
Once this is known, we can get the PDF of \((U,V)\) by
\begin{equation}
f_{U,V}(u,v)=f_{X,Y}(x,y)\left|\frac{1}{\frac{\partial(u,v)}{\partial(x,y)}}\right|.
\end{equation}
In some cases there will be a cancellation and the work will be lot shorter. Of course, it is not always true that
\begin{equation}
\label{eq-biv-jacob-recip}
\frac{\partial(x,y)}{\partial(u,v)}=\frac{1}{\frac{\partial(u,v)}{\partial(x,y)}},
\end{equation}
but for the well-behaved examples that we will see in this book it works just fine... do you see the connection between Equations [[eq-biv-jacob-recip]] and [[eq-univ-jacob-recip]]?
#+end_rem


#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Let \((X,Y)\sim\mathsf{mvnorm}(\mathtt{mean}=\mathbf{0}_{2\times1},\,\mathtt{sigma}=\mathbf{I}_{2\times2})\) and consider the transformation
\begin{align*}
U= & \ 3X+4Y,\\
V= & \ 5X+6Y.
\end{align*}
We can solve the system of equations to find the inverse transformations; they are
\begin{align*}
X= & -3U+2V,\\
Y= & \ \frac{5}{2}U-\frac{3}{2}V,
\end{align*}
in which case the Jacobian of the inverse transformation is
\[
\begin{vmatrix}
-3 & 2\\
\frac{5}{2} & -\frac{3}{2}
\end{vmatrix}
= -3\left(-\frac{3}{2}\right)-2\left(\frac{5}{2}\right) = -\frac{1}{2}.
\]
As \((x,y)\) traverses \(\mathbb{R}^{2}\), so too does \((u,v)\). Since the joint PDF of \((X,Y)\) is
\[
f_{X,Y}(x,y)=\frac{1}{2\pi}\exp\left\{ -\frac{1}{2}\left(x^{2}+y^{2}\right)\right\} ,\quad (x,y)\in\mathbb{R}^{2},
\]
we get that the joint PDF of \((U,V)\) is
\begin{equation}
\label{eq-biv-norm-hidden}
f_{U,V}(u,v)=\frac{1}{2\pi}\exp\left\{ -\frac{1}{2}\left[\left(-3u+2v\right)^{2}+\left(\frac{5u-3v}{2}\right)^{2}\right]\right\} \cdot\frac{1}{2},\quad (u,v)\in\mathbb{R}^{2}.
\end{equation}
#+latex: \end{exampletoo}
#+html: </div>

#+begin_rem
It may not be obvious, but Equation [[eq-biv-norm-hidden]] is the PDF of a \(\mathsf{mvnorm}\) distribution. For a more general result see Theorem [[thm-mvnorm-dist-matrix-prod]].
#+end_rem

*** How to do it with \(\mathsf{R}\)
:PROPERTIES:
:CUSTOM_ID: sub-bivariate-transf-R
:END:

It is possible to do the computations above in \(\mathsf{R}\) with the =Ryacas= package. The package is an interface to the open-source computer algebra system, ``Yacas''. The user installs Yacas, then employs =Ryacas= to submit commands to Yacas, after which the output is displayed in the \(\mathsf{R}\) console.

There are not yet any examples of Yacas in this book, but there are online materials to help the interested reader: see [[http://code.google.com/p/ryacas/][here]] to get started.

** Remarks for the Multivariate Case
:PROPERTIES:
:CUSTOM_ID: sec-Remarks-for-the-Multivariate
:END:

There is nothing spooky about \(n\geq3\) random variables. We just have a whole bunch of them: \(X_{1}\), \(X_{2}\),..., \(X_{n}\), which we can shorten to \(\mathbf{X}=(X_{1},X_{2},\ldots,X_{n})^{\mathrm{T}}\) to make the formulas prettier (now may be a good time to check out Appendix [[sec-Linear-Algebra][Linear Algebra]]). For \(\mathbf{X}\) supported on the set \(S_{\mathbf{X}}\), the joint PDF \(f_{\mathbf{X}}\) (if it exists) satisfies
\begin{equation}
f_{\mathbf{X}}(\mathbf{x})>0,\quad \mbox{for }\mathbf{x}\in S_{\mathbf{X}},
\end{equation}
and
\begin{equation}
\int\!\!\!\int\cdots\int f_{\mathbf{X}}(\mathbf{x})\,\mathrm{d} x_{1}\mathrm{d} x_{2}\cdots\mathrm{d} x_{n}=1,
\end{equation}
or even shorter: \(\int f_{\mathbf{X}}(\mathbf{x})\,\mathrm{d}\mathbf{x}=1\). The joint CDF \(F_{\mathbf{X}}\) is defined by 
\begin{equation}
F_{\mathbf{X}}(\mathbf{x})=\mathbb{P}(X_{1}\leq x_{1},\, X_{2}\leq x_{2},\ldots,\, X_{n}\leq x_{n}),
\end{equation}
for \(\mathbf{x}\in\mathbb{R}^{n}\). The expectation of a function \(g(\mathbf{X})\) is defined just as we would imagine:
\begin{equation}
\mathbb{E} g(\mathbf{X})=\int g(\mathbf{x})\, f_{\mathbf{X}}(\mathbf{x})\,\mathrm{d}\mathbf{x}.
\end{equation}
provided the integral exists and is finite. And the moment generating function in the multivariate case is defined by
\begin{eqnarray} 
M_{\mathbf{X}}(\mathbf{t}) & = & \mathbb{E}\exp\left\{ \mathbf{t}^{\mathrm{T}}\mathbf{X}\right\},
\end{eqnarray}
whenever the integral exists and is finite for all \(\mathbf{t}\) in a neighborhood of \(\mathbf{0}_{\mathrm{n}\times1}\) (note that \(\mathbf{t}^{\mathrm{T}}\mathbf{X}\) is shorthand for \(t_{1}X_{1}+t_{2}X_{2}+\cdots+t_{n}X_{n}\)). The only difference in any of the above for the discrete case is that integrals are replaced by sums. 

Marginal distributions are obtained by integrating out remaining variables from the joint distribution. And even if we are given all of the univariate marginals it is not enough to determine the joint distribution uniquely.

We say that \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) are /mutually independent/ if their joint PDF factors into the product of the marginals
\begin{equation}
f_{\mathbf{X}}(\mathbf{x})=f_{X_{1}}(x_{1})\, f_{X_{2}}(x_{2})\,\cdots\, f_{X_{n}}(x_{n}),
\end{equation}
for every \(\mathbf{x}\) in their joint support \(S_{\mathbf{X}}\), and we say that \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) are /exchangeable/ if their joint PDF (or CDF) is a symmetric function of its \(n\) arguments, that is, if 
\begin{equation}
f_{\mathbf{X}}(\mathbf{x^{\ast}})=f_{\mathbf{X}}(\mathbf{x}),
\end{equation}
for any reordering (permutation) \(\mathbf{x^{\ast}}\) of the elements of \(\mathbf{x}=(x_{1},x_{2},\ldots,x_{n})\) in the joint support.

#+begin_prop
# <<pro-mean-sd-lin-comb>>
Let \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) be independent with respective population means \(\mu_{1}\), \(\mu_{2}\), ..., \(\mu_{n}\) and standard deviations \(\sigma_{1}\), \(\sigma_{2}\), ..., \(\sigma_{n}\). For given constants \(a_{1}\), \(a_{2}\), ...,\(a_{n}\) define \(Y=\sum_{i=1}^{n}a_{i}X_{i}\). Then the mean and standard deviation of \(Y\) are given by the formulas
\begin{equation}
\mu_{Y}=\sum_{i=1}^{n}a_{i}\mu_{i},\quad \sigma_{Y}=\left(\sum_{i=1}^{n}a_{i}^{2}\sigma_{i}^{2}\right)^{1/2}.
\end{equation}
#+end_prop

#+begin_proof
The mean is easy:
\[
\mathbb{E} Y=\mathbb{E}\left(\sum_{i=1}^{n}a_{i}X_{i}\right)=\sum_{i=1}^{n}a_{i}\mathbb{E} X_{i}=\sum_{i=1}^{n}a_{i}\mu_{i}.
\]
The variance is not too difficult to compute either. As an intermediate step, we calculate \(\mathbb{E} Y^{2}\). 
\[
\mathbb{E} Y^{2}=\mathbb{E}\left(\sum_{i=1}^{n}a_{i}X_{i}\right)^{2}=\mathbb{E}\left(\sum_{i=1}^{n}a_{i}^{2}X_{i}^{2}+2\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}a_{i}a_{j}X_{i}X_{j}\right).
\]
Using linearity of expectation the \(\mathbb{E}\) distributes through the sums. Now \(\mathbb{E} X_{i}^{2}=\sigma_{i}^{2}+\mu_{i}^{2}\) and \(\mathbb{E} X_{i}X_{j}=\mathbb{E} X_{i}\mathbb{E} X_{j}=\mu_{i}\mu_{j}\) when \(i\neq j\) because of independence. Thus
\begin{eqnarray*}
\mathbb{E} Y^{2} & = & \sum_{i=1}^{n}a_{i}^{2}(\sigma_{i}^{2}+\mu_{i}^{2})+2\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}a_{i}a_{j}\mu_{i}\mu_{j}\\
 & = & \sum_{i=1}^{n}a_{i}^{2}\sigma_{i}^{2}+\left(\sum_{i=1}^{n}a_{i}^{2}\mu_{i}^{2}+2\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}a_{i}a_{j}\mu_{i}\mu_{j}\right)
\end{eqnarray*}
To complete the proof, note that the expression in the parentheses is exactly \(\left(\mathbb{E} Y\right)^{2}\), and recall the identity \(\sigma_{Y}^{2}=\mathbb{E} Y^{2}-\left(\mathbb{E} Y\right)^{2}\).  
#+end_proof

There is a corresponding statement of Fact [[fac-indep-then-function-indep]] for the multivariate case. The proof is also omitted here. 

#+begin_fact
If \(\mathbf{X}\) and \(\mathbf{Y}\) are mutually independent random vectors, then \(u(\mathbf{X})\) and \(v(\mathbf{Y})\) are independent for any functions \(u\) and \(v\).
#+end_fact

Bruno de Finetti was a strong proponent of the subjective approach to probability. He proved an important theorem in 1931 which illuminates the link between exchangeable random variables and independent random variables. Here it is in one of its simplest forms. 

#+begin_thm
*De Finetti's Theorem.* Let \(X_{1}\), \(X_{2}\), ... be a sequence of \(\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p)\) random variables such that \((X_{1},\ldots,X_{k})\) are exchangeable for every \(k\). Then there exists a random variable \(\Theta\) with support \([0,1]\) and PDF \(f_{\Theta}(\theta)\) such that
\begin{equation}
\label{eq-definetti-binary}
\mathbb{P}(X_{1}=x_{1},\ldots,\, X_{k}=x_{k})=\int_{0}^{1}\theta^{\sum x_{i}}(1-\theta)^{k-\sum x_{i}}\, f_{\Theta}(\theta)\,\mathrm{d}\theta,
\end{equation}
for all \(x_{i}=0,\,1\), \(i=1,\,2,\ldots,k\).
#+end_thm

To get a handle on the intuitive content de Finetti's theorem, imagine that we have a /bunch/ of coins in our pocket with each having its own unique value of \(\theta=\mathbb{P}(\mbox{Heads})\). We reach into our pocket and select a coin at random according to some probability -- say, \(f_{\Theta}(\theta)\). We take the randomly selected coin and flip it \(k\) times. 

Think carefully: the conditional probability of observing a sequence  \(X_{1}=x_{1},\ldots,\, X_{k}=x_{k}\), given a specific coin \(\theta\) would just be \(\theta^{\sum x_{i}}(1-\theta)^{k-\sum x_{i}}\), because the coin flips are an independent sequence of Bernoulli trials. But the coin is random, so the Theorem of Total Probability says we can get the /unconditional/ probability \(\mathbb{P}(X_{1}=x_{1},\ldots,\, X_{k}=x_{k})\) by adding up terms that look like
\begin{equation}
\theta^{\sum x_{i}}(1-\theta)^{k-\sum x_{i}}\, f_{\Theta}(\theta),
\end{equation}
where we sum over all possible coins. The right-hand side of Equation [[eq-definetti-binary]] is a sophisticated way to denote this process.

Of course, the integral's value does not change if we jumble the \(x_{i}\)'s, so \((X_{1},\ldots,X_{k})\) are clearly exchangeable. The power of de Finetti's Theorem is that /every/ infinite binary exchangeable sequence can be written in the above form.

The connection to subjective probability: our prior information about \(\theta\) corresponds to \(f_{\Theta}(\theta)\) and the likelihood of the sequence \(X_{1}=x_{1},\ldots,\, X_{k}=x_{k}\) (conditional on \(\theta\)) corresponds to \(\theta^{\sum x_{i}}(1-\theta)^{k-\sum x_{i}}\). Compare Equation [[eq-definetti-binary]] to Section [[sec-Bayes-Rule][Bayes' Rule]] and Section [[sec-Conditional-Distributions][Conditional Distributions]].

The multivariate normal distribution immediately generalizes from the bivariate case. If the matrix \(\Sigma\) is nonsingular then the joint PDF of \(\mathbf{X}\sim\mathsf{mvnorm}(\mathtt{mean}=\upmu,\,\mathtt{sigma}=\Sigma)\) is
\begin{equation}
f_{\mathbf{X}}(\mathbf{x})=\frac{1}{(2\pi)^{n/2}\left|\Sigma\right|^{1/2}}\exp\left\{ -\frac{1}{2}\left(\mathbf{x}-\upmu\right)^{\top}\Sigma^{-1}\left(\mathbf{x}-\upmu\right)\right\},
\end{equation}
and the MGF is
\begin{equation}
M_{\mathbf{X}}(\mathbf{t})=\exp\left\{ \upmu^{\top}\mathbf{t}+\frac{1}{2}\mathbf{t}^{\top}\Sigma\mathbf{t}\right\}.
\end{equation}
We will need the following in Chapter [[cha-multiple-linear-regression][Multiple Linear Regression]].

#+begin_thm
# <<thm-mvnorm-dist-matrix-prod>>
If \(\mathbf{X}\sim\mathsf{mvnorm}(\mathtt{mean}=\upmu,\,\mathtt{sigma}=\Sigma)\) and \(\mathbf{A}\) is any matrix, then the random vector \(\mathbf{Y}=\mathbf{AX}\)
is distributed
\begin{equation}
\mathbf{Y}\sim\mathsf{mvnorm}(\mathtt{mean}=\mathbf{A}\upmu,\,\mathtt{sigma}=\mathbf{A}\Sigma\mathbf{A}^{\mathrm{T}}).
\end{equation}
#+end_thm

#+begin_proof
Look at the MGF of \(\mathbf{Y}\):
\begin{eqnarray*}
M_{\mathbf{Y}}(\mathbf{t}) & = & \mathbb{E}\,\exp\left\{ \mathbf{t}^{\mathrm{T}}(\mathbf{AX})\right\} ,\\
 & = & \mathbb{E}\,\exp\left\{ (\mathbf{A}^{\mathrm{T}}\mathbf{t})^{\mathrm{T}}\mathbf{X}\right\} ,\\
 & = & \exp\left\{ \upmu^{\mathrm{T}}(\mathbf{A}^{\top}\mathbf{t})+\frac{1}{2}(\mathbf{A}^{\mathrm{T}}\mathbf{t})^{\mathrm{T}}\Sigma(\mathbf{A}^{\mathrm{T}}\mathbf{t})\right\} ,\\
 & = & \exp\left\{ \left(\mathbf{A}\upmu\right)^{\mathrm{T}}\mathbf{t}+\frac{1}{2}\mathbf{t}^{\mathrm{T}}\left(\mathbf{A}\Sigma\mathbf{A}^{\mathrm{T}}\right)\mathbf{t}\right\},
\end{eqnarray*}
and the last expression is the MGF of an \(\mathsf{mvnorm}(\mathtt{mean}=\mathbf{A}\upmu,\,\mathtt{sigma}=\mathbf{A}\Sigma\mathbf{A}^{\mathrm{T}})\) distribution.
#+end_proof

** The Multinomial Distribution
:PROPERTIES:
:CUSTOM_ID: sec-Multinomial
:END:

We sample \(n\) times, with replacement, from an urn that contains balls of \(k\) different types. Let \(X_{1}\) denote the number of balls in our sample of type 1, let \(X_{2}\) denote the number of balls of type 2, ..., and let \(X_{k}\) denote the number of balls of type \(k\). Suppose the urn has proportion \(p_{1}\) of balls of type 1, proportion \(p_{2}\) of balls of type 2, ..., and proportion \(p_{k}\) of balls of type \(k\). Then the joint PMF of \((X_{1},\ldots,X_{k})\) is
\begin{eqnarray}
f_{X_{1},\ldots,X_{k}}(x_{1},\ldots,x_{k}) & = & {n \choose x_{1}\, x_{2}\,\cdots\, x_{k}}\, p_{1}^{x_{1}}p_{2}^{x_{2}}\cdots p_{k}^{x_{k}},
\end{eqnarray}
for \((x_{1},\ldots,x_{k})\) in the joint support \(S_{X_{1},\ldots X_{K}}\). We write
\begin{equation}
(X_{1},\ldots,X_{k})\sim\mathsf{multinom}(\mathtt{size}=n,\,\mathtt{prob}=\mathbf{p}_{\mathrm{k}\times1}).
\end{equation}
Several comments are in order. First, the joint support set \(S_{X_{1},\ldots X_{K}}\) contains all nonnegative integer \(k\)-tuples \((x_{1},\ldots,x_{k})\) such that \(x_{1}+x_{2}+\cdots+x_{k}=n\). A support set like this is called a /simplex/. Second, the proportions \(p_{1}\), \(p_{2}\), ..., \(p_{k}\) satisfy \(p_{i}\geq0\) for all \(i\) and \(p_{1}+p_{2}+\cdots+p_{k}=1\). Finally, the symbol
\begin{equation}
{n \choose x_{1}\, x_{2}\,\cdots\, x_{k}}=\frac{n!}{x_{1}!\, x_{2}!\,\cdots x_{k}!}
\end{equation}
is called a /multinomial coefficient/ which generalizes the notion of a binomial coefficient we saw in Equation [[eq-binomial-coefficient]]. 

The form and notation we have just described matches the \(\mathsf{R}\) usage but is not standard among other texts. Most other books use the above for a \(k-1\) dimension multinomial distribution, because the linear constraint \(x_{1}+x_{2}+\cdots+x_{k}=n\) means that once the values of \(X_{1}\), \(X_{2}\), ..., \(X_{k-1}\) are known the final value \(X_{k}\) is determined, not random. Another term used for this is a /singular/ distribution. 

For the most part we will ignore these difficulties, but the careful reader should keep them in mind. There is not much of a difference in practice, except that below we will use a two-dimensional support set for a three-dimension multinomial distribution. See Figure [[fig-multinom-pmf2][multinom-pmf2]].

When \(k=2\), we have \(x_{1}=x\) and \(x_{2}=n-x\), we have \(p_{1}=p\) and \(p_{2}=1-p\), and the multinomial coefficient is literally a binomial coefficient. In the previous notation we have thus shown that the \(\mathsf{multinom}(\mathtt{size}=n,\,\mathtt{prob}=\mathbf{p}_{2\times1})\) distribution is the same as a \(\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p)\) distribution.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
*Dinner with Barack Obama.* During the 2008 U.S. presidential primary, Barack Obama offered to have dinner with three randomly selected monetary contributors to his campaign. Imagine the thousands of people in the contributor database. For the sake of argument, Suppose that the database was approximately representative of the U.S. population as a whole, Suppose Barack Obama wants to have [[http://pewresearch.org/pubs/773/fewer-voters-identify-as-republicans][dinner]] with 36 democrat, 27 republican, and 37 independent.
#+latex: \end{exampletoo}
#+html: </div>

#+begin_rem
Here are some facts about the multinomial distribution.
1. The expected value of \((X_{1},\, X_{2},\,\ldots,\, X_{k})\) is \(n\mathbf{p}_{k\times1}\).
1. The variance-covariance matrix \(\Sigma\) is symmetric with diagonal entries \(\sigma_{i}^{2}=np_{i}(1-p_{i})\), \(i=1,\,2,\,\ldots,\, k\) and off-diagonal entries \(\mbox{Cov}(X_{i},\, X_{j})=-np_{i}p_{j}\), for \(i\neq j\). The correlation between \(X_{i}\) and \(X_{j}\) is therefore \(\mbox{Corr}(X_{i},\, X_{j})=-\sqrt{p_{i}p_{j}/(1-p_{i})(1-p_{j})}\). 
1. The marginal distribution of \((X_{1},\, X_{2},\,\ldots,\, X_{k-1})\) is \(\mathsf{multinom}(\mathtt{size}=n,\,\mathtt{prob}=\mathbf{p}_{(k-1)\times1})\) with
   \begin{equation}
   \mathbf{p}_{(k-1)\times1}=\left(p_{1},\, p_{2},\,\ldots,\, p_{k-2},\, p_{k-1}+p_{k}\right),
   \end{equation}
   and in particular, \(X_{i}\sim\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p_{i})\).
#+end_rem

*** How to do it with \(\mathsf{R}\)

There is support for the multinomial distribution in base \(\mathsf{R}\), namely in the =stats= package \cite{stats}. The =dmultinom= function represents the PMF and the =rmultinom= function generates random variates.

#+begin_src R :exports both :results output pp 
tmp <- t(xsimplex(3, 6))
p <- apply(tmp, MARGIN = 1, FUN = dmultinom, prob = c(36,27,37))
S <- probspace(tmp, probs = p)
ProbTable <- xtabs(probs ~ X1 + X2, data = S)
round(ProbTable, 3)
#+end_src

Do some examples of =rmultinom=.

Another way to do the plot is with the =scatterplot3d= function in the =scatterplot3d= package \cite{scatterplot3d}. It looks like this: 
: library("scatterplot3d")
: X <- t(as.matrix(expand.grid(0:6, 0:6)))
: X <- X[, colSums(X) <= 6 ]; X <- rbind(X, 6 - colSums(X))
: Z <- round(apply(X, 2, function(x) dmultinom(x, prob = 1:3)), 3)
: A <- data.frame(x = X[1, ], y = X[2, ], probability = Z)
: scatterplot3d(A, type = "h", lwd = 3, box = FALSE)
The =scatterplot3d= graph looks better in this example, but the code is more difficult to understand. And with =cloud= one can easily do conditional plots of the form =cloud(z ~ x + y | f)=, where =f= is a factor.

#+name: multinom-pmf2
#+begin_src R :exports code :results silent
print(cloud(probs ~ X1 + X2, data = S, type = c("p","h"), lwd = 2, 
            pch = 16, cex = 1.5), screen = list(z = 15, x = -70))
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/multdist/multinom-pmf2.ps
  <<multinom-pmf2>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/multdist/multinom-pmf2.svg
  <<multinom-pmf2>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/multdist/multinom-pmf2.ps}
  \caption[Plot of a multinomial PMF.]{\small A plot of a multinomial PMF.}
    \label{fig-multinom-pmf2}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-multinom-pmf2" class="figure">
  <p><img src="svg/multdist/multinom-pmf2.svg" width=500 alt="svg/multdist/multinom-pmf2.svg" /></p>
  <p>A plot of a multinomial PMF.</p>
</div>
#+end_html

#+latex: \newpage{}

** Exercises

#+latex: \setcounter{thm}{0}

#+begin_xca
# <<xca-Prove-cov-shortcut>>
Prove that \( \mbox{Cov}(X,Y)=\mathbb{E}(XY)-(\mathbb{E} X)(\mathbb{E} Y). \)
#+end_xca

#+begin_xca
# <<xca-sum-indep-chisq>>
Suppose \(X\sim\mathsf{chisq}(\mathtt{df}=p_{1})\) and \(Y\sim\mathsf{chisq}(\mathtt{df}=p_{2})\) are independent. Find the distribution of \(X+Y\) (you may want to refer to Equation [[eq-mgf-chisq]]).
#+end_xca

#+begin_xca
# <<xca-diff-indep-norm>>
Show that when \(X\) and \(Y\) are independent the MGF of \(X-Y\) is \(M_{X}(t)M_{Y}(-t)\). Use this to find the distribution of \(X-Y\) when \(X\sim\mathsf{norm}(\mathtt{mean}=\mu_{1},\,\mathtt{sd}=\sigma_{1})\) and \(Y\sim\mathsf{norm}(\mathtt{mean}=\mu_{2},\,\mathtt{sd}=\sigma_{2})\)  are independent. 
#+end_xca

* Sampling Distributions                                           :sampdist:
:PROPERTIES:
:tangle: R/sampdist.R
:CUSTOM_ID: cha-Sampling-Distributions
:END:

#+begin_src R :exports none :eval never
# Chapter: Sampling Distributions
# All code released under GPL Version 3
#+end_src

#+latex: \noindent 
This is an important chapter; it is the bridge from probability and descriptive statistics that we studied in Chapters [[cha-Describing-Data-Distributions][Data Description]] through [[cha-Multivariable-Distributions][Multivariate Distributions]] to inferential statistics which forms the latter part of this book.

Here is the link: we are presented with a /population/ about which we would like to learn. And while it would be desirable to examine every single member of the population, we find that it is either impossible or infeasible to for us to do so, thus, we resort to collecting a /sample/ instead. We do not lose heart. Our method will suffice, provided the sample is /representative/ of the population. A good way to achieve this is to sample /randomly/ from the population.

Supposing for the sake of argument that we have collected a random sample, the next task is to make some /sense/ out of the data because the complete list of sample information is usually cumbersome, unwieldy. We summarize the data set with a descriptive /statistic/, a quantity calculated from the data (we saw many examples of these in Chapter [[cha-Describing-Data-Distributions][Data Description]]). But our sample was random... therefore, it stands to reason that our statistic will be random, too. How is the statistic distributed?

The probability distribution associated with the population (from which we sample) is called the /population distribution/, and the probability distribution associated with our statistic is called its /sampling distribution/; clearly, the two are interrelated. To learn about the population distribution, it is imperative to know everything we can about the sampling distribution. Such is the goal of this chapter.

We begin by introducing the notion of simple random samples and cataloguing some of their more convenient mathematical properties. Next we focus on what happens in the special case of sampling from the normal distribution (which, again, has several convenient mathematical properties), and in particular, we meet the sampling distribution of \(\overline{X}\) and \(S^{2}\). Then we explore what happens to \(\overline{X}\)'s sampling distribution when the population is not normal and prove one of the most remarkable theorems in statistics, the Central Limit Theorem (CLT).

With the CLT in hand, we then investigate the sampling distributions of several other popular statistics, taking full advantage of those with a tractable form. We finish the chapter with an exploration of statistics whose sampling distributions are not quite so tractable, and to accomplish this goal we will use simulation methods that are grounded in all of our work in the previous four chapters.

*What do I want them to know?*
- the notion of population versus simple random sample, parameter versus statistic, and population distribution versus sampling distribution
- the classical sampling distributions of the standard one and two sample statistics
- how to generate a simulated sampling distribution when the statistic is crazy
- the Central Limit Theorem, period.
- some basic concepts related to sampling distribution utility, such as bias and variance

** Simple Random Samples
:PROPERTIES:
:CUSTOM_ID: sec-simple-random-samples
:END:

#+begin_defn
If \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) are independent with \(X_{i}\sim f\) for \(i=1,2,\ldots,n\), then we say that \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) are /independent and identically distributed/ (IID) from the population \(f\) or alternatively we say that \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) are a /simple random sample of size/ \(n\), denoted \(SRS(n)\), from the population \(f\). 
#+end_defn

#+begin_prop
# <<pro-mean-sd-xbar>>
Let \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) be a \(SRS(n)\) from a population distribution with mean \(\mu\) and finite standard deviation \(\sigma\). Then the mean and standard deviation of \(\overline{X}\) are given by the formulas \(\mu_{\overline{X}}=\mu\) and \(\sigma_{\overline{X}}=\sigma/\sqrt{n}\).
#+end_prop

#+begin_proof
Plug in \(a_{1}=a_{2}=\cdots=a_{n}=1/n\) in Proposition [[pro-mean-sd-lin-comb]].
#+end_proof

The next fact will be useful to us when it comes time to prove the Central Limit Theorem in Section [[sec-Central-Limit-Theorem][Central Limit Theorem]].

#+begin_prop
# <<pro-mgf-xbar>>
Let \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) be a \(SRS(n)\) from a population distribution with MGF \(M(t)\). Then the MGF of \(\overline{X}\) is given by
\begin{equation}
M_{\overline{X}}(t)=\left[M\left(\frac{t}{n}\right)\right]^{n}.
\end{equation}
#+end_prop

#+begin_proof
Go from the definition:
\begin{eqnarray*}
M_{\overline{X}}(t) & = & \mathbb{E}\,\mathrm{e}^{t\overline{X}},\\
 & = & \mathbb{E}\,\mathrm{e}^{t(X_{1}+\cdots+X_{n})/n},\\
 & = & \mathbb{E}\,\mathrm{e}^{tX_{1}/n}\mathrm{e}^{tX_{2}/n}\cdots\mathrm{e}^{tX_{n}/n}.
\end{eqnarray*}
And because \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) are independent, Proposition [[pro-indep-implies-prodexpect]] allows us to distribute the expectation among each term in the product, which is
\[
\mathbb{E}\mathrm{e}^{tX_{1}/n}\,\mathbb{E}\mathrm{e}^{tX_{2}/n}\cdots\mathbb{E}\mathrm{e}^{tX_{n}/n}.
\]
The last step is to recognize that each term in last product above is exactly \(M(t/n)\).
#+end_proof

** Sampling from a Normal Distribution
:PROPERTIES:
:CUSTOM_ID: sec-sampling-from-normal-dist
:END:

*** The Distribution of the Sample Mean
:PROPERTIES:
:CUSTOM_ID: sub-samp-mean-dist-of
:END:

#+begin_prop
Let \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) be a \(SRS(n)\) from a \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) distribution. Then the sample mean \(\overline{X}\) has a \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma/\sqrt{n})\) sampling distribution.
#+end_prop

#+begin_proof
The mean and standard deviation of \(\overline{X}\) follow directly from Proposition [[pro-mean-sd-xbar]]. To address the shape, first remember from Section [[sec-The-Normal-Distribution][Normal Distribution]] that the \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) MGF is of the form
\[
M(t)=\exp\left[ \mu t+\sigma^{2}t^{2}/2\right] .
\]
Now use Proposition [[pro-mgf-xbar]] to find
\begin{eqnarray*}
M_{\overline{X}}(t) & = & \left[M\left(\frac{t}{n}\right)\right]^{n},\\
 & = & \left[\exp\left( \mu(t/n)+\sigma^{2}(t/n)^{2}/2\right) \right]^{n},\\
 & = & \exp\left( \, n\cdot\left[\mu(t/n)+\sigma^{2}(t/n)^{2}/2\right]\right) ,\\
 & = & \exp\left( \mu t+(\sigma/\sqrt{n})^{2}t^{2}/2\right),
\end{eqnarray*}
and we recognize this last quantity as the MGF of a \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma/\sqrt{n})\) distribution.
#+end_proof

*** The Distribution of the Sample Variance
:PROPERTIES:
:CUSTOM_ID: sub-Samp-Var-Dist
:END:

#+begin_thm
# <<thm-Xbar-andS>>
Let \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) be a \(SRS(n)\) from a \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) distribution, and let
\begin{equation}
\overline{X}=\sum_{i=1}^{n}X_{i}\quad \mbox{and}\quad S^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}.
\end{equation}
Then
1. \(\overline{X}\) and \(S^{2}\) are independent, and
2. The rescaled sample variance
    \begin{equation}
    \frac{(n-1)}{\sigma^{2}}S^{2}=\frac{\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}}{\sigma^{2}}
    \end{equation}
    has a \(\mathsf{chisq}(\mathtt{df}=n-1)\) sampling distribution.
#+end_thm

#+begin_proof
The proof is beyond the scope of the present book, but the theorem is simply too important to be omitted. The interested reader could consult Casella and Berger \cite{Casella2002}, or Hogg /et al/ \cite{Hogg2005}. 
#+end_proof

*** The Distribution of Student's \(T\) Statistic
:PROPERTIES:
:CUSTOM_ID: sub-Student's-t-Distribution
:END:

#+begin_prop
Let \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) be a \(SRS(n)\) from a \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) distribution. Then the quantity 
\begin{equation}
T=\frac{\overline{X}-\mu}{S/\sqrt{n}}
\end{equation}
has a \(\mathsf{t}(\mathtt{df}=n-1)\) sampling distribution.
#+end_prop

#+begin_proof
Divide the numerator and denominator by \(\sigma\) and rewrite
\[
T=\frac{\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}}{S/\sigma}=\frac{\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}}{\sqrt{\left.\frac{(n-1)S^{2}}{\sigma^{2}}\right/ (n-1)}}.
\]
Now let 
\[
Z=\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\quad \mbox{and}\quad V=\frac{(n-1)S^{2}}{\sigma^{2}},
\]
so that
\begin{equation}
T=\frac{Z}{\sqrt{V/r}},
\end{equation}
where \(r=n-1\).

We know from Section [[sub-samp-mean-dist-of][Distribution of Xbar]] that \(Z\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) and we know from Section [[sub-Samp-Var-Dist][Sample Variance Distribution]] that \(V\sim\mathsf{chisq}(\mathtt{df}=n-1)\). Further, since we are sampling from a normal distribution, Theorem [[thm-Xbar-andS][Xbar and S]] gives that \(\overline{X}\) and \(S^{2}\) are independent and by Fact [[fac-indep-then-function-indep]] so are \(Z\) and \(V\). In summary, the distribution of \(T\) is the same as the distribution of the quantity \(Z/\sqrt{V/r}\), where \(Z\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) and \(V\sim\mathsf{chisq}(\mathtt{df}=r)\) are independent. This is in fact the definition of Student's \(t\) distribution.
#+end_proof

This distribution was first published by W. S. Gosset (1900) under the pseudonym Student, and the distribution has consequently come to be known as Student's \(t\) distribution. The PDF of \(T\) can be derived explicitly using the techniques of Section [[sec-Functions-of-Continuous][Functions of Continuous Random Variables]]; it takes the form 
\begin{equation}
f_{X}(x)=\frac{\Gamma[(r+1)/2]}{\sqrt{r\pi}\ \Gamma(r/2)}\left(1+\frac{x^{2}}{r}\right)^{-(r+1)/2},\quad -\infty < x < \infty.
\end{equation}
Any random variable \(X\) with the preceding PDF is said to have Student's \(t\) distribution with \(r\) /degrees of freedom/, and we write \(X\sim\mathsf{t}(\mathtt{df}=r)\). The shape of the PDF is similar to the normal, but the tails are considerably heavier. See Figure [[fig-Students-t-dist-vary-df][Student's-t-dist-vary-df]]. As with the normal distribution, there are four functions in \(\mathsf{R}\) associated with the \(t\) distribution, namely =dt=, =pt=,=qt=, and =rt=, which compute the PDF, CDF, quantile function, and generate random variates, respectively.

The code to produce Figure [[fig-Students-t-dist-vary-df][Student's-t-dist-vary-df]] is

#+name: Students-t-dist-vary-df
#+begin_src R :exports code :results silent
curve(dt(x, df = 30), from = -3, to = 3, lwd = 3, ylab = "y")
ind <- c(1, 2, 3, 5, 10)
for (i in ind) curve(dt(x, df = i), -3, 3, add = TRUE)
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/sampdist/Students-t-dist-vary-df.ps
  <<Students-t-dist-vary-df>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/sampdist/Students-t-dist-vary-df.svg
  <<Students-t-dist-vary-df>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/sampdist/Student's-t-dist-vary-df.ps}
  \caption[Student's \(t\) distribution for various degrees of freedom]{\small A plot of Student's \(t\) distribution for various degrees of freedom.}
  \label{fig-Students-t-dist-vary-df}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-Students-t-dist-vary-df" class="figure">
  <p><img src="svg/sampdist/Students-t-dist-vary-df.svg" width=500 alt="svg/sampdist/Students-t-dist-vary-df.svg" /></p>
  <p>A plot of Student's t distribution for various degrees of freedom.</p>
</div>
#+end_html

Similar to that done for the normal we may define \(\mathsf{t}_{\alpha}(\mathtt{df}=n-1)\) as the number on the \(x\)-axis such that there is exactly \(\alpha\) area under the \(\mathsf{t}(\mathtt{df}=n-1)\) curve to its right.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Find \(\mathsf{t}{}_{0.01}(\mathtt{df}=23)\) with the quantile function.
#+latex: \end{exampletoo}
#+html: </div>

#+begin_src R :exports both :results output pp 
qt(0.01, df = 23, lower.tail = FALSE)
#+end_src

#+begin_rem
There are a few things to note about the \(\mathtt{t}(\mathtt{df}=r)\) distribution.
1. The \(\mathtt{t}(\mathtt{df}=1)\) distribution is the same as the \(\mathsf{cauchy}(\mathtt{location}=0,\,\mathtt{scale}=1)\) distribution. The Cauchy distribution is rather pathological and is a counterexample to many famous results. 
2. The standard deviation of \(\mathsf{t}(\mathtt{df}=r)\) is undefined (that is, infinite) unless \(r>2\). When \(r\) is more than 2, the standard deviation is always bigger than one, but decreases to 1 as \(r\to\infty\).
3. As \(r\to\infty\), the \(\mathtt{t}(\mathtt{df}=r)\) distribution approaches the \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) distribution.
#+end_rem

** The Central Limit Theorem
:PROPERTIES:
:CUSTOM_ID: sec-Central-Limit-Theorem
:END:

In this section we study the distribution of the sample mean when the underlying distribution is /not/ normal. We saw in Section [[sec-sampling-from-normal-dist][Sampling from Normal]] that when \(X_{1}\), \(X_{2}\), ... , \(X_{n}\) is a \(SRS(n)\) from a \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) distribution then \(\overline{X}\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma/\sqrt{n})\). In other words, we may say (owing to Fact [[fac-lin-trans-norm-is-norm]]) when the underlying population is normal that the sampling distribution of \(Z\) defined by
\begin{equation}
Z=\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}
\end{equation}
is \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\). 

However, there are many populations that are /not/ normal ... and the statistician often finds herself sampling from such populations. What can be said in this case? The surprising answer is contained in the following theorem.

#+begin_thm
# <<thm-central-limit-thrm>>
*The Central Limit Theorem.* Let \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) be a \(SRS(n)\) from a population distribution with mean \(\mu\) and finite standard deviation \(\sigma\). Then the sampling distribution of 
\begin{equation}
Z=\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}
\end{equation}
approaches a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) distribution as \(n\to\infty\).
#+end_thm

#+begin_rem
We suppose that \(X_{1}\), \(X_{2}\), ... , \(X_{n}\) are IID, and we learned in Section [[sec-simple-random-samples][Simple Random Samples]] that \(\overline{X}\) has mean \(\mu\) and standard deviation \(\sigma/\sqrt{n}\), so we already knew that \(Z\) has mean zero and standard deviation one. The beauty of the CLT is that it addresses the /shape/ of \(Z\)'s distribution when the sample size is large.
#+end_rem

#+begin_rem
Notice that the shape of the underlying population's distribution is not mentioned in Theorem [[thm-central-limit-thrm][CLT]]; indeed, the result is true for any population that is well-behaved enough to have a finite standard deviation. In particular, if the population is normally distributed then we know from Section [[sub-samp-mean-dist-of][Distribution of Xbar]] that the distribution of \(\overline{X}\) (and \(Z\) by extension) is /exactly/ normal, for /every/ \(n\).
#+end_rem

#+begin_rem
How large is ``sufficiently large''? It is here that the shape of the underlying population distribution plays a role. For populations with distributions that are approximately symmetric and mound-shaped, the samples may need to be only of size four or five, while for highly skewed or heavy-tailed populations the samples may need to be much larger for the distribution of the sample means to begin to show a bell-shape. Regardless, for a given population distribution (with finite standard deviation) the approximation tends to be better for larger sample sizes.
#+end_rem

*** How to do it with \(\mathsf{R}\)

The =TeachingDemos= package \cite{TeachingDemos} has =clt.examp= and the =distrTeach= \cite{distrTeach} package has =illustrateCLT=. Try the following at the command line (output omitted):
#+begin_src R :exports code :eval never
example(clt.examp)
#+end_src
and
#+begin_src R :exports code :eval never
example(illustrateCLT)
#+end_src

The =IPSUR=  package \cite{IPSUR} has the functions =clt1=, =clt2=, and =clt3= (see Exercise [[xca-clt123][clt123]] at the end of this chapter). Its purpose is to investigate what happens to the sampling distribution of \(\overline{X}\) when the population distribution is mound shaped, finite support, and skewed, namely \(\mathsf{t}(\mathtt{df}=3)\), \(\mathsf{unif}(\mathtt{a}=0,\,\mathtt{b}=10)\), and \(\mathsf{gamma}(\mathtt{shape}=1.21,\,\mathtt{scale}=1/2.37)\), respectively. 

For example, when the command =clt1()=  is issued a plot window opens to show a graph of the PDF of a \(\mathsf{t}(\mathtt{df}=3)\) distribution. On the display are shown numerical values of the population mean and variance. While the students examine the graph the computer is simulating random samples of size =sample.size = 2= from the population distribution =rt= a total of =N.iter = 100000= times, and sample means are calculated of each sample. Next follows a histogram of the simulated sample means, which closely approximates the sampling distribution of \(\overline{X}\), see Section [[sec-Simulated-Sampling-Distributions][Simulated Sampling Distributions]]. Also shown are the sample mean and sample variance of all of the simulated  \( \overline{X} \) values. As a final step, when the student clicks the second plot, a normal curve with the same mean and variance as the simulated \( \overline{X} \) values is superimposed over the histogram. Students should compare the population theoretical mean and variance to the simulated mean and variance of the sampling distribution. They should also compare the shape of the simulated sampling distribution to the shape of the normal distribution.

The three separate =clt1=, =clt2=, and =clt3= functions were written so that students could compare what happens overall when the shape of the population distribution changes. It would be possible to combine all three into one big function, =clt= which covers all three cases (and more). 

** Sampling Distributions of Two-Sample Statistics
:PROPERTIES:
:CUSTOM_ID: sec-Samp-Dist-Two-Samp
:END:

There are often two populations under consideration, and it sometimes of interest to compare properties between groups. To do so we take independent samples from each population and calculate respective sample statistics for comparison. In some simple cases the sampling distribution of the comparison is known and easy to derive; such cases are the subject of the present section.

*** Difference of Independent Sample Means

#+begin_prop
Let \(X_{1}\), \(X_{2}\), ... , \(X_{n_{1}}\) be an \(SRS(n_{1})\) from a \(\mathsf{norm}(\mathtt{mean}=\mu_{X},\,\mathtt{sd}=\sigma_{X})\) distribution and let \(Y_{1}\), \(Y_{2}\), ... , \(Y_{n_{2}}\) be an \(SRS(n_{2})\) from a \(\mathsf{norm}(\mathtt{mean}=\mu_{Y},\,\mathtt{sd}=\sigma_{Y})\) distribution. Suppose that \(X_{1}\), \(X_{2}\), ... , \(X_{n_{1}}\) and \(Y_{1}\), \(Y_{2}\), ... , \(Y_{n_{2}}\) are independent samples. Then the quantity
\begin{equation}
\label{eq-diff-indep-sample-means}
\frac{\overline{X}-\overline{Y}-(\mu_{X}-\mu_{Y})}{\sqrt{\left.\sigma_{X}^{2}\right/ n_{1}+\left.\sigma_{Y}^{2}\right/ n_{2}}}
\end{equation}
has a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) sampling distribution. Equivalently, \(\overline{X}-\overline{Y}\) has a \(\mathsf{norm}(\mathtt{mean}=\mu_{X}-\mu_{Y},\,\mathtt{sd}=\sqrt{\left.\sigma_{X}^{2}\right/ n_{1}+\left.\sigma_{Y}^{2}\right/ n_{2}})\) sampling distribution.
#+end_prop

#+begin_proof
We know that \(\overline{X}\) is \(\mathsf{norm}(\mathtt{mean}=\mu_{X},\,\mathtt{sd}=\sigma_{X}/\sqrt{n_{1}})\) and we also know that \(\overline{Y}\) is \(\mathsf{norm}(\mathtt{mean}=\mu_{Y},\,\mathtt{sd}=\sigma_{Y}/\sqrt{n_{2}})\). And since the samples \(X_{1}\), \(X_{2}\), ..., \(X_{n_{1}}\) and \(Y_{1}\), \(Y_{2}\), ..., \(Y_{n_{2}}\) are independent, so too are \(\overline{X}\) and \(\overline{Y}\). The distribution of their difference is thus normal as well, and the mean and standard deviation are given by Proposition [[pro-mean-sd-lin-comb-two]].
#+end_proof

#+begin_rem
Even if the distribution of one or both of the samples is not normal, the quantity in Equation [[eq-diff-indep-sample-means]] will be approximately normal provided both sample sizes are large.
#+end_rem

#+begin_rem
For the special case of \(\mu_{X}=\mu_{Y}\) we have shown that 
\begin{equation} \frac{\overline{X}-\overline{Y}}{\sqrt{\sigma_{X}^{2}/n_{1}+\sigma_{Y}^{2}/n_{2}}}
\end{equation}
has a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) sampling distribution, or in other words, \(\overline{X}-\overline{Y}\) has a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=\sqrt{\sigma_{X}^{2}/n_{1}+\sigma_{Y}^{2}/n_{2}})\) sampling distribution. This will be important when it comes time to do hypothesis tests; see Section [[sec-Conf-Interv-for-Diff-Means][Confidence Intervals for Difference of Means]].
#+end_rem

*** Difference of Independent Sample Proportions

#+begin_prop
Let \(X_{1}\), \(X_{2}\), ..., \(X_{n_{1}}\) be an \(SRS(n_{1})\) from a \(\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p_{1})\) distribution and let \(Y_{1}\), \(Y_{2}\), ..., \(Y_{n_{2}}\) be an \(SRS(n_{2})\) from a \(\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p_{2})\) distribution. Suppose that \(X_{1}\), \(X_{2}\), ... , \(X_{n_{1}}\) and \(Y_{1}\), \(Y_{2}\), ... , \(Y_{n_{2}}\) are independent samples. Define 
\begin{equation}
\hat{p}_{1}=\frac{1}{n_{1}}\sum_{i=1}^{n_{1}}X_{i}\quad \mbox{and}\quad \hat{p}_{2}=\frac{1}{n_{2}}\sum_{j=1}^{n_{2}}Y_{j}.
\end{equation}
Then the sampling distribution of
\begin{equation}
\frac{\hat{p}_{1}-\hat{p}_{2}-(p_{1}-p_{2})}{\sqrt{\frac{p_{1}(1-p_{1})}{n_{1}}+\frac{p_{2}(1-p_{2})}{n_{2}}}}
\end{equation}
approaches a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) distribution as both \(n_{1},\, n_{2}\to\infty\). In other words, the sampling distribution of \(\hat{p}_{1}-\hat{p}_{2}\) is approximately
\begin{equation}
\mathsf{norm}\left(\mathtt{mean}=p_{1}-p_{2},\,\mathtt{sd}=\sqrt{\frac{p_{1}(1-p_{1})}{n_{1}}+\frac{p_{2}(1-p_{2})}{n_{2}}}\right),
\end{equation}
provided both \(n_{1}\) and \(n_{2}\) are sufficiently large.
#+end_prop

#+begin_proof
We know that \(\hat{p}_{1}\) is approximately normal for \(n_{1}\) sufficiently large by the CLT, and we know that \(\hat{p}_{2}\) is approximately normal for \(n_{2}\) sufficiently large, also by the CLT. Further, \(\hat{p}_{1}\) and \(\hat{p}_{2}\) are independent since they are derived from independent samples. And a difference of independent (approximately) normal distributions is (approximately) normal, by Exercise [[xca-diff-indep-norm]].
#+latex: \footnote{This does not explicitly follow because of our cavalier use of ``approximately'' in too many places. To be more thorough, however, would require more concepts than we can afford at the moment. The interested reader may consult a more advanced text, specifically the topic of weak convergence, that is, convergence in distribution.}
The expressions for the mean and standard deviation follow immediately from Proposition [[pro-mean-sd-lin-comb-two]] combined with the formulas for the \(\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p)\) distribution from Chapter [[cha-Discrete-Distributions][Discrete Distributions]].
#+end_proof

*** Ratio of Independent Sample Variances

#+begin_prop
Let \(X_{1}\), \(X_{2}\), ..., \(X_{n_{1}}\) be an \(SRS(n_{1})\) from a \(\mathsf{norm}(\mathtt{mean}=\mu_{X},\,\mathtt{sd}=\sigma_{X})\) distribution and let \(Y_{1}\), \(Y_{2}\), ... , \(Y_{n_{2}}\) be an \(SRS(n_{2})\) from a \(\mathsf{norm}(\mathtt{mean}=\mu_{Y},\,\mathtt{sd}=\sigma_{Y})\) distribution. Suppose that \(X_{1}\), \(X_{2}\), ... , \(X_{n_{1}}\) and \(Y_{1}\), \(Y_{2}\), ... , \(Y_{n_{2}}\) are independent samples. Then the ratio
\begin{equation}
F=\frac{\sigma_{Y}^{2}S_{X}^{2}}{\sigma_{X}^{2}S_{Y}^{2}}
\end{equation}
has an \(\mathsf{f}(\mathtt{df1}=n_{1}-1,\,\mathtt{df2}=n_{2}-1)\) sampling distribution.
#+end_prop

#+begin_proof
We know from Theorem [[thm-Xbar-andS][Xbar and S]] that \((n_{1}-1)S_{X}^{2}/\sigma_{X}^{2}\) is distributed \(\mathsf{chisq}(\mathtt{df}=n_{1}-1)\) and \((n_{2}-1)S_{Y}^{2}/\sigma_{Y}^{2}\) is distributed \(\mathsf{chisq}(\mathtt{df}=n_{2}-1)\). Now write
\[
F=\frac{\sigma_{Y}^{2}S_{X}^{2}}{\sigma_{X}^{2}S_{Y}^{2}}=\frac{\left.(n_{1}-1)S_{X}^{2}\right/ (n_{1}-1)}{\left.(n_{2}-1)S_{Y}^{2}\right/ (n_{2}-1)}\cdot\frac{\left.1\right/ \sigma_{X}^{2}}{\left.1\right/ \sigma_{Y}^{2}},
\]
by multiplying and dividing the numerator with \(n_{1}-1\) and doing likewise for the denominator with \(n_{2}-1\). Now we may regroup the terms into
\[
F=\frac{\left.\frac{(n_{1}-1)S_{X}^{2}}{\sigma_{X}^{2}}\right/ (n_{1}-1)}{\left.\frac{(n_{2}-1)S_{Y}^{2}}{\sigma_{Y}^{2}}\right/ (n_{2}-1)},
\]
and we recognize \(F\) to be the ratio of independent \(\mathsf{chisq}\) distributions, each divided by its respective numerator \(\mathtt{df}=n_{1}-1\) and denominator \(\mathtt{df}=n_{1}-1\) degrees of freedom. This is, indeed, the definition of Snedecor's \(F\) distribution. 
#+end_proof

#+begin_rem
For the special case of \(\sigma_{X}=\sigma_{Y}\) we have shown that
\begin{equation}
F=\frac{S_{X}^{2}}{S_{Y}^{2}}
\end{equation}
has an \(\mathsf{f}(\mathtt{df1}=n_{1}-1,\,\mathtt{df2}=n_{2}-1)\) sampling distribution. This will be important in Chapters [[cha-Estimation][Estimation]] onward.
#+end_rem

** Simulated Sampling Distributions
:PROPERTIES:
:CUSTOM_ID: sec-Simulated-Sampling-Distributions
:END:

Some comparisons are meaningful, but their sampling distribution is not quite so tidy to describe analytically. What do we do then?

As it turns out, we do not need to know the exact analytical form of the sampling distribution; sometimes it is enough to approximate it with a simulated distribution. In this section we will show you how. Note that \(\mathsf{R}\) is particularly well suited to compute simulated sampling distributions, much more so than, say, SPSS or SAS.

*** The Interquartile Range

#+begin_src R :exports code :results silent 
iqrs <- replicate(100, IQR(rnorm(100)))
#+end_src

We can look at the mean of the simulated values
#+begin_src R :exports both :results output pp 
mean(iqrs)    # close to 1
#+end_src

and we can see the standard deviation
#+begin_src R :exports both :results output pp 
sd(iqrs)
#+end_src

Now let's take a look at a plot of the simulated values

#+name: simulated-IQR
#+begin_src R :exports code :results silent
hist(iqrs, breaks = 20)
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/sampdist/simulated-IQR.ps
  <<simulated-IQR>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/sampdist/simulated-IQR.svg
  <<simulated-IQR>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/sampdist/simulated-IQR.ps}
  \caption[Plot of simulated IQRs]{\small A plot of simulated IQRs.}
  \label{fig-simulated-IQR}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-simulated-IQR" class="figure">
  <p><img src="svg/sampdist/simulated-IQR.svg" width=500 alt="svg/sampdist/simulated-IQR.svg" /></p>
  <p>A plot of simulated IQRs.</p>
</div>
#+end_html

*** The Median Absolute Deviation

#+begin_src R :exports code :results silent
mads <- replicate(100, mad(rnorm(100)))
#+end_src

We can look at the mean of the simulated values

#+begin_src R :exports both :results output pp 
mean(mads)    # close to 1.349
#+end_src

and we can see the standard deviation

#+begin_src R :exports both :results output pp 
sd(mads)
#+end_src

Now let's take a look at a plot of the simulated values

#+name: simulated-MAD
#+begin_src R :exports code :results silent
hist(mads, breaks = 20)
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/sampdist/simulated-MAD.ps
  <<simulated-MAD>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/sampdist/simulated-MAD.svg
  <<simulated-MAD>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/sampdist/simulated-MAD.ps}
  \caption[Plot of simulated MADs]{\small A plot of simulated MADs.}
  \label{fig-simulated-MAD}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-simulated-MAD" class="figure">
  <p><img src="svg/sampdist/simulated-MAD.svg" width=500 alt="svg/sampdist/simulated-MAD.svg" /></p>
  <p>A plot of simulated MADs.</p>
</div>
#+end_html

#+latex: \newpage{}

** Exercises
#+latex: \setcounter{thm}{0}

#+begin_src R :exports none :results silent
k <- 1
n <- sample(10:30, size=10, replace = TRUE)
mu <- round(rnorm(10, mean = 20))
#+end_src

#+begin_xca
Suppose that we observe a random sample \(X_{1}\), \(X_{2}\), ... , \(X_{n}\) of size SRS( n =  \(SRC_R{n[1]}\) ) from a \( \mathsf{norm}(\mathtt{mean}= SRC_R{mu[1]} ) \) distribution. 
1. What is the mean of \(\overline{X}\)?
1. What is the standard deviation of \(\overline{X}\)?
1. What is the distribution of \(\overline{X}\)? (approximately)
1. Find \(\mathbb{P}(a< \overline{X} \leq b)\)
1. Find \(\mathbb{P}(\overline{X} > c)\).
#+end_xca

#+begin_xca
# <<xca-clt123>>
In this exercise we will investigate how the shape of the population distribution affects the time until the distribution of \(\overline{X}\) is acceptably normal.
#+end_xca

#+begin_xca
Let \(X_{1}\),..., \(X_{25}\) be a random sample from a \(\mathsf{norm}(\mathtt{mean}=37,\,\mathtt{sd}=45)\) distribution, and let \(\overline{X}\) be the sample mean of these \(n=25\) observations.
1. How is \(\overline{X}\) distributed? 
   \(\mathsf{norm}(\mathtt{mean}=37,\,\mathtt{sd}=45/\sqrt{25})\) 
1. Find \(\mathbb{P}(\overline{X} > 43.1)\).
And that's all she wrote.
#+end_xca

* Estimation                                                       :estimate:
:PROPERTIES:
:tangle: R/estimate.R
:CUSTOM_ID: cha-Estimation
:END:

#+begin_src R :exports none :eval never
# Chapter: Data Description
# All code released under GPL Version 3
#+end_src

#+latex: \noindent 
We will discuss two branches of estimation procedures: point estimation and interval estimation. We briefly discuss point estimation first and then spend the rest of the chapter on interval estimation.

We find an estimator with the methods of Section [[sec-Point-Estimation-1][Point Estimation 1]]. We make some assumptions about the underlying population distribution and use what we know from Chapter [[cha-Sampling-Distributions][Sampling Distributions]] about sampling distributions both to study how the estimator will perform, and to find intervals of confidence for underlying parameters associated with the population distribution. Once we have confidence intervals we can do inference in the form of hypothesis tests in the next chapter.

*What do I want them to know?*
- how to look at a problem, identify a reasonable model, and estimate a parameter associated with the model
- about maximum likelihood, and in particular, how to
   - eyeball a likelihood to get a maximum
   - use calculus to find an MLE for one-parameter families
- about properties of the estimators they find, such as bias, minimum variance, MSE
- point versus interval estimation, and how to find and interpret confidence intervals for basic experimental designs
- the concept of margin of error and its relationship to sample size

** Point Estimation
:PROPERTIES:
:CUSTOM_ID: sec-Point-Estimation-1
:END:

The following example is how I was introduced to maximum likelihood.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-how-many-fish>>
Suppose we have a small pond in our backyard, and in the pond there live some fish. We would like to know how many fish live in the pond. How can we estimate this? One procedure developed by researchers is the capture-recapture method. Here is how it works.

We will fish from the pond and suppose that we capture \(M=7\) fish. On each caught fish we attach an unobtrusive tag to the fish's tail, and release it back into the water.

Next, we wait a few days for the fish to remix and become accustomed to their new tag. Then we go fishing again. On the second trip some of the fish we catch may be tagged; some may not be. Let \(X\) denote the number of caught fish which are tagged
#+latex: \footnote{It is theoretically possible that we could catch the same tagged fish more than once, which would inflate our count of tagged fish. To avoid this difficulty, suppose that on the second trip we use a tank on the boat to hold the caught fish until data collection is completed.},
and suppose for the sake of argument that we catch \(K=4\) fish and we find that 3 of them are tagged.

Now let \(F\) denote the (unknown) total number of fish in the pond. We know that \(F\geq7\), because we tagged that many on the first trip. In fact, if we let \(N\) denote the number of untagged fish in the pond, then \(F=M+N\). Using our earlier language, we have sampled \(K=4\) times, without replacement, from an urn (pond) which has \(M=7\) white balls (tagged fish) and \(N=F-M\) black balls (untagged fish), and we have observed \(x=3\) of them to be white (tagged). What is the probability of this?

Looking back to Section [[sec-other-discrete-distributions][Other Discrete Distributions]], we see that the random variable \(X\) has a \(\mathsf{hyper}(\mathtt{m}=M,\,\mathtt{n}=F-M,\,\mathtt{k}=K)\) distribution. Therefore, for an observed value \(X=x\) the probability would be
\[
\mathbb{P}(X=x)=\frac{{M \choose x}{F-M \choose K-x}}{{F \choose K}}.
\]
First we notice that \(F\) must be at least 7. Could \(F\) be equal to seven? If \(F=7\) then all of the fish would have been tagged on the first run, and there would be no untagged fish in the pond, thus, \(\mathbb{P}(\mbox{3 successes in 4 trials})=0\). 
What about \(F=8\); what would be the probability of observing \(X=3\) tagged fish?
\[
\mathbb{P}(\mbox{3 successes in 4 trials})=\frac{{7 \choose 3}{1 \choose 1}}{{8 \choose 4}}=\frac{35}{70}=0.5.
\]
Similarly, if \(F=9\) then the probability of observing \(X=3\) tagged fish would be
\[
\mathbb{P}(\mbox{3 successes in 4 trials})=\frac{{7 \choose 3}{2 \choose 1}}{{9 \choose 4}}=\frac{70}{126}\approx0.556.
\]
We can see already that the observed data \(X=3\) is more likely when \(F=9\) than it is when \(F=8\). And here lies the genius of Sir Ronald Aylmer Fisher: he asks, ``What is the value of \(F\) which has the highest likelihood?'' In other words, for all of the different possible values of \(F\), which one makes the above probability the biggest? We can answer this question with a plot of \(\mathbb{P}(X=x)\) versus \(F\). See Figure [[fig-capture-recapture][capture-recapture]].
#+latex: \end{exampletoo}
#+html: </div>

#+name: capture-recapture
#+begin_src R :exports none :results silent
heights = rep(0, 16)
for (j in 7:15) heights[j] <- dhyper(3, m = 7, n = j - 7, k = 4)
plot(6:15, heights[6:15], pch = 16, cex = 1.5, xlab = "number of fish in pond", ylab = "Likelihood")
abline(h = 0)
lines(6:15, heights[6:15], type = "h", lwd = 2, lty = 3)
text(9, heights[9]/6, bquote(hat(F)==.(9)), cex = 2, pos = 4)
lines(9, heights[9], type = "h", lwd = 2)
points(9, 0, pch = 4, lwd = 3, cex = 2)
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/estimate/capture-recapture.ps
  <<capture-recapture>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/estimate/capture-recapture.svg
  <<capture-recapture>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/estimate/capture-recapture.ps}
  \caption[Capture-recapture experiment]{\small A plot of maximum likelihood for the capture-recapture experiment.}
  \label{fig-capture-recapture}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-capture-recapture" class="figure">
  <p><img src="svg/estimate/capture-recapture.svg" width=500 alt="svg/estimate/capture-recapture.svg" /></p>
  <p>A plot of maximum likelihood for the capture-recapture experiment.</p>
</div>
#+end_html

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-bass-bluegill>>
In the last example we were only concerned with how many fish were in the pond, but now, we will ask a different question. Suppose it is known that there are only two species of fish in the pond: smallmouth bass (/Micropterus dolomieu/) and bluegill (/Lepomis macrochirus/); perhaps we built the pond some years ago and stocked it with only these two species. We would like to estimate the proportion of fish in the pond which are bass.

Let \(p=\mbox{the proportion of bass}\). Without any other information, it is conceivable for \(p\) to be any value in the interval \([0,1]\), but for the sake of argument we will suppose that \(p\) falls strictly between zero and one. How can we learn about the true value of \(p\)? Go fishing! As before, we will use catch-and-release, but unlike before, we will not tag the fish. We will simply note the species of any caught fish before returning it to the pond. 

Suppose we catch \(n\) fish. Let
\[
X_{i}=
\begin{cases}
1, & \mbox{if the \(i\mathrm{th}\) fish is a bass,}\\
0, & \mbox{if the \(i\mathrm{th}\) fish is a bluegill.}
\end{cases}
\]
Since we are returning the fish to the pond once caught, we may think of this as a sampling scheme with replacement where the proportion of bass \(p\) does not change. Given that we allow the fish sufficient time to ``mix'' once returned, it is not completely unreasonable to model our fishing experiment as a sequence of Bernoulli trials, so that the \(X_{i}\)'s would be IID \(\mathsf{binom(\mathtt{size}}=1,\,\mathtt{prob}=p)\). Under those assumptions we would have
\begin{eqnarray*}
\mathbb{P}(X_{1}=x_{1},\, X_{2}=x_{2},\,\ldots,\, X_{n}=x_{n}) & = & \mathbb{P}(X_{1}=x_{1})\,\mathbb{P}(X_{2}=x_{2})\,\cdots\mathbb{P}(X_{n}=x_{n}),\\
 & = & p^{x_{1}}(1-p)^{x_{1}}\, p^{x_{2}}(1-p)^{x_{2}}\cdots\, p^{x_{n}}(1-p)^{x_{n}},\\
 & = & p^{\sum x_{i}}(1-p)^{n-\sum x_{i}}.
\end{eqnarray*}
That is, 
\[
\mathbb{P}(X_{1}=x_{1},\, X_{2}=x_{2},\,\ldots,\, X_{n}=x_{n})=p^{\sum x_{i}}(1-p)^{n-\sum x_{i}}.
\]
This last quantity is a function of \(p\), called the /likelihood function/ \(L(p)\):
\[
L(p)=p^{\sum x_{i}}(1-p)^{n-\sum x_{i}}.
\]
A graph of \(L\) for values of \(\sum x_{i}=3,\ 4\), and 5 when \(n=7\) is shown in Figure [[fig-fishing-part-two][fishing-part-two]]. 

#+name: fishing-part-two
#+begin_src R :exports code :results silent
curve(x^5*(1-x)^2, 0, 1, xlab = "p", ylab = "L(p)")
curve(x^4*(1-x)^3, 0, 1, add = TRUE)
curve(x^3*(1-x)^4, 0, 1, add = TRUE)
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/estimate/fishing-part-two.ps
  <<fishing-part-two>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/estimate/fishing-part-two.svg
  <<fishing-part-two>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/estimate/fishing-part-two.ps}
  \caption[Assorted likelihood functions for fishing, part two]{\small Assorted likelihood functions for fishing, part two.   Three graphs are shown of \(L\) when \(\sum x_{i}\) equals 3, 4, and 5, respectively, from left to right. We pick an \(L\) that matches the observed data and then maximize \(L\) as a function of \(p\). If \(\sum x_{i}=4\), then the maximum appears to occur somewhere around \(p \approx 0.6\).}
  \label{fig-fishing-part-two}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-fishing-part-two" class="figure">
  <p><img src="svg/estimate/fishing-part-two.svg" width=500 alt="svg/estimate/fishing-part-two.svg" /></p>
  <p>Assorted likelihood functions for fishing, part two.</p>
</div>
#+end_html

We want the value of \(p\) which has the highest likelihood, that is, we again wish to maximize the likelihood. We know from calculus (see Appendix [[sec-Differential-and-Integral][Differential and Integral Calculus]]) to differentiate \(L\) and set \(L'=0\) to find a maximum.
\[
L'(p)=\left(\sum x_{i}\right)p^{\sum x_{i}-1}(1-p)^{n-\sum x_{i}}+p^{\sum x_{i}}\left(n-\sum x_{i}\right)(1-p)^{n-\sum x_{i}-1}(-1).
\]
The derivative vanishes (\(L'=0\)) when
\begin{eqnarray*}
\left(\sum x_{i}\right)p^{\sum x_{i}-1}(1-p)^{n-\sum x_{i}} & = & p^{\sum x_{i}}\left(n-\sum x_{i}\right)(1-p)^{n-\sum x_{i}-1},\\
\sum x_{i}(1-p) & = & \left(n-\sum x_{i}\right)p,\\
\sum x_{i}-p\sum x_{i} & = & np-p\sum x_{i},\\
\frac{1}{n}\sum_{i=1}^{n}x_{i} & = & p.
\end{eqnarray*}
This ``best'' \(p\), the one which maximizes the likelihood, is called the maximum likelihood estimator (MLE) of \(p\) and is denoted \(\hat{p}\). That is, 
\begin{equation} 
\hat{p}=\frac{\sum_{i=1}^{n}x_{i}}{n}=\overline{x}.
\end{equation}

#+begin_rem
Strictly speaking we have only shown that the derivative equals zero at \(\hat{p}\), so it is theoretically possible that the critical value \(\hat{p}=\overline{x}\) is located at a minimum
#+latex: \footnote{We can tell from the graph that our value of \(\hat{p}\) is a maximum instead of a minimum so we do not really need to worry for this example. Other examples are not so easy, however, and we should be careful to be cognizant of this extra step.}
instead of a maximum! We should be thorough and check that \(L'>0\) when \(p<\overline{x}\) and \(L'<0\) when \(p>\overline{x}\). Then by the First Derivative Test (Theorem [[thm-First-Derivative-Test][FDT]]) we could be certain that \(\hat{p}=\overline{x}\) is indeed a maximum likelihood estimator, and not a minimum likelihood estimator.
#+end_rem

The result is shown in Figure [[fig-species-mle][species-mle]].
#+latex: \end{exampletoo}
#+html: </div>

#+name: species-mle
#+begin_src R :exports none :results silent
dat <- rbinom(27, size = 1, prob = 0.3)
like <- function(x){
r <- 1
for (k in 1:27){ r <- r*dbinom(dat[k], size = 1, prob = x)}
return(r)
}
curve(like, from = 0, to = 1, xlab = "parameter space", ylab = "Likelihood", lwd = 3, col = "blue")
abline(h = 0, lwd = 1, lty = 3, col = "grey")
mle <- mean(dat)
mleobj <- like(mle)
lines(mle, mleobj, type = "h", lwd = 2, lty = 3, col = "red")
points(mle, 0, pch = 4, lwd = 2, cex = 2, col = "red")
text(mle, mleobj/6, substitute(hat(theta)==a, list(a=round(mle, 4))), cex = 2, pos = 4)
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/estimate/species-mle.ps
  <<species-mle>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/estimate/species-mle.svg
  <<species-mle>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/estimate/species-mle.ps}
  \caption[Species maximum likelihood]{\small Species maximum likelihood.}
  \label{fig-species-mle}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-species-mle" class="figure">
  <p><img src="svg/estimate/species-mle.svg" width=500 alt="svg/estimate/species-mle.svg" /></p>
  <p>Species maximum likelihood.</p>
</div>
#+end_html

In general, we have a family of PDFs \(f(x|\theta)\) indexed by a parameter \(\theta\) in some parameter space \(\Theta\). We want to learn about \(\theta\). We take a \(SRS(n)\):
\begin{equation}
X_{1},\, X_{2},\,\ldots,X_{n}\mbox{ which are IID \( f(x| \theta ) \).}
\end{equation}

#+begin_defn
Given the observed data \(x_{1}\), \(x_{2}\), ..., \(x_{n}\), the /likelihood function/ \(L\) is defined by 
\[ 
L(\theta)=\prod_{i=1}^{n}f(x_{i}|\theta),\quad \theta\in\Theta.
\]
#+end_defn

The next step is to maximize \(L\). The method we will use in this book is to find the derivative \(L'\) and solve the equation \(L'(\theta)=0\). Call a solution \(\hat{\theta}\). We will check that \(L\) is maximized at \(\hat{\theta}\) using the First Derivative Test or the Second Derivative Test \(\left(L''(\hat{\theta})<0\right)\).

#+begin_defn
A value \(\theta\) that maximizes \(L\) is called a /maximum likelihood estimator/ (MLE) and is denoted \(\hat{\theta}\). It is a function of the sample, \(\hat{\theta}=\hat{\theta}\left(X_{1},\, X_{2},\,\ldots,X_{n}\right)\), and is called a /point estimator/ of \(\theta\).
#+end_defn

#+begin_rem
Some comments about maximum likelihood estimators:
- Often it is easier to maximize the /log-likelihood/ \(l(\theta)=\ln L(\theta)\) instead of the likelihood \(L\). Since the logarithmic function \(y=\ln x\) is a monotone transformation, the solutions to both problems are the same.
- MLEs do not always exist (for instance, sometimes the likelihood has a vertical asymptote), and even when they do exist, they are not always unique (imagine a function with a bunch of humps of equal height). For any given problem, there could be zero, one, or any number of values of \(\theta\) for which \(L(\theta)\) is a maximum.
- The problems we encounter in this book are all very nice with likelihood functions that have closed form representations and which are optimized by some calculus acrobatics. In practice, however, likelihood functions are sometimes nasty in which case we are obliged to use numerical methods to find maxima (if there are any).
- MLEs are just one of _many_ possible estimators. One of the more popular alternatives are the /method of moments estimators/; see Casella and Berger \cite{Casella2002} for more.
#+end_rem

Notice, in Example [[exa-bass-bluegill]] we had \(X_{i}\) IID \(\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p)\), and we saw that the MLE was \(\hat{p}=\overline{X}\). But further
\begin{eqnarray*}
\mathbb{E}\overline{X} & = & \mathbb{E}\frac{X_{1}+X_{2}+\cdots+X_{n}}{n},\\
 & = & \frac{1}{n}\left(\mathbb{E} X_{1}+\mathbb{E} X_{2}+\cdots+\mathbb{E} X_{n}\right),\\
 & = & \frac{1}{n}\left(np\right),\\
 & = & p,
\end{eqnarray*}
which is exactly the same as the parameter which we estimated. More concisely, \(\mathbb{E}\hat{p}=p\), that is, on the average, the estimator is exactly right.

#+begin_defn
Let \(s(X_{1},X_{2},\ldots,X_{n})\) be a statistic which estimates \(\theta\). If 
\[
\mathbb{E} s(X_{1},X_{2},\ldots,X_{n})=\theta,
\]
then the statistic \(s(X_{1},X_{2},\ldots,X_{n})\) is said to be an /unbiased estimator/ of \(\theta\). Otherwise, it is /biased/.
#+end_defn

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-normal-MLE-both>>

Let \(X_{1}\), \(X_{2}\), ... , \(X_{n}\) be an \(SRS(n)\) from a \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) distribution. It can be shown (in Exercise [[xca-norm-mu-sig-MLE]]) that if \(\mbox{$\theta$}=(\mu,\sigma^{2})\) then the MLE of \(\theta\) is
\begin{equation}
\hat{\theta}=(\hat{\mu},\hat{\sigma}^{2}),
\end{equation}
where \(\hat{\mu}=\overline{X}\) and
\begin{equation}
\hat{\sigma^{2}}=\frac{1}{n}\sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}=\frac{n-1}{n}S^{2}.
\end{equation}
We of course know from [[pro-mean-sd-xbar]] that \(\hat{\mu}\) is unbiased. What about \(\hat{\sigma^{2}}\)? Let us check: 
\begin{eqnarray*}
\mathbb{E}\,\hat{\sigma^{2}} & = & \mathbb{E}\,\frac{n-1}{n}S^{2}\\
 & = & \mathbb{E}\left(\frac{\sigma^{2}}{n}\frac{(n-1)S^{2}}{\sigma^{2}}\right)\\
 & = & \frac{\sigma^{2}}{n}\mathbb{E}\ \mathsf{chisq}(\mathtt{df}=n-1)\\
 & = & \frac{\sigma^{2}}{n}(n-1),
\end{eqnarray*}
from which we may conclude two things:
- \(\hat{\sigma^{2}}\) is a biased estimator of \(\sigma^{2}\), and 
- \(S^{2}=n\hat{\sigma^{2}}/(n-1)\) is an unbiased estimator of \(\sigma^{2}\).

#+latex: \end{exampletoo}
#+html: </div>

One of the most common questions in an introductory statistics class is, ``Why do we divide by \(n-1\) when we compute the sample variance? Why do we not divide by \(n\)?'' We see now that division by \(n\) amounts to the use of a /biased/ estimator for \(\sigma^{2}\), that is, if we divided by \(n\) then on the average we would /underestimate/ the true value of \(\sigma^{2}\). We use \(n-1\) so that, on the average, our estimator of \(\sigma^{2}\) will be exactly right. 

*** How to do it with \(\mathsf{R}\)

\(\mathsf{R}\) can be used to find maximum likelihood estimators in a lot of diverse settings. We will discuss only the most basic here and will leave the rest to more sophisticated texts.

For one parameter estimation problems we may use the =optimize= function to find MLEs. The arguments are the function to be maximized (the likelihood function), the range over which the optimization is to take place, and optionally any other arguments to be passed to the likelihood if needed.

Let us see how to do Example [[exa-bass-bluegill]]. Recall that our likelihood function was given by
\begin{equation}
L(p)=p^{\sum x_{i}}(1-p)^{n-\sum x_{i}}.
\end{equation}
Notice that the likelihood is just a product of \(\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p)\) PMFs. We first give some sample data (in the vector =datavals=), next we define the likelihood function =L=, and finally we =optimize= =L= over the range =c(0,1)=.

#+begin_src R :exports both :results output pp 
x <- mtcars$am
L <- function(p,x) prod(dbinom(x, size = 1, prob = p))
optimize(L, interval = c(0,1), x = x, maximum = TRUE)
#+end_src

#+begin_src R :exports none :results silent
A <- optimize(L, interval = c(0,1), x = x, maximum = TRUE)
#+end_src

Note that the =optimize= function by default minimizes the function =L=, so we have to set =maximum = TRUE= to get an MLE. The returned value of =$maximum= gives an approximate value of the MLE to be \( SRC_R{round(A$maximum, 3)} \) and =objective= gives =L= evaluated at the MLE which is approximately \( SRC_R{round(A$objective, 3)} \).

We previously remarked that it is usually more numerically convenient to maximize the log-likelihood (or minimize the negative log-likelihood), and we can just as easily do this with \(\mathsf{R}\). We just need to calculate the log-likelihood beforehand which (for this example) is
\[
-l(p)=-\sum x_{i}\ln\, p-\left(n-\sum x_{i}\right)\ln(1-p).
\]

It is done in \(\mathsf{R}\) with

#+begin_src R :exports both :results output pp 
minuslogL <- function(p,x){
                -sum(dbinom(x, size = 1, prob = p, log = TRUE))
             }
optimize(minuslogL, interval = c(0,1), x = x)
#+end_src

Note that we did not need =maximum = TRUE= because we minimized the negative log-likelihood. The answer for the MLE is essentially the same as before, but the =$objective= value was different, of course.

For multiparameter problems we may use a similar approach by way of the =mle= function in the =stats4= package \cite{stats4}. 

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">

*Plant Growth.* We will investigate the =weight= variable of the =PlantGrowth= data. We will suppose that the weights constitute a random observations \(X_{1}\), \(X_{2}\), ... , \(X_{n}\) that are IID \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) which is not unreasonable based on a histogram and other exploratory measures. We will find the MLE of \(\theta=(\mu,\sigma^{2})\). We claimed in Example [[exa-normal-MLE-both]] that \(\hat{\theta}=(\hat{\mu},\hat{\sigma}^{2})\) had the form given above. Let us check whether this is plausible numerically. The negative log-likelihood function is

#+begin_src R :exports code :results silent
minuslogL <- function(mu, sigma2){
  -sum(dnorm(x, mean = mu, sd = sqrt(sigma2), log = TRUE))
}
#+end_src

Note that we omitted the data as an argument to the log-likelihood function; the only arguments were the parameters over which the maximization is to take place. Now we will simulate some data and find the MLE. The optimization algorithm requires starting values (intelligent guesses) for the parameters. We choose values close to the sample mean and variance (which turn out to be approximately 5 and 0.5, respectively) to illustrate the procedure.

#+begin_src R :exports both :results output pp 
x <- PlantGrowth$weight
MaxLikeEst <- mle(minuslogL, start = list(mu = 5, sigma2 = 0.5))
summary(MaxLikeEst)
#+end_src

The outputted MLEs are shown above, and =mle= even gives us estimates for the standard errors of \(\hat{\mu}\) and \(\hat{\sigma}^{2}\) (which were obtained by inverting the numerical Hessian matrix at the optima; see Appendix [[sec-Multivariable-Calculus][Multivariable Calculus]]). Let us check how close the numerical MLEs came to the theoretical MLEs:

#+begin_src R :exports both :results output pp 
mean(x); var(x)*29/30; sd(x)/sqrt(30)
#+end_src

The numerical MLEs were very close to the theoretical MLEs. We already knew that the standard error of \(\hat{\mu}=\overline{X}\) is \(\sigma/\sqrt{n}\), and the numerical estimate of this was very close too.

#+latex: \end{exampletoo}
#+html: </div>

There is functionality in the =distrTest= package \cite{distrTest} to calculate theoretical MLEs; we will skip examples of these for the time being.

** Confidence Intervals for Means
:PROPERTIES:
:CUSTOM_ID: sec-Confidence-Intervals-for-Means
:END:

We are given \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) that are an \(SRS(n)\) from a \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) distribution, where \(\mu\) is unknown. We know that we may estimate \(\mu\) with \(\overline{X}\), and we have seen that this estimator is the MLE. But how good is our estimate? We know that 
\begin{equation} 
\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1).
\end{equation}
For a big probability \(1-\alpha\), for instance, 95%, we can calculate the quantile \(z_{\alpha/2}\). Then
\begin{equation}
\mathbb{P}\left(-z_{\alpha/2}\leq\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\leq z_{\alpha/2}\right)=1-\alpha.
\end{equation}
But now consider the following string of equivalent inequalities:
\[
-z_{\alpha/2}\leq\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\leq z_{\alpha/2},
\]
\[
-z_{\alpha/2}\left(\frac{\sigma}{\sqrt{n}}\right)\leq\overline{X}-\mu\leq z_{\alpha/2}\left(\frac{\sigma}{\sqrt{n}}\right),
\]
\[
-\overline{X}-z_{\alpha/2}\left(\frac{\sigma}{\sqrt{n}}\right)\leq-\mu\leq-\overline{X}+z_{\alpha/2}\left(\frac{\sigma}{\sqrt{n}}\right),
\]
\[
\overline{X}-z_{\alpha/2}\left(\frac{\sigma}{\sqrt{n}}\right)\leq\mu\leq\overline{X}+z_{\alpha/2}\left(\frac{\sigma}{\sqrt{n}}\right).
\]
That is, 
\begin{equation}
\mathbb{P}\left(\overline{X}-z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\leq\mu\leq\overline{X}+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right)=1-\alpha.
\end{equation}

#+begin_defn
The interval
\begin{equation}
\left[\overline{X}-z_{\alpha/2}\frac{\sigma}{\sqrt{n}},\ \overline{X}+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right]
\end{equation}
is a \(100(1-\alpha)\%\) /confidence interval for/ \(\mu\). The quantity \(1-\alpha\) is called the /confidence coefficient/.
#+end_defn

#+begin_rem
The interval is also sometimes written more compactly as
\begin{equation}
\label{eq-z-interval}
\overline{X}\pm z_{\alpha/2}\frac{\sigma}{\sqrt{n}}.
\end{equation}
#+end_rem

The interpretation of confidence intervals is tricky and often mistaken by novices. When I am teaching the concept ``live'' during class, I usually ask the students to imagine that my piece of chalk represents the ``unknown'' parameter, and I lay it down on the desk in front of me. Once the chalk has been lain, it is /fixed/; it does not move. Our goal is to estimate the parameter. For the estimator I pick up a sheet of loose paper lying nearby. The estimation procedure is to randomly drop the piece of paper from above, and observe where it lands. If the piece of paper covers the piece of chalk, then we are successful -- our estimator covers the parameter. If it falls off to one side or the other, then we are unsuccessful; our interval fails to cover the parameter.

Then I ask them: suppose we were to repeat this procedure hundreds, thousands, millions of times. Suppose we kept track of how many times we covered and how many times we did not. What percentage of the time would we be successful?

In the demonstration, the parameter corresponds to the chalk, the sheet of paper corresponds to the confidence interval, and the random experiment corresponds to dropping the sheet of paper. The percentage of the time that we are successful /exactly/ corresponds to the /confidence coefficient/. That is, if we use a 95% confidence interval, then we can say that, in the long run, approximately 95% of our intervals will cover the true parameter (which is fixed, but unknown). 

See Figure [[fig-ci-examp][ci-examp]], which is a graphical display of these ideas.

#+name: ci-examp
#+begin_src R :exports code :results silent
ci.examp()
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/estimate/ci-examp.ps
  <<ci-examp>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/estimate/ci-examp.svg
  <<ci-examp>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/estimate/ci-examp.ps}
  \caption[Simulated confidence intervals]{\small The graph was generated by the \texttt{ci.examp} function from the \texttt{TeachingDemos} package. Fifty (50) samples of size twenty five (25) were generated from a \( \mathsf{norm}(\mathtt{mean}=100,\,\mathtt{sd}=10) \) distribution, and each sample was used to find a 95\% confidence interval for the population mean using Equation \ref{eq-z-interval}. The 50 confidence intervals are represented above by horizontal lines, and the respective sample means are denoted by vertical slashes. Confidence intervals that ``cover'' the true mean value of 100 are plotted in black; those that fail to cover are plotted in a lighter color. In the plot we see that only one (1) of the simulated intervals out of the 50 failed to cover \(\mu=100\), which is a success rate of 98\%. If the number of generated samples were to increase from 50 to 500 to 50000, ..., then we would expect our success rate to approach the exact value of 95\%.}
  \label{fig-ci-examp}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-ci-examp" class="figure">
  <p><img src="svg/estimate/ci-examp.svg" width=500 alt="svg/estimate/ci-examp.svg" /></p>
  <p>Simulated confidence intervals.</p>
</div>
#+end_html

Under the above framework, we can reason that an ``interval'' with a /larger/ confidence coefficient corresponds to a /wider/ sheet of paper. Furthermore, the width of the confidence interval (sheet of paper) should be /somehow/ related to the amount of information contained in the random sample, \(X_{1}\), \(X_{2}\), ...,
\(X_{n}\). The following remarks makes these notions precise. 

#+begin_rem
For a fixed confidence coefficient \(1-\alpha\), if \(n\) increases, then the confidence interval gets /SHORTER/.

For a fixed sample size \(n\), if \(1-\alpha\) increases, then the confidence interval gets /WIDER/.
#+end_rem

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-plant-one-samp-z-int>>
*Results from an Experiment on Plant Growth.* The =PlantGrowth= data frame gives the results of an experiment to measure plant yield (as measured by the weight of the plant). We would like to a 95% confidence interval for the mean weight of the plants. Suppose that we know from prior research that the true population standard deviation of the plant weights is \(0.7\) g.

The parameter of interest is \(\mu\), which represents the true mean weight of the population of all plants of the particular species in the study. We will first take a look at a stem-and-leaf display of the data:

#+latex: \end{exampletoo}
#+html: </div>

#+begin_src R :exports both :results output pp 
with(PlantGrowth, stem.leaf(weight))
#+end_src

The data appear to be approximately normal with no extreme values. The data come from a designed experiment, so it is reasonable to suppose that the observations constitute a simple random sample of weights
#+latex: \footnote{Actually we will see later that there is reason to believe that the observations are simple random samples from three distinct populations. See Section \ref{sec-Analysis-of-Variance}.}. 
We know the population standard deviation \(\sigma=0.70\) from prior research. We are going to use the one-sample \(z\)-interval.

#+begin_src R :exports both :results output pp 
dim(PlantGrowth)   # sample size is first entry
#+end_src

#+begin_src R :exports both :results output pp 
with(PlantGrowth, mean(weight))
#+end_src

#+begin_src R :exports both :results output pp 
qnorm(0.975)
#+end_src

We find the sample mean of the data to be \(\overline{x}=5.073\) and \(z_{\alpha/2}=z_{0.025}\approx1.96\). Our interval is therefore
\[
\overline{x}\pm z_{\alpha/2}\frac{\sigma}{\sqrt{n}}=5.073\pm1.96\cdot\frac{0.70}{\sqrt{30}},
\]
which comes out to approximately \([4.823,\,5.323]\). In conclusion, we are 95% confident that the true mean weight \(\mu\) of all plants of this species lies somewhere between 4.823 g and 5.323 g, that is, we are 95% confident that the interval \([4.823,\,5.323]\) covers \(\mu\).

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Give some data with \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) an \(SRS(n)\) from a \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) distribution. Maybe small sample?
#+latex: \end{exampletoo}
#+html: </div>

1. What is the parameter of interest? in the context of the problem.
2. Give a point estimate for \(\mu\).
3. What are the assumptions being made in the problem? Do they meet the conditions of the interval?
4. Calculate the interval.
5. Draw the conclusion.

#+begin_rem
What if \(\sigma\) is unknown? We instead use the interval
\begin{equation}
\overline{X}\pm z_{\alpha/2}\frac{S}{\sqrt{n}},
\end{equation}
where \(S\) is the sample standard deviation.
- If \(n\) is large, then \(\overline{X}\) will have an approximately normal distribution regardless of the underlying population (by the CLT) and \(S\) will be very close to the parameter \(\sigma\) (by the SLLN); thus the above interval will have approximately \(100(1-\alpha)\%\) confidence of covering \(\mu\).
- If \(n\) is small, then
   - If the underlying population is normal then we may replace \(z_{\alpha/2}\) with \(t_{\alpha/2}(\mathtt{df}=n-1)\). The resulting \(100(1-\alpha)\%\) confidence interval is
     \begin{equation}
     \label{eq-one-samp-t-int}
     \overline{X}\pm t_{\alpha/2}(\mathtt{df}=n-1)\frac{S}{\sqrt{n}}.
     \end{equation}
   - if the underlying population is not normal, but approximately normal, then we may use the \(t\) interval, Equation [[eq-one-samp-t-int]]. The interval will have approximately \(100(1-\alpha)\%\) confidence of covering \(\mu\). However, if the population is highly skewed or the data have outliers, then we should ask a professional statistician for advice.
#+end_rem

The author learned of a handy acronym from AP Statistics Exam graders that summarizes the important parts of confidence interval estimation, which is PANIC: /P/-arameter, /A/-ssumptions, /N/-ame, /I/-nterval, and /C/-onclusion.
- Parameter: :: identify the parameter of interest with the proper symbols. Write down what the parameter means in the context of the problem.
- Assumptions: :: list any assumptions made in the experiment. If there are any other assumptions needed or that were not checked, state what they are and why they are important.
- Name: :: choose a statistical procedure from your bag of tricks based on the answers to the previous two parts. The assumptions of the procedure you choose should match those of the problem; if they do not match then either pick a different procedure or openly admit that the results may not be reliable. Write down any underlying formulas used.
- Interval: :: calculate the interval from the sample data. This can be done by hand but will more often be done with the aid of a computer. Regardless of the method, all calculations or code should be shown so that the entire process is repeatable by a subsequent reader.
- Conclusion: :: state the final results, using language in the context of the problem. Include the appropriate interpretation of the interval, making reference to the confidence coefficient.

#+begin_rem
All of the above intervals for \(\mu\) were two-sided, but there are also one-sided intervals for \(\mu\). They look like
\begin{equation}
\left[\overline{X}-z_{\alpha}\frac{\sigma}{\sqrt{n}},\ \infty\right)\quad \mbox{or}\quad \left(-\infty,\ \overline{X}+z_{\alpha}\frac{\sigma}{\sqrt{n}}\right]
\end{equation}
and satisfy
\begin{equation}
\mathbb{P}\left(\overline{X}-z_{\alpha}\frac{\sigma}{\sqrt{n}}\leq\mu\right)=1-\alpha\quad \mbox{and}\quad \mathbb{P}\left(\overline{X}+z_{\alpha}\frac{\sigma}{\sqrt{n}}\geq\mu\right)=1-\alpha.
\end{equation}
#+end_rem


#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Small sample, some data with \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) an \(SRS(n)\) from a \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) distribution.  PANIC
#+latex: \end{exampletoo}
#+html: </div>

*** How to do it with \(\mathsf{R}\)
We can do Example [[exa-plant-one-samp-z-int]] with the following code.

#+begin_src R :exports both :results output pp 
temp <- with(PlantGrowth, z.test(weight, stdev = 0.7))
temp
#+end_src

The confidence interval bounds are shown in the sixth line down of the output (please disregard all of the additional output information for now -- we will use it in Chapter [[cha-Hypothesis-Testing][Hypothesis Testing]]). We can make the plot for Figure [[fig-plant-z-int-plot][plant-z-int-plot]] with

#+begin_src R :exports code :eval never
plot(temp, "Conf")
#+end_src

** Confidence Intervals for Differences of Means
:PROPERTIES:
:CUSTOM_ID: sec-Conf-Interv-for-Diff-Means
:END:

Let \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) be a \(SRS(n)\) from a \(\mathsf{norm}(\mathtt{mean}=\mu_{X},\,\mathtt{sd}=\sigma_{X})\) distribution and let \(Y_{1}\), \(Y_{2}\), ..., \(Y_{m}\) be a \(SRS(m)\) from a \(\mathsf{norm}(\mathtt{mean}=\mu_{Y},\,\mathtt{sd}=\sigma_{Y})\) distribution. Further, assume that the \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) sample is independent of the \(Y_{1}\), \(Y_{2}\), ..., \(Y_{m}\) sample.

Suppose that \(\sigma_{X}\) and \(\sigma_{Y}\) are known. We would like a confidence interval for \(\mu_{X}-\mu_{Y}\). We know that 
\begin{equation}
\overline{X}-\overline{Y}\sim\mathsf{norm}\left(\mathtt{mean}=\mu_{X}-\mu_{Y},\,\mathtt{sd}=\sqrt{\frac{\sigma_{X}^{2}}{n}+\frac{\sigma_{Y}^{2}}{m}}\right).
\end{equation}
Therefore, a \( 100(1-\alpha)\% \) confidence interval for \(\mu_{X}-\mu_{Y}\) is given by
\begin{equation}
\label{eq-two-samp-mean-CI}
\left(\overline{X}-\overline{Y}\right)\pm z_{\alpha/2}\sqrt{\frac{\sigma_{X}^{2}}{n}+\frac{\sigma_{Y}^{2}}{m}}.
\end{equation}
Unfortunately, most of the time the values of \(\sigma_{X}\) and \(\sigma_{Y}\) are unknown. This leads us to the following:
- If both sample sizes are large, then we may appeal to the CLT/SLLN (see Section [[sec-Central-Limit-Theorem][Central Limit Theorem]]) and substitute \(S_{X}^{2}\) and \(S_{Y}^{2}\) for \(\sigma_{X}^{2}\) and \(\sigma_{Y}^{2}\) in the interval [[eq-two-samp-mean-CI]]. The resulting confidence interval will have approximately \(100(1-\alpha)\%\) confidence.
- If one or more of the sample sizes is small then we are in trouble, unless
    -the underlying populations are both normal and \(\sigma_{X}=\sigma_{Y}\). In this case (setting \(\sigma=\sigma_{X}=\sigma_{Y}\)), 
    \begin{equation}
    \overline{X}-\overline{Y}\sim\mathsf{norm}\left(\mathtt{mean}=\mu_{X}-\mu_{Y},\,\mathtt{sd}=\sigma\sqrt{\frac{1}{n}+\frac{1}{m}}\right).
    \end{equation}
Now let
\begin{equation}
U=\frac{n-1}{\sigma^{2}}S_{X}^{2}+\frac{m-1}{\sigma^{2}}S_{Y}^{2}.
\end{equation}
Then by Exercise [[xca-sum-indep-chisq]] we know that \(U\sim\mathsf{chisq}(\mathtt{df}=n+m-2)\) and it is not a large leap to believe that \(U\) is independent of \(\overline{X}-\overline{Y}\); thus
\begin{equation}
T = \frac{Z}{\sqrt{\left.U\right/ (n+m-2)}}\sim\mathsf{t}(\mathtt{df}=n+m-2).
\end{equation}
But
\begin{align*}
T & =\frac{\frac{\overline{X}-\overline{Y}-(\mu_{X}-\mu_{Y})}{\sigma\sqrt{\frac{1}{n}+\frac{1}{m}}}}{\sqrt{\left.\frac{n-1}{\sigma^{2}}S_{X}^{2}+\frac{m-1}{\sigma^{2}}S_{Y}^{2}\right/ (n+m-2)}},\\
 & =\frac{\overline{X}-\overline{Y}-(\mu_{X}-\mu_{Y})}{\sqrt{\left(\frac{1}{n}+\frac{1}{m}\right)\left(\frac{(n-1)S_{X}^{2}+(m-1)S_{Y}^{2}}{n+m-2}\right)}},\\
 & \sim\mathsf{t}(\mathtt{df}=n+m-2).
\end{align*}
Therefore a \(100(1-\alpha)\%\) confidence interval for \(\mu_{X}-\mu_{Y}\) is given by
\begin{equation}
\left(\overline{X}-\overline{Y}\right)\pm t_{\alpha/2}(\mathtt{df}=n+m-2)\, S_{p}\sqrt{\frac{1}{n}+\frac{1}{m}},
\end{equation}
where
\begin{equation}
S_{p}=\sqrt{\frac{(n-1)S_{X}^{2}+(m-1)S_{Y}^{2}}{n+m-2}}
\end{equation}
is called the ``pooled'' estimator of \(\sigma\).
    - If one of the samples is small, and both underlying populations are normal, but \(\sigma_{X}\neq\sigma_{Y}\), then we may use a Welch (or Satterthwaite) approximation to the degrees of freedom. See Welch \cite{Welch1947}, Satterthwaite \cite{Satterthwaite1946}, or Neter /et al/ \cite{Neter1996}. The idea is to use an interval of the form 
\begin{equation}
\left(\overline{X}-\overline{Y}\right)\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=r)\,\sqrt{\frac{S_{X}^{2}}{n}+\frac{S_{Y}^{2}}{m}},
\end{equation}
where the degrees of freedom \(r\) is chosen so that the interval has nice statistical properties. It turns out that a good choice for \(r\) is given by
\begin{equation}
r=\frac{\left(S_{X}^{2}/n+S_{Y}^{2}/m\right)^{2}}{\frac{1}{n-1}\left(S_{X}^{2}/n\right)^{2}+\frac{1}{m-1}\left(S_{Y}^{2}/m\right)^{2}},
\end{equation}
where we understand that \(r\) is rounded down to the nearest integer. The resulting interval has approximately \(100(1-\alpha)\%\) confidence.

*** How to do it with \(\mathsf{R}\)

The basic function is =t.test= which has a =var.equal= argument that may be set to =TRUE= or =FALSE=. The confidence interval is shown as part of the output, although there is a lot of additional information that is not needed until Chapter [[cha-Hypothesis-Testing][Hypothesis Testing]].

There is not any specific functionality to handle the \(z\)-interval for small samples, but if the samples are large then =t.test= with =var.equal = FALSE= will be essentially the same thing. The standard deviations are never (?) known in advance anyway so it does not really matter in practice. 

** Confidence Intervals for Proportions
:PROPERTIES:
:CUSTOM_ID: sec-Confidence-Intervals-Proportions
:END:

We would like to know \(p\) which is the ``proportion of successes''. For instance, \(p\) could be:
- the proportion of U.S. citizens that support Obama,
- the proportion of smokers among adults age 18 or over,
- the proportion of people worldwide infected by the H1N1 virus.

We are given an \(SRS(n)\) \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) distributed \(\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p)\). Recall from Section [[sec-binom-dist][Binomial Distribution]] that the common mean of these variables is \(\mathbb{E} X=p\) and the variance is \(\mathbb{E}(X-p)^{2}=p(1-p)\). If we let \(Y=\sum X_{i}\), then from Section [[sec-binom-dist][Binomial Distribution]] we know that \(Y\sim\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p)\) and that 
\[
\overline{X}=\frac{Y}{n}\mbox{ has }\mathbb{E}\overline{X}=p\mbox{ and }\mathrm{Var}(\overline{X})=\frac{p(1-p)}{n}.
\]
Thus if \(n\) is large (here is the CLT) then an approximate \(100(1-\alpha)\%\) confidence interval for \(p\) would be given by
\begin{equation}
\label{eq-ci-p-no-good}
\overline{X}\pm z_{\alpha/2}\sqrt{\frac{p(1-p)}{n}}.
\end{equation}
OOPS...! Equation [[eq-ci-p-no-good]] is of no use to us because the \underbar{unknown} parameter \(p\) is in the formula! (If we knew what \(p\) was to plug in the formula then we would not need a confidence interval in the first place.) There are two solutions to this problem.
1. Replace \(p\) with \(\hat{p}=\overline{X}\). Then an approximate \(100(1-\alpha)\%\) confidence interval for \(p\) is given by 
   \begin{equation}
   \hat{p}\pm z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}.
   \end{equation}
   This approach is called the /Wald interval/ and is also known as the /asymptotic interval/ because it appeals to the CLT for large sample sizes.
2. Go back to first principles. Note that
   \[
   -z_{\alpha/2}\leq\frac{Y/n-p}{\sqrt{p(1-p)/n}}\leq z_{\alpha/2}
   \]
   exactly when the function \(f\) defined by
   \[
   f(p)=\left(Y/n-p\right)^{2}-z_{\alpha/2}^{2}\frac{p(1-p)}{n}
   \]
   satisfies \(f(p)\leq0\). But \(f\) is quadratic in \(p\) so its graph is a parabola; it has two roots, and these roots form the limits of the confidence interval. We can find them with the quadratic formula (see Exercise [[xca-CI-quad-form]]):
   \begin{equation}
   \left.\left[\left(\hat{p}+\frac{z_{\alpha/2}^{2}}{2n}\right)\pm z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}+\frac{z_{\alpha/2}^{2}}{(2n)^{2}}}\right]\right/ \left(1+\frac{z_{\alpha/2}^{2}}{n}\right)
   \end{equation}
   This approach is called the /score interval/ because it is based on the inversion of the ``Score test''. See Chapter [[cha-Categorical-Data-Analysis][Categorical Data Analysis]]. It is also known as the /Wilson interval/; see Agresti \cite{Agresti2002}.


For two proportions \(p_{1}\) and \(p_{2}\), we may collect independent \(\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p)\) samples of size \(n_{1}\) and \(n_{2}\), respectively. Let \(Y_{1}\) and \(Y_{2}\) denote the number of successes in the respective samples. 
We know that
\[
\frac{Y_{1}}{n_{1}}\approx\mathsf{norm}\left(\mathtt{mean}=p_{1},\,\mathtt{sd}=\sqrt{\frac{p_{1}(1-p_{1})}{n_{1}}}\right)
\]
and
\[
\frac{Y_{2}}{n_{2}}\approx\mathsf{norm}\left(\mathtt{mean}=p_{2},\,\mathtt{sd}=\sqrt{\frac{p_{2}(1-p_{2})}{n_{2}}}\right)
\]
so it stands to reason that an approximate \(100(1-\alpha)\%\) confidence interval for \(p_{1}-p_{2}\) is given by
\begin{equation}
\left(\hat{p}_{1}-\hat{p}_{2}\right)\pm z_{\alpha/2}\sqrt{\frac{\hat{p}_{1}(1-\hat{p}_{1})}{n_{1}}+\frac{\hat{p}_{2}(1-\hat{p}_{2})}{n_{2}}},
\end{equation}
where \(\hat{p}_{1}=Y_{1}/n_{1}\) and \(\hat{p}_{2}=Y_{2}/n_{2}\).

#+begin_rem
When estimating a single proportion, one-sided intervals are sometimes needed. They take the form
\begin{equation}
\left[0,\ \hat{p}+z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}\right]
\end{equation}
or
\begin{equation}
\left[\hat{p}-z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}},\ 1\right]
\end{equation}
or in other words, we know in advance that the true proportion is restricted to the interval \([0,1]\), so we can truncate our confidence interval to those values on either side.
#+end_rem

*** How to do it with \(\mathsf{R}\)

#+begin_src R :exports both :results output pp 
binconf(x = 7, n = 25, method = "asymptotic")
#+end_src

#+begin_src R :exports both :results output pp 
binconf(x = 7, n = 25, method = "wilson")
#+end_src

The default value of the =method= argument is =wilson=.  An alternate way is 
#+begin_src R :exports none :results silent
data(RcmdrTestDrive)
#+end_src

#+begin_src R :exports both :results output pp 
tab <- xtabs(~gender, data = RcmdrTestDrive)
prop.test(rbind(tab), conf.level = 0.95, correct = FALSE)
#+end_src

#+begin_src R :exports code :results silent
A <- as.data.frame(Titanic)
B <- with(A, untable(A, Freq))
#+end_src

** Confidence Intervals for Variances
:PROPERTIES:
:CUSTOM_ID: sec-Confidence-Intervals-for-Variances
:END:

I am thinking one and two sample problems here.

*** How to do it with \(\mathsf{R}\)

I am thinking about =sigma.test= in the =TeachingDemos= package \cite{TeachingDemos} and =var.test= in base \(\mathsf{R}\) \cite{base} here.

** Fitting Distributions
:PROPERTIES:
:CUSTOM_ID: sec-Fitting-Distributions
:END:

*** How to do it with \(\mathsf{R}\)

I am thinking about =fitdistr= from the =MASS= package \cite{MASS}.

** Sample Size and Margin of Error
:PROPERTIES:
:CUSTOM_ID: sec-Sample-Size-and-MOE
:END:

Sections [[sec-Confidence-Intervals-for-Means][Confidence Intervals for Means]] through [[sec-Confidence-Intervals-for-Variances][Confidence Intervals for Variances]] all began the same way: we were given the sample size \(n\) and the confidence coefficient \(1-\alpha\), and our task was to find a margin of error \(E\) so that 
\[
\hat{\theta}\pm E\mbox{ is a }100(1-\alpha)\%\mbox{ confidence interval for }\theta.
\]

Some examples we saw were:
- \(E=z_{\alpha/2}\sigma/\sqrt{n}\), in the one-sample \(z\)-interval,
- \(E=t_{\alpha/2}(\mathtt{df}=n+m-2)S_{p}\sqrt{n^{-1}+m^{-1}}\), in the two-sample pooled \(t\)-interval. 

We already know (we can see in the formulas above) that \(E\) decreases as \(n\) increases. Now we would like to use this information to our advantage: suppose that we have a fixed margin of error \(E,\) say \(E=3\), and we want a \(100(1-\alpha)\%\) confidence interval for \(\mu\). The question is: how big does \(n\) have to be?

For the case of a population mean the answer is easy: we set up an equation and solve for \(n\).

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Given a situation, given \(\sigma\), given \(E\), we would like to know how big \(n\) has to be to ensure that \(\overline{X}\pm5\) is a 95% confidence interval for \(\mu\).
#+latex: \end{exampletoo}
#+html: </div>

#+begin_rem
Always round up any decimal values of \(n\), no matter how small the decimal is. Another name for \(E\) is the ``maximum error of the estimate''.
#+end_rem

For proportions, recall that the asymptotic formula to estimate \(p\) was
\[
\hat{p}\pm z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}.
\]
Reasoning as above we would want
\begin{align}
\label{eq-samp-size-prop-ME}
E & =z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}},\mbox{ or}\\
n & =z_{\alpha/2}^{2}\frac{\hat{p}(1-\hat{p})}{E^{2}}.
\end{align}
OOPS! Recall that \(\hat{p}=Y/n\), which would put the variable \(n\) on both sides of Equation [[eq-samp-size-prop-ME]]. Again, there are two solutions to the problem.

1. If we have a good idea of what \(p\) is, say \(p^{\ast}\) then we can plug it in to get
   \begin{equation}
   n=z_{\alpha/2}^{2}\frac{p^{\ast}(1-p^{\ast})}{E^{2}}.
   \end{equation}
2. Even if we have no idea what \(p\) is, we do know from calculus that \(p(1-p)\leq1/4\) because the function \(f(x)=x(1-x)\) is quadratic (so its graph is a parabola which opens downward) with maximum value attained at \(x=1/2\). Therefore, regardless of our choice for \(p^{\ast}\) the sample size must satisfy
   \begin{equation}
   n=z_{\alpha/2}^{2}\frac{p^{\ast}(1-p^{\ast})}{E^{2}}\leq\frac{z_{\alpha/2}^{2}}{4E^{2}}.
   \end{equation}
   The quantity \(z_{\alpha/2}^{2}/4E^{2}\) is large enough to guarantee \(100(1-\alpha)\%\) confidence.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Proportion example.
#+latex: \end{exampletoo}
#+html: </div>

#+begin_rem
For very small populations sometimes the value of \(n\) obtained from the formula is too big. In this case we should use the hypergeometric distribution for a sampling model rather than the binomial model. With this modification the formulas change to the following: if \(N\) denotes the population size then let
\begin{equation}
m=z_{\alpha/2}^{2}\frac{p^{\ast}(1-p^{\ast})}{E^{2}}
\end{equation}
and the sample size needed to ensure \(100(1-\alpha)\%\) confidence is achieved is
\begin{equation}
n=\frac{m}{1+\frac{m-1}{N}}.
\end{equation}
If we do not have a good value for the estimate \(p^{\ast}\) then we may use \(p^{\ast}=1/2\).
#+end_rem

*** How to do it with \(\mathsf{R}\)
I am thinking about =power.t.test=, =power.prop.test=, =power.anova.test=, and I am also thinking about =replicate=.

** Other Topics
:PROPERTIES:
:CUSTOM_ID: sec-Other-Topics
:END:

Mention =mle= from the =stats4= package \cite{stats4}.

#+latex: \newpage{}

** Exercises
#+latex: \setcounter{thm}{0}

#+begin_xca
# <<xca-norm-mu-sig-MLE>>
Let \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) be an \(SRS(n)\) from a \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) distribution. Find a two-dimensional MLE for \(\theta=(\mu,\sigma)\).
#+end_xca

#+begin_xca
# <<xca-CI-quad-form>>
Find the upper and lower limits for the confidence interval procedure by finding the roots of \(f\) defined by 
\[
f(p)=\left(Y/n-p\right)^{2}-z_{\alpha/2}^{2}\frac{p(1-p)}{n}.
\]
You are going to need the quadratic formula.
#+end_xca

* Hypothesis Testing                                                 :hypoth:
:PROPERTIES:
:tangle: R/hypoth.R
:CUSTOM_ID: cha-Hypothesis-Testing
:END:

#+begin_src R :exports none :eval never
# Chapter: Hypothesis Testing
# All code released under GPL Version 3
#+end_src

*What do I want them to know?*
- basic terminology and philosophy of the Neyman-Pearson paradigm
- classical hypothesis tests for the standard one and two sample problems with means, variances, and proportions
- the notion of between versus within group variation and how it plays out with one-way ANOVA
- the concept of statistical power and its relation to sample size

** Introduction
:PROPERTIES:
:CUSTOM_ID: sec-Introduction-Hypothesis
:END:

I spent a week during the summer of 2005 at the University of Nebraska at Lincoln grading Advanced Placement Statistics exams, and while I was there I attended a presentation by Dr. Roxy Peck. At the end of her talk she described an activity she had used with students to introduce the basic concepts of hypothesis testing. I was impressed by the activity and have used it in my own classes several times since.

#+begin_quote
The instructor (with a box of cookies in hand) enters a class of fifteen or more students and produces a brand-new, sealed deck of ordinary playing cards. The instructor asks for a student volunteer to break the seal, and then the instructor prominently shuffles the deck
#+latex: \footnote{The jokers are removed before shuffling.}
several times in front of the class, after which time the students are asked to line up in a row. They are going to play a game. Each student will draw a card from the top of the deck, in turn. If the card is black, then the lucky student will get a cookie. If the card is red, then the unlucky student will sit down empty-handed. Let the game begin.

The first student draws a card: red. There are jeers and outbursts, and the student slinks off to his/her chair. (S)he is disappointed, of course, but not really. After all, (s)he had a 50-50 chance of getting black, and it did not happen. Oh well.

The second student draws a card: red, again. There are more jeers, and the second student slips away. This student is also disappointed, but again, not so much, because it is probably his/her unlucky day. On to the next student.

The student draws: red again! There are a few wiseguys who yell (happy to make noise, more than anything else), but there are a few other students who are not yelling any more -- they are thinking. This is the third red in a row, which is possible, of course, but what is going on, here? They are not quite sure. They are now concentrating on the next card... it is bound to be black, right?

The fourth student draws: red. Hmmm... now there are groans instead of outbursts. A few of the students at the end of the line shrug their shoulders and start to make their way back to their desk, complaining that the teacher does not want to give away any cookies. There are still some students in line though, salivating, waiting for the inevitable black to appear.

The fifth student draws red. Now it isn't funny any more. As the remaining students make their way back to their seats an uproar ensues, from an entire classroom demanding cookies.
#+end_quote

Keep the preceding experiment in the back of your mind as you read the following sections. When you have finished the entire chapter, come back and read this introduction again. All of the mathematical jargon that follows is connected to the above paragraphs. In the meantime, I will get you started:

- Null hypothesis: :: it is an ordinary deck of playing cards, shuffled thoroughly.
- Alternative hypothesis: :: either it is a trick deck of cards, or the instructor did some fancy shufflework.
- Observed data: :: a sequence of draws from the deck, five reds in a row.

If it were truly an ordinary, well-shuffled deck of cards, the probability of observing zero blacks out of a sample of size five (without replacement) from a deck with 26 black cards and 26 red cards would be

#+begin_src R :exports both :results output pp 
dhyper(0, m = 26, n = 26, k = 5)
#+end_src

There are two very important final thoughts. First, everybody gets a cookie in the end. Second, the students invariably (and aggressively) attempt to get me to open up the deck and reveal the true nature of the cards. I never do.

** Tests for Proportions
:PROPERTIES:
:CUSTOM_ID: sec-Tests-for-Proportions
:END:

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-widget-machine>>

We have a machine that makes widgets. 

- Under normal operation, about 0.10 of the widgets produced are defective.
- Go out and purchase a torque converter.
- Install the torque converter, and observe \(n=100\) widgets from the machine.
- Let \(Y=\mbox{number of defective widgets observed}\).

If

- \(Y=0\), then the torque converter is great!
- \(Y=4\), then the torque converter seems to be helping. 
- \(Y=9\), then there is not much evidence that the torque converter helps.
- \(Y=17\), then throw away the torque converter.

Let \(p\) denote the proportion of defectives produced by the machine. Before the installation of the torque converter \(p\) was \(0.10\). Then we installed the torque converter. Did \(p\) change? Did it go up or down? We use statistics to decide. Our method is to observe data and construct a 95% confidence interval for \(p\),
\begin{equation}
\hat{p}\pm z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}.
\end{equation}
If the confidence interval is 
- \([0.01,\,0.05]\), then we are 95% confident that \(0.01\leq p\leq0.05\), so there is evidence that the torque converter is helping.
- \([0.15,\,0.19]\), then we are 95% confident that \(0.15\leq p\leq0.19\), so there is evidence that the torque converter is hurting.
- \([0.07,\,0.11]\), then there is not enough evidence to conclude that the torque converter is doing anything at all, positive or negative.

#+latex: \end{exampletoo}
#+html: </div>

*** Terminology

The /null hypothesis/ \(H_{0}\) is a ``nothing'' hypothesis, whose interpretation could be that nothing has changed, there is no difference, there is nothing special taking place, /etc/. In Example [[exa-widget-machine]] the null hypothesis would be \(H_{0}:\, p=0.10.\) The /alternative hypothesis/ \(H_{1}\) is the hypothesis that something has changed, in this case, \(H_{1}:\, p\neq0.10\). Our goal is to statistically /test/ the hypothesis \(H_{0}:\, p=0.10\) versus the alternative \(H_{1}:\, p\neq0.10\). Our procedure will be:
1. Go out and collect some data, in particular, a simple random sample of observations from the machine.
2. Suppose that \(H_{0}\) is true and construct a \(100(1-\alpha)\%\) confidence interval for \(p\).
3. If the confidence interval does not cover \(p=0.10\), then we /reject/ \(H_{0}\). Otherwise, we /fail to reject/ \(H_{0}\).

#+begin_rem
Every time we make a decision it is possible to be wrong, and there are two possible mistakes that we could make. We have committed a 
- Type I Error :: if we reject \(H_{0}\) when in fact \(H_{0}\) is true. This would be akin to convicting an innocent person for a crime (s)he did not commit.
- Type II Error :: if we fail to reject \(H_{0}\) when in fact \(H_{1}\) is true. This is analogous to a guilty person escaping conviction.
#+end_rem

Type I Errors are usually considered worse
#+latex: \footnote{There is no mathematical difference between the errors, however. The bottom line is that we choose one type of error to control with an iron fist, and we try to minimize the probability of making the other type. That being said, null hypotheses are often by design to correspond to the ``simpler'' model, so it is often easier to analyze (and thereby control) the probabilities associated with Type I Errors.}, 
and we design our statistical procedures to control the probability of making such a mistake. We define the
\begin{equation}
\mbox{significance level of the test}=\mathbb{P}(\mbox{Type I Error})=\alpha.
\end{equation}
We want \(\alpha\) to be small which conventionally means, say, \(\alpha=0.05\), \(\alpha=0.01\), or \(\alpha=0.005\) (but could mean anything, in principle).
- The /rejection region/ (also known as the /critical region/) for the test is the set of sample values which would result in the rejection of \(H_{0}\). For Example [[exa-widget-machine]], the rejection region would be all possible samples that result in a 95% confidence interval that does not cover \(p=0.10\).
- The above example with \(H_{1}:p\neq0.10\) is called a /two-sided/ test. Many times we are interested in a /one-sided/ test, which would look like \(H_{1}:p<0.10\) or \(H_{1}:p>0.10\).

We are ready for tests of hypotheses for one proportion.
Table here.
Don't forget the assumptions.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Find
1. The null and alternative hypotheses.
2. Check your assumptions.
3. Define a critical region with an \(\alpha=0.05\) significance level.
4. Calculate the value of the test statistic and state your conclusion.
#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-prop-test-pvalue-A>>

Suppose \(p=\mbox{the proportion of students}\) who are admitted to the graduate school of the University of California at Berkeley, and suppose that a public relations officer boasts that UCB has historically had a 40% acceptance rate for its graduate school. Consider the data stored in the table =UCBAdmissions= from 1973. Assuming these observations constituted a simple random sample, are they consistent with the officer's claim, or do they provide evidence that the acceptance rate was significantly less than 40%? Use an \(\alpha=0.01\) significance level.

Our null hypothesis in this problem is \(H_{0}:\, p=0.4\) and the alternative hypothesis is \(H_{1}:\, p<0.4\). We reject the null hypothesis if \(\hat{p}\) is too small, that is, if
\begin{equation} 
\frac{\hat{p}-0.4}{\sqrt{0.4(1-0.4)/n}}<-z_{\alpha},
\end{equation}
where \(\alpha=0.01\) and \(-z_{0.01}\) is 
#+begin_src R :exports both :results output pp 
-qnorm(0.99)
#+end_src

Our only remaining task is to find the value of the test statistic and see where it falls relative to the critical value. We can find the number of people admitted and not admitted to the UCB graduate school with the following. 

#+begin_src R :exports both :results output pp 
A <- as.data.frame(UCBAdmissions)
head(A)
xtabs(Freq ~ Admit, data = A)
#+end_src

Now we calculate the value of the test statistic.

#+begin_src R :exports both :results output pp 
phat <- 1755/(1755 + 2771)
(phat - 0.4)/sqrt(0.4 * 0.6/(1755 + 2771)) 
#+end_src

Our test statistic is not less than \(-2.32\), so it does not fall into the critical region. Therefore, we /fail/ to reject the null hypothesis that the true proportion of students admitted to graduate school is less than 40% and say that the observed data are consistent with the officer's claim at the \(\alpha=0.01\) significance level. 

#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-prop-test-pvalue-B>>
We are going to do Example [[exa-prop-test-pvalue-A]] all over again. Everything will be exactly the same except for one change. Suppose we choose significance level \(\alpha=0.05\) instead of \(\alpha=0.01\). Are the 1973 data consistent with the officer's claim?

Our null and alternative hypotheses are the same. Our observed test statistic is the same: it was approximately \(-1.68\). But notice that our critical value has changed: \(\alpha=0.05\) and \(-z_{0.05}\) is 
#+latex: \end{exampletoo}
#+html: </div>
#+begin_src R :exports both :results output pp 
-qnorm(0.95)
#+end_src

Our test statistic is less than \(-1.64\) so it now falls into the critical region! We now /reject/ the null hypothesis and conclude that the 1973 data provide evidence that the true proportion of students admitted to the graduate school of UCB in 1973 was significantly less than 40%. The data are /not/ consistent with the officer's claim at the \(\alpha=0.05\) significance level.

What is going on, here? If we choose \(\alpha=0.05\) then we reject the null hypothesis, but if we choose \(\alpha=0.01\) then we fail to reject the null hypothesis. Our final conclusion seems to depend on our selection of the significance level. This is bad; for a particular test, we never know whether our conclusion would have been different if we had chosen a different significance level. 

Or do we?

Clearly, for some significance levels we reject, and for some significance levels we do not. Where is the boundary? That is, what is the significance level for which we would /reject/ at any significance level /bigger/, and we would /fail to reject/ at any significance level /smaller/? This boundary value has a special name: it is called the /p-value/ of the test.

#+begin_defn
The /p-value/, or /observed significance level/, of a hypothesis test is the probability when the null hypothesis is true of obtaining the observed value of the test statistic (such as \(\hat{p}\)) or values more extreme -- meaning, in the direction of the alternative hypothesis
#+latex: \footnote{Bickel and Doksum \cite{Bickel2001} state the definition particularly well: the \(p\)-value is ``the smallest level of significance \(\alpha\) at which an experimenter using the test statistic \(T\) would reject \(H_{0}\) on the basis of the observed sample outcome \(x\)''.}. 
#+end_defn

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Calculate the \(p\)-value for the test in Examples [[exa-prop-test-pvalue-A]] and [[exa-prop-test-pvalue-B]].

The \(p\)-value for this test is the probability of obtaining a \(z\)-score equal to our observed test statistic (which had \(z\)-score \(\approx-1.680919\)) or more extreme, which in this example is less than the observed test statistic. In other words, we want to know the area under a standard normal curve on the interval \((-\infty,\,-1.680919]\). We can get this easily with
#+latex: \end{exampletoo}
#+html: </div>

#+begin_src R :exports both :results output pp 
pnorm(-1.680919)
#+end_src

We see that the \(p\)-value is strictly between the significance levels \(\alpha=0.01\) and \(\alpha=0.05\). This makes sense: it has to be bigger than \(\alpha=0.01\) (otherwise we would have rejected \(H_{0}\) in Example [[exa-prop-test-pvalue-A]]) and it must also be smaller than \(\alpha=0.05\) (otherwise we would not have rejected \(H_{0}\) in Example [[exa-prop-test-pvalue-B]]). Indeed, \(p\)-values are a characteristic indicator of whether or not we would have rejected at assorted significance levels, and for this reason a statistician will often skip the calculation of critical regions and critical values entirely. If (s)he knows the \(p\)-value, then (s)he knows immediately whether or not (s)he would have rejected at /any/ given significance level.

Thus, another way to phrase our significance test procedure is: we will reject \(H_{0}\) at the \(\alpha\)-level of significance if the \(p\)-value is less than \(\alpha\).

#+begin_rem
If we have two populations with proportions \(p_{1}\) and \(p_{2}\) then we can test the null hypothesis \(H_{0}:p_{1}=p_{2}\).
#+end_rem

Table Here.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Example.
#+latex: \end{exampletoo}
#+html: </div>

**** How to do it with \(\mathsf{R}\)

The following does the test.

#+begin_src R :exports both :results output pp 
prop.test(1755, 1755 + 2771, p = 0.4, alternative = "less", 
          conf.level = 0.99, correct = FALSE)
#+end_src

Do the following to make the plot.


Use Yates' continuity correction when the expected frequency of successes is less than 10. You can use it all of the time, but you will have a decrease in power. For large samples the correction does not matter. 

**** With the \(\mathsf{R}\) Commander

If you already know the number of successes and failures, then you can use the menu =Statistics= \(\triangleright\) =Proportions= \(\triangleright\) =IPSUR Enter table for single sample...=

Otherwise, your data -- the raw successes and failures -- should be in a column of the Active Data Set. Furthermore, the data must be stored as a ``factor'' internally. If the data are not a factor but are numeric then you can use the menu =Data= \(\triangleright\) =Manage variables in active data set= \(\triangleright\) =Convert numeric variables to factors...= to convert the variable to a factor. Or, you can always use the =factor= function.

Once your unsummarized data is a column, then you can use the menu =Statistics= \(\triangleright\) =Proportions= \(\triangleright\) =Single-sample proportion test...=

** One Sample Tests for Means and Variances
:PROPERTIES:
:CUSTOM_ID: sec-One-Sample-Tests
:END:

*** For Means

Here, \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) are a \(SRS(n)\) from a \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) distribution. We would like to test \(H_{0}:\mu=\mu_{0}\).

- Case A: :: Suppose \(\sigma\) is known. Then under \(H_{0}\),
   \[
   Z=\frac{\overline{X}-\mu_{0}}{\sigma/\sqrt{n}}\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1).
   \]
   Table here.
- Case B: :: When \(\sigma\) is unknown, under \(H_{0}\)
   \[
   T = \frac{\overline{X}-\mu_{0}}{S/\sqrt{n}}\sim\mathsf{t}(\mathtt{df}=n-1).
   \]
   Table here.


#+begin_rem
If \(\sigma\) is unknown but \(n\) is large then we can use the \(z\)-test.
#+end_rem

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
In this example we
1. Find the null and alternative hypotheses.
2. Choose a test and find the critical region.
3. Calculate the value of the test statistic and state the conclusion.
4. Find the \(p\)-value.

#+latex: \end{exampletoo}
#+html: </div>

#+begin_rem
Another name for a \(p\)-value is /tail end probability/. We reject \(H_{0}\) when the \(p\)-value is small.
The quantity \(\sigma/\sqrt{n}\), when \(\sigma\) is known, is called the /standard error of the sample mean/. In general, if we have an estimator \(\hat{\theta}\) then \(\sigma_{\hat{\theta}}\) is called the /standard error/ of \(\hat{\theta}\). We usually need to estimate \(\sigma_{\hat{\theta}}\) with \(\hat{\sigma_{\hat{\theta}}}\).
#+end_rem

**** How to do it with \(\mathsf{R}\)

I am thinking =z.test=\index{z.test@\texttt{z.test}} in =TeachingDemos=, =t.test=\index{t.test@\texttt{t.test}} in base \(\mathsf{R}\).

#+begin_src R :exports both :results output pp 
x <- rnorm(37, mean = 2, sd = 3)
z.test(x, mu = 1, sd = 3, conf.level = 0.90)
#+end_src

The =RcmdrPlugin.IPSUR= package \cite{RcmdrPlugin.IPSUR} does not have a menu for =z.test= yet. 

#+begin_src R :exports both :results output pp 
x <- rnorm(13, mean = 2, sd = 3)
t.test(x, mu = 0, conf.level = 0.90, alternative = "greater")
#+end_src

**** With the \(\mathsf{R}\) Commander

Your data should be in a single numeric column (a variable) of the Active Data Set. Use the menu =Statistics= \(\triangleright\) =Means= \(\triangleright\) =Single-sample t-test...= 

*** Tests for a Variance

Here, \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) are a \(SRS(n)\) from a \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) distribution. We would like to test \(H_{0}:\sigma^{2}=\sigma_{0}\). We know that under \(H_{0}\),
\[
X^{2}=\frac{(n-1)S^{2}}{\sigma^{2}}\sim\mathsf{chisq}(\mathtt{df}=n-1).
\]
Table here.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Give some data and a hypothesis.
- Give an \(\alpha\)-level and test the critical region way.
- Find the \(p\)-value for the test.
#+latex: \end{exampletoo}
#+html: </div>

**** How to do it with \(\mathsf{R}\)
I am thinking about =sigma.test=\index{sigma.test@\texttt{sigma.test}} in the =TeachingDemos= package \cite{TeachingDemos}.

#+begin_src R :exports both :results output pp 
sigma.test(women$height, sigma = 8)
#+end_src

** Two-Sample Tests for Means and Variances
:PROPERTIES:
:CUSTOM_ID: sec-Two-Sample-Tests-for-Means
:END:

The basic idea for this section is the following. We have \(X\sim\mathsf{norm}(\mathtt{mean}=\mu_{X},\,\mathtt{sd}=\sigma_{X})\) and \(Y\sim\mathsf{norm}(\mathtt{mean}=\mu_{Y},\,\mathtt{sd}=\sigma_{Y})\). distributed independently. We would like to know whether \(X\) and \(Y\) come from the same population distribution, that is, we would like to know:
\begin{equation}
\mbox{Does }X\overset{\mathrm{d}}{=}Y?
\end{equation}
where the symbol \(\overset{\mathrm{d}}{=}\) means equality of probability distributions.
Since both \(X\) and \(Y\) are normal, we may rephrase the question:
\begin{equation}
\mbox{Does }\mu_{X}=\mu_{Y}\mbox{ and }\sigma_{X}=\sigma_{Y}?
\end{equation}
Suppose first that we do not know the values of \(\sigma_{X}\) and \(\sigma_{Y}\), but we know that they are equal, \(\sigma_{X}=\sigma_{Y}\). Our test would then simplify to \(H_{0}:\mu_{X}=\mu_{Y}\). We collect data \(X_{1}\), \(X_{2}\), ..., \(X_{n}\) and \(Y_{1}\), \(Y_{2}\), ..., \(Y_{m}\), both simple random samples of size \(n\) and \(m\) from their respective normal distributions. Then under \(H_{0}\) (that is, assuming \(H_{0}\) is true) we have \(\mu_{X}=\mu_{Y}\) or rewriting, \(\mu_{X}-\mu_{Y}=0\), so 
\begin{equation}
T=\frac{\overline{X}-\overline{Y}}{S_{p}\sqrt{\frac{1}{n}+\frac{1}{m}}}=\frac{\overline{X}-\overline{Y}-(\mu_{X}-\mu_{Y})}{S_{p}\sqrt{\frac{1}{n}+\frac{1}{m}}}\sim\mathsf{t}(\mathtt{df}=n+m-2).
\end{equation}

*** Independent Samples

#+begin_rem
If the values of \(\sigma_{X}\) and \(\sigma_{Y}\) are known, then we can plug them in to our statistic:
\begin{equation} 
Z=\frac{\overline{X}-\overline{Y}}{\sqrt{\sigma_{X}^{2}/n+\sigma_{Y}^{2}/m}};
\end{equation}
the result will have a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) distribution when \(H_{0}:\mu_{X}=\mu_{Y}\) is true. 
#+end_rem

#+begin_rem
Even if the values of \(\sigma_{X}\) and \(\sigma_{Y}\) are not known, if both \(n\) and \(m\) are large then we can plug in the sample estimates and the result will have approximately a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) distribution when \(H_{0}:\mu_{X}=\mu_{Y}\) is true.
\begin{equation} 
Z=\frac{\overline{X}-\overline{Y}}{\sqrt{S_{X}^{2}/n+S_{Y}^{2}/m}}.
\end{equation}
#+end_rem

#+begin_rem
It is usually important to construct side-by-side boxplots and other visual displays in concert with the hypothesis test. This gives a visual comparison of the samples and helps to identify departures from the test's assumptions -- such as outliers.
#+end_rem

#+begin_rem
WATCH YOUR ASSUMPTIONS.
- The normality assumption can be relaxed as long as the population distributions are not highly skewed.
- The equal variance assumption can be relaxed as long as both sample sizes \(n\) and \(m\) are large. However, if one (or both) samples is small, then the test does not perform well; we should instead use the methods of Chapter [[cha-resampling-methods][Resampling Methods]].
#+end_rem

For a nonparametric alternative to the two-sample \(F\) test see Chapter [[cha-Nonparametric-Statistics][Nonparametric Statistics]].

*** Paired Samples

**** How to do it with \(\mathsf{R}\)

#+begin_src R :exports both :results output pp 
t.test(extra ~ group, data = sleep, paired = TRUE)
#+end_src

** Other Hypothesis Tests
:PROPERTIES:
:CUSTOM_ID: sec-Other-Hypothesis-Tests
:END:

*** Kolmogorov-Smirnov Goodness-of-Fit Test
:PROPERTIES:
:CUSTOM_ID: sub-Kolmogorov-Smirnov-Goodness-of-Fit-Test
:END:

**** How to do it with \(\mathsf{R}\)

#+begin_src R :exports both :results output pp 
ks.test(randu$x, "punif")
#+end_src

*** Shapiro-Wilk Normality Test
:PROPERTIES:
:CUSTOM_ID: sub-Shapiro-Wilk-Normality-Test
:END:

**** How to do it with \(\mathsf{R}\)

#+begin_src R :exports both :results output pp 
shapiro.test(women$height)
#+end_src

** Analysis of Variance
:PROPERTIES:
:CUSTOM_ID: sec-Analysis-of-Variance
:END:

*** How to do it with \(\mathsf{R}\)

I am thinking 
#+begin_src R :exports both :results output pp 
with(chickwts, by(weight, feed, shapiro.test))
#+end_src
and
#+begin_src R :exports code :results silent 
temp <- lm(weight ~ feed, data = chickwts)
#+end_src
and 
#+begin_src R :exports both :results output pp 
anova(temp)
#+end_src

Plot for the intuition of between versus within group variation.

#+name: Between-versus-within
#+begin_src R :exports none :results silent
y1 <- rnorm(300, mean = c(2,8,22))
plot(y1, xlim = c(-1,25), ylim = c(0,0.45) , type = "n")
f <- function(x){dnorm(x, mean = 2)}
curve(f, from = -1, to = 5, add = TRUE, lwd = 2)
f <- function(x){dnorm(x, mean = 8)}
curve(f, from = 5, to = 11, add = TRUE, lwd = 2)
f <- function(x){dnorm(x, mean = 22)}
curve(f, from = 19, to = 25, add = TRUE, lwd = 2)
rug(y1)
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/hypoth/Between-versus-within.ps
  <<Between-versus-within>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/hypoth/Between-versus-within.svg
  <<Between-versus-within>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/hypoth/Between-versus-within.ps}
  \caption[Between group versus within group variation]{A plot of between group versus within group variation.}
  \label{fig-Between-versus-within}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-Between-versus-within" class="figure">
  <p><img src="svg/hypoth/Between-versus-within.svg" width=500 alt="svg/hypoth/Between-versus-within.svg" /></p>
  <p>A plot of between group versus within group variation.</p>
</div>
#+end_html

#+name: Some-F-plots-HH
#+begin_src R :exports code :results silent
old.omd <- par(omd = c(.05,.88, .05,1))
F.setup(df1 = 5, df2 = 30)
F.curve(df1 = 5, df2 = 30, col='blue')
F.observed(3, df1 = 5, df2 = 30)
par(old.omd)
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/hypoth/Some-F-plots-HH.ps
  <<Some-F-plots-HH>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/hypoth/Some-F-plots-HH.svg
  <<Some-F-plots-HH>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/hypoth/Some-F-plots-HH.ps}
  \caption[Some \(F\) plots from the \texttt{HH} package]{\small Some \(F\) plots from the \texttt{HH} package.}
  \label{fig-Some-F-plots-HH}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-Some-F-plots-HH" class="figure">
  <p><img src="svg/hypoth/Some-F-plots-HH.svg" width=500 alt="svg/hypoth/Some-F-plots-HH.svg" /></p>
  <p>Some \(F\) plots from the <code>HH</code> package.</p>
</div>
#+end_html

** Sample Size and Power
:PROPERTIES:
:CUSTOM_ID: sec-Sample-Size-and-Power
:END:

The power function of a test for a parameter \(\theta\) is
\[
\beta(\theta)=\mathbb{P}_{\theta}(\mbox{Reject }H_{0}),\quad -\infty < \theta < \infty.
\]
Here are some properties of power functions:
1. \(\beta(\theta)\leq\alpha\) for any \(\theta\in\Theta_{0}\), and \(\beta(\theta_{0})=\alpha\). We interpret this by saying that no matter what value \(\theta\) takes inside the null parameter space, there is never more than a chance of \(\alpha\) of rejecting the null hypothesis. We have controlled the Type I error rate to be no greater than \(\alpha\).
2. \(\lim_{n\to\infty}\beta(\theta)=1\) for any fixed \(\theta\in\Theta_{1}\). In other words, as the sample size grows without bound we are able to detect a nonnull value of \(\theta\) with increasing accuracy, no matter how close it lies to the null parameter space. This may appear to be a good thing at first glance, but it often turns out to be a curse, because this means that our Type II error rate grows as the sample size increases. 

*** How to do it with \(\mathsf{R}\)

I am thinking about =replicate=\index{replicate@\texttt{replicate}} here, and also =power.examp=\index{power.examp@\texttt{power.examp}} from the =TeachingDemos= package \cite{TeachingDemos}. There is an even better plot in upcoming work from the =HH= package \cite{HH}.

#+name: power-examp
#+begin_src R :exports code :results silent
power.examp()
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/hypoth/power-examp.ps
  <<power-examp>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/hypoth/power-examp.svg
  <<power-examp>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/hypoth/power-examp.ps}
  \caption[Plot of significance level and power]{\small This graph was generated by the \texttt{power.examp} function from the \texttt{TeachingDemos} package. The plot corresponds to the hypothesis test \(H_{0}:\,\mu=\mu_{0}\) versus \(H_{1}:\,\mu=\mu_{1}\) (where \(\mu_{0}=0\) and \(\mu_{1}=1\), by default) based on a single observation \(X\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\). The top graph is of the \(H_{0}\) density while the bottom is of the \(H_{1}\) density. The significance level is set at \(\alpha=0.05\), the sample size is \(n=1\), and the standard deviation is \(\sigma=1\). The pink area is the significance level, and the critical value \(z_{0.05}\approx1.645\) is marked at the left boundary -- this defines the rejection region. When \(H_{0}\) is true, the probability of falling in the rejection region is exactly \(\alpha=0.05\). The same rejection region is marked on the bottom graph, and the probability of falling in it (when \(H_{1}\)  is true) is the blue area shown at the top of the display to be approximately \(0.26\). This probability represents the \emph{power} to detect a non-null mean value of \(\mu=1\). With the command the \texttt{run.power.examp()} at the command line the same plot opens, but in addition, there are sliders available that allow the user to interactively change the sample size \(n\), the standard deviation \(\sigma\), the true difference between the means \(\mu_{1}-\mu_{0}\), and the significance level \(\alpha\). By playing around the student can investigate the effect each of the aforementioned parameters has on the statistical power. Note that you need the \texttt{tkrplot} package \cite{tkrplot} for \texttt{run.power.examp}.}
  \label{fig-power-examp}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-power-examp" class="figure">
  <p><img src="svg/hypoth/power-examp.svg" width=500 alt="svg/hypoth/power-examp.svg" /></p>
  <p>A plot of significance level and power.</p>
</div>
#+end_html

#+latex: \newpage{}

** Exercises

#+latex: \setcounter{thm}{0}

* Simple Linear Regression                                              :slr:
:PROPERTIES:
:tangle: R/slr.R
:CUSTOM_ID: cha-simple-linear-regression
:END:

#+begin_src R :exports none :eval never
# Chapter: Simple Linear Regression
# All code released under GPL Version 3
#+end_src

*What do I want them to know?*

- basic philosophy of SLR and the regression assumptions
- point and interval estimation of the model parameters, and how to use it to make predictions
- point and interval estimation of future observations from the model
- regression diagnostics, including \( R^{2} \) and basic residual analysis
- the concept of influential versus outlying observations, and how to tell the difference

** Basic Philosophy
:PROPERTIES:
:CUSTOM_ID: sec-Basic-Philosophy
:END:

Here we have two variables \(X\) and \(Y\). For our purposes, \(X\) is not random (so we will write \(x\)), but \(Y\) is random. We believe that \(Y\) depends in /some/ way on \(x\). Some typical examples of \( (x,Y) \) pairs are

- \( x = \) study time and \( Y = \) score on a test.
- \( x = \) height and \( Y = \) weight.
- \( x = \) smoking frequency and \( Y = \) age of first heart attack.

Given information about the relationship between \(x\) and \(Y\), we would like to /predict/ future values of \(Y\) for particular values of \(x\). This turns out to be a difficult problem \!\!
#+latex: \footnote{Yogi Berra once said, ``It is always difficult to make predictions, especially about the future.''}
, so instead we first tackle an easier problem: we estimate \( \mathbb{E} Y \). How can we accomplish this? Well, we know that \(Y\) depends somehow on \(x\), so it stands to reason that
\begin{equation}
\mathbb{E} Y = \mu(x),\ \mbox{a function of }x.
\end{equation}

But we should be able to say more than that. To focus our efforts we impose some structure on the functional form of \(\mu\). For instance, 
- if \(\mu(x)=\beta_{0}+\beta_{1}x\), we try to estimate \( \beta_{0} \) and \( \beta_{1} \).
- if \( \mu(x) = \beta_{0} + \beta_{1}x + \beta_{2}x^{2} \), we try to estimate \(\beta_{0}\), \(\beta_{1}\), and \(\beta_{2}\).
- if \( \mu(x) = \beta_{0} \mathrm{e}^{\beta_{1}x} \), we try to estimate \(\beta_{0}\) and \(\beta_{1}\).

This helps us in the sense that we concentrate on the estimation of just a few parameters, \(\beta_{0}\) and \(\beta_{1}\), say, rather than some nebulous function. Our /modus operandi/ is simply to perform the random experiment \(n\) times and observe the \(n\) ordered pairs of data \( (x_{1},Y_{1}),\ (x_{2},Y_{2}),\ \ldots,(x_{n},Y_{n}) \). We use these \(n\) data points to estimate the parameters.

More to the point, there are /three simple linear regression/ (SLR) assumptions\index{regression assumptions} that will form the basis for the rest of this chapter:

#+begin_assumption
We assume that \(\mu\) is a linear function of \(x\), that is, 
\begin{equation}
\mu(x)=\beta_{0}+\beta_{1}x,
\end{equation}
where \(\beta_{0}\) and \(\beta_{1}\) are unknown constants to be estimated.
#+end_assumption

#+begin_assumption
We further assume that \( Y_{i} \) is \( \mu(x_{i}) \), a ``signal'', plus some ``error'' (represented by the symbol \( \epsilon_{i} \)):
\begin{equation}
Y_{i} = \beta_{0} + \beta_{1}x_{i} + \epsilon_{i}, \quad i = 1,2,\ldots,n.
\end{equation}
#+end_assumption

#+begin_assumption
We lastly assume that the errors are IID normal with mean 0 and variance \( \sigma^{2} \):
\begin{equation}
\epsilon_{1},\epsilon_{2},\ldots,\epsilon_{n}\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=\sigma).
\end{equation}
#+end_assumption

#+begin_rem
We assume both the normality of the errors \(\epsilon\) and the linearity of the mean function \( \mu \). Recall from Proposition [[pro-mvnorm-cond-dist]] of Chapter [[cha-Multivariable-Distributions][Multivariate Distributions]] that if \( (X,Y)\sim\mathsf{mvnorm} \) then the mean of \(Y|x\) is a linear function of \(x\). This is not a coincidence. In more advanced classes we study the case that both \(X\) and \(Y\) are random, and in particular, when they are jointly normally distributed.
#+end_rem

*** What does it all mean?
See Figure [[fig-philosophy][philosophy]]. Shown in the figure is a solid line, the regression line\index{regression line} \(\mu\), which in this display has slope 0.5 and /y/-intercept 2.5, that is, \( \mu(x) = 2.5 + 0.5x \). The intuition is that for each given value of \(x\), we observe a random value of \(Y\) which is normally distributed with a mean equal to the height of the regression line at that \(x\) value. Normal densities are superimposed on the plot to drive this point home; in principle, the densities stand outside of the page, perpendicular to the plane of the paper. The figure shows three such values of \(x\), namely, \( x = 1 \), \( x = 2.5 \), and \( x = 4 \). Not only do we assume that the observations at the three locations are independent, but we also assume that their distributions have the same spread. In mathematical terms this means that the normal densities all along the line have identical standard deviations -- there is no ``fanning out'' or ``scrunching in'' of the normal densities as \(x\) increases
#+latex: \footnote{In practical terms, this constant variance assumption is often violated, in that we often observe scatterplots that fan out from the line as \(x\) gets large or small. We say under those circumstances that the data show \emph{heteroscedasticity}. There are methods to address it, but they fall outside the realm of SLR.}.

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/slr/philosophy.ps}
  \caption[Philosophical foundations of SLR]{\small Philosophical foundations of SLR.}
  \label{fig-philosophy}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-philosophy" class="figure">
  <p><img src="svg/slr/philosophy.svg" width=500 alt="svg/slr/philosophy.svg" /></p>
  <p>Philosophical foundations of SLR.</p>
</div>
#+end_html

#+name: philosophy
#+begin_src R :exports none :results silent
# open window
plot(c(0,5), c(0,6.5), type = "n", xlab="x", ylab="y")
abline(h = 0, v = 0, col = "gray60")
abline(a = 2.5, b = 0.5, lwd = 2)
x <- 600:3000/600
y <- dnorm(x, mean = 3, sd = 0.5)
lines(y + 1.0, x)
lines(y + 2.5, x + 0.75)
lines(y + 4.0, x + 1.5)
abline(v = c(1, 2.5, 4), lty = 2, col = "grey")
segments(1, 3, 1 + dnorm(0,0,0.5),3, lty = 2, col = "gray")
segments(2.5, 3.75, 2.5 + dnorm(0,0,0.5), 3.75, lty = 2, col = "gray")
segments(4,4.5, 4 + dnorm(0,0,0.5),4.5, lty = 2, col = "gray")
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/slr/philosophy.ps
  <<philosophy>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/slr/philosophy.svg
  <<philosophy>>
#+end_src

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-Speed-and-Stopping>>
*Speed and stopping distance of cars.* We will use the data frame \texttt{cars}\index{Data sets!cars@\texttt{cars}} from the =datasets= package \cite{datasets}. It has two variables: =speed= and =dist=. We can take a look at some of the values in the data frame: 
#+begin_src R :exports both :results output pp 
head(cars)
#+end_src

The =speed= represents how fast the car was going (\(x\)) in miles per hour and =dist= (\(Y\)) measures how far it took the car to stop, in feet. We can make a simple scatterplot of the data with the =qplot= command in the =ggplot2= package \cite{ggplot2}. 

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/slr/carscatter.ps}
  \caption[Scatterplot of \texttt{dist} versus \texttt{speed} for the \texttt{cars} data]{\small A scatterplot of \texttt{dist} versus \texttt{speed} for the \texttt{cars} data.  There is clearly an upward trend to the plot which is approximately linear.}
  \label{fig-Scatter-cars}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-Scatter-cars" class="figure">
  <p><img src="svg/slr/carscatter.svg" width=500 alt="svg/slr/carscatter.svg" /></p>
  <p>Scatterplot of <code>dist</code> versus <code>speed</code> for the <code>cars</code> data.</p>
</div>
#+end_html

#+name: carscatter
#+begin_src R :exports code :results silent
qplot(speed, dist, data = cars)
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/slr/carscatter.ps
  <<carscatter>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/slr/carscatter.svg
  <<carscatter>>
#+end_src

You can see the output in Figure [[fig-Scatter-cars][carscatter]], which was produced by the following code.

#+begin_src R :exports code :eval never :results silent
qplot(speed, dist, data = cars)
plot(dist ~ speed, data = cars)
#+end_src

There is a pronounced upward trend to the data points, and the pattern looks approximately linear. There does not appear to be substantial fanning out of the points or extreme values. 
#+latex: \end{exampletoo}
#+html: </div>

** Estimation
:PROPERTIES:
:CUSTOM_ID: sec-SLR-Estimation
:END:

*** Point Estimates of the Parameters
:PROPERTIES:
:CUSTOM_ID: sub-point-estimate-mle-slr
:END:

Where is \( \mu(x) \)? In essence, we would like to ``fit'' a line to the points. But how do we determine a ``good'' line? Is there a /best/ line? We will use maximum likelihood\index{maximum likelihood} to find it. We know:
\begin{equation}
Y_{i} = \beta_{0} + \beta_{1}x_{i} + \epsilon_{i},\quad i=1,\ldots,n,
\end{equation}
where the \( \epsilon_{i} \) are IID \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=\sigma) \). Thus \( Y_{i}\sim\mathsf{norm}(\mathtt{mean}=\beta_{0}+\beta_{1}x_{i},\,\mathtt{sd}=\sigma),\ i=1,\ldots,n \). Furthermore, \( Y_{1},\ldots,Y_{n} \) are independent -- but not identically distributed. The likelihood function\index{likelihood function} is:
\begin{alignat}{1}
L(\beta_{0},\beta_{1},\sigma)= & \prod_{i=1}^{n}f_{Y_{i}}(y_{i}),\\
= & \prod_{i=1}^{n}(2\pi\sigma^{2})^{-1/2}\exp\left\{ \frac{-(y_{i}-\beta_{0}-\beta_{1}x_{i})^{2}}{2\sigma^{2}}\right\} ,\\
= & (2\pi\sigma^{2})^{-n/2}\exp\left\{ \frac{-\sum_{i=1}^{n}(y_{i}-\beta_{0}-\beta_{1}x_{i})^{2}}{2\sigma^{2}}\right\} .
\end{alignat}
We take the natural logarithm to get
\begin{equation}
\label{eq-regML-lnL}
\ln L(\beta_{0},\beta_{1},\sigma)=-\frac{n}{2}\ln(2\pi\sigma^{2})-\frac{\sum_{i=1}^{n}(y_{i}-\beta_{0}-\beta_{1}x_{i})^{2}}{2\sigma^{2}}.
\end{equation}
 We would like to maximize this function of \( \beta_{0} \) and \( \beta_{1} \). See Appendix [[sec-Multivariable-Calculus][Multivariable Calculus]] which tells us that we should find critical points by means of the partial derivatives. Let us start by differentiating with respect to \( \beta_{0} \):
\begin{equation}
\frac{\partial}{\partial\beta_{0}}\ln L=0-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}2(y_{i}-\beta_{0}-\beta_{1}x_{i})(-1),
\end{equation}
and the partial derivative equals zero when \( \sum_{i=1}^{n}(y_{i}-\beta_{0}-\beta_{1}x_{i}) = 0 \), that is, when
\begin{equation}
\label{eq-regML-a}
n \beta_{0} + \beta_{1} \sum_{i=1}^{n} x_{i} = \sum_{i = 1}^{n}y_{i}.
\end{equation}
Moving on, we next take the partial derivative of \( \ln L \) (Equation [[eq-regML-lnL]]) with respect to \( \beta_{1} \) to get
\begin{alignat}{1}
\frac{\partial}{\partial\beta_{1}}\ln L=\  & 0-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}2(y_{i}-\beta_{0}-\beta_{1}x_{i})(-x_{i}),\\
= & \frac{1}{\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}y_{i}-\beta_{0}x_{i}-\beta_{1}x_{i}^{2}\right),
\end{alignat}
and this equals zero when the last sum equals zero, that is, when
\begin{equation}
\label{eq-regML-b}
\beta_{0} \sum_{i = 1}^{n}x_{i} + \beta_{1} \sum_{i = 1}^{n}x_{i}^{2} = \sum_{i = 1}^{n}x_{i}y_{i}.
\end{equation}
Solving the system of equations [[eq-regML-a]] and [[eq-regML-b]]
\begin{eqnarray}
n\beta_{0} + \beta_{1}\sum_{i = 1}^{n}x_{i} & = & \sum_{i = 1}^{n}y_{i}\\
\beta_{0}\sum_{i = 1}^{n}x_{i}+\beta_{1}\sum_{i = 1}^{n}x_{i}^{2} & = & \sum_{i = 1}^{n}x_{i}y_{i}
\end{eqnarray}
for \( \beta_{0} \) and \( \beta_{1} \) (in Exercise [[xca-find-mles-SLR]]) gives
\begin{equation}
\label{eq-regline-slope-formula}
\hat{\beta}_{1} = \frac{\sum_{i = 1}^{n}x_{i}y_{i} - \left.\left(\sum_{i = 1}^{n}x_{i}\right)\left(\sum_{i = 1}^{n}y_{i}\right)\right] n}{\sum_{i = 1}^{n}x_{i}^{2} - \left.\left(\sum_{i = 1}^{n}x_{i}\right)^{2}\right/ n}
\end{equation}
and
\begin{equation}
\hat{\beta}_{0} = \overline{y} - \hat{\beta}_{1}\overline{x}.
\end{equation}

The conclusion? To estimate the mean line 
\begin{equation}
\mu(x) = \beta_{0} + \beta_{1}x,
\end{equation}
we use the ``line of best fit''
\begin{equation}
\hat{\mu}(x) = \hat{\beta}_{0} + \hat{\beta}_{1}x,
\end{equation}
where \(\hat{\beta}_{0}\) and \(\hat{\beta}_{1}\) are given as above. For notation we will usually write \( b_{0} = \hat{\beta_{0}} \) and \( b_{1}=\hat{\beta_{1}} \) so that \( \hat{\mu}(x) = b_{0} + b_{1}x \).

#+begin_rem
The formula for \( b_{1} \) in Equation [[eq-regline-slope-formula]] gets the job done but does not really make any sense. There are many equivalent formulas for \( b_{1} \) that are more intuitive, or at the least are easier to remember. One of the author's favorites is
\begin{equation}
\label{eq-sample-correlation-formula}
b_{1} = r\frac{s_{y}}{s_{x}},
\end{equation}
where \(r\), \( s_{y} \), and \( s_{x} \) are the sample correlation coefficient and the sample standard deviations of the \(Y\) and \(x\) data, respectively. See Exercise [[xca-show-alternate-slope-formula]]. Also, notice the similarity between Equation [[eq-sample-correlation-formula]] and Equation [[eq-population-slope-slr]].
#+end_rem

**** How to do it with \(\mathsf{R}\)

#+begin_src R :exports none :results silent
tmpcoef <- round(as.numeric(coef(lm(dist ~ speed, cars))), 2)
#+end_src

Here we go. \(\mathsf{R}\) will calculate the linear regression line with the =lm= function. We will store the result in an object which we will call =cars.lm=. Here is how it works:

#+begin_src R :exports code :results silent
cars.lm <- lm(dist ~ speed, data = cars)
#+end_src

The first part of the input to the =lm= function, =dist ~ speed=, is a /model formula/, read like this: =dist= is described (or modeled) by =speed=. The =data = cars= argument tells \(\mathsf{R}\) where to look for the variables quoted in the model formula. The output object =cars.lm= contains a multitude of information. Let's first take a look at the coefficients of the fitted regression line, which are extracted by the =coef= function (alternatively, we could just type =cars.lm= to see the same thing):

#+begin_src R :exports both :results output pp 
coef(cars.lm)
#+end_src

The parameter estimates \( b_{0} \) and \( b_{1} \) for the intercept and slope, respectively, are shown above. The regression line is thus given by \( \hat{\mu}(\mathtt{speed}) = SRC_R{tmpcoef[1]} + SRC_R{tmpcoef[2]} \cdot \mathtt{speed} \).

It is good practice to visually inspect the data with the regression line added to the plot. To do this we first scatterplot the original data and then follow with a call to the =abline= function. The inputs to =abline= are the coefficients of =cars.lm=; see Figure [[fig-Scatter-cars-regline][Scatter-cars-regline]].

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/slr/carline.ps}
  \caption[Scatterplot with added regression line for the \texttt{cars} data]{\small A scatterplot with an added regression line for the \texttt{cars} data.}
  \label{fig-Scatter-cars-regline}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-Scatter-cars-regline" class="figure">
  <p><img src="svg/slr/carline.svg" width=500 alt="svg/slr/carline.svg" /></p>
  <p>A scatterplot with an added regression line for the <code>cars</code> data.</p>
</div>
#+end_html

#+name: carline
#+begin_src R :exports code :results silent
ggplot(cars, aes(x = speed, y = dist)) + geom_point(shape = 19) + geom_smooth(method = lm, se = FALSE)
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/slr/carline.ps
  <<carline>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/slr/carline.svg
  <<carline>>
#+end_src

To calculate points on the regression line we may simply plug the desired \(x\) value(s) into \( \hat{\mu} \), either by hand, or with the =predict= function. The inputs to =predict= are the fitted linear model object, =cars.lm=, and the desired \(x\) value(s) represented by a data frame. See the example below.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-regline-cars-interpret>>

Using the regression line for the =cars= data:

1. What is the meaning of \( \mu(60) = \beta_{0} + \beta_{1}(8) \)? 
   This represents the average stopping distance (in feet) for a car going 8 mph. 
1. Interpret the slope \(\beta_{1}\). 
   The true slope \(\beta_{1}\) represents the increase in average stopping distance for each mile per hour faster that the car drives. In this case, we estimate the car to take approximately \( SRC_R{tmpcoef[2]} \) additional feet to stop for each additional mph increase in speed.
1. Interpret the intercept \( \beta_{0} \).
   This would represent the mean stopping distance for a car traveling 0 mph (which our regression line estimates to be \( SRC_R{tmpcoef[1]} \) ). Of course, this interpretation does not make any sense for this example, because a car travelling 0 mph takes 0 ft to stop (it was not moving in the first place)! What went wrong? Looking at the data, we notice that the smallest speed for which we have measured data is 4 mph. Therefore, if we predict what would happen for slower speeds then we would be /extrapolating/, a dangerous practice which often gives nonsensical results.
#+latex: \end{exampletoo}
#+html: </div>

*** Point Estimates of the Regression Line
:PROPERTIES:
:CUSTOM_ID: sub-slr-point-est-regline
:END:

We said at the beginning of the chapter that our goal was to estimate \( \mu = \mathbb{E} Y \), and the arguments in Section [[sub-point-estimate-mle-slr][Point Estimation]] showed how to obtain an estimate \( \hat{\mu} \) of \( \mu \) when the regression assumptions hold. Now we will reap the benefits of our work in more ways than we previously disclosed. Given a particular value \(x_{0}\), there are two values we would like to estimate:
1. the mean value of \(Y\) at \(x_{0}\), and
1. a future value of \(Y\) at \(x_{0}\).
The first is a number, \(\mu(x_{0})\), and the second is a random variable, \(Y(x_{0})\), but our point estimate is the same for both: \(\hat{\mu}(x_{0})\).

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-regline-cars-pe-8mph>>
We may use the regression line to obtain a point estimate of the mean stopping distance for a car traveling 8 mph: \( \hat{\mu}(15) = b_{0} + (8) (b_{1}) \approx SRC_R{tmpcoef[1]} + (8) ( SRC_R{tmpcoef[2]} ) \approx 13.88 \). We would also use 13.88 as a point estimate for the stopping distance of a future car traveling 8 mph. 
#+latex: \end{exampletoo}
#+html: </div>

Note that we actually have observed data for a car traveling 8 mph; its stopping distance was 16 ft as listed in the fifth row of the =cars= data (which we saw in Example [[exa-Speed-and-Stopping]]).

#+begin_src R :exports both :results output pp
cars[5, ]
#+end_src

There is a special name for estimates \( \hat{\mu}(x_{0}) \) when \( x_{0} \) matches an observed value \(x_{i}\) from the data set. They are called /fitted values/, they are denoted by \(\hat{Y}_{1}\), \(\hat{Y}_{2}\), ..., \(\hat{Y}_{n}\) (ignoring repetition), and they play an important role in the sections that follow. 

In an abuse of notation we will sometimes write \(\hat{Y}\) or \(\hat{Y}(x_{0})\) to denote a point on the regression line even when \(x_{0}\) does not belong to the original data if the context of the statement obviates any danger of confusion.

We saw in Example [[exa-regline-cars-interpret]] that spooky things can happen when we are cavalier about point estimation. While it is usually acceptable to predict/estimate at values of \(x_{0}\) that fall within the range of the original \(x\) data, it is reckless to use \(\hat{\mu}\) for point estimates at locations outside that range. Such estimates are usually worthless. /Do not extrapolate/ unless there are compelling external reasons, and even then, temper it with a good deal of caution.

**** How to do it with \(\mathsf{R}\)

The fitted values are automatically computed as a byproduct of the model fitting procedure and are already stored as a component of the =cars.lm= object. We may access them with the =fitted= function (we only show the first five entries):

#+begin_src R :exports both :results output pp 
fitted(cars.lm)[1:5]
#+end_src

Predictions at \(x\) values that are not necessarily part of the original data are done with the =predict= function. The first argument is the original =cars.lm= object and the second argument =newdata= accepts a dataframe (in the same form that was used to fit =cars.lm=) that contains the locations at which we are seeking predictions. Let us predict the average stopping distances of cars traveling 6 mph, 8 mph, and 21 mph:

#+begin_src R :exports both :results output pp 
predict(cars.lm, newdata = data.frame(speed = c(6, 8, 21)))
#+end_src

Note that there were no observed cars that traveled 6 mph or 21 mph. Also note that our estimate for a car traveling 8 mph matches the value we computed by hand in Example [[exa-regline-cars-pe-8mph]].

*** Mean Square Error and Standard Error

To find the MLE of \(\sigma^{2}\) we consider the partial derivative
\begin{equation}
\frac{\partial}{\partial\sigma^{2}}\ln L=\frac{n}{2\sigma^{2}}-\frac{1}{2(\sigma^{2})^{2}}\sum_{i=1}^{n}(y_{i}-\beta_{0}-\beta_{1}x_{i})^{2},
\end{equation}
and after plugging in \(\hat{\beta}_{0}\) and \(\hat{\beta}_{1}\) and setting equal to zero we get
\begin{equation}
\hat{\sigma^{2}}=\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i})^{2}=\frac{1}{n}\sum_{i=1}^{n}[y_{i}-\hat{\mu}(x_{i})]^{2}.
\end{equation}
We write \(\hat{Yi}=\hat{\mu}(x_{i})\), and we let \(E_{i}=Y_{i}-\hat{Y_{i}}\) be the \(i^{\mathrm{th}}\) /residual/. We see 
\begin{equation}
n\hat{\sigma^{2}}=\sum_{i=1}^{n}E_{i}^{2}=SSE=\mbox{ the sum of squared errors.}
\end{equation}
For a point estimate of \(\sigma^{2}\) we use the /mean square error/ \(S^{2}\) defined by 
\begin{equation}
S^{2}=\frac{SSE}{n-2},
\end{equation}
and we estimate \(\sigma\) with the /standard error/
#+latex: \footnote{Be careful not to confuse the mean square error \(S^{2}\) with the sample variance \(S^{2}\) in Chapter \ref{cha-Describing-Data-Distributions}. Other notation the reader may encounter is the lowercase \(s^{2}\) or the bulky \(MSE\).}
\(S=\sqrt{S^{2}}\).

**** How to do it with \(\mathsf{R}\)

The residuals for the model may be obtained with the =residuals= function; we only show the first few entries in the interest of space:

#+begin_src R :exports both :results output pp 
residuals(cars.lm)[1:5]
#+end_src

#+begin_src R :exports none :results silent
tmpred <- round(as.numeric(predict(cars.lm, newdata = data.frame(speed = 8))), 2)
tmps <- round(summary(cars.lm)$sigma, 2)
#+end_src

In the last section, we calculated the fitted value for \(x=8\) and found it to be approximately \( \hat{\mu}(8)\approx SRC_R{tmpred} \). Now, it turns out that there was only one recorded observation at \(x = 8\), and we have seen this value in the output of =head(cars)= in Example [[exa-Speed-and-Stopping]]; it was \(\mathtt{dist} = 16\) ft for a car with \( \mathtt{speed} = 8 \) mph. Therefore, the residual should be \(E = Y - \hat{Y}\) which is \(E \approx 16 - SRC_R{tmpred} \). Now take a look at the last entry of =residuals(cars.lm)=, above. It is not a coincidence.

The estimate \(S\) for \(\sigma\) is called the =Residual standard error= and for the =cars= data is shown a few lines up on the =summary(cars.lm)= output (see How to do it with \(\mathsf{R}\) in Section [[sub-slr-interval-est-params][Interval Estimation]]). We may read it from there to be \( S\approx SRC_R{tmps} \), or we can access it directly from the =summary= object.

#+begin_src R :exports both :results output pp
carsumry <- summary(cars.lm)
carsumry$sigma
#+end_src

*** Interval Estimates of the Parameters
:PROPERTIES:
:CUSTOM_ID: sub-slr-interval-est-params
:END:

We discussed general interval estimation in Chapter [[cha-Estimation][Estimation]]. There we found that we could use what we know about the sampling distribution of certain statistics to construct confidence intervals for the parameter being estimated. We will continue in that vein, and to get started we will determine the sampling distributions of the parameter estimates, \(b_{1}\) and \(b_{0}\).

To that end, we can see from Equation [[eq-regline-slope-formula]] (and it is made clear in Chapter [[cha-multiple-linear-regression][Multiple Linear Regression]]) that \(b_{1}\) is just a linear combination of normally distributed random variables, so \(b_{1}\) is normally distributed too. Further, it can be shown that
\begin{equation}
b_{1}\sim\mathsf{norm}\left(\mathtt{mean}=\beta_{1},\,\mathtt{sd}=\sigma_{b_{1}}\right)
\end{equation}
where
\begin{equation}
\sigma_{b_{1}}=\frac{\sigma}{\sqrt{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}}
\end{equation}
is called /the standard error of/ \(b_{1}\) which unfortunately depends on the unknown value of \(\sigma\). We do not lose heart, though, because we can estimate \(\sigma\) with the standard error \(S\) from the last section. This gives us an estimate \(S_{b_{1}}\) for \(\sigma_{b_{1}}\) defined by
\begin{equation}
S_{b_{1}}=\frac{S}{\sqrt{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}}.
\end{equation}

Now, it turns out that \(b_{0}\), \(b_{1}\), and \(S\) are mutually independent (see the footnote in Section [[sub-mlr-interval-est-params][MLR Interval Estimation]]). Therefore, the quantity
\begin{equation}
T=\frac{b_{1}-\beta_{1}}{S_{b_{1}}}
\end{equation}
has a \(\mathsf{t}(\mathtt{df}=n-2)\) distribution and a \(100(1 - \alpha)\% \) confidence interval for \(\beta_{1}\) is given by 
\begin{equation}
b_{1}\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)\, S_{b_{1}.}
\end{equation}

It is also sometimes of interest to construct a confidence interval for \(\beta_{0}\) in which case we will need the sampling distribution of \(b_{0}\). It is shown in Chapter [[cha-multiple-linear-regression][Multiple Linear Regression]] that
\begin{equation}
b_{0}\sim\mathsf{norm}\left(\mathtt{mean}=\beta_{0},\,\mathtt{sd}=\sigma_{b_{0}}\right),
\end{equation}
where \(\sigma_{b_{0}}\) is given by
\begin{equation}
\sigma_{b_{0}}=\sigma\sqrt{\frac{1}{n}+\frac{\overline{x}^{2}}{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}},
\end{equation}
and which we estimate with the \(S_{b_{0}}\) defined by
\begin{equation}
S_{b_{0}}=S\sqrt{\frac{1}{n}+\frac{\overline{x}^{2}}{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}}.
\end{equation}
Thus the quantity
\begin{equation}
T=\frac{b_{0}-\beta_{0}}{S_{b_{0}}}
\end{equation}
has a \(\mathsf{t}(\mathtt{df}=n-2)\) distribution and a \(100(1-\alpha)\%\) confidence interval for \(\beta_{0}\) is given by
\begin{equation}
b_{0}\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)\, S_{b_{0}}.
\end{equation}

**** How to do it with \(\mathsf{R}\)

#+begin_src R :exports none :results silent
A <- matrix(as.numeric(round(carsumry$coef, 3)), nrow = 2)
B <- round(confint(cars.lm), 3)
#+end_src

Let us take a look at the output from =summary(cars.lm)=:

#+begin_src R :exports both :results output pp 
summary(cars.lm)
#+end_src

In the =Coefficients= section we find the parameter estimates and their respective standard errors in the second and third columns; the other columns are discussed in Section [[sec-Model-Utility-SLR][SLR Model Utility]]. If we wanted, say, a 95% confidence interval for \(\beta_{1}\) we could use \( b_{1} = SRC_R{A[2,1]} \) and \( S_{b_{1}} = SRC_R{A[2,2]} \) together with a \( \mathsf{t}_{0.025}(\mathtt{df}=23) \) critical value to calculate \( b_{1} \pm \mathsf{t}_{0.025}(\mathtt{df} = 23) \cdot S_{b_{1}} \).  Or, we could use the =confint= function.

#+begin_src R :exports both :results output pp 
confint(cars.lm)
#+end_src

With 95% confidence, the random interval \( [ SRC_R{B[2,1]}, SRC_R{B[2,2]} ] \) covers the parameter \(\beta_{1}\).

*** Interval Estimates of the Regression Line
:PROPERTIES:
:CUSTOM_ID: sub-slr-interval-est-regline
:END:

We have seen how to estimate the coefficients of regression line with both point estimates and confidence intervals. We even saw how to estimate a value \(\hat{\mu}(x)\) on the regression line for a given value of \(x\), such as \(x=15\). 

But how good is our estimate \(\hat{\mu}(15)\)? How much confidence do we have in /this/ estimate? Furthermore, suppose we were going to observe another value of \(Y\) at \(x=15\). What could we say?

Intuitively, it should be easier to get bounds on the mean (average) value of \(Y\) at \(x_{0}\) -- called a /confidence interval for the mean value of/ \(Y\) /at/ \(x_{0}\) -- than it is to get bounds on a future observation of \(Y\) (called a /prediction interval for/ \(Y\) /at/ \(x_{0}\)). As we shall see, the intuition serves us well and confidence intervals are shorter for the mean value, longer for the individual value.

Our point estimate of \(\mu(x_{0})\) is of course \(\hat{Y}=\hat{Y}(x_{0})\), so for a confidence interval we will need to know \(\hat{Y}\)'s sampling distribution. It turns out (see Section ) that \(\hat{Y}=\hat{\mu}(x_{0})\) is distributed
\begin{equation}
\hat{Y}\sim\mathsf{norm}\left(\mathtt{mean}=\mu(x_{0}),\:\mathtt{sd}=\sigma\sqrt{\frac{1}{n}+\frac{(x_{0}-\overline{x})^{2}}{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}}\right).
\end{equation}

Since \(\sigma\) is unknown we estimate it with \(S\) (we should expect the appearance of a \(\mathsf{t}(\mathtt{df}=n-2)\) distribution in the near future). 

A \( 100(1-\alpha)\% \) /confidence interval (CI) for/ \(\mu(x_{0})\) is given by
\begin{equation}
\label{eq-SLR-conf-int-formula}
\hat{Y}\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-2)\, S\sqrt{\frac{1}{n}+\frac{(x_{0}-\overline{x}^{2})}{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}}.
\end{equation}
Prediction intervals are a little bit different. In order to find confidence bounds for a new observation of \(Y\) (we will denote it \(Y_{\mbox{new}}\)) we use the fact that
\begin{equation}
Y_{\mbox{new}}\sim\mathtt{norm}\left(\mathtt{mean}=\mu(x_{0}),\,\mathtt{sd}=\sigma\sqrt{1+\frac{1}{n}+\frac{(x_{0}-\overline{x})^{2}}{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}}\right).
\end{equation}
Of course, \(\sigma\) is unknown so we estimate it with \(S\) and a \( 100(1-\alpha)\% \) prediction interval (PI) for a future value of \(Y\) at \(x_{0}\) is given by 
\begin{equation}
\label{eq-SLR-pred-int-formula}
\hat{Y}(x_{0})\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)\: S\,\sqrt{1+\frac{1}{n}+\frac{(x_{0}-\overline{x})^{2}}{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}}.
\end{equation}
We notice that the prediction interval in Equation [[eq-SLR-pred-int-formula]] is wider than the confidence interval in Equation [[eq-SLR-conf-int-formula]], as we expected at the beginning of the section.

**** How to do it with \(\mathsf{R}\)

Confidence and prediction intervals are calculated in \(\mathsf{R}\) with the =predict=\index{predict@\texttt{predict}} function, which we encountered in Section [[sub-slr-point-est-regline][SLR Point Estimation]]. There we neglected to take advantage of its additional =interval= argument. The general syntax follows. 

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
We will find confidence and prediction intervals for the stopping distance of a car travelling 5, 6, and 21 mph (note from the graph that there are no collected data for these speeds). We have computed =cars.lm= earlier, and we will use this for input to the =predict= function. Also, we need to tell \(\mathsf{R}\) the values of \(x_{0}\) at which we want the predictions made, and store the \(x_{0}\) values in a data frame whose variable is labeled with the correct name. /This is important/. 

#+begin_src R :exports code :results silent
new <- data.frame(speed = c(5, 6, 21))
#+end_src

Next we instruct \(\mathsf{R}\) to calculate the intervals. Confidence intervals are given by 

#+begin_src R :exports both :results output pp 
predict(cars.lm, newdata = new, interval = "confidence")
#+end_src

#+begin_src R :exports none
carsCI <- round(predict(cars.lm, newdata = new, interval = "confidence"), 2)
#+end_src

Prediction intervals are given by

#+begin_src R :exports both :results output pp 
predict(cars.lm, newdata = new, interval = "prediction")
#+end_src

#+begin_src R :exports none :results silent
carsPI <- round(predict(cars.lm, newdata = new, interval = "prediction"), 2)
#+end_src

#+latex: \end{exampletoo}
#+html: </div>

The type of interval is dictated by the =interval= argument (which is =none= by default), and the default confidence level is 95\% (which can be changed with the =level= argument). 

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Using the =cars= data,
1. Report a point estimate of and a 95% confidence interval for the mean stopping distance for a car travelling 5 mph.
   The fitted value for \(x=5\) is \( SRC_R{carsCI[1, 1]} \), so a point estimate would be \( SRC_R{carsCI[1, 1]} \) ft. The 95% CI is given by \( [ SRC_R{carsCI[1, 2]}, SRC_R{carsCI[1, 3]} ] \), so with 95% confidence the mean stopping distance lies somewhere between \( SRC_R{carsCI[1, 2]} \) ft and \( SRC_R{carsCI[1, 3]} \) ft.
2. Report a point prediction for and a 95% prediction interval for the stopping distance of a hypothetical car travelling 21 mph.
   The fitted value for \(x = 21\) is \( SRC_R{carsPI[3, 1]} \), so a point prediction for the stopping distance is \( SRC_R{carsPI[3, 1]} \) ft. The 95% PI is \( [ SRC_R{carsPI[3, 2]}, SRC_R{carsPI[3, 3]} ] \), so with 95% confidence we may assert that the hypothetical stopping distance for a car travelling 21 mph would lie somewhere between \( SRC_R{carsPI[3, 2]} \) ft and \( SRC_R{carsPI[3, 3]} \) ft.
#+latex: \end{exampletoo}
#+html: </div>

*** Graphing the Confidence and Prediction Bands

We earlier guessed that a bound on the value of a single new observation would be inherently less certain than a bound for an average (mean) value; therefore, we expect the CIs for the mean to be tighter than the PIs for a new observation. A close look at the standard deviations in Equations [[eq-SLR-conf-int-formula]] and [[eq-SLR-pred-int-formula]] confirms our guess, but we would like to see a picture to drive the point home.

We may plot the confidence and prediction intervals with one fell swoop using the =ci.plot= function from the =HH= package \cite{HH}. The graph is displayed in Figure [[fig-Scatter-cars-CIPI][Scatter-cars-CIPI]].

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/slr/carscipi.ps}
  \caption[Scatterplot with confidence/prediction bands for the \texttt{cars} data]{\small A scatterplot with confidence/prediction bands for the \texttt{cars} data.}
  \label{fig-Scatter-cars-CIPI}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-Scatter-cars-CIPI" class="figure">
  <p><img src="svg/slr/carscipi.svg" width=500 alt="svg/slr/carscipi.svg" /></p>
  <p>A scatterplot with confidence/prediction bands for the <code>cars</code> data.</p>
</div>
#+end_html

#+begin_src R :exports code :eval never
library(HH)
ci.plot(cars.lm)
#+end_src

Notice that the bands curve outward from the regression line as the \(x\) values move away from the center. This is expected once we notice the \((x_{0}-\overline{x})^{2}\) term in the standard deviation formulas in Equations [[eq-SLR-conf-int-formula]] and [[eq-SLR-pred-int-formula]].

#+name: carscipi
#+begin_src R :exports none :results silent
print(ci.plot(cars.lm))
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/slr/carscipi.ps
  <<carscipi>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/slr/carscipi.svg
  <<carscipi>>
#+end_src

** Model Utility and Inference
:PROPERTIES:
:CUSTOM_ID: sec-Model-Utility-SLR
:END:

*** Hypothesis Tests for the Parameters
:PROPERTIES:
:CUSTOM_ID: sub-slr-hypoth-test-params
:END:

Much of the attention of SLR is directed toward \(\beta_{1}\) because when \( \beta_{1}\neq 0 \) the mean value of \(Y\) increases (or decreases) as \(x\) increases. It is really boring when \(\beta_{1}=0\), because in that case the mean value of \(Y\) remains the same, regardless of the value of \(x\) (when the regression assumptions hold, of course). It is thus very important to decide whether or not \( \beta_{1} = 0 \). We address the question with a statistical test of the null hypothesis \(H_{0}:\beta_{1}=0\) versus the alternative hypothesis \(H_{1}:\beta_{1}\neq0\), and to do that we need to know the sampling distribution of \(b_{1}\) when the null hypothesis is true.

To this end we already know from Section [[sub-slr-interval-est-params][SLR Interval Estimation]] that the quantity

\begin{equation} 
T=\frac{b_{1}-\beta_{1}}{S_{b_{1}}}
\end{equation}
has a \(\mathsf{t}(\mathtt{df}=n-2)\) distribution; therefore, when \(\beta_{1}=0\) the quantity \(b_{1}/S_{b_{1}}\) has a \(\mathsf{t}(\mathtt{df}=n-2)\) distribution and we can compute a \(p\)-value by comparing the observed value of \(b_{1}/S{}_{b_{1}}\) with values under a \(\mathsf{t}(\mathtt{df}=n-2)\) curve. 

Similarly, we may test the hypothesis \(H_{0}:\beta_{0}=0\) versus the alternative \(H_{1}:\beta_{0}\neq0\) with the statistic \(T=b_{0}/S_{b_{0}}\), where \(S_{b_{0}}\) is given in Section [[sub-slr-interval-est-params][SLR Interval Estimation]]. The test is conducted the same way as for \(\beta_{1}\). 

**** How to do it with \(\mathsf{R}\)

Let us take another look at the output from =summary(cars.lm)=:

#+begin_src R :exports both :results output pp 
summary(cars.lm)
#+end_src

In the =Coefficients= section we find the \(t\) statistics and the \(p\)-values associated with the tests that the respective parameters are zero in the fourth and fifth columns. Since the \(p\)-values are (much) less than 0.05, we conclude that there is strong evidence that the parameters \(\beta_{1}\neq0\) and \(\beta_{0}\neq0\), and as such, we say that there is a statistically significant linear relationship between =dist= and =speed=. 

*** Simple Coefficient of Determination

It would be nice to have a single number that indicates how well our linear regression model is doing, and the /simple coefficient of determination/ is designed for that purpose. In what follows, we observe the values \(Y_{1}\), \(Y_{2}\), ...,\(Y_{n}\), and the goal is to estimate \(\mu(x_{0})\), the mean value of \(Y\) at the location \(x_{0}\). 

If we disregard the dependence of \(Y\) and \(x\) and base our estimate only on the \(Y\) values then a reasonable choice for an estimator is just the MLE of \(\mu\), which is \(\overline{Y}\). Then the errors incurred by the estimate are just \(Y_{i}-\overline{Y}\) and the variation about the estimate as measured by the sample variance is proportional to
\begin{equation}
SSTO=\sum_{i=1}^{n}(Y_{i}-\overline{Y})^{2}.
\end{equation}
The acronym \(SSTO\) stands for /total sum of squares/.  And we have additional information, namely, we have values \(x_{i}\) associated with each value of \(Y_{i}\). We have seen that this information leads us to the estimate \(\hat{Y_{i}}\) and the errors incurred are just the residuals, \(E_{i}=Y_{i}-\hat{Y_{i}}\). The variation associated with these errors can be measured with 
\begin{equation}
SSE=\sum_{i=1}^{n}(Y_{i}-\hat{Y_{i}})^{2}.
\end{equation}
We have seen the \(SSE\) before, which stands for the /sum of squared errors/ or /error sum of squares/. Of course, we would expect the error to be less in the latter case, since we have used more information. The improvement in our estimation as a result of the linear regression model can be measured with the difference
\[
(Y_{i}-\overline{Y})-(Y_{i}-\hat{Y_{i}})=\hat{Y_{i}}-\overline{Y},
\]
and we measure the variation in these errors with
\begin{equation}
SSR=\sum_{i=1}^{n}(\hat{Y_{i}}-\overline{Y})^{2},
\end{equation}
also known as the /regression sum of squares/. It is not obvious, but some algebra proved a famous result known as the *ANOVA Equality*:
\begin{equation}
\label{eq-anovaeq}
\sum_{i=1}^{n}(Y_{i}-\overline{Y})^{2}=\sum_{i=1}^{n}(\hat{Y_{i}}-\overline{Y})^{2}+\sum_{i=1}^{n}(Y_{i}-\hat{Y_{i}})^{2}
\end{equation}
or in other words,
\begin{equation}
SSTO=SSR+SSE.
\end{equation}
This equality has a nice interpretation. Consider \(SSTO\) to be the /total variation/ of the errors. Think of a decomposition of the total variation into pieces: one piece measuring the reduction of error from using the linear regression model, or /explained variation/ (\(SSR\)), while the other represents what is left over, that is, the errors that the linear regression model doesn't explain, or /unexplained variation/ (\(SSE\)). In this way we see that the ANOVA equality merely partitions the variation into 
\[
\mbox{total variation}=\mbox{explained variation}+\mbox{unexplained variation}.
\]
For a single number to summarize how well our model is doing we use the /simple coefficient of determination/ \(r^{2}\), defined by
\begin{equation}
r^{2}=1-\frac{SSE}{SSTO}.
\end{equation}
We interpret \(r^{2}\) as the proportion of total variation that is explained by the simple linear regression model. When \(r^{2}\) is large, the model is doing a good job; when \(r^{2}\) is small, the model is not doing a good job.

Related to the simple coefficient of determination is the sample correlation coefficient, \(r\). As you can guess, the way we get \(r\) is by the formula \(|r|=\sqrt{r^{2}}\). The sign of \(r\) is equal the sign of the slope estimate \(b_{1}\). That is, if the regression line \(\hat{\mu}(x)\) has positive slope, then \(r=\sqrt{r^{2}}\). Likewise, if the slope of \(\hat{\mu}(x)\) is negative, then \(r=-\sqrt{r^{2}}\).

**** How to do it with \(\mathsf{R}\)

The primary method to display partitioned sums of squared errors is with an /ANOVA table/. The command in \(\mathsf{R}\) to produce such a table is =anova=. The input to =anova= is the result of an =lm= call which for the =cars= data is =cars.lm=.

#+begin_src R :exports both :results output pp 
anova(cars.lm)
#+end_src

The output gives
\[
r^{2}=1-\frac{SSE}{SSR+SSE}=1-\frac{11353.5}{21185.5+11353.5}\approx0.65.
\]

The interpretation should be: ``The linear regression line accounts for approximately 65% of the variation of =dist= as explained by =speed=''.

The value of \(r^{2}\) is stored in the =r.squared= component of =summary(cars.lm)=, which we called =carsumry=.

#+begin_src R :exports both :results output pp 
carsumry$r.squared
#+end_src

We already knew this. We saw it in the next to the last line of the =summary(cars.lm)= output where it was called =Multiple R-squared=. Listed right beside it is the =Adjusted R-squared= which we will discuss in Chapter [[cha-multiple-linear-regression][Multiple Linear Regression]].  For the =cars= data, we find \(r\) to be

#+begin_src R :exports both :results output pp 
sqrt(carsumry$r.squared)
#+end_src

We choose the principal square root because the slope of the regression line is positive.

*** Overall /F/ statistic
:PROPERTIES:
:CUSTOM_ID: sub-slr-overall-F-statistic
:END:

There is another way to test the significance of the linear regression model. In SLR, the new way also tests the hypothesis \(H_{0}:\beta_{1}=0\) versus \(H_{1}:\beta_{1}\neq0\), but it is done with a new test statistic called the /overall F statistic/. It is defined by
\begin{equation}
\label{eq-slr-overall-F-statistic}
F=\frac{SSR}{SSE/(n-2)}.
\end{equation}

Under the regression assumptions and when \(H_{0}\) is true, the \(F\) statistic has an \(\mathtt{f}(\mathtt{df1}=1,\,\mathtt{df2}=n-2)\) distribution. We reject \(H_{0}\) when \(F\) is large -- that is, when the explained variation is large relative to the unexplained variation.

All this being said, we have not yet gained much from the overall \(F\) statistic because we already knew from Section [[sub-slr-hypoth-test-params][SLR Hypothesis Testing]] how to test \(H_{0}:\beta_{1}=0\)... we use the Student's \(t\) statistic. What is worse is that (in the simple linear regression model) it can be proved that the \(F\) in Equation [[eq-slr-overall-F-statistic][SLR Overall F]] is exactly the Student's \(t\) statistic for \(\beta_{1}\) squared,

\begin{equation}
F=\left(\frac{b_{1}}{S_{b_{1}}}\right)^{2}.
\end{equation}

So why bother to define the \(F\) statistic? Why not just square the \(t\) statistic and be done with it? The answer is that the \(F\) statistic has a more complicated interpretation and plays a more important role in the multiple linear regression model which we will study in Chapter [[cha-multiple-linear-regression][Multiple Linear Regression]]. See Section [[sub-mlr-Overall-F-Test][MLR Overall F]] for details.

**** How to do it with \(\mathsf{R}\)

The overall \(F\) statistic and \(p\)-value are displayed in the bottom line of the =summary(cars.lm)= output. It is also shown in the final columns of =anova(cars.lm)=:

#+begin_src R :exports both :results output pp 
anova(cars.lm)
#+end_src

#+begin_src R :exports none :results silent
tmpf <- round(as.numeric(carsumry$fstatistic[1]), 2)
#+end_src

Here we see that the \(F\) statistic is \( SRC_R{tmpf} \) with a \(p\)-value very close to zero. The conclusion: there is very strong evidence that \(H_{0}:\beta_{1} = 0 \) is false, that is, there is strong evidence that \( \beta_{1} \neq 0 \). Moreover, we conclude that the regression relationship between =dist= and =speed= is significant.

Note that the value of the \(F\) statistic is the same as the Student's \(t\) statistic for =speed= squared.

** Residual Analysis
:PROPERTIES:
:CUSTOM_ID: sec-Residual-Analysis-SLR
:END:

We know from our model that \(Y=\mu(x)+\epsilon\), or in other words, \(\epsilon=Y-\mu(x)\). Further, we know that \(\epsilon\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=\sigma)\). We may estimate \(\epsilon_{i}\) with the /residual/ \(E_{i}=Y_{i}-\hat{Y_{i}}\), where \(\hat{Y_{i}}=\hat{\mu}(x_{i})\). If the regression assumptions hold, then the residuals should be normally distributed. We check this in Section [[sub-Normality-Assumption][Normality Assumption]]. Further, the residuals should have mean zero with constant variance \(\sigma^{2}\), and we check this in Section [[sub-Constant-Variance-Assumption][Constant Variance Assumption]]. Last, the residuals should be independent, and we check this in Section [[sub-Independence-Assumption][Independence Assumption]].

In every case, we will begin by looking at residual plots -- that is, scatterplots of the residuals \(E_{i}\) versus index or predicted values \(\hat{Y_{i}}\) -- and follow up with hypothesis tests.

*** Normality Assumption
:PROPERTIES:
:CUSTOM_ID: sub-Normality-Assumption
:END:

We can assess the normality of the residuals with graphical methods and hypothesis tests. To check graphically whether the residuals are normally distributed we may look at histograms or /q-q/ plots. We first examine a histogram in Figure [[fig-Normal-q-q-plot-cars][Normal-q-q-plot-cars]]. There we see that the distribution of the residuals appears to be mound shaped, for the most part. We can plot the order statistics of the sample versus quantiles from a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) distribution with the command =plot(cars.lm, which = 2)=, and the results are in Figure [[fig-Normal-q-q-plot-cars][Normal-q-q-plot-cars]]. If the assumption of normality were true, then we would expect points randomly scattered about the dotted straight line displayed in the figure. In this case, we see a slight departure from normality in that the dots show systematic clustering on one side or the other of the line. The points on the upper end of the plot also appear begin to stray from the line. We would say there is some evidence that the residuals are not perfectly normal. 

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/slr/Normal-q-q-plot-cars.ps}
  \caption[Normal q-q plot of the residuals for the \texttt{cars} data]{\small Used for checking the normality assumption. Look out for
any curvature or substantial departures from the straight line; hopefully
the dots hug the line closely.}
  \label{fig-Normal-q-q-plot-cars}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-Normal-q-q-plot-cars" class="figure">
  <p><img src="svg/slr/Normal-q-q-plot-cars.svg" width=500 alt="svg/slr/Normal-q-q-plot-cars.svg" /></p>
  <p>Normal q-q plot of the residuals for the <code>cars</code> data.</p>
</div>
#+end_html

#+name: Normal-q-q-plot-cars
#+begin_src R :exports code :results silent
plot(cars.lm, which = 2)
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/slr/Normal-q-q-plot-cars.ps
  <<Normal-q-q-plot-cars>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/slr/Normal-q-q-plot-cars.svg
  <<Normal-q-q-plot-cars>>
#+end_src

**** Testing the Normality Assumption

Even though we may be concerned about the plots, we can use tests to determine if the evidence present is statistically significant, or if it could have happened merely by chance. There are many statistical tests of normality. We will use the Shapiro-Wilk test, since it is known to be a good test and to be quite powerful. However, there are many other fine tests of normality including the Anderson-Darling test and the Lillefors test, just to mention two of them.  

The Shapiro-Wilk test is based on the statistic
\begin{equation}
W=\frac{\left(\sum_{i=1}^{n}a_{i}E_{(i)}\right)^{2}}{\sum_{j=1}^{n}E_{j}^{2}},
\end{equation}
where the \(E_{(i)}\) are the ordered residuals and the \(a_{i}\) are constants derived from the order statistics of a sample of size \(n\) from a normal distribution. See Section [[sub-Shapiro-Wilk-Normality-Test][Shapiro Wilk Normality Test]].
We perform the Shapiro-Wilk test below, using the =shapiro.test= function from the =stats= package \cite{stats}. The hypotheses are
\[
H_{0}:\mbox{ the residuals are normally distributed }
\]
versus
\[
H_{1}:\mbox{ the residuals are not normally distributed.}
\]
The results from \(\mathsf{R}\) are

#+begin_src R :exports both :results output pp 
shapiro.test(residuals(cars.lm))
#+end_src

For these data we would reject the assumption of normality of the residuals at the \(\alpha=0.05\) significance level, but do not lose heart, because the regression model is reasonably robust to departures from the normality assumption. As long as the residual distribution is not highly skewed, then the regression estimators will perform reasonably well. Moreover, departures from constant variance and independence will sometimes affect the quantile plots and histograms, therefore it is wise to delay final decisions regarding normality until all diagnostic measures have been investigated.

*** Constant Variance Assumption
:PROPERTIES:
:CUSTOM_ID: sub-Constant-Variance-Assumption
:END:

We will again go to residual plots to try and determine if the spread of the residuals is changing over time (or index). However, it is unfortunately not that easy because the residuals do not have constant variance! In fact, it can be shown that the variance of the residual \(E_{i}\) is 
\begin{equation}
\mbox{Var$(E_{i})$}=\sigma^{2}(1-h_{ii}),\quad i=1,2,\ldots,n,
\end{equation}
where \(h_{ii}\) is a quantity called the /leverage/ which is defined below. Consequently, in order to check the constant variance assumption we must standardize the residuals before plotting. We estimate the standard error of \(E_{i}\) with \(s_{E_{i}}=s\sqrt{(1-h_{ii})}\) and define the /standardized residuals/ \(R_{i}\), \(i=1,2,\ldots,n\), by 
\begin{equation} 
R_{i}=\frac{E_{i}}{s\,\sqrt{1-h_{ii}}},\quad i=1,2,\ldots,n.
\end{equation}
For the constant variance assumption we do not need the sign of the residual so we will plot \(\sqrt{|R_{i}|}\) versus the fitted values. As we look at a scatterplot of \(\sqrt{|R_{i}|}\) versus \(\hat{Y}_{i}\) we would expect under the regression assumptions to see a constant band of observations, indicating no change in the magnitude of the observed distance from the line. We want to watch out for a fanning-out of the residuals, or a less common funneling-in of the residuals. Both patterns indicate a change in the residual variance and a consequent departure from the regression assumptions, the first an increase, the second a decrease.

In this case, we plot the standardized residuals versus the fitted values. The graph may be seen in Figure [[fig-std-resids-fitted-cars][std-resids-fitted-cars]]. For these data there does appear to be somewhat of a slight fanning-out of the residuals.

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/slr/std-resids-fitted-cars.ps}
  \caption[Plot of standardized residuals against the fitted values for the \texttt{cars} data]{\small Used for checking the constant variance assumption. Watch out for any fanning out (or in) of the dots; hopefully they fall in a constant band.}
  \label{fig-std-resids-fitted-cars}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-std-resids-fitted-cars" class="figure">
  <p><img src="svg/slr/std-resids-fitted-cars.svg" width=500 alt="svg/slr/std-resids-fitted-cars.svg" /></p>
  <p>Plot of standardized residuals against the fitted values for the <code>cars</code> data.</p>
</div>
#+end_html

#+name: std-resids-fitted-cars
#+begin_src R :exports code :results silent
plot(cars.lm, which = 3)
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/slr/std-resids-fitted-cars.ps
  <<std-resids-fitted-cars>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/slr/std-resids-fitted-cars.svg
  <<std-resids-fitted-cars>>
#+end_src

**** Testing the Constant Variance Assumption

We will use the Breusch-Pagan test to decide whether the variance of the residuals is nonconstant. The null hypothesis is that the variance is the same for all observations, and the alternative hypothesis is that the variance is not the same for all observations. The test statistic is found by fitting a linear model to the centered squared residuals,
\begin{equation}
W_{i} = E_{i}^{2} - \frac{SSE}{n}, \quad i=1,2,\ldots,n.
\end{equation}

By default the same explanatory variables are used in the new model which produces fitted values \(\hat{W}_{i}\), \(i=1,2,\ldots,n\). The Breusch-Pagan test statistic in \(\mathsf{R}\) is then calculated with 
\begin{equation}
BP=n\sum_{i=1}^{n}\hat{W}_{i}^{2}\div\sum_{i=1}^{n}W_{i}^{2}.
\end{equation}
We reject the null hypothesis if \(BP\) is too large, which happens when the explained variation i the new model is large relative to the unexplained variation in the original model.
We do it in \(\mathsf{R}\) with the =bptest= function from the =lmtest= package \cite{lmtest}. 
#+begin_src R :exports both :results output pp
bptest(cars.lm)
#+end_src

For these data we would not reject the null hypothesis at the \(\alpha=0.05\) level. There is relatively weak evidence against the assumption of constant variance. 

*** Independence Assumption
:PROPERTIES:
:CUSTOM_ID: sub-Independence-Assumption
:END:

One of the strongest of the regression assumptions is the one regarding independence. Departures from the independence assumption are often exhibited by correlation (or autocorrelation, literally, self-correlation) present in the residuals. There can be positive or negative correlation.

Positive correlation is displayed by positive residuals followed by positive residuals, and negative residuals followed by negative residuals. Looking from left to right, this is exhibited by a cyclical feature in the residual plots, with long sequences of positive residuals being followed by long sequences of negative ones.

On the other hand, negative correlation implies positive residuals followed by negative residuals, which are then followed by positive residuals, /etc/. Consequently, negatively correlated residuals are often associated with an alternating pattern in the residual plots. We examine the residual plot in Figure [[fig-resids-fitted-cars][resids-fitted-cars]]. There is no obvious cyclical wave pattern or structure to the residual plot. 

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/slr/resids-fitted-cars.ps}
  \caption[Plot of the residuals versus the fitted values for the \texttt{cars}
data]{\small Used for checking the independence assumption. Watch out for any patterns or structure; hopefully the points are randomly scattered on the plot.}
  \label{fig-resids-fitted-cars}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-resids-fitted-cars" class="figure">
  <p><img src="svg/slr/resids-fitted-cars.svg" width=500 alt="svg/slr/resids-fitted-cars.svg" /></p>
  <p>A plot of the residuals versus the fitted values for the <code>cars</code>
 data.</p>
</div>
#+end_html

#+name: resids-fitted-cars
#+begin_src R :exports code :results silent
plot(cars.lm, which = 1)
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/slr/resids-fitted-cars.ps
  <<resids-fitted-cars>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/slr/resids-fitted-cars.svg
  <<resids-fitted-cars>>
#+end_src

**** Testing the Independence Assumption

We may statistically test whether there is evidence of autocorrelation in the residuals with the Durbin-Watson test. The test is based on the statistic
\begin{equation}
D=\frac{\sum_{i=2}^{n}(E_{i}-E_{i-1})^{2}}{\sum_{j=1}^{n}E_{j}^{2}}.
\end{equation}
Exact critical values are difficult to obtain, but \(\mathsf{R}\) will calculate the /p-value/ to great accuracy. It is performed with the =dwtest= function from the =lmtest= package \cite{lmtest}. We will conduct a two sided test that the correlation is not zero, which is not the default (the default is to test that the autocorrelation is positive).

#+begin_src R :exports both :results output pp 
dwtest(cars.lm, alternative = "two.sided")
#+end_src

In this case we do not reject the null hypothesis at the \(\alpha=0.05\) significance level; there is very little evidence of nonzero autocorrelation in the residuals.

*** Remedial Measures

We often find problems with our model that suggest that at least one of the three regression assumptions is violated. What do we do then? There are many measures at the statistician's disposal, and we mention specific steps one can take to improve the model under certain types of violation.

- Mean response is not linear :: We can directly modify the model to better approximate the mean response. In particular, perhaps a polynomial regression function of the form 
  \[
  \mu(x) = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{1}^{2}
  \]
  would be appropriate. Alternatively, we could have a function of the form
  \[
  \mu(x)=\beta_{0}\mathrm{e}^{\beta_{1}x}.
  \]
  Models like these are studied in nonlinear regression courses.
- Error variance is not constant :: Sometimes a transformation of the dependent variable will take care of the problem. There is a large class of them called /Box-Cox transformations/. They take the form 
  \begin{equation}
  Y^{\ast}=Y^{\lambda},
  \end{equation}
  where \(\lambda\) is a constant. (The method proposed by Box and Cox will determine a suitable value of \(\lambda\) automatically by maximum likelihood). The class contains the transformations 
  \begin{alignat*}{1}
  \lambda=2,\quad & Y^{\ast}=Y^{2}\\
  \lambda=0.5,\quad & Y^{\ast}=\sqrt{Y}\\
  \lambda=0,\quad & Y^{\ast}=\ln\: Y\\
  \lambda=-1,\quad & Y^{\ast}=1/Y
  \end{alignat*}
  Alternatively, we can use the method of /weighted least squares/. This is studied in more detail in later classes. 
- Error distribution is not normal :: The same transformations for stabilizing the variance are equally appropriate for smoothing the residuals to a more Gaussian form. In fact, often we will kill two birds with one stone.
- Errors are not independent :: There is a large class of autoregressive models to be used in this situation which occupy the latter part of Chapter [[cha-Time-Series][Time Series]].

** Other Diagnostic Tools
:PROPERTIES:
:CUSTOM_ID: sec-Other-Diagnostic-Tools-SLR
:END:

There are two types of observations with which we must be especially careful:
- Influential observations :: are those that have a substantial effect on our estimates, predictions, or inferences. A small change in an influential observation is followed by a large change in the parameter estimates or inferences. 
- Outlying observations :: are those that fall fall far from the rest of the data. They may be indicating a lack of fit for our regression model, or they may just be a mistake or typographical error that should be corrected. Regardless, special attention should be given to these observations. An outlying observation may or may not be influential.

We will discuss outliers first because the notation builds sequentially in that order.
*** Outliers
There are three ways that an observation \((x_{i},y_{i})\) might be identified as an  outlier: it can have an \(x_{i}\) value which falls far from the other  \(x\) values, it can have a \(y_{i}\) value which falls far from the other \(y\) values, or it can have both its \(x_{i}\) and \(y_{i}\) values falling far from the other \(x\) and \(y\) values.
*** Leverage
Leverage statistics are designed to identify observations which have \(x\) values that are far away from the rest of the data. In the simple linear regression model the leverage of \(x_{i}\) is denoted by \(h_{ii}\) and defined by 
\begin{equation}
h_{ii}=\frac{1}{n}+\frac{(x_{i}-\overline{x})^{2}}{\sum_{k=1}^{n}(x_{k}-\overline{x})^{2}},\quad i=1,2,\ldots,n.
\end{equation}
The formula has a nice interpretation in the SLR model: if the distance from \(x_{i}\) to \(\overline{x}\) is large relative to the other \(x\)'s then \(h_{ii}\) will be close to 1. 

Leverages have nice mathematical properties; for example, they satisfy
\begin{equation}
\label{eq-slr-leverage-between}
0\leq h_{ii}\leq1,
\end{equation}
and their sum is
\begin{eqnarray}
\label{eq-slr-average-leverage}
\sum_{i=1}^{n}h_{ii} & = & \sum_{i=1}^{n}\left[\frac{1}{n}+\frac{(x_{i}-\overline{x})^{2}}{\sum_{k=1}^{n}(x_{k}-\overline{x})^{2}}\right],\\
 & = & \frac{n}{n}+\frac{\sum_{i}(x_{i}-\overline{x})^{2}}{\sum_{k}(x_{k}-\overline{x})^{2}},\\
 & = & 2.
\end{eqnarray}

A rule of thumb is to consider leverage values to be large if they are more than double their average size (which is \(2/n\) according to Equation [[eq-slr-average-leverage]]). So leverages larger than \(4/n\) are suspect. Another rule of thumb is to say that values bigger than 0.5 indicate high leverage, while values between 0.3 and 0.5 indicate moderate leverage.

*** Standardized and Studentized Deleted Residuals

We have already encountered the /standardized residuals/ \(r_{i}\) in Section [[sub-Constant-Variance-Assumption][Constant Variance Assumption]]; they are merely residuals that have been divided by their respective standard deviations: 
\begin{equation}
R_{i}=\frac{E_{i}}{S\sqrt{1-h_{ii}}},\quad i=1,2,\ldots,n.
\end{equation}
Values of \(|R_{i}| > 2\) are extreme and suggest that the observation has an outlying \(y\)-value. 

Now delete the \(i^{\mathrm{th}}\) case and fit the regression function to the remaining \(n - 1\) cases, producing a fitted value \(\hat{Y}_{(i)}\) with /deleted residual/ \(D_{i}=Y_{i}-\hat{Y}_{(i)}\). It is shown in later classes that 
\begin{equation}
\mbox{Var $(D_{i})$}=\frac{S_{(i)}^{2}}{1-h_{ii}},\quad i=1,2,\ldots,n,
\end{equation}
so that the /studentized deleted residuals/ \(t_{i}\) defined by
\begin{equation}
\label{eq-slr-studentized-deleted-resids}
t_{i}=\frac{D_{i}}{S_{(i)}/(1-h_{ii})},\quad i=1,2,\ldots,n,
\end{equation}
have a \(\mathsf{t}(\mathtt{df}=n-3)\) distribution and we compare observed values of \(t_{i}\) to this distribution to decide whether or not an observation is extreme. 

The folklore in regression classes is that a test based on the statistic in Equation [[eq-slr-studentized-deleted-resids]] can be too liberal. A rule of thumb is if we suspect an observation to be an outlier /before/ seeing the data then we say it is significantly outlying if its two-tailed \(p\)-value is less than \(\alpha\), but if we suspect an observation to be an outlier /after/ seeing the data then we should only say it is significantly outlying if its two-tailed \(p\)-value is less than \(\alpha/n\). The latter rule of thumb is called the /Bonferroni approach/ and can be overly conservative for large data sets. The responsible statistician should look at the data and use his/her best judgement, in every case.

**** How to do it with \(\mathsf{R}\)

We can calculate the standardized residuals with the =rstandard= function. The input is the =lm= object, which is =cars.lm=.

#+begin_src R :exports both :results output pp 
sres <- rstandard(cars.lm)
sres[1:5]
#+end_src

We can find out which observations have studentized residuals larger than two with the command

#+begin_src R :exports both :results output pp 
sres[which(abs(sres) > 2)]
#+end_src

In this case, we see that observations 23, 35, and 49 are potential outliers with respect to their \(y\)-value.  We can compute the studentized deleted residuals with =rstudent=:

#+begin_src R :exports both :results output pp 
sdelres <- rstudent(cars.lm)
sdelres[1:5]
#+end_src

We should compare these values with critical values from a \(\mathsf{t}(\mathtt{df}=n-3)\) distribution, which in this case is \(\mathsf{t}(\mathtt{df}=50-3=47)\). We can calculate a 0.005 quantile and check with 

#+begin_src R :exports both :results output pp 
t0.005 <- qt(0.005, df = 47, lower.tail = FALSE)
sdelres[which(abs(sdelres) > t0.005)]
#+end_src

This means that observations 23 and 49 have a large studentized deleted residual. The leverages can be found with the =hatvalues= function:

#+begin_src R :exports both :results output pp 
leverage <- hatvalues(cars.lm)
leverage[which(leverage > 4/50)]
#+end_src

Here we see that observations 1, 2, and 50 have leverages bigger than double their mean value. These observations would be considered outlying with respect to their \(x\) value (although they may or may not be influential).

*** Influential Observations

**** \(DFBETAS\) and \(DFFITS\)

Any time we do a statistical analysis, we are confronted with the variability of data. It is always a concern when an observation plays too large a role in our regression model, and we would not like or procedures to be overly influenced by the value of a single observation. Hence, it becomes desirable to check to see how much our estimates and predictions would change if one of the observations were not included in the analysis. If an observation changes the estimates/predictions a large amount, then the observation is influential and should be subjected to a higher level of scrutiny.

We measure the change in the parameter estimates as a result of deleting an observation with \(DFBETAS\). The \(DFBETAS\) for the intercept \(b_{0}\) are given by
\begin{equation}
(DFBETAS)_{0(i)}=\frac{b_{0}-b_{0(i)}}{S_{(i)}\sqrt{\frac{1}{n}+\frac{\overline{x}^{2}}{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}}},\quad i=1,2,\ldots,n.
\end{equation}
and the \(DFBETAS\) for the slope \(b_{1}\) are given by
\begin{equation}
(DFBETAS)_{1(i)}=\frac{b_{1}-b_{1(i)}}{S_{(i)}\left[\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}\right]^{-1/2}},\quad i=1,2,\ldots,n.
\end{equation}
See Section [[sec-Residual-Analysis-MLR][MLR Residual Analysis]] for a better way to write these. The signs of the \(DFBETAS\) indicate whether the coefficients would increase or decrease as a result of including the observation. If the \(DFBETAS\) are large, then the observation has a large impact on those regression coefficients. We label observations as suspicious if their \(DFBETAS\) have magnitude greater 1 for small data or \(2/\sqrt{n}\) for large data sets.
We can calculate the \(DFBETAS\) with the =dfbetas= function (some output has been omitted):

#+begin_src R :exports both :results output pp 
dfb <- dfbetas(cars.lm)
head(dfb)
#+end_src

We see that the inclusion of the first observation slightly increases the =Intercept= and slightly decreases the coefficient on =speed=.

We can measure the influence that an observation has on its fitted value with \(DFFITS\). These are calculated by deleting an observation, refitting the model, recalculating the fit, then standardizing. The formula is 
\begin{equation}
(DFFITS)_{i}=\frac{\hat{Y_{i}}-\hat{Y}_{(i)}}{S_{(i)}\sqrt{h_{ii}}},\quad i=1,2,\ldots,n.
\end{equation}
The value represents the number of standard deviations of \(\hat{Y_{i}}\) that the fitted value \(\hat{Y_{i}}\) increases or decreases with the inclusion of the \(i^{\textrm{th}}\) observation. We can compute them with the =dffits= function.

#+begin_src R :exports both :results output pp
dff <- dffits(cars.lm)
dff[1:5]
#+end_src

A rule of thumb is to flag observations whose \(DFFIT\) exceeds one in absolute value, but there are none of those in this data set.

**** Cook's Distance

The \(DFFITS\) are good for measuring the influence on a single fitted value, but we may want to measure the influence an observation has on all of the fitted values simultaneously. The statistics used for measuring this are Cook's distances which may be calculated
#+latex: \footnote{Cook's distances are actually defined by a different formula than the one shown. The formula in Equation \ref{eq-slr-cooks-distance} is algebraically equivalent to the defining formula and is, in the author's opinion, more transparent.}
by the formula
\begin{equation}
\label{eq-slr-cooks-distance}
D_{i}=\frac{E_{i}^{2}}{(p+1)S^{2}}\cdot\frac{h_{ii}}{(1-h_{ii})^{2}},\quad i=1,2,\ldots,n.
\end{equation}
It shows that Cook's distance depends both on the residual \(E_{i}\) and the leverage \(h_{ii}\) and in this way \(D_{i}\) contains information about outlying \(x\) and \(y\) values.

To assess the significance of \(D\), we compare to quantiles of an \(\mathsf{f}(\mathtt{df1}=2,\,\mathtt{df2}=n-2)\) distribution. A rule of thumb is to classify observations falling higher than the \(50^{\mathrm{th}}\) percentile as being extreme. 

**** How to do it with \(\mathsf{R}\)

We can calculate the Cook's Distances with the =cooks.distance= function.

#+begin_src R :exports both :results output pp 
cooksD <- cooks.distance(cars.lm)
cooksD[1:4]
#+end_src

We can look at a plot of the Cook's distances with the command =plot(cars.lm, which = 4)=.

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/slr/Cooks-distance-cars.ps}
  \caption[Cook's distances for the \texttt{cars} data]{\small Used for checking for influential and/our outlying observations. Values with large Cook's distance merit further investigation.}
  \label{fig-Cooks-distance-cars}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-Cooks-distance-cars" class="figure">
  <p><img src="svg/slr/Cooks-distance-cars.svg" width=500 alt="svg/slr/Cooks-distance-cars.svg" /></p>
  <p>Cook's distances for the <code>cars</code> data.</p>
</div>
#+end_html

#+name: Cooks-distance-cars
#+begin_src R :exports code :results silent
plot(cars.lm, which = 4)
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/slr/Cooks-distance-cars.ps
  <<Cooks-distance-cars>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/slr/Cooks-distance-cars.svg
  <<Cooks-distance-cars>>
#+end_src

Observations with the largest Cook's D values are labeled, hence we see that observations 23, 39, and 49 are suspicious. However, we need to compare to the quantiles of an \( \mathsf{f}(\mathtt{df1} = 2, \, \mathtt{df2} = 48) \) distribution:

#+begin_src R :exports both :results output pp 
F0.50 <- qf(0.5, df1 = 2, df2 = 48)
any(cooksD > F0.50)
#+end_src

We see that with this data set there are no observations with extreme Cook's distance, after all.

*** All Influence Measures Simultaneously

We can display the result of diagnostic checking all at once in one table, with potentially influential points displayed. We do it with the command =influence.measures(cars.lm)=:

#+begin_src R :exports code :eval never
influence.measures(cars.lm)
#+end_src

The output is a huge matrix display, which we have omitted in the interest of brevity. A point is identified if it is classified to be influential with respect to any of the diagnostic measures. Here we see that observations 2, 11, 15, and 18 merit further investigation.  

We can also look at all diagnostic plots at once with the commands

#+begin_src R :exports code :eval never
par(mfrow = c(2,2))
plot(cars.lm)
par(mfrow = c(1,1))
#+end_src

The =par= command is used so that \(2\times 2 = 4\) plots will be shown on the same display. The diagnostic plots for the =cars= data are shown in Figure [[fig-Diagnostic-plots-cars][Diagnostic-plots-cars]]:

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/slr/Diagnostic-plots-cars.ps}
  \caption[Diagnostic plots for the \texttt{cars} data]{\small Diagnostic plots for the \texttt{cars} data.}
  \label{fig-Diagnostic-plots-cars}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-Diagnostic-plots-cars" class="figure">
  <p><img src="svg/slr/Diagnostic-plots-cars.svg" width=500 alt="svg/slr/Diagnostic-plots-cars.svg" /></p>
  <p>Diagnostic plots for the <code>cars</code> data.</p>
</div>
#+end_html

#+name: Diagnostic-plots-cars
#+begin_src R :exports none :results silent
par(mfrow = c(2,2))
plot(cars.lm)
par(mfrow = c(1,1))
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/slr/Diagnostic-plots-cars.ps
  <<Diagnostic-plots-cars>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/slr/Diagnostic-plots-cars.svg
  <<Diagnostic-plots-cars>>
#+end_src

We have discussed all of the plots except the last, which is possibly the most interesting. It shows Residuals vs. Leverage, which will identify outlying \(y\) values versus outlying \(x\) values. Here we see that observation 23 has a high residual, but low leverage, and it turns out that observations 1 and 2 have relatively high leverage but low/moderate leverage (they are on the right side of the plot, just above the horizontal line). Observation 49 has a large residual with a comparatively large leverage. 

We can identify the observations with the =identify= command; it allows us to display the observation number of dots on the plot. First, we plot the graph, then we call =identify=:

#+begin_src R :exports code :eval never
plot(cars.lm, which = 5)          # std'd resids vs lev plot
identify(leverage, sres, n = 4)   # identify 4 points
#+end_src

The graph with the identified points is omitted (but the plain plot is shown in the bottom right corner of Figure [[fig-Diagnostic-plots-cars][Diagnostic-plots-cars]]). Observations 1 and 2 fall on the far right side of the plot, near the horizontal axis.

#+latex: \newpage{}

** Exercises
#+latex: \setcounter{thm}{0}

#+begin_xca
Prove the ANOVA equality, Equation [[eq-anovaeq]]. /Hint/:
show that
\[
\sum_{i=1}^{n}(Y_{i}-\hat{Y_{i}})(\hat{Y_{i}}-\overline{Y})=0.
\]
#+end_xca

#+begin_xca
# <<xca-find-mles-SLR>>
Solve the following system of equations for \(\beta_{1}\) and \(\beta_{0}\) to find the MLEs for slope and intercept in the simple linear regression model.
\begin{eqnarray*}
n\beta_{0}+\beta_{1}\sum_{i=1}^{n}x_{i} & = & \sum_{i=1}^{n}y_{i}\\
\beta_{0}\sum_{i=1}^{n}x_{i}+\beta_{1}\sum_{i=1}^{n}x_{i}^{2} & = & \sum_{i=1}^{n}x_{i}y_{i}
\end{eqnarray*}
#+end_xca

#+begin_xca
# <<xca-show-alternate-slope-formula>>
Show that the formula given in Equation [[eq-sample-correlation-formula]] is equivalent to
\[
\hat{\beta}_{1} = \frac{\sum_{i=1}^{n}x_{i}y_{i}-\left.\left(\sum_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}y_{i}\right)\right/ n}{\sum_{i=1}^{n}x_{i}^{2}-\left.\left(\sum_{i=1}^{n}x_{i}\right)^{2}\right/ n}.
\]
#+end_xca

* Multiple Linear Regression                                            :mlr:
:PROPERTIES:
:tangle: R/mlr.R
:CUSTOM_ID: cha-multiple-linear-regression
:END:

#+begin_src R :exports none :eval never
# Chapter: Multiple Linear Regression
# All code released under GPL Version 3
#+end_src

#+latex: \noindent 
We know a lot about simple linear regression models, and a next step is to study multiple regression models that have more than one independent (explanatory) variable. In the discussion that follows we will assume that we have \(p\) explanatory variables, where \(p>1\).

The language is phrased in matrix terms -- for two reasons. First, it is quicker to write and (arguably) more pleasant to read. Second, the matrix approach will be required for later study of the subject; the reader might as well be introduced to it now.

Most of the results are stated without proof or with only a cursory justification. Those yearning for more should consult an advanced text in linear regression for details, such as /Applied Linear Regression Models/ \cite{Neter1996} or /Linear Models: Least Squares and Alternatives/ \cite{Rao1999}.

*What do I want them to know?*
- the basic MLR model, and how it relates to the SLR
- how to estimate the parameters and use those estimates to make predictions
- basic strategies to determine whether or not the model is doing a good job
- a few thoughts about selected applications of the MLR, such as polynomial, interaction, and dummy variable models
- some of the uses of residuals to diagnose problems
- hints about what will be coming later

** The Multiple Linear Regression Model
:PROPERTIES:
:CUSTOM_ID: sec-The-MLR-Model
:END:

The first thing to do is get some better notation. We will write 
\begin{equation}
\mathbf{Y}_{\mathrm{n}\times1}=
\begin{bmatrix}y_{1}\\
y_{2}\\
\vdots\\
y_{n}
\end{bmatrix},
\quad \mbox{and}\quad \mathbf{X}_{\mathrm{n}\times(\mathrm{p}+1)}=
\begin{bmatrix}1 & x_{11} & x_{21} & \cdots & x_{p1}\\
1 & x_{12} & x_{22} & \cdots & x_{p2}\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
1 & x_{1n} & x_{2n} & \cdots & x_{pn}
\end{bmatrix}.
\end{equation}
The vector \(\mathbf{Y}\) is called the /response vector/ \index{response vector} and the matrix \(\mathbf{X}\) is called the /model matrix/ \index{model matrix}. As in Chapter [[cha-simple-linear-regression][Simple Linear Regression]], the most general assumption that relates \(\mathbf{Y}\) to \(\mathbf{X}\) is
\begin{equation}
\mathbf{Y}=\mu(\mathbf{X})+\upepsilon,
\end{equation}
where \(\mu\) is some function (the /signal/) and \(\upepsilon\) is the /noise/ (everything else). We usually impose some structure on \(\mu\) and \(\upepsilon\). In particular, the standard multiple linear regression model\index{model!multiple linear regression} assumes
\begin{equation}
\mathbf{Y}=\mathbf{X}\upbeta+\upepsilon,
\end{equation}
where the parameter vector \(\upbeta\) looks like 
\begin{equation}
\upbeta_{(\mathrm{p}+1)\times1}=\begin{bmatrix}\beta_{0} & \beta_{1} & \cdots & \beta_{p}\end{bmatrix}^{\mathrm{T}},
\end{equation}
and the random vector \(\upepsilon_{\mathrm{n}\times1}=\begin{bmatrix}\epsilon_{1} & \epsilon_{2} & \cdots & \epsilon_{n}\end{bmatrix}^{\mathrm{T}}\) is assumed to be distributed
\begin{equation}
\upepsilon\sim\mathsf{mvnorm}\left(\mathtt{mean}=\mathbf{0}_{\mathrm{n}\times1},\,\mathtt{sigma}=\sigma^{2}\mathbf{I}_{\mathrm{n}\times\mathrm{n}}\right).
\end{equation}

The assumption on \(\upepsilon\) is equivalent to the assumption that \(\epsilon_{1}\), \(\epsilon_{2}\), ..., \(\epsilon_{n}\) are IID \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=\sigma)\). It is a linear model because the quantity \(\mu(\mathbf{X})=\mathbf{X}\upbeta\) is linear in the parameters \(\beta_{0}\), \(\beta_{1}\), ..., \(\beta_{p}\). It may be helpful to see the model in expanded form; the above matrix formulation is equivalent to the more lengthy
\begin{equation} 
Y_{i}=\beta_{0}+\beta_{1}x_{1i}+\beta_{2}x_{2i}+\cdots+\beta_{p}x_{pi}+\epsilon_{i},\quad i=1,2,\ldots,n.
\end{equation}

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
*Girth, Height, and Volume for Black Cherry trees.* \index{Data sets!trees@\texttt{trees}}
Measurements were made of the girth, height, and volume of timber in 31 felled black cherry trees. Note that girth is the diameter of the tree (in inches) measured at 4 ft 6 in above the ground. The variables are

1. =Girth=: tree diameter in inches (denoted \(x_{1}\))
2. =Height=: tree height in feet (\(x_{2}\)).
3. =Volume=: volume of the tree in cubic feet. (\(y\))

The data are in the =datasets= package \cite{datasets} and are already on the search path; they can be viewed with

#+begin_src R :exports both :results output pp 
head(trees)
#+end_src

Let us take a look at a visual display of the data. For multiple variables, instead of a simple scatterplot we use a scatterplot matrix which is made with the =splom= function in the =lattice= package \cite{lattice} as shown below. The plot is shown in Figure [[fig-splom-trees][splom-trees]].

#+name: splom-trees
#+begin_src R :exports code :results silent
splom(trees)
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/mlr/splom-trees.ps
  <<splom-trees>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/mlr/splom-trees.svg
  <<splom-trees>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/mlr/splom-trees.ps}
  \caption[Scatterplot matrix of \texttt{trees} data]{\small A scatterplot matrix of \texttt{trees} data.}
  \label{fig-splom-trees}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-splom-trees" class="figure">
  <p><img src="svg/mlr/splom-trees.svg" width=500 alt="svg/mlr/splom-trees.svg" /></p>
  <p>A scatterplot matrix of the <code>trees</code> data.</p>
</div>
#+end_html

The dependent (response) variable =Volume= is listed in the first row of the scatterplot matrix. Moving from left to right, we see an approximately linear relationship between =Volume= and the independent (explanatory) variables =Height= and =Girth=. A first guess at a model for these data might be
\begin{equation}
Y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\epsilon,
\end{equation}
in which case the quantity \(\mu(x_{1},x_{2})=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}\) would represent the mean value of \(Y\) at the point \((x_{1},x_{2})\).
#+latex: \end{exampletoo}
#+html: </div>

*** What does it mean?

The interpretation is simple. The intercept \(\beta_{0}\) represents the mean =Volume= when all other independent variables are zero. The parameter \(\beta_{i}\) represents the change in mean =Volume= when there is a unit increase in \(x_{i}\), while the other independent variable is held constant. For the =trees= data, \(\beta_{1}\) represents the change in average =Volume= as =Girth= increases by one unit when the =Height= is held constant, and \(\beta_{2}\) represents the change in average =Volume= as =Height= increases by one unit when the =Girth= is held constant. 


In simple linear regression, we had one independent variable and our linear regression surface was 1D, simply a line. In multiple regression there are many independent variables and so our linear regression surface will be many-D... in general, a hyperplane. But when there are only two explanatory variables the hyperplane is just an ordinary plane and we can look at it with a 3D scatterplot. 

One way to do this is with the \(\mathsf{R}\) Commander in the =Rcmdr= package \cite{Rcmdr}. It has a 3D scatterplot option under the =Graphs= menu. It is especially great because the resulting graph is dynamic; it can be moved around with the mouse, zoomed, /etc/. But that particular display does not translate well to a printed book.

Another way to do it is with the =scatterplot3d= function in the =scatterplot3d= package \cite{scatterplot3d}. The code follows, and the result is shown in Figure [[fig-3D-scatterplot-trees][3D-scatterplot-trees]].

#+name: 3D-scatterplot-trees
#+begin_src R :exports code :results silent
s3d <- with(trees, scatterplot3d(Girth, Height, Volume, pch = 16, highlight.3d = TRUE, angle = 60))
fit <- lm(Volume ~ Girth + Height, data = trees)
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/mlr/3D-scatterplot-trees.ps
  <<3D-scatterplot-trees>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/mlr/3D-scatterplot-trees.svg
  <<3D-scatterplot-trees>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/mlr/3D-scatterplot-trees.ps}
  \caption[3D scatterplot with regression plane for the \texttt{trees} data]{\small A 3D scatterplot with regression plane for the \texttt{trees} data.}
  \label{fig-3D-scatterplot-trees}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-3D-scatterplot-trees" class="figure">
  <p><img src="svg/mlr/3D-scatterplot-trees.svg" width=500 alt="svg/mlr/3D-scatterplot-trees.svg" /></p>
  <p> A 3D scatterplot with regression plane for the <code>trees</code> data.</p>
</div>
#+end_html

Looking at the graph we see that the data points fall close to a plane in three dimensional space. (The plot looks remarkably good. In the author's experience it is rare to see points fit so well to the plane without some additional work.)

** Estimation and Prediction
:PROPERTIES:
:CUSTOM_ID: sec-Estimation-and-Prediction-MLR
:END:

*** Parameter estimates
:PROPERTIES:
:CUSTOM_ID: sub-mlr-parameter-estimates
:END:

We will proceed exactly like we did in Section [[sec-SLR-Estimation][SLR Estimation]]. We know
\begin{equation}
\upepsilon\sim\mathsf{mvnorm}\left(\mathtt{mean}=\mathbf{0}_{\mathrm{n}\times1},\,\mathtt{sigma}=\sigma^{2}\mathbf{I}_{\mathrm{n}\times\mathrm{n}}\right),
\end{equation}
which means that \(\mathbf{Y}=\mathbf{X}\upbeta+\upepsilon\) has an \(\mathsf{mvnorm}\left(\mathtt{mean}=\mathbf{X}\upbeta,\,\mathtt{sigma}=\sigma^{2}\mathbf{I}_{\mathrm{n}\times\mathrm{n}}\right)\) distribution. Therefore, the likelihood function\index{likelihood function} is
\begin{equation}
L(\upbeta,\sigma)=\frac{1}{2\pi^{n/2}\sigma}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(\mathbf{Y}-\mathbf{X}\upbeta\right)^{\mathrm{T}}\left(\mathbf{Y}-\mathbf{X}\upbeta\right)\right\}.
\end{equation}

To /maximize/ the likelihood\index{maximum likelihood} in \(\upbeta\), we need to /minimize/ the quantity \(g(\upbeta)=\left(\mathbf{Y}-\mathbf{X}\upbeta\right)^{\mathrm{T}}\left(\mathbf{Y}-\mathbf{X}\upbeta\right)\). We do this by differentiating \(g\) with respect to \(\upbeta\). (It may be a good idea to brush up on the material in Appendices [[sec-Linear-Algebra][Linear Algebra]] and [[sec-Multivariable-Calculus][Multivariable Calculus]].) First we will rewrite \(g\):
\begin{equation}
g(\upbeta)=\mathbf{Y}^{\mathrm{T}}\mathbf{Y}-\mathbf{Y}^{\mathrm{T}}\mathbf{X}\upbeta-\upbeta^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\mathbf{Y}+\upbeta^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\mathbf{X}\upbeta,
\end{equation}
which can be further simplified to \(g(\upbeta)=\mathbf{Y}^{\mathrm{T}}\mathbf{Y}-2\upbeta^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\mathbf{Y}+\upbeta^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\mathbf{X}\upbeta\) since \(\upbeta^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\mathbf{Y}\) is \(1\times1\) and thus equal to its transpose. Now we differentiate to get
\begin{equation}
\frac{\partial g}{\partial\upbeta}=\mathbf{0}-2\mathbf{X}^{\mathrm{T}}\mathbf{Y}+2\mathbf{X}^{\mathrm{T}}\mathbf{X}\upbeta,
\end{equation}
since \(\mathbf{X}^{\mathrm{T}}\mathbf{X}\) is symmetric. Setting the derivative equal to the zero vector yields the so called ``normal equations''\index{normal equations}
\begin{equation}
\mathbf{X}^{\mathrm{T}}\mathbf{X}\upbeta=\mathbf{X}^{\mathrm{T}}\mathbf{Y}.
\end{equation}

In the case that \(\mathbf{X}^{\mathrm{T}}\mathbf{X}\) is invertible
#+latex: \footnote{We can find solutions of the normal equations even when \(\mathbf{X}^{\mathrm{T}}\mathbf{X}\) is not of full rank, but the topic falls outside the scope of this book. The interested reader can consult an advanced text such as Rao \cite{Rao1999}.},
we may solve the equation for \(\upbeta\) to get the maximum likelihood estimator of \(\upbeta\) which we denote by \(\mathbf{b}\):
\begin{equation}
\label{eq-b-formula-matrix}
\mathbf{b}=\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}\mathbf{Y}.
\end{equation}

#+begin_rem
The formula in Equation [[eq-b-formula-matrix]] is convenient for mathematical study but is inconvenient for numerical computation. Researchers have devised much more efficient algorithms for the actual calculation of the parameter estimates, and we do not explore them here.
#+end_rem

#+begin_rem
We have only found a critical value, and have not actually shown that the critical value is a minimum. We omit the details and refer the interested reader to \cite{Rao1999}.
#+end_rem

**** How to do it with \(\mathsf{R}\)

We do all of the above just as we would in simple linear regression. The powerhouse is the =lm=\index{lm@\texttt{lm}} function. Everything else is based on it. We separate explanatory variables in the model formula by a plus sign.

#+begin_src R :exports both :results output pp 
trees.lm <- lm(Volume ~ Girth + Height, data = trees)
trees.lm
#+end_src

We see from the output that for the =trees= data our parameter estimates are 
\[
\mathbf{b}=\begin{bmatrix}-58.0 & 4.7 & 0.3\end{bmatrix},
\] 
and consequently our estimate of the mean response is \(\hat{\mu}\) given by 
\begin{alignat}{1}
\hat{\mu}(x_{1},x_{2})= & \ b_{0}+b_{1}x_{1}+b_{2}x_{2},\\
\approx & -58.0+4.7x_{1}+0.3x_{2}.
\end{alignat}
We could see the entire model matrix \(\mathbf{X}\) with the =model.matrix=\index{model.matrix@\texttt{model.matrix}} function, but in the interest of brevity we only show the first few rows. 

#+begin_src R :exports both :results output pp 
head(model.matrix(trees.lm))
#+end_src

*** Point Estimates of the Regression Surface
:PROPERTIES:
:CUSTOM_ID: sub-mlr-point-est-regsurface
:END:

The parameter estimates \(\mathbf{b}\) make it easy to find the fitted values\index{fitted values}, \(\hat{\mathbf{Y}}\). We write them individually as \(\hat{Y}_{i}\), \(i=1,2,\ldots,n\), and recall that they are defined by
\begin{eqnarray}
\hat{Y}_{i} & = & \hat{\mu}(x_{1i},x_{2i}),\\
 & = & b_{0}+b_{1}x_{1i}+b_{2}x_{2i},\quad i=1,2,\ldots,n.
\end{eqnarray}
They are expressed more compactly by the matrix equation
\begin{equation}
\hat{\mathbf{Y}}=\mathbf{X}\mathbf{b}.
\end{equation}
From Equation [[eq-b-formula-matrix]] we know that \(\mathbf{b}=\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}\mathbf{Y}\), so we can rewrite
\begin{eqnarray}
\hat{\mathbf{Y}} & = & \mathbf{X}\left[\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}\mathbf{Y}\right],\\
 & = & \mathbf{H}\mathbf{Y},
\end{eqnarray}
where \(\mathbf{H}=\mathbf{X}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}\) is appropriately named /the hat matrix/\index{hat matrix} because it ``puts the hat on \(\mathbf{Y}\)''. The hat matrix is very important in later courses. Some facts about \(\mathbf{H}\) are
- \(\mathbf{H}\) is a symmetric square matrix, of dimension \(\mathrm{n}\times\mathrm{n}\).
- The diagonal entries \(h_{ii}\) satisfy \(0\leq h_{ii}\leq1\) (compare to Equation [[eq-slr-leverage-between]]).
- The trace is \(\mathrm{tr}(\mathbf{H})=p\).
- \(\mathbf{H}\) is /idempotent/ (also known as a /projection matrix/) which means that \(\mathbf{H}^{2}=\mathbf{H}\). The same is true of \(\mathbf{I}-\mathbf{H}\).

Now let us write a column vector \(\mathbf{x}_{0}=(x_{10},x_{20})^{\mathrm{T}}\) to denote given values of the explanatory variables =Girth == \(x_{10}\) and =Height == \(x_{20}\). These values may match those of the collected data, or they may be completely new values not observed in the original data set. We may use the parameter estimates to find \(\hat{Y}(\mathbf{x}_{0})\), which will give us

1. an estimate of \(\mu(\mathbf{x}_{0})\), the mean value of a future observation at \(\mathbf{x}_{0}\), and

2. a prediction for \(Y(\mathbf{x}_{0})\), the actual value of a future observation at \(\mathbf{x}_{0}\).

We can represent \(\hat{Y}(\mathbf{x}_{0})\) by the matrix equation
\begin{equation}
\label{eq-mlr-single-yhat-matrix}
\hat{Y}(\mathbf{x}_{0})=\mathbf{x}_{0}^{\mathrm{T}}\mathbf{b},
\end{equation}
which is just a fancy way to write
\begin{equation}
\hat{Y}(x_{10},x_{20})=b_{0}+b_{1}x_{10}+b_{2}x_{20}.
\end{equation}
 
#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
If we wanted to predict the average volume of black cherry trees that have =Girth = 15= in and are =Height = 77= ft tall then we would use the estimate 
\begin{alignat*}{1}
\hat{\mu}(15,\,77)= & -58+4.7(15)+0.3(77),\\
\approx & 35.6\mbox{\,\ ft}^{3}.
\end{alignat*}

We would use the same estimate \(\hat{Y}=35.6\) to predict the measured =Volume= of another black cherry tree -- yet to be observed -- that has =Girth = 15= in and is =Height = 77= ft tall.
#+latex: \end{exampletoo}
#+html: </div>

**** How to do it with \(\mathsf{R}\)

The fitted values are stored inside =trees.lm= and may be accessed with the =fitted= function. We only show the first five fitted values.

#+begin_src R :exports both :results output pp 
fitted(trees.lm)[1:5]
#+end_src

The syntax for general prediction does not change much from simple linear regression. The computations are done with the =predict= function as described below. 

The only difference from SLR is in the way we tell \(\mathsf{R}\) the values of the explanatory variables for which we want predictions. In SLR we had only one independent variable but in MLR we have many (for the =trees= data we have two). We will store values for the independent variables in the data frame =new=, which has two columns (one for each independent variable) and three rows (we shall make predictions at three different locations).

#+begin_src R :exports code :results silent 
new <- data.frame(Girth = c(9.1, 11.6, 12.5), Height = c(69, 74, 87))
#+end_src

We can view the locations at which we will predict:

#+begin_src R :exports both :results output pp 
new
#+end_src

We continue just like we would have done in SLR.

#+begin_src R :exports both :results output pp 
predict(trees.lm, newdata = new)
#+end_src

#+begin_src R :exports none :results silent
treesFIT <- round(predict(trees.lm, newdata = new), 1)
#+end_src

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Using the =trees= data,
1. Report a point estimate of the mean =Volume= of a tree of =Girth= 9.1 in and =Height= 69 ft.

   The fitted value for \(x_{1}=9.1\) and \(x_{2}=69\) is \( SRC_R{treesFIT[ 1 ]} \), so a point estimate would be \( SRC_R{treesFIT[ 1 ]} \) cubic feet. 

2. Report a point prediction for and a 95% prediction interval for the =Volume= of a hypothetical tree of =Girth= 12.5 in and =Height= 87 ft.

   The fitted value for \(x_{1} = 12.5\) and \(x_{2} = 87\) is \( SRC_R{treesFIT[ 3 ]} \), so a point prediction for the =Volume= is \( SRC_R{treesFIT[ 3 ]} \) cubic feet. 
#+latex: \end{exampletoo}
#+html: </div>

*** Mean Square Error and Standard Error
:PROPERTIES:
:CUSTOM_ID: sub-mlr-mse-se
:END:

The residuals are given by
\begin{equation}
\mathbf{E}=\mathbf{Y}-\hat{\mathbf{Y}}=\mathbf{Y}-\mathbf{H}\mathbf{Y}=(\mathbf{I}-\mathbf{H})\mathbf{Y}.
\end{equation}
Now we can use Theorem [[thm-mvnorm-dist-matrix-prod]] to see that the residuals are distributed
\begin{equation}
\mathbf{E}\sim\mathsf{mvnorm}(\mathtt{mean}=\mathbf{0},\,\mathtt{sigma}=\sigma^{2}(\mathbf{I}-\mathbf{H})),
\end{equation}
since \((\mathbf{I}-\mathbf{H})\mathbf{X}\upbeta=\mathbf{X}\upbeta-\mathbf{X}\upbeta=\mathbf{0}\) and \((\mathbf{I}-\mathbf{H})\,(\sigma^{2}\mathbf{I})\,(\mathbf{I}-\mathbf{H})^{\mathrm{T}}=\sigma^{2}(\mathbf{I}-\mathbf{H})^{2}=\sigma^{2}(\mathbf{I}-\mathbf{H})\). The sum of squared errors \(SSE\) is just
\begin{equation}
SSE=\mathbf{E}^{\mathrm{T}}\mathbf{E}=\mathbf{Y}^{\mathrm{T}}(\mathbf{I}-\mathbf{H})(\mathbf{I}-\mathbf{H})\mathbf{Y}=\mathbf{Y}^{\mathrm{T}}(\mathbf{I}-\mathbf{H})\mathbf{Y}.
\end{equation}
Recall that in SLR we had two parameters (\(\beta_{0}\) and \(\beta_{1}\)) in our regression model and we estimated \(\sigma^{2}\) with \(s^{2}=SSE/(n-2)\). In MLR, we have \(p+1\) parameters in our regression model and we might guess that to estimate \(\sigma^{2}\) we would use the /mean square error/ \(S^{2}\) defined by 
\begin{equation}
S^{2}=\frac{SSE}{n-(p+1)}.
\end{equation}
That would be a good guess. The /residual standard error/ is \(S=\sqrt{S^{2}}\).

**** How to do it with \(\mathsf{R}\)

The residuals are also stored with =trees.lm= and may be accessed with the =residuals= function. We only show the first five residuals.

#+begin_src R :exports both :results output pp 
residuals(trees.lm)[1:5]
#+end_src

The =summary= function output (shown later) lists the =Residual Standard Error= which is just \(S=\sqrt{S^{2}}\). It is stored in the =sigma= component of the =summary= object.

#+begin_src R :exports both :results output pp 
treesumry <- summary(trees.lm)
treesumry$sigma
#+end_src

For the =trees= data we find \(s\approx SRC_R{round(treesumry$sigma, 3)} \).

*** Interval Estimates of the Parameters
:PROPERTIES:
:CUSTOM_ID: sub-mlr-interval-est-params
:END:

We showed in Section [[sub-mlr-parameter-estimates][MLR Parameter Estimates]] that \(\mathbf{b}=\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}\mathbf{Y}\), which is really just a big matrix -- namely \(\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}\) -- multiplied by \(\mathbf{Y}\). It stands to reason that the sampling distribution of \(\mathbf{b}\) would be intimately related to the distribution of \(\mathbf{Y}\), which we assumed to be
\begin{equation}
\mathbf{Y}\sim\mathsf{mvnorm}\left(\mathtt{mean}=\mathbf{X}\upbeta,\,\mathtt{sigma}=\sigma^{2}\mathbf{I}\right).
\end{equation}
Now recall Theorem [[thm-mvnorm-dist-matrix-prod]] that we said we were going to need eventually (the time is now). That proposition guarantees that
\begin{equation}
\label{eq-distn-b-mlr}
\mathbf{b}\sim\mathsf{mvnorm}\left(\mathtt{mean}=\upbeta,\,\mathtt{sigma}=\sigma^{2}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\right),
\end{equation}
since
\begin{equation}
\mathbb{E}\mathbf{b}=\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}(\mathbf{X}\upbeta)=\upbeta,
\end{equation}
and
\begin{equation}
\mbox{Var}(\mathbf{b})=\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}(\sigma^{2}\mathbf{I})\mathbf{X}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}=\sigma^{2}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1},
\end{equation}
the first equality following because the matrix \(\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\) is symmetric.

There is a lot that we can glean from Equation [[eq-distn-b-mlr]]. First, it follows that the estimator \(\mathbf{b}\) is unbiased (see Section [[sec-Point-Estimation-1][Point Estimation 1]]). Second, the variances of \(b_{0}\), \(b_{1}\), ..., \(b_{n}\) are exactly the diagonal elements of \(\sigma^{2}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\), which is completely known except for that pesky parameter \(\sigma^{2}\). Third, we can estimate the standard error of \(b_{i}\) (denoted \(S_{b_{i}}\)) with the mean square error \(S\) (defined in the previous section) multiplied by the corresponding diagonal element of \(\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\). Finally, given estimates of the standard errors we may construct confidence intervals for \(\beta_{i}\) with an interval that looks like
\begin{equation}
b_{i}\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-p-1)S_{b_{i}}.
\end{equation}
The degrees of freedom for the Student's \(t\) distribution
#+latex: \footnote{We are taking great leaps over the mathematical details. In particular, we have yet to show that \(s^{2}\) has a chi-square distribution and we have not even come close to showing that \(b_{i}\) and \(s_{b_{i}}\) are independent. But these are entirely outside the scope of the present book and the reader may rest assured that the proofs await in later classes. See C.R. Rao for more.}
are the same as the denominator of \(S^{2}\). 

**** How to do it with \(\mathsf{R}\)

To get confidence intervals for the parameters we need only use =confint=\index{confint@\texttt{confint}}:

#+begin_src R :exports both :results output pp 
confint(trees.lm)
#+end_src

#+begin_src R :exports none :results silent
treesPAR <- round(confint(trees.lm), 1)
#+end_src

For example, using the calculations above we say that for the regression model =Volume ~ Girth + Height= we are 95% confident that the parameter \(\beta_{1}\) lies somewhere in the interval \( [ SRC_R{treesPAR[2, 1]}, SRC_R{treesPAR[2, 2]} ] \).

*** Confidence and Prediction Intervals

We saw in Section [[sub-mlr-point-est-regsurface][MLR Estimate Regsurface]] how to make point estimates of the mean value of additional observations and predict values of future observations, but how good are our estimates? We need confidence and prediction intervals to gauge their accuracy, and lucky for us the formulas look similar to the ones we saw in SLR.

In Equation [[eq-mlr-single-yhat-matrix]] we wrote \( \hat{Y}(\mathbf{x}_{0})=\mathbf{x}_{0}^{\mathrm{T}}\mathbf{b} \), and in Equation [[eq-distn-b-mlr]] we saw that
\begin{equation}
\mathbf{b}\sim\mathsf{mvnorm}\left(\mathtt{mean}=\upbeta,\,\mathtt{sigma}=\sigma^{2}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\right).
\end{equation}
The following is therefore immediate from Theorem [[thm-mvnorm-dist-matrix-prod]]:
\begin{equation}
\hat{Y}(\mathbf{x}_{0})\sim\mathsf{mvnorm}\left(\mathtt{mean}=\mathbf{x}_{0}^{\mathrm{T}}\upbeta,\,\mathtt{sigma}=\sigma^{2}\mathbf{x}_{0}^{\mathrm{T}}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{x}_{0}\right).
\end{equation}
It should be no surprise that confidence intervals for the mean value of a future observation at the location \(\mathbf{x}_{0}=\begin{bmatrix}x_{10} & x_{20} & \ldots & x_{p0}\end{bmatrix}^{\mathrm{T}}\) are given by
\begin{equation}
\hat{Y}(\mathbf{x}_{0})\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-p-1)\, S\sqrt{\mathbf{x}_{0}^{\mathrm{T}}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{x}_{0}}.
\end{equation}
Intuitively, \(\mathbf{x}_{0}^{\mathrm{T}}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{x}_{0}\) measures the distance of \(\mathbf{x}_{0}\) from the center of the data. The degrees of freedom in the Student's \(t\) critical value are \(n-(p+1)\) because we need to estimate \(p+1\) parameters.

Prediction intervals for a new observation at \(\mathbf{x}_{0}\) are given by
\begin{equation}
\hat{Y}(\mathbf{x}_{0})\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-p-1)\, S\sqrt{1+\mathbf{x}_{0}^{\mathrm{T}}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{x}_{0}}.
\end{equation}
The prediction intervals are wider than the confidence intervals, just as in Section [[sub-slr-interval-est-regline][SLR Interval Estimate Regline]].

**** How to do it with \(\mathsf{R}\)

The syntax is identical to that used in SLR, with the proviso that we need to specify values of the independent variables in the data frame =new= as we did in Section [[sub-slr-interval-est-regline][SLR Interval Estimate Regline]] (which we repeat here for illustration).

#+begin_src R :exports code :results silent 
new <- data.frame(Girth = c(9.1, 11.6, 12.5), Height = c(69, 74, 87))
#+end_src

Confidence intervals are given by

#+begin_src R :exports both :results output pp 
predict(trees.lm, newdata = new, interval = "confidence")
#+end_src

#+begin_src R :exports none :results silent
treesCI <- round(predict(trees.lm, newdata = new, interval = "confidence"), 1)
#+end_src

Prediction intervals are given by

#+begin_src R :exports both :results output pp 
predict(trees.lm, newdata = new, interval = "prediction")
#+end_src

#+begin_src R :exports none :results silent
treesPI <- round(predict(trees.lm, newdata = new, interval = "prediction"), 1)
#+end_src

As before, the interval type is decided by the =interval= argument and the default confidence level is 95% (which can be changed with the =level= argument).

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Using the =trees= data, 

1. Report a 95% confidence interval for the mean =Volume= of a tree of =Girth= 9.1 in and =Height= 69 ft.

   The 95% CI is given by \( [ SRC_R{treesCI[1, 2]}, SRC_R{treesCI[1, 3]} ] \), so with 95% confidence the mean =Volume= lies somewhere between \( SRC_R{treesCI[1, 2]} \) cubic feet and \( SRC_R{treesCI[1, 3]} \) cubic feet.

2. Report a 95% prediction interval for the =Volume= of a hypothetical tree of =Girth= 12.5 in and =Height= 87 ft.

   The 95% prediction interval is given by \( [ SRC_R{treesCI[3, 2]}, SRC_R{treesCI[3, 3]} ] \), so with 95% confidence we may assert that the hypothetical =Volume= of a tree of =Girth= 12.5 in and =Height= 87 ft would lie somewhere between \( SRC_R{treesCI[3, 2]} \) cubic feet and \( SRC_R{treesCI[3, 3]} \) feet.

#+latex: \end{exampletoo}
#+html: </div>

** Model Utility and Inference
:PROPERTIES:
:CUSTOM_ID: sec-Model-Utility-and-MLR
:END:

*** Multiple Coefficient of Determination

We saw in Section [[sub-mlr-mse-se][MLR MSE SE]] that the error sum of squares \(SSE\) can be conveniently written in MLR as 
\begin{equation}
\label{eq-mlr-sse-matrix}
SSE=\mathbf{Y}^{\mathrm{T}}(\mathbf{I}-\mathbf{H})\mathbf{Y}.
\end{equation}
It turns out that there are equally convenient formulas for the total sum of squares \(SSTO\) and the regression sum of squares \(SSR\). They are:
\begin{alignat}{1}
\label{eq-mlr-ssto-matrix}
SSTO= & \mathbf{Y}^{\mathrm{T}}\left(\mathbf{I}-\frac{1}{n}\mathbf{J}\right)\mathbf{Y}
\end{alignat}
and
\begin{alignat}{1}
\label{eq-mlr-ssr-matrix}
SSR= & \mathbf{Y}^{\mathrm{T}}\left(\mathbf{H}-\frac{1}{n}\mathbf{J}\right)\mathbf{Y}.
\end{alignat}
(The matrix \(\mathbf{J}\) is defined in Appendix [[sec-Linear-Algebra][Linear Algebra]].) Immediately from Equations [[eq-mlr-sse-matrix]], [[eq-mlr-ssto-matrix]], and [[eq-mlr-ssr-matrix]] we get the /Anova Equality/
\begin{equation} 
SSTO=SSE+SSR.
\end{equation}
(See Exercise [[xca-anova-equality]].) We define the /multiple coefficient of determination/ by the formula
\begin{equation} 
R^{2}=1-\frac{SSE}{SSTO}.
\end{equation}

We interpret \(R^{2}\) as the proportion of total variation that is explained by the multiple regression model. In MLR we must be careful, however, because the value of \(R^{2}\) can be artificially inflated by the addition of explanatory variables to the model, regardless of whether or not the added variables are useful with respect to prediction of the response variable. In fact, it can be proved that the addition of a single explanatory variable to a regression model will increase the value of \(R^{2}\), /no matter how worthless/ the explanatory variable is. We could model the height of the ocean tides, then add a variable for the length of cheetah tongues on the Serengeti plain, and our \(R^{2}\) would inevitably increase. 

This is a problem, because as the philosopher, Occam, once said: ``causes should not be multiplied beyond necessity''. We address the problem by penalizing \(R^{2}\) when parameters are added to the model. The result is an /adjusted/ \(R^{2}\) which we denote by \(\overline{R}^{2}\).
\begin{equation}
\overline{R}^{2}=\left(R^{2}-\frac{p}{n-1}\right)\left(\frac{n-1}{n-p-1}\right).
\end{equation}
It is good practice for the statistician to weigh both \(R^{2}\) and \(\overline{R}^{2}\) during assessment of model utility. In many cases their values will be very close to each other. If their values differ substantially, or if one changes dramatically when an explanatory variable is added, then (s)he should take a closer look at the explanatory variables in the model.

**** How to do it with \(\mathsf{R}\)
For the =trees= data, we can get \(R^{2}\) and \(\overline{R}^{2}\) from the =summary= output or access the values directly by name as shown (recall that we stored the =summary= object in =treesumry=).

#+begin_src R :exports both :results output pp 
treesumry$r.squared
#+end_src

#+begin_src R :exports both :results output pp 
treesumry$adj.r.squared
#+end_src

High values of \(R^{2}\) and \( \overline{R}^2 \) such as these indicate that the model fits very well, which agrees with what we saw in Figure [[fig-3D-scatterplot-trees][3D-scatterplot-trees]].

*** Overall /F/-Test
:PROPERTIES:
:CUSTOM_ID: sub-mlr-Overall-F-Test
:END:

Another way to assess the model's utility is to to test the hypothesis
\[
H_{0}:\beta_{1}=\beta_{2}=\cdots=\beta_{p}=0\mbox{ versus }H_{1}:\mbox{ at least one $\beta_{i}\neq0$}.
\]
The idea is that if all \(\beta_{i}\)'s were zero, then the explanatory variables \(X_{1},\ldots,X_{p}\) would be worthless predictors for the response variable \(Y\). We can test the above hypothesis with the overall \(F\) statistic, which in MLR is defined by
\begin{equation}
F=\frac{SSR/p}{SSE/(n-p-1)}.
\end{equation}
When the regression assumptions hold and under \(H_{0}\), it can be shown that \(F\sim\mathsf{f}(\mathtt{df1}=p,\,\mathtt{df2}=n-p-1)\). We reject \(H_{0}\) when \(F\) is large, that is, when the explained variation is large relative to the unexplained variation.

**** How to do it with \(\mathsf{R}\)

The overall \(F\) statistic and its associated /p/-value is listed at the bottom of the =summary= output, or we can access it directly by name; it is stored in the =fstatistic= component of the =summary= object. 

#+begin_src R :exports both :results output pp 
treesumry$fstatistic
#+end_src

For the =trees= data, we see that \( F = SRC_R{treesumry$fstatistic[ 1 ]} \) with a /p/-value =< 2.2e-16=. Consequently we reject \(H_{0}\), that is, the data provide strong evidence that not all \(\beta_{i}\)'s are zero.

*** Student's /t/ Tests
:PROPERTIES:
:CUSTOM_ID: sub-mlr-Students-t-Tests
:END:

We know that
\begin{equation}
\mathbf{b}\sim\mathsf{mvnorm}\left(\mathtt{mean}=\upbeta,\,\mathtt{sigma}=\sigma^{2}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\right)
\end{equation}
and we have seen how to test the hypothesis \(H_{0}:\beta_{1}=\beta_{2}=\cdots=\beta_{p}=0\), but let us now consider the test
\begin{equation}
H_{0}:\beta_{i}=0\mbox{ versus }H_{1}:\beta_{i}\neq0,
\end{equation}
where \(\beta_{i}\) is the coefficient for the \(i^{\textrm{th}}\) independent variable. We test the hypothesis by calculating a statistic, examining it's null distribution, and rejecting \(H_{0}\) if the /p-value/ is small. If \(H_{0}\) is rejected, then we conclude that there is a significant relationship between \(Y\) and \(x_{i}\) /in the regression model/ \(Y\sim(x_{1},\ldots,x_{p})\). This last part of the sentence is very important because the significance of the variable \(x_{i}\) sometimes depends on the presence of other independent variables in the model
#+latex: \footnote{In other words, a variable might be highly significant one moment but then fail to be significant when another variable is added to the model. When this happens it often indicates a problem with the explanatory variables, such as /multicollinearity/. See Section \ref{sub-Multicollinearity}.}.

To test the hypothesis we go to find the sampling distribution of \( b_{i} \), the estimator of the corresponding parameter \( \beta_{i} \), when the null hypothesis is true. We saw in Section [[sub-mlr-interval-est-params][MLR Interval Estimate Parameters]] that 
\begin{equation}
T_{i}=\frac{b_{i}-\beta_{i}}{S_{b_{i}}}
\end{equation}
has a Student's \(t\) distribution with \(n-(p+1)\) degrees of freedom. (Remember, we are estimating \(p+1\) parameters.) Consequently, under the null hypothesis \(H_{0}:\beta_{i}=0\) the statistic \(t_{i}=b_{i}/S_{b_{i}}\) has a \(\mathsf{t}(\mathtt{df}=n-p-1)\) distribution.

**** How to do it with \(\mathsf{R}\)

The Student's \(t\) tests for significance of the individual explanatory variables are shown in the =summary= output.

#+begin_src R :exports both :results output pp 
treesumry
#+end_src

We see from the /p-values/ that there is a significant linear relationship between =Volume= and =Girth= and between =Volume= and =Height= in the regression model =Volume ~ Girth + Height=. Further, it appears that the =Intercept= is significant in the aforementioned model.

** Polynomial Regression
:PROPERTIES:
:CUSTOM_ID: sec-Polynomial-Regression
:END:

*** Quadratic Regression Model

In each of the previous sections we assumed that \(\mu\) was a linear function of the explanatory variables. For example, in SLR we assumed that \(\mu(x)=\beta_{0}+\beta_{1}x\), and in our previous MLR examples we assumed \(\mu(x_{1},x_{2})=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}\). In every case the scatterplots indicated that our assumption was reasonable. Sometimes, however, plots of the data suggest that the linear model is incomplete and should be modified.

#+name: Scatterplot-Volume-Girth-trees
#+begin_src R :exports code :results silent
qplot(Girth, Volume, data = trees)
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/mlr/Scatterplot-Volume-Girth-trees.ps
  <<Scatterplot-Volume-Girth-trees>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/mlr/Scatterplot-Volume-Girth-trees.svg
  <<Scatterplot-Volume-Girth-trees>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/mlr/Scatterplot-Volume-Girth-trees.ps}
  \caption[Scatterplot of \texttt{Volume} versus \texttt{Girth} for the \texttt{trees} data]{\small A scatterplot of \texttt{Volume} versus \texttt{Girth} for the \texttt{trees} data.}
  \label{fig-Scatterplot-Volume-Girth-trees}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-Scatterplot-Volume-Girth-trees" class="figure">
  <p><img src="svg/mlr/Scatterplot-Volume-Girth-trees.svg" width=500 alt="svg/mlr/Scatterplot-Volume-Girth-trees.svg" /></p>
  <p>A scatterplot of <code>Volume</code> versus <code>Girth</code> for the <code>trees</code> data.</p>
</div>
#+end_html

For example, let us examine a scatterplot of =Volume= versus =Girth= a little more closely. See Figure [[fig-Scatterplot-Volume-Girth-trees][Scatterplot-Volume-Girth-trees]]. There might be a slight curvature to the data; the volume curves ever so slightly upward as the girth increases. After looking at the plot we might try to capture the curvature with a mean response such as 
\begin{equation}
\mu(x_{1})=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{1}^{2}.
\end{equation}
The model associated with this choice of \(\mu\) is
\begin{equation}
Y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{1}^{2}+\epsilon.
\end{equation}
The regression assumptions are the same. Almost everything indeed is the same. In fact, it is still called a ``linear regression model'', since the mean response \(\mu\) is linear /in the parameters/ \(\beta_{0}\), \(\beta_{1}\), and \(\beta_{2}\). 

*However, there is one important difference.* When we introduce the squared variable in the model we inadvertently also introduce strong dependence between the terms which can cause significant numerical problems when it comes time to calculate the parameter estimates. Therefore, we should usually rescale the independent variable to have mean zero (and even variance one if we wish) *before* fitting the model. That is, we replace the \(x_{i}\)'s with \(x_{i}-\overline{x}\) (or \((x_{i}-\overline{x})/s\)) before fitting the model
#+latex: \footnote{Rescaling the data gets the job done but a better way to avoid the multicollinearity introduced by the higher order terms is with /orthogonal polynomials/, whose coefficients are chosen just right so that the polynomials are not correlated with each other. This is beginning to linger outside the scope of this book, however, so we will content ourselves with a brief mention and then stick with the rescaling approach in the discussion that follows. A nice example of orthogonal polynomials in action can be run with \texttt{example(cars)}.}.

**** How to do it with \(\mathsf{R}\)

There are multiple ways to fit a quadratic model to the variables =Volume= and =Girth= using \(\mathsf{R}\).
1. One way would be to square the values for =Girth= and save them in a vector =Girthsq=. Next, fit the linear model =Volume ~ Girth + Girthsq=. 
2. A second way would be to use the /insulate/ function in \(\mathsf{R}\), denoted by =I=:
   : Volume ~ Girth + I(Girth^2)
The second method is shorter than the first but the end result is the same. And once we calculate and store the fitted model (in, say, =treesquad.lm=) all of the previous comments regarding \(\mathsf{R}\) apply.  
3. A third and ``right'' way to do it is with orthogonal polynomials:
   :  Volume ~ poly(Girth, degree = 2)
   See =?poly= and =?cars= for more information. Note that we can recover the approach in 2 with =poly(Girth, degree = 2, raw = TRUE)=.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
We will fit the quadratic model to the =trees= data and display the results with =summary=, being careful to rescale the data before fitting the model. We may rescale the =Girth= variable to have zero mean and unit variance on-the-fly with the =scale= function.

#+begin_src R :exports both :results output pp 
treesquad.lm <- lm(Volume ~ scale(Girth) + I(scale(Girth)^2), data = trees)
summary(treesquad.lm)
#+end_src

We see that the \(F\) statistic indicates the overall model including =Girth= and =Girth^2= is significant. Further, there is strong evidence that both =Girth= and =Girth^2= are significantly related to =Volume=. We may examine a scatterplot together with the fitted quadratic function using the =lines= function, which adds a line to the plot tracing the estimated mean response.

#+name: Fitting-the-Quadratic
#+begin_src R :exports code :results silent
a <- ggplot(trees, aes(scale(Girth), Volume))
a + stat_smooth(method = lm, formula = y ~ poly(x, 2)) + geom_point()
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/mlr/Fitting-the-Quadratic.ps
  <<Fitting-the-Quadratic>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/mlr/Fitting-the-Quadratic.svg
  <<Fitting-the-Quadratic>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/mlr/Fitting-the-Quadratic.ps}
  \caption[Quadratic model for the \texttt{trees} data]{\small A quadratic model for the \texttt{trees} data.}
  \label{fig-Fitting-the-Quadratic}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-Fitting-the-Quadratic" class="figure">
  <p><img src="svg/mlr/Fitting-the-Quadratic.svg" width=500 alt="svg/mlr/Fitting-the-Quadratic.svg" /></p>
  <p>A quadratic model for the <code>trees</code> data.</p>
</div>
#+end_html

The plot is shown in Figure [[fig-Fitting-the-Quadratic][Fitting-the-Quadratic]]. Pay attention to the scale on the \(x\)-axis: it is on the scale of the transformed =Girth= data and not on the original scale.

#+latex: \end{exampletoo}
#+html: </div>


#+begin_rem
When a model includes a quadratic term for an independent variable, it is customary to also include the linear term in the model. The principle is called /parsimony/. More generally, if the researcher decides to include \(x^{m}\) as a term in the model, then (s)he should also include all lower order terms \(x\), \(x^{2}\), ...,\(x^{m-1}\) in the model.
#+end_rem

We do estimation/prediction the same way that we did in Section [[sub-mlr-point-est-regsurface][MLR Point Estimation Regsurface]], except we do not need a =Height= column in the dataframe =new= since the variable is not included in the quadratic model.

#+begin_src R :exports both :results output pp 
new <- data.frame(Girth = c(9.1, 11.6, 12.5))
predict(treesquad.lm, newdata = new, interval = "prediction")
#+end_src

The predictions and intervals are slightly different from what they were previously. Notice that it was not necessary to rescale the =Girth= prediction data before input to the =predict= function; the model did the rescaling for us automatically.

#+begin_rem
We have mentioned on several occasions that it is important to rescale the explanatory variables for polynomial regression. Watch what happens if we ignore this advice:

#+begin_src R :exports both :results output pp 
summary(lm(Volume ~ Girth + I(Girth^2), data = trees))
#+end_src

Now nothing is significant in the model except =Girth^2=. We could delete the =Intercept= and =Girth= from the model, but the model would no longer be /parsimonious/. A novice may see the output and be confused about how to proceed, while the seasoned statistician recognizes immediately that =Girth= and =Girth^2= are highly correlated (see Section [[sub-Multicollinearity][Multicollinearity]]). The only remedy to this ailment is to rescale =Girth=, which we should have done in the first place.

In Example [[exa-mlr-trees-poly-no-rescale]] of Section [[sec-Partial-F-Statistic][Partial F Statistic]] we investigate this issue further.

#+end_rem

** Interaction
:PROPERTIES:
:CUSTOM_ID: sec-Interaction
:END:

In our model for tree volume there have been two independent variables: =Girth= and =Height=. We may suspect that the independent variables are related, that is, values of one variable may tend to influence values of the other. It may be desirable to include an additional term in our model to try and capture the dependence between the variables. Interaction terms are formed by multiplying one (or more) explanatory variable(s) by another. 


#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
Perhaps the =Girth= and =Height= of the tree interact to influence the its =Volume=; we would like to investigate whether the model (=Girth= = \(x_{1}\) and =Height= = \(x_{2}\)) 
\begin{equation}
Y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\epsilon
\end{equation}
would be significantly improved by the model
\begin{equation}
Y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\beta_{1:2}x_{1}x_{2}+\epsilon,
\end{equation}
where the subscript \(1:2\) denotes that \(\beta_{1:2}\) is a coefficient
of an interaction term between \(x{}_{1}\) and \(x_{2}\). 

#+latex: \end{exampletoo}
#+html: </div>

*** What does it mean?

Consider the mean response \(\mu(x_{1},x_{2})\) as a function of \(x_{2}\):
\begin{equation}
\mu(x_{2})=(\beta_{0}+\beta_{1}x_{1})+\beta_{2}x_{2}.
\end{equation}
This is a linear function of \(x_{2}\) with slope \(\beta_{2}\). As \(x_{1}\) changes, the \(y\)-intercept of the mean response in \(x_{2}\) changes, but the slope remains the same. Therefore, the mean response in \(x_{2}\) is represented by a collection of parallel lines all with common slope \(\beta_{2}\).

Now think about what happens when the interaction term \(\beta_{1:2}x_{1}x_{2}\) is included. The mean response in \(x_{2}\) now looks like
\begin{equation}
\mu(x_{2})=(\beta_{0}+\beta_{1}x_{1})+(\beta_{2}+\beta_{1:2}x_{1})x_{2}.
\end{equation}
In this case we see that not only the \(y\)-intercept changes when \(x_{1}\) varies, but the slope also changes in \(x_{1}\). Thus, the interaction term allows the slope of the mean response in \(x_{2}\) to increase and decrease as \(x_{1}\) varies. 

*** How to do it with \(\mathsf{R}\)

There are several ways to introduce an interaction term into the model.
1. Make a new variable =prod <- Girth * Height=, then include =prod= in the model formula =Volume ~ Girth + Height + prod=. This method is perhaps the most transparent, but it also reserves memory space unnecessarily.
2. Construct an interaction term directly in \(\mathsf{R}\) with a colon =:=. For this example, the model formula would look like 
   : Volume ~ Girth + Height + Girth:Height 

For the =trees= data, we fit the model with the interaction using method two and see if it is significant:

#+begin_src R :exports both :results output pp 
treesint.lm <- lm(Volume ~ Girth + Height + Girth:Height, data = trees)
summary(treesint.lm)
#+end_src

We can see from the output that the interaction term is highly significant. Further, the estimate \(b_{1:2}\) is positive. This means that the slope of \(\mu(x_{2})\) is steeper for bigger values of =Girth=. Keep in mind: the same interpretation holds for \(\mu(x_{1})\); that is, the slope of \(\mu(x_{1})\) is steeper for bigger values of =Height=.

For the sake of completeness we calculate confidence intervals for the parameters and do prediction as before.

#+begin_src R :exports both :results output pp 
confint(treesint.lm)
#+end_src

#+begin_src R :exports both :results output pp 
new <- data.frame(Girth = c(9.1, 11.6, 12.5), Height = c(69, 74, 87))
predict(treesint.lm, newdata = new, interval = "prediction")
#+end_src

#+begin_rem
There are two other ways to include interaction terms in model formulas. For example, we could have written =Girth * Height= or even =(Girth + Height)^2= and both would be the same as =Girth + Height + Girth:Height=. 
#+end_rem

These examples can be generalized to more than two independent variables, say three, four, or even more. We may be interested in seeing whether any pairwise interactions are significant. We do this with a model formula that looks something like =y ~ (x1 + x2 + x3 + x4)^2=.  

** Qualitative Explanatory Variables
:PROPERTIES:
:CUSTOM_ID: sec-Qualitative-Explanatory-Variables
:END:

We have so far been concerned with numerical independent variables taking values in a subset of real numbers. In this section, we extend our treatment to include the case in which one of the explanatory variables is qualitative, that is, a /factor/. Qualitative variables take values in a set of /levels/, which may or may not be ordered. See Section [[sub-Qualitative-Data][Qualitative Data]].

*Note:*
The =trees= data do not have any qualitative explanatory variables, so we will construct one for illustrative purposes
#+latex: \footnote{This procedure of replacing a continuous variable by a discrete/qualitative one is called \emph{binning}, and is almost \emph{never} the right thing to do. We are in a bind at this point, however, because we have invested this chapter in the \texttt{trees} data and I do not want to switch mid-discussion. I am currently searching for a data set with pre-existing qualitative variables that also conveys the same points present in the trees data, and when I find it I will update this chapter accordingly.}.
We will leave the =Girth= variable alone, but we will replace the variable =Height= by a new variable =Tall= which indicates whether or not the cherry tree is taller than a certain threshold (which for the sake of argument will be the sample median height of 76 ft). That is, =Tall= will be defined by
\begin{equation}
\mathtt{Tall}=
\begin{cases}
\mathtt{yes}, & \mbox{if }\mathtt{Height}>76,\\
\mathtt{no}, & \mbox{if }\mathtt{Height}\leq76.
\end{cases}
\end{equation}

We can construct =Tall= very quickly in \(\mathsf{R}\) with the =cut= function:

#+begin_src R :exports both :results output pp 
trees$Tall <- cut(trees$Height, breaks = c(-Inf, 76, Inf), 
                  labels = c("no","yes"))
trees$Tall[1:5]
#+end_src

Note that =Tall= is automatically generated to be a factor with the labels in the correct order. See =?cut= for more. 


Once we have =Tall=, we include it in the regression model just like we would any other variable. It is handled internally in a special way. Define a ``dummy variable'' =Tallyes= that takes values
\begin{equation}
\mathtt{Tallyes}=
\begin{cases}
1, & \mbox{if }\mathtt{Tall}=\mathtt{yes},\\
0, & \mbox{otherwise.}
\end{cases}
\end{equation}
That is, =Tallyes= is an /indicator variable/ which indicates when a respective tree is tall. The model may now be written as 
\begin{equation}
\mathtt{Volume}=\beta_{0}+\beta_{1}\mathtt{Girth}+\beta_{2}\mathtt{Tallyes}+\epsilon.
\end{equation}
Let us take a look at what this definition does to the mean response. Trees with =Tall = yes= will have the mean response
\begin{equation}
\mu(\mathtt{Girth})=(\beta_{0}+\beta_{2})+\beta_{1}\mathtt{Girth},
\end{equation}
while trees with =Tall = no= will have the mean response
\begin{equation} 
\mu(\mathtt{Girth})=\beta_{0}+\beta_{1}\mathtt{Girth}.
\end{equation}
In essence, we are fitting two regression lines: one for tall trees, and one for short trees. The regression lines have the same slope but they have different \(y\) intercepts (which are exactly \(|\beta_{2}|\) far apart).

*** How to do it with \(\mathsf{R}\)

The important thing is to double check that the qualitative variable in question is stored as a factor. The way to check is with the =class= command. For example,

#+begin_src R :exports both :results output pp 
class(trees$Tall)
#+end_src

If the qualitative variable is not yet stored as a factor then we may convert it to one with the =factor= command. See Section [[sub-Qualitative-Data][Qualitative Data]]. Other than this we perform MLR as we normally would.

#+begin_src R :exports both :results output pp 
treesdummy.lm <- lm(Volume ~ Girth + Tall, data = trees)
summary(treesdummy.lm)
#+end_src

From the output we see that all parameter estimates are statistically significant and we conclude that the mean response differs for trees with =Tall = yes= and trees with =Tall = no=.

#+begin_rem
We were somewhat disingenuous when we defined the dummy variable =Tallyes= because, in truth, \(\mathsf{R}\) defines =Tallyes= automatically without input from the user
#+latex: \footnote{That is, \(\mathsf{R}\) by default handles contrasts according to its internal settings which may be customized by the user for fine control. Given that we will not investigate contrasts further in this book it does not serve the discussion to delve into those settings, either. The interested reader should check \texttt{?contrasts} for details.}. 
Indeed, the author fit the model beforehand and wrote the discussion afterward with the knowledge of what \(\mathsf{R}\) would do so that the output the reader saw would match what (s)he had previously read. The way that \(\mathsf{R}\) handles factors internally is part of a much larger topic concerning /contrasts/, which falls outside the scope of this book. The interested reader should see Neter et al \cite{Neter1996} or Fox \cite{Fox1997} for more. 
#+end_rem

#+begin_rem
In general, if an explanatory variable =foo= is qualitative with \(n\) levels =bar1=, =bar2=, ..., =barn= then \(\mathsf{R}\) will by default automatically define \(n-1\) indicator variables in the following way:
\begin{eqnarray*}
\mathtt{foobar2} & = & \begin{cases}
1, & \mbox{if }\mathtt{foo}=\mathtt{"bar2"},\\
0, & \mbox{otherwise.}\end{cases},\,\ldots,\,\mathtt{foobarn}=\begin{cases}
1, & \mbox{if }\mathtt{foo}=\mathtt{"barn"},\\
0, & \mbox{otherwise.}\end{cases}
\end{eqnarray*}
The level =bar1= is represented by \(\mathtt{foobar2}=\cdots=\mathtt{foobarn}=0\). We just need to make sure that =foo= is stored as a factor and \(\mathsf{R}\) will take care of the rest. 
#+end_rem

*** Graphing the Regression Lines

We can see a plot of the two regression lines with the following mouthful of code.

#+name: dummy-variable-trees
#+begin_src R :exports none :results silent
treesTall <- split(trees, trees$Tall)
treesTall[["yes"]]$Fit <- predict(treesdummy.lm, treesTall[["yes"]])
treesTall[["no"]]$Fit <- predict(treesdummy.lm, treesTall[["no"]])
plot(Volume ~ Girth, data = trees, type = "n")
points(Volume ~ Girth, data = treesTall[["yes"]], pch = 1)
points(Volume ~ Girth, data = treesTall[["no"]], pch = 2)
lines(Fit ~ Girth, data = treesTall[["yes"]])
lines(Fit ~ Girth, data = treesTall[["no"]])
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/mlr/dummy-variable-trees.ps
  <<dummy-variable-trees>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/mlr/dummy-variable-trees.svg
  <<dummy-variable-trees>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/mlr/dummy-variable-trees.ps}
  \caption[A dummy variable model for the \texttt{trees} data]{\small A dummy variable model for the \texttt{trees} data.}
  \label{fig-dummy-variable-trees}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-dummy-variable-trees" class="figure">
  <p><img src="svg/mlr/dummy-variable-trees.svg" width=500 alt="svg/mlr/dummy-variable-trees.svg" /></p>
  <p>A dummy variable model for the <code>trees</code> data.</p>
</div>
#+end_html

It may look intimidating but there is reason to the madness. First we =split= the =trees= data into two pieces, with groups determined by the =Tall= variable. Next we add the fitted values to each piece via =predict=. Then we set up a =plot= for the variables =Volume= versus =Girth=, but we do not plot anything yet (=type = n=) because we want to use different symbols for the two groups. Next we add =points= to the plot for the =Tall = yes= trees and use an open circle for a plot character (=pch = 1=), followed by =points= for the =Tall = no= trees with a triangle character (=pch = 2=). Finally, we add regression =lines= to the plot, one for each group.

There are other -- shorter -- ways to plot regression lines by groups, namely the =scatterplot= function in the =car= package \cite{car}  and the =xyplot= function in the =lattice= package \cite{lattice}. We elected to introduce the reader to the above approach since many advanced plots in \(\mathsf{R}\) are done in a similar, consecutive fashion.

** Partial /F/ Statistic
:PROPERTIES:
:CUSTOM_ID: sec-Partial-F-Statistic
:END:

We saw in Section [[sub-mlr-Overall-F-Test][MLR Overall F]] how to test \(H_{0}:\beta_{0}=\beta_{1}=\cdots=\beta_{p}=0\) with the overall \(F\) statistic and we saw in Section [[sub-mlr-Students-t-Tests][MLR Student's t Tests]] how to test \(H_{0}:\beta_{i}=0\) that a particular coefficient \(\beta_{i}\) is zero. Sometimes, however, we would like to test whether a certain part of the model is significant. Consider the regression model
\begin{equation}
Y=\beta_{0}+\beta_{1}x_{1}+\cdots+\beta_{j}x_{j}+\beta_{j+1}x_{j+1}+\cdots+\beta_{p}x_{p}+\epsilon,
\end{equation}
where \(j\geq1\) and \(p\geq2\). Now we wish to test the hypothesis
\begin{equation}
H_{0}:\beta_{j+1}=\beta_{j+2}=\cdots=\beta_{p}=0
\end{equation}
versus the alternative 
\begin{equation}
H_{1}:\mbox{at least one of $\beta_{j+1},\ \beta_{j+2},\ ,\ldots,\beta_{p}\neq0$}.
\end{equation}

The interpretation of \(H_{0}\) is that none of the variables \(x_{j+1}\), ...,\(x_{p}\) is significantly related to \(Y\) and the interpretation of \(H_{1}\) is that at least one of \(x_{j+1}\), ...,\(x_{p}\) is significantly related to \(Y\). In essence, for this hypothesis test there are two competing models under consideration:
\begin{align}
\mbox{the full model:} & \quad y=\beta_{0}+\beta_{1}x_{1}+\cdots+\beta_{p}x_{p}+\epsilon,\\
\mbox{the reduced model:} & \quad y=\beta_{0}+\beta_{1}x_{1}+\cdots+\beta_{j}x_{j}+\epsilon,
\end{align}

Of course, the full model will always explain the data /better/ than the reduced model, but does the full model explain the data /significantly better/ than the reduced model? This question is exactly what the partial \(F\) statistic is designed to answer.

We first calculate \(SSE_{f}\), the unexplained variation in the full model, and \(SSE_{r}\), the unexplained variation in the reduced model. We base our test on the difference \(SSE_{r}-SSE_{f}\) which measures the reduction in unexplained variation attributable to the variables \(x_{j+1}\), ..., \(x_{p}\). In the full model there are \(p+1\) parameters and in the reduced model there are \(j+1\) parameters, which gives a difference of \(p-j\) parameters (hence degrees of freedom). The partial /F/ statistic is 
\begin{equation}
F=\frac{(SSE_{r}-SSE_{f})/(p-j)}{SSE_{f}/(n-p-1)}.
\end{equation}
It can be shown when the regression assumptions hold under \(H_{0}\) that the partial \(F\) statistic has an \(\mathsf{f}(\mathtt{df1}=p-j,\,\mathtt{df2}=n-p-1)\) distribution. We calculate the \(p\)-value of the observed partial \(F\) statistic and reject \(H_{0}\) if the \(p\)-value is small. 

*** How to do it with \(\mathsf{R}\)

The key ingredient above is that the two competing models are /nested/ in the sense that the reduced model is entirely contained within the complete model. The way to test whether the improvement is significant is to compute =lm= objects both for the complete model and the reduced model then compare the answers with the =anova= function.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-mlr-trees-poly-no-rescale>>

For the =trees= data, let us fit a polynomial regression model and for the sake of argument we will ignore our own good advice and fail to rescale the explanatory variables. 

#+begin_src R :exports both :results output pp 
treesfull.lm <- lm(Volume ~ Girth + I(Girth^2) + Height + 
                   I(Height^2), data = trees)
summary(treesfull.lm)
#+end_src

In this ill-formed model nothing is significant except =Girth= and =Girth^2=. Let us continue down this path and suppose that we would like to try a reduced model which contains nothing but =Girth= and =Girth^2= (not even an =Intercept=). Our two models are now
\begin{align*} 
\mbox{the full model:} & \quad Y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{1}^{2}+\beta_{3}x_{2}+\beta_{4}x_{2}^{2}+\epsilon,\\
\mbox{the reduced model:} & \quad Y=\beta_{1}x_{1}+\beta_{2}x_{1}^{2}+\epsilon,
\end{align*}
We fit the reduced model with =lm= and store the results:

#+begin_src R :exports code :results silent
treesreduced.lm <- lm(Volume ~ -1 + Girth + I(Girth^2), data = trees)
#+end_src

To delete the intercept from the model we used =-1= in the model formula. Next we compare the two models with the =anova= function. The convention is to list the models from smallest to largest.

#+begin_src R :exports both :results output pp 
anova(treesreduced.lm, treesfull.lm)
#+end_src

We see from the output that the complete model is highly significant compared to the model that does not incorporate =Height= or the =Intercept=. We wonder (with our tongue in our cheek) if the =Height^2= term in the full model is causing all of the trouble. We will fit an alternative reduced model that only deletes =Height^2=. 

#+begin_src R :exports both :results output pp 
treesreduced2.lm <- lm(Volume ~ Girth + I(Girth^2) + Height, 
                       data = trees)
anova(treesreduced2.lm, treesfull.lm)
#+end_src

In this case, the improvement to the reduced model that is attributable to =Height^2= is not significant, so we can delete =Height^2= from the model with a clear conscience. We notice that the /p-value/ for this latest partial \(F\) test is 0.8865, which seems to be remarkably close to the /p-value/ we saw for the univariate /t/ test of =Height^2= at the beginning of this example. In fact, the /p-values/ are /exactly/ the same. Perhaps now we gain some insight into the true meaning of the univariate tests.

#+latex: \end{exampletoo}
#+html: </div>

** Residual Analysis and Diagnostic Tools
:PROPERTIES:
:CUSTOM_ID: sec-Residual-Analysis-MLR
:END:

We encountered many, many diagnostic measures for simple linear regression in Sections [[sec-Residual-Analysis-SLR][SLR Residual Analysis]] and [[sec-Other-Diagnostic-Tools-SLR][SLR Other Diagnostic Tools]]. All of these are valid in multiple linear regression, too, but there are some slight changes that we need to make for the multivariate case. We list these below, and apply them to the trees example.  

- Shapiro-Wilk, Breusch-Pagan, Durbin-Watson: :: unchanged from SLR, but we are now equipped to talk about the Shapiro-Wilk test statistic for the residuals. It is defined by the formula 
   \begin{equation}
   W=\frac{\mathbf{a}^{\mathrm{T}}\mathbf{E}^{\ast}}{\mathbf{E}^{\mathrm{T}}\mathbf{E}},
   \end{equation}
   where \(\mathbf{E}^{\ast}\) is the sorted residuals and \(\mathbf{a}_{1\times\mathrm{n}}\) is defined by 
   \begin{equation}
   \mathbf{a}=\frac{\mathbf{m}^{\mathrm{T}}\mathbf{V}^{-1}}{\sqrt{\mathbf{m}^{\mathrm{T}}\mathbf{V}^{-1}\mathbf{V}^{-1}\mathbf{m}}},
   \end{equation}
   where \(\mathbf{m}_{\mathrm{n}\times1}\) and \(\mathbf{V}_{\mathrm{n}\times\mathrm{n}}\) are the mean and covariance matrix, respectively, of the order statistics from an \(\mathsf{mvnorm}\left(\mathtt{mean}=\mathbf{0},\,\mathtt{sigma}=\mathbf{I}\right)\) distribution. 
- Leverages: :: are defined to be the diagonal entries of the hat matrix \(\mathbf{H}\) (which is why we called them \(h_{ii}\) in Section [[sub-mlr-point-est-regsurface][MLR Point Estimation Regsurface]]). The sum of the leverages is \(\mbox{tr}(\mathbf{H})=p+1\). One rule of thumb considers a leverage extreme if it is larger than double the mean leverage value, which is \(2(p+1)/n\), and another rule of thumb considers leverages bigger than 0.5 to indicate high leverage, while values between 0.3 and 0.5 indicate moderate leverage.
- Standardized residuals: :: unchanged. Considered extreme if \(|R_{i}|>2\). 
- Studentized residuals: :: compared to a \(\mathsf{t}(\mathtt{df}=n-p-2)\) distribution.  
- DFBETAS: :: The formula is generalized to
   \begin{equation}
   (DFBETAS)_{j(i)}=\frac{b_{j}-b_{j(i)}}{S_{(i)}\sqrt{c_{jj}}},\quad j=0,\ldots p,\ i=1,\ldots,n,
   \end{equation}
   where \(c_{jj}\) is the \(j^{\mathrm{th}}\) diagonal entry of \((\mathbf{X}^{\mathrm{T}}\mathbf{X})^{-1}\). Values larger than one for small data sets or \(2/\sqrt{n}\) for large data sets should be investigated.
- DFFITS: :: unchanged. Larger than one in absolute value is considered extreme.
- Cook's D: :: compared to an \(\mathsf{f}(\mathtt{df1}=p+1,\,\mathtt{df2}=n-p-1)\) distribution. Observations falling higher than the 50\(^{\textrm{th}}\) percentile are extreme. 
Note that plugging the value \(p=1\) into the formulas will recover all of the ones we saw in Chapter [[cha-simple-linear-regression][Simple Linear Regression]].

** Additional Topics
:PROPERTIES:
:CUSTOM_ID: sec-Additional-Topics-MLR
:END:

*** Nonlinear Regression

We spent the entire chapter talking about the =trees= data, and all of our models looked like =Volume ~ Girth + Height= or a variant of this model. But let us think again: we know from elementary school that the volume of a rectangle is \(V=lwh\) and the volume of a cylinder (which is closer to what a black cherry tree looks like) is
\begin{equation}
V=\pi r^{2}h\quad \mbox{or}\quad V=4\pi dh,
\end{equation}
where \(r\) and \(d\) represent the radius and diameter of the tree, respectively. With this in mind, it would seem that a more appropriate model for \(\mu\) might be
\begin{equation}
\label{eq-trees-nonlin-reg}
\mu(x_{1},x_{2})=\beta_{0}x_{1}^{\beta_{1}}x_{2}^{\beta_{2}},
\end{equation}
where \(\beta_{1}\) and \(\beta_{2}\) are parameters to adjust for the fact that a black cherry tree is not a perfect cylinder.

How can we fit this model? The model is not linear in the parameters any more, so our linear regression methods will not work... or will they? In the =trees= example we may take the logarithm of both sides of Equation [[eq-trees-nonlin-reg]] to get
\begin{equation}
\mu^{\ast}(x_{1},x_{2})=\ln\left[\mu(x_{1},x_{2})\right]=\ln\beta_{0}+\beta_{1}\ln x_{1}+\beta_{2}\ln x_{2},
\end{equation}
and this new model \(\mu^{\ast}\) is linear in the parameters \(\beta_{0}^{\ast}=\ln\beta_{0}\), \(\beta_{1}^{\ast}=\beta_{1}\) and \(\beta_{2}^{\ast}=\beta_{2}\). We can use what we have learned to fit a linear model =log(Volume) ~ log(Girth) + log(Height)=, and everything will proceed as before, with one exception: we will need to be mindful when it comes time to make predictions because the model will have been fit on the log scale, and we will need to transform our predictions back to the original scale (by exponentiating with =exp=) to make sense.

#+begin_src R :exports both :results output pp 
treesNonlin.lm <- lm(log(Volume) ~ log(Girth) + log(Height), data = trees)
summary(treesNonlin.lm)
#+end_src

This is our best model yet (judging by \(R^{2}\) and \(\overline{R}^{2}\)), all of the parameters are significant, it is simpler than the quadratic or interaction models, and it even makes theoretical sense. It rarely gets any better than that.

We may get confidence intervals for the parameters, but remember that it is usually better to transform back to the original scale for interpretation purposes :

#+begin_src R :exports both :results output pp 
exp(confint(treesNonlin.lm))
#+end_src

(Note that we did not update the row labels of the matrix to show that we exponentiated and so they are misleading as written.) We do predictions just as before. Remember to transform the response variable back to the original scale after prediction. 

#+begin_src R :exports both :results output pp 
new <- data.frame(Girth = c(9.1, 11.6, 12.5), Height = c(69, 74, 87))
exp(predict(treesNonlin.lm, newdata = new, interval = "confidence"))
#+end_src

The predictions and intervals are slightly different from those calculated earlier, but they are close. Note that we did not need to transform the =Girth= and =Height= arguments in the dataframe =new=. All transformations are done for us automatically.

*** Real Nonlinear Regression

We saw with the =trees= data that a nonlinear model might be more appropriate for the data based on theoretical considerations, and we were lucky because the functional form of \(\mu\) allowed us to take logarithms to transform the nonlinear model to a linear one. The same trick will not work in other circumstances, however. We need techniques to fit general models of the form
\begin{equation}
\mathbf{Y}=\mu(\mathbf{X})+\epsilon,
\end{equation}
where \(\mu\) is some crazy function that does not lend itself to linear transformations.

There are a host of methods to address problems like these which are studied in advanced regression classes. The interested reader should see Neter /et al/ \cite{Neter1996} or Tabachnick and Fidell \cite{Tabachnick2006}. 

It turns out that John Fox has posted an Appendix to his book \cite{Fox2002} which discusses some of the methods and issues associated with nonlinear regression; see [[http://cran.r-project.org/doc/contrib/Fox-Companion/appendix.html][here]] for more.  Here is an example of how it works, based on a question from R-help.

#+begin_src R :exports both :results output pp
# fake data 
set.seed(1) 
x <- seq(from = 0, to = 1000, length.out = 200) 
y <- 1 + 2*(sin((2*pi*x/360) - 3))^2 + rnorm(200, sd = 2)
# plot(x, y)
acc.nls <- nls(y ~ a + b*(sin((2*pi*x/360) - c))^2, 
               start = list(a = 0.9, b = 2.3, c = 2.9))
summary(acc.nls)
#plot(x, fitted(acc.nls))
#+end_src

*** Multicollinearity
:PROPERTIES:
:CUSTOM_ID: sub-Multicollinearity
:END:

A multiple regression model exhibits /multicollinearity/ when two or more of the explanatory variables are substantially correlated with each other. We can measure multicollinearity by having one of the explanatory play the role of ``dependent variable'' and regress it on the remaining explanatory variables. The the \(R^{2}\) of the resulting model is near one, then we say that the model is multicollinear or shows multicollinearity.

Multicollinearity is a problem because it causes instability in the regression model. The instability is a consequence of redundancy in the explanatory variables: a high \(R^{2}\) indicates a strong dependence between the selected independent variable and the others. The redundant information inflates the variance of the parameter estimates which can cause them to be statistically insignificant when they would have been significant otherwise. To wit, multicollinearity is usually measured by what are called /variance inflation factors/.

Once multicollinearity has been diagnosed there are several approaches to remediate it. Here are a couple of important ones. 
- Principal Components Analysis. :: This approach casts out two or more of the original explanatory variables and replaces them with new variables, derived from the original ones, that are by design uncorrelated with one another. The redundancy is thus eliminated and we may proceed as usual with the new variables in hand. Principal Components Analysis is important for other reasons, too, not just for fixing multicollinearity problems.
- Ridge Regression. :: The idea of this approach is to replace the original parameter estimates with a different type of parameter estimate which is more stable under multicollinearity. The estimators are not found by ordinary least squares but rather a different optimization procedure which incorporates the variance inflation factor information. 

We decided to omit a thorough discussion of multicollinearity because we are not equipped to handle the mathematical details. Perhaps the topic will receive more attention in a later edition.

- What to do when data are not normal
   - Bootstrap (see Chapter [[cha-resampling-methods][Resampling Methods]]).

*** Akaike's Information Criterion

\[
AIC=-2\ln L+2(p+1)
\]


#+latex: \newpage{}

** Exercises

#+latex: \setcounter{thm}{0}

#+begin_xca
# <<xca-anova-equality>>
Use Equations [[eq-mlr-sse-matrix]], [[eq-mlr-ssto-matrix]], and [[eq-mlr-ssr-matrix]] to prove the Anova Equality:
\[
SSTO=SSE+SSR.
\]
#+end_xca

* Resampling Methods                                                 :resamp:
:PROPERTIES:
:tangle: R/resamp.R
:CUSTOM_ID: cha-resampling-methods
:END:

#+begin_src R :exports none :eval never
# Chapter: Resampling Methods
# All code released under GPL Version 3
#+end_src

#+latex: \noindent 
Computers have changed the face of statistics. Their quick computational speed and flawless accuracy, coupled with large data sets acquired by the researcher, make them indispensable for many modern analyses. In particular, resampling methods (due in large part to Bradley Efron) have gained prominence in the modern statistician's repertoire. We first look at a classical problem to get some insight why. 

I have seen /Statistical Computing with \(\mathsf{R}\)/ by Rizzo \cite{Rizzo2008} and I recommend it to those looking for a more advanced treatment with additional topics. I believe that /Monte Carlo Statistical Methods/ by Robert and Casella \cite{Robert2004} has a new edition that integrates \(\mathsf{R}\) into the narrative.

*What do I want them to know?*
- basic philosophy of resampling and why it is important
- resampling for standard errors and confidence intervals
- resampling for hypothesis tests (permutation tests)

** Introduction
:PROPERTIES:
:CUSTOM_ID: sec-Introduction-Resampling
:END:

-  Classical question :: Given a population of interest, how may we effectively learn some of its salient features, /e.g./, the population's mean? One way is through representative random sampling. Given a random sample, we summarize the information contained therein by calculating a reasonable statistic, /e.g./, the sample mean. Given a value of a statistic, how do we know whether that value is significantly different from that which was expected? We don't; we look at the /sampling distribution/ of the statistic, and we try to make probabilistic assertions based on a confidence level or other consideration. For example, we may find ourselves saying things like, "With 95% confidence, the true population mean is greater than zero".

- Problem :: Unfortunately, in most cases the sampling distribution is /unknown/. Thus, in the past, in efforts to say something useful, statisticians have been obligated to place some restrictive assumptions on the underlying population. For example, if we suppose that the population has a normal distribution, then we can say that the distribution of \(\overline{X}\) is normal, too, with the same mean (and a smaller standard deviation). It is then easy to draw conclusions, make inferences, and go on about our business. 

- Alternative :: We don't know what the underlying population distributions is, so let us /estimate/ it, just like we would with any other parameter. The statistic we use is the /empirical CDF/, that is, the function that places mass \(1/n\) at each of the observed data points \(x_{1},\ldots,x_{n}\) (see Section [[sec-empirical-distribution][Empirical Distribution]]). As the sample size increases, we would expect the approximation to get better and better (with IID observations, it does, and there is a wonderful theorem by Glivenko and Cantelli that proves it). And now that we have an (estimated) population distribution, it is easy to find the sampling distribution of any statistic we like: just *sample* from the empirical CDF many, many times, calculate the statistic each time, and make a histogram. Done! Of course, the number of samples needed to get a representative histogram is prohibitively large... human beings are simply too slow (and clumsy) to do this tedious procedure.

Fortunately, computers are very skilled at doing simple, repetitive tasks very quickly and accurately. So we employ them to give us a reasonable idea about the sampling distribution of our statistic, and we use the generated sampling distribution to guide our inferences and draw our conclusions. If we would like to have a better approximation for the sampling distribution (within the confines of the information contained in the original sample), we merely tell the computer to sample more. In this (restricted) sense, we are limited only by our current computational speed and pocket book.

In short, here are some of the benefits that the advent of resampling methods has given us:
- Fewer assumptions. :: We are no longer required to assume the population is normal or the sample size is large (though, as before, the larger the sample the better). 
- Greater accuracy. :: Many classical methods are based on rough upper bounds or Taylor expansions. The bootstrap procedures can be iterated long enough to give results accurate to several decimal places, often beating classical approximations.  
- Generality. :: Resampling methods are easy to understand and apply to a large class of seemingly unrelated procedures. One no longer needs to memorize long complicated formulas and algorithms.

#+begin_rem
Due to the special structure of the empirical CDF, to get an IID sample we just need to take a random sample of size \(n\), with replacement, from the observed data \(x_{1},\ldots,x_{n}\). Repeats are expected and acceptable. Since we already sampled to get the original data, the term /resampling/ is used to describe the procedure.
#+end_rem

*** General bootstrap procedure.

The above discussion leads us to the following general procedure to approximate the sampling distribution of a statistic \(S=S(x_{1},x_{2},\ldots,x_{n})\) based on an observed simple random sample \(\mathbf{x}=(x_{1},x_{2},\ldots,x_{n})\) of size \(n\): 

1. Create many many samples \(\mathbf{x}_{1}^{\ast},\ldots,\mathbf{x}_{M}^{\ast}\), called /resamples/, by sampling with replacement from the data. 

1. Calculate the statistic of interest \(S(\mathbf{x}_{1}^{\ast}),\ldots,S(\mathbf{x}_{M}^{\ast})\) for each resample. The distribution of the resample statistics is called a /bootstrap distribution/.
 
1. The bootstrap distribution gives information about the sampling distribution of the original statistic \(S\). In particular, the bootstrap distribution gives us some idea about the center, spread, and shape of the sampling distribution of \(S\).

** Bootstrap Standard Errors
:PROPERTIES:
:CUSTOM_ID: sec-Bootstrap-Standard-Errors
:END:

Since the bootstrap distribution gives us information about a statistic's sampling distribution, we can use the bootstrap distribution to estimate properties of the statistic. We will illustrate the bootstrap procedure in the special case that the statistic \(S\) is a standard error. 

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-Bootstrap-se-mean>>

*Standard error of the mean.*  In this example we illustrate the bootstrap by estimating the standard error of the sample meanand we will do it in the special case that the underlying population is \(\mathsf{norm}(\mathtt{mean}=3,\,\mathtt{sd}=1)\).  

Of course, we do not really need a bootstrap distribution here because from Section [[sec-sampling-from-normal-dist][Sampling from Normal]] we know that \(\overline{X}\sim\mathsf{norm}(\mathtt{mean}=3,\,\mathtt{sd}=1/\sqrt{n})\), but we proceed anyway to investigate how the bootstrap performs when we know what the answer should be ahead of time.

We will take a random sample of size \(n=25\) from the population. Then we will /resample/ the data 1000 times to get 1000 resamples of size 25. We will calculate the sample mean of each of the resamples, and will study the data distribution of the 1000 values of \(\overline{x}\).

#+begin_src R :exports code :results silent 
srs <- rnorm(25, mean = 3)
resamps <- replicate(1000, sample(srs, 25, TRUE), simplify = FALSE)
xbarstar <- sapply(resamps, mean, simplify = TRUE)
#+end_src

A histogram of the 1000 values of \(\overline{x}\) is shown in Figure [[fig-Bootstrap-se-mean][Bootstrap-se-mean]], and was produced by the following code.

#+name: Bootstrap-se-mean
#+begin_src R :exports code :results silent
hist(xbarstar, breaks = 40, prob = TRUE)
curve(dnorm(x, 3, 0.2), add = TRUE) # overlay true normal density
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/resamp/Bootstrap-se-mean.ps
  <<Bootstrap-se-mean>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/resamp/Bootstrap-se-mean.svg
  <<Bootstrap-se-mean>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/resamp/Bootstrap-se-mean.ps}
  \caption[Bootstrapping the standard error of the mean, simulated data]{\small The original data were 25 observations generated from a \(\mathsf{norm}(\mathtt{mean}=3,\,\mathtt{sd}=1)\) distribution. We next resampled to get 1000 resamples, each of size 25, and calculated the sample mean for each resample. A histogram of the 1000 values of \(\overline{x}\) is shown above. Also shown (with a solid line) is the true sampling distribution of \(\overline{X}\), which is a \(\mathsf{norm}(\mathtt{mean}=3,\,\mathtt{sd}=0.2)\) distribution. Note that the histogram is centered at the sample mean of the original data, while the true sampling distribution is centered at the true value of \(\mu=3\). The shape and spread of the histogram is similar to the shape and spread of the true sampling distribution.}
  \label{fig-Bootstrap-se-mean}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-Bootstrap-se-mean" class="figure">
  <p><img src="svg/resamp/Bootstrap-se-mean.svg" width=500 alt="svg/resamp/Bootstrap-se-mean.svg" /></p>
  <p>Bootstrapping the standard error of the mean, simulated data.</p>
</div>
#+end_html

We have overlain what we know to be the true sampling distribution of \(\overline{X}\), namely, a \(\mathsf{norm}(\mathtt{mean}=3,\,\mathtt{sd}=1/\sqrt{25})\) distribution. The histogram matches the true sampling distribution pretty well with respect to shape and spread... but notice how the histogram is off-center a little bit. This is not a coincidence -- in fact, it can be shown that the mean of the bootstrap distribution is exactly the mean of the original sample, that is, the value of the statistic that we originally observed. Let us calculate the mean of the bootstrap distribution and compare it to the mean of the original sample:

#+begin_src R :exports both :results output pp 
mean(xbarstar)
mean(srs)
mean(xbarstar) - mean(srs)
#+end_src

#+latex: \end{exampletoo}
#+html: </div>

Notice how close the two values are. The difference between them is an estimate of how biased the original statistic is, the so-called /bootstrap estimate of bias/. Since the estimate is so small we would expect our original statistic (\(\overline{X}\)) to have small bias, but this is no surprise to us because we already knew from Section [[sec-simple-random-samples][Simple Random Samples]] that \(\overline{X}\) is an unbiased estimator of the population mean.

Now back to our original problem, we would like to estimate the standard error of \(\overline{X}\). Looking at the histogram, we see that the spread of the bootstrap distribution is similar to the spread of the sampling distribution. Therefore, it stands to reason that we could estimate the standard error of \(\overline{X}\) with the sample standard deviation of the resample statistics. Let us try and see.

#+begin_src R :exports both :results output pp 
sd(xbarstar)
#+end_src

We know from theory that the true standard error is \(1/\sqrt{25}=0.20\). Our bootstrap estimate is not very far from the theoretical value. 

#+begin_rem
What would happen if we take more resamples? Instead of 1000 resamples, we could increase to, say, 2000, 3000, or even 4000... would it help? The answer is both yes and no. Keep in mind that with resampling methods there are two sources of randomness: that from the original sample, and that from the subsequent resampling procedure. An increased number of resamples would reduce the variation due to the second part, but would do nothing to reduce the variation due to the first part.

We only took an original sample of size \(n=25\), and resampling more and more would never generate more information about the population than was already there. In this sense, the statistician is limited by the information contained in the original sample. 
#+end_rem

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-Bootstrap-se-median>>

*Standard error of the median.* We look at one where we do not know the answer ahead of time. This example uses the =rivers=\index{Data sets!rivers@\texttt{rivers}} data set. Recall the stemplot on page \vpageref{ite-stemplot-rivers} that we made for these data which shows them to be markedly right-skewed, so a natural estimate of center would be the sample median. Unfortunately, its sampling distribution falls out of our reach. We use the bootstrap to help us with this problem, and the modifications to the last example are trivial.

#+begin_src R :exports both :results output pp 
resamps <- replicate(1000, sample(rivers, 141, TRUE), simplify = FALSE)
medstar <- sapply(resamps, median, simplify = TRUE)
sd(medstar)
#+end_src

#+name: Bootstrapping-se-median
#+begin_src R :exports code :results silent
hist(medstar, breaks = 40, prob = TRUE)
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/resamp/Bootstrapping-se-median.ps
  <<Bootstrapping-se-median>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/resamp/Bootstrapping-se-median.svg
  <<Bootstrapping-se-median>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/resamp/Bootstrapping-se-median.ps}
  \caption[Bootstrapping the standard error of the median for the \texttt{rivers} data]{\small Bootstrapping the standard error of the median for the \texttt{rivers} data.}
  \label{fig-Bootstrapping-se-median}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-Bootstrapping-se-median" class="figure">
  <p><img src="svg/resamp/Bootstrapping-se-median.svg" width=500 alt="svg/resamp/Bootstrapping-se-median.svg" /></p>
  <p>Bootstrapping the standard error of the median for the <code>rivers</code> data.</p>
</div>
#+end_html

The graph is shown in Figure [[fig-Bootstrapping-se-median][Bootstrapping-se-median]], and was produced by the following code.

#+begin_src R :exports code :eval never
hist(medstar, breaks = 40, prob = TRUE)
#+end_src

#+begin_src R :exports both :results output pp 
median(rivers)
mean(medstar)
mean(medstar) - median(rivers)
#+end_src

#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
*The boot package in R*. It turns out that there are many bootstrap procedures and commands already built into base \(\mathsf{R}\), in the =boot= package. Further, inside the =boot= package \cite{boot:1} there is even a function called =boot=\index{boot@\texttt{boot}}. The basic syntax is of the form:

:  boot(data, statistic, R)

#+latex: \end{exampletoo}
#+html: </div>
 Here, =data= is a vector (or matrix) containing the data to be resampled, =statistic= is a defined function, /of two arguments/, that tells which statistic should be computed, and the parameter \(\mathsf{R}\) specifies how many resamples should be taken.

For the standard error of the mean (Example [[exa-Bootstrap-se-mean]]):

#+begin_src R :exports both :results output pp 
mean_fun <- function(x, indices) mean(x[indices])
boot(data = srs, statistic = mean_fun, R = 1000)
#+end_src

For the standard error of the median (Example [[exa-Bootstrap-se-median]]):

#+begin_src R :exports both :results output pp 
median_fun <- function(x, indices) median(x[indices])
boot(data = rivers, statistic = median_fun, R = 1000)
#+end_src

We notice that the output from both methods of estimating the standard errors produced similar results. In fact, the =boot= procedure is to be preferred since it invisibly returns much more information (which we will use later) than our naive script and it is much quicker in its computations.

#+begin_rem
Some things to keep in mind about the bootstrap:

- For many statistics, the bootstrap distribution closely resembles the sampling distribution with respect to spread and shape. However, the bootstrap will not have the same center as the true sampling distribution. While the sampling distribution is centered at the population mean (plus any bias), the bootstrap distribution is centered at the original value of the statistic (plus any bias). The =boot= function gives an empirical estimate of the bias of the statistic as part of its output. 

- We tried to estimate the standard error, but we could have (in principle) tried to estimate something else. Note from the previous remark, however, that it would be useless to estimate the population mean \(\mu\) using the bootstrap since the mean of the bootstrap distribution is the observed \(\overline{x}\).  

- You don't get something from nothing. We have seen that we can take a random sample from a population and use bootstrap methods to get a very good idea about standard errors, bias, and the like. However, one must not get lured into believing that by doing some random resampling somehow one gets more information about the parameters than that which was contained in the original sample. Indeed, there is some uncertainty about the parameter due to the randomness of the original sample, and there is even more uncertainty introduced by resampling. One should think of the bootstrap as just another estimation method, nothing more, nothing less.

#+end_rem

** Bootstrap Confidence Intervals
:PROPERTIES:
:CUSTOM_ID: sec-Bootstrap-Confidence-Intervals
:END:

*** Percentile Confidence Intervals

As a first try, we want to obtain a 95% confidence interval for a parameter. Typically the statistic we use to estimate the parameter is centered at (or at least close by) the parameter; in such cases a 95% confidence interval for the parameter is nothing more than a 95% confidence interval for the statistic. And to find a 95% confidence interval for the statistic we need only go to its sampling distribution to find an interval that contains 95% of the area. (The most popular choice is the equal-tailed interval with 2.5% in each tail.)

This is incredibly easy to accomplish with the bootstrap. We need only to take a bunch of bootstrap resamples, order them, and choose the \(\alpha/2\)th and \((1-\alpha)\)th percentiles. There is a function =boot.ci=\index{boot.ci@\texttt{boot.ci}} in \(\mathsf{R}\) already created to do just this. Note that in order  to use the function =boot.ci= we must first run the =boot= function and save the output in a variable, for example, =data.boot=. We then plug =data.boot= into the function =boot.ci=.


#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
# <<exa-percentile-interval-median-first>>

*Percentile interval for the expected value of the median.* We will try the naive approach where we generate the resamples and calculate the percentile interval by hand.

#+begin_src R :exports both :results output pp 
btsamps <- replicate(2000, sample(stack.loss, 21, TRUE), simplify = FALSE)
thetast <- sapply(btsamps, median, simplify = TRUE)
mean(thetast)
median(stack.loss)
quantile(thetast, c(0.025, 0.975))
#+end_src

#+latex: \end{exampletoo}
#+html: </div>

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
*Confidence interval for expected value of the median, second try.*  Now we will do it the right way with the =boot= function.

#+begin_src R :exports both :results output pp 
med_fun <- function(x, ind) median(x[ind])
med_boot <- boot(stack.loss, med_fun, R = 2000)
boot.ci(med_boot, type = c("perc", "norm", "bca"))
#+end_src

#+latex: \end{exampletoo}
#+html: </div>

*** Student's \(t\) intervals (``normal intervals'')

The idea is to use confidence intervals that we already know and let the bootstrap help us when we get into trouble. We know that a \(100(1-\alpha)\%\) confidence interval for the mean of a \(SRS(n)\) from a normal distribution is 
\begin{equation} 
\overline{X}\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)\frac{S}{\sqrt{n}},
\end{equation} 
where \(\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)\) is the appropriate critical value from Student's \(t\) distribution, and we remember that an estimate for the standard error of \(\overline{X}\) is \(S/\sqrt{n}\). Of course, the estimate for the standard error will change when the underlying population distribution is not normal, or when we use a statistic more complicated than \(\overline{X}\). In those situations the bootstrap will give us quite reasonable estimates for the standard error. And as long as the sampling distribution of our statistic is approximately bell-shaped with small bias, the interval 
\begin{equation}
\mbox{statistic}\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)*\mathrm{SE}(\mbox{statistic})
\end{equation}
 will have approximately \(100(1-\alpha)\%\) confidence of containing \(\mathbb{E}(\mathrm{statistic})\). 

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo">
We will use the t-interval method to find the bootstrap CI for the median. We have looked at the bootstrap distribution; it appears to be symmetric and approximately mound shaped. Further, we may check that the bias is approximately 40, which on the scale of these data is practically negligible. Thus, we may consider looking at the \(t\)-intervals. Note that, since our sample is so large, instead of \(t\)-intervals we will essentially be using \(z\)-intervals. 
#+latex: \end{exampletoo}
#+html: </div>

We see that, considering the scale of the data, the confidence intervals compare with each other quite well.

#+begin_rem
We have seen two methods for bootstrapping confidence intervals for a statistic. Which method should we use? If the bias of the bootstrap distribution is small and if the distribution is close to normal, then the percentile and \(t\)-intervals will closely agree. If the intervals are noticeably different, then it should be considered evidence that the normality and bias conditions are not met. In this case, /neither/ interval should be used.
#+end_rem

- \(BC_{a}\): bias-corrected and accelerated
   - transformation invariant
   - more correct and accurate
   - not monotone in coverage level?
- \(t\)-intervals
   - more natural
   - numerically unstable
- Can do things like transform scales, compute confidence intervals, and then transform back.
- Studentized bootstrap confidence intervals where is the Studentized version of is the  order statistic of the simulation

** Resampling in Hypothesis Tests
:PROPERTIES:
:CUSTOM_ID: sec-Resampling-in-Hypothesis
:END:

The classical two-sample problem can be stated as follows: given two groups of interest, we would like to know whether these two groups are significantly different from one another or whether the groups are reasonably similar. The standard way to decide is to 
1. Go collect some information from the two groups and calculate an associated statistic, for example, \(\overline{X}_{1}-\overline{X}_{2}\). 
1. Suppose that there is no difference in the groups, and find the distribution of the statistic in 1. 
1. Locate the observed value of the statistic with respect to the distribution found in 2. A value in the main body of the distribution is not spectacular, it could reasonably have occurred by chance. A value in the tail of the distribution is unlikely, and hence provides evidence /against/ the null hypothesis that the population distributions are the same.  

Of course, we usually compute a /p-value/, defined to be the probability of the observed value of the statistic or more extreme when the null hypothesis is true. Small \(p\)-values are evidence against the null hypothesis. It is not immediately obvious how to use resampling methods here, so we discuss an example.

#+latex: \begin{exampletoo}
#+html: <div class="exampletoo"> 
A study concerned differing dosages of the antiretroviral drug AZT. The common dosage is 300mg daily. Higher doses cause more side affects, but are they significantly higher? We examine for a 600mg dose. The data are as follows: We compare the scores from the two groups by computing the difference in their sample means. The 300mg data were entered in x1 and the 600mg data were entered into x2. The observed difference was

| 300mg  | 284 | 279 | 289 | 292 | 287 | 295 | 285 | 279 | 306 | 298 |
| 600mg  | 298 | 307 | 297 | 279 | 291 | 335 | 299 | 300 | 306 | 291 |
The average amounts can be found:
: > mean(x1)
: [1] 289.4
: > mean(x2)
: [1] 300.3

with an observed difference of =mean(x2) - mean(x1) = 10.9=. As expected, the 600 mg measurements seem to have a higher average, and we might be interested in trying to decide if the average amounts are =significantly= different. The null hypothesis should be that there is no difference in the amounts, that is, the groups are more or less the same. If the null hypothesis were true, then the two groups would indeed be the same, or just one big group. In that case, the observed difference in the sample means just reflects the random assignment into the arbitrary =x1= and =x2= categories. It is now clear how we may resample, consistent with the null hypothesis.
#+latex: \end{exampletoo}
#+html: </div>

*** Procedure:

1. Randomly resample 10 scores from the combined scores of =x1= and =x2=, and assign then to the =x1= group. The rest will then be in the =x2= group. Calculate the difference in (re)sampled means, and store that value.  
1. Repeat this procedure many, many times and draw a histogram of the resampled statistics, called the /permutation distribution/. Locate the observed difference 10.9 on the histogram to get the \(p\)-value. If the \(p\)-value is small, then we consider that evidence against the hypothesis that the groups are the same. 

#+begin_rem
In calculating the permutation test /p-value/, the formula is essentially the proportion of resample statistics that are greater than or equal to the observed value. Of course, this is merely an /estimate/ of the true \(p\)-value. As it turns out, an adjustment of \(+1\) to both the numerator and denominator of the proportion improves the performance of the estimated \(p\)-value, and this adjustment is implemented in the =ts.perm= function.
#+end_rem

#+begin_src R :exports both :results output pp 
oneway_test(len ~ supp, data = ToothGrowth)
#+end_src

*** Comparison with the Two Sample /t/ test

We know from Chapter [[cha-Hypothesis-Testing][Hypothesis Testing]] to use the two-sample \(t\)-test to tell whether there is an improvement as a result of taking the intervention class. Note that the \(t\)-test assumes normal underlying populations, with unknown variance, and small sample \(n=10\). What does the \(t\)-test say? Below is the output. 

#+begin_src R :exports both :results output pp 
t.test(len ~ supp, data = ToothGrowth, 
       alt = "greater", var.equal = TRUE)
#+end_src

#+begin_src R :exports none :results silent
A <- show(oneway_test(len ~ supp, data = ToothGrowth))
B <- t.test(len ~ supp, data = ToothGrowth, alt = "greater", var.equal = TRUE)
#+end_src

The \(p\)-value for the \(t\)-test was \( SRC_R{round(B$p.value, 3)} \), while the permutation test \(p\)-value was \( SRC_R{round(A$p.value, 3)} \). Note that there is an underlying normality assumption for the \(t\)-test, which isn't present in the permutation test. If the normality assumption may be questionable, then the permutation test would be more reasonable. We see what can happen when using a test in a situation where the assumptions are not met: smaller \(p\)-values. In situations where the normality assumptions are not met, for example, small sample scenarios, the permutation test is to be preferred. In particular, if accuracy is very important then we should use the permutation test. 

#+begin_rem
Here are some things about permutation tests to keep in mind.
- While the permutation test does not require normality of the populations (as contrasted with the \(t\)-test), nevertheless it still requires that the two groups are exchangeable; see Section [[sec-Exchangeable-Random-Variables][Exchangeable Random Variables]]. In particular, this means that they must be identically distributed under the null hypothesis. They must have not only the same means, but they must also have the same spread, shape, and everything else. This assumption may or may not be true in a given example, but it will rarely cause the \(t\)-test to outperform the permutation test, because even if the sample standard deviations are markedly different it does not mean that the population standard deviations are different. In many situations the permutation test will also carry over to the \(t\)-test.
- If the distribution of the groups is close to normal, then the \(t\)-test \(p\)-value and the bootstrap \(p\)-value will be approximately equal. If they differ markedly, then this should be considered evidence that the normality assumptions do not hold.  
- The generality of the permutation test is such that one can use all kinds of statistics to compare the two groups. One could compare the difference in variances or the difference in (just about anything). Alternatively, one could compare the ratio of sample means, \(\overline{X}_{1}/\overline{X}_{2}\). Of course, under the null hypothesis this last quantity should be near 1. 
- Just as with the bootstrap, the answer we get is subject to variability due to the inherent randomness of resampling from the data. We can make the variability as small as we like by taking sufficiently many resamples. How many? If the conclusion is very important (that is, if lots of money is at stake), then take thousands. For point estimation problems typically, \(R=1000\) resamples, or so, is enough. In general, if the true \(p\)-value is \(p\) then the standard error of the estimated \(p\)-value is \(\sqrt{p(1-p)/R}\). You can choose \(R\) to get whatever accuracy desired.
#+end_rem

- Other possible testing designs:
   - Matched Pairs Designs. 
   - Relationship between two variables. 

#+latex: \newpage{}

** Exercises
#+latex: \setcounter{thm}{0}

* Nonparametric Statistics                                         :nonparam:
:PROPERTIES:
:tangle: R/nonparam.R
:CUSTOM_ID: cha-Nonparametric-Statistics
:END:

#+begin_src R :exports none :eval never
# Chapter: Nonparametric Statistics
# All code released under GPL Version 3
#+end_src

This chapter is still under substantial revision. At any time you can preview any released drafts with the development version of the =IPSUR= package  which is available from \(\mathsf{R}\)-Forge:

#+begin_src R :exports code :eval never
install.packages("IPSUR", repos="http://R-Forge.R-project.org")
library("IPSUR")
read(IPSUR)
#+end_src

* Categorical Data Analysis                                           :categ:
:PROPERTIES:
:tangle: R/categ.R
:CUSTOM_ID: cha-Categorical-Data-Analysis
:END:

#+begin_src R :exports none :eval never
# Chapter: Categorical Data Analysis
# All code released under GPL Version 3
#+end_src

This chapter is still under substantial revision. At any time you can preview any released drafts with the development version of the =IPSUR= package which is available from \(\mathsf{R}\)-Forge:

#+begin_src R :exports code :eval never
install.packages("IPSUR", repos="http://R-Forge.R-project.org")
library("IPSUR")
read(IPSUR)
#+end_src

* Time Series                                                    :timeseries:
:PROPERTIES:
:tangle: R/timeseries.R
:CUSTOM_ID: cha-Time-Series
:END:

#+begin_src R :exports none :eval never
# Chapter: Time Series
# All code released under GPL Version 3
#+end_src

This chapter is still under substantial revision. At any time you can preview any released drafts with the development version of the =IPSUR= package which is available from \(\mathsf{R}\)-Forge:

#+begin_src R :exports code :eval never
install.packages("IPSUR", repos="http://R-Forge.R-project.org")
library("IPSUR")
read(IPSUR)
#+end_src

#+latex: \appendix


* R Session Information                                            :appendix:
:PROPERTIES:
:CUSTOM_ID: cha-R-Session-Information
:END:

If you ever write the \(\mathsf{R}\) help mailing list with a question, then you should include your session information in the email; it makes the reader's job easier and is requested by the Posting Guide. Here is how to do that, and below is what the output looks like.

#+begin_src R :exports both :results output pp 
sessionInfo()
#+end_src

#+latex: \vfill{}

* GNU Free Documentation License                                   :appendix:
:PROPERTIES:
:CUSTOM_ID: cha-GNU-Free-Documentation
:END:

#+begin_latex
\begin{center}
\textbf{\large Version 1.3, 3 November 2008}\bigskip{}

\par\end{center}

\noindent Copyright (C) 2000, 2001, 2002, 2007, 2008 Free Software
Foundation, Inc.

\begin{center}
\url{http://fsf.org/}
\par\end{center}
#+end_latex

#+latex: \noindent 
Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.

** 0. PREAMBLE

The purpose of this License is to make a manual, textbook, or other functional and useful document free  in the sense of freedom: to assure everyone the effective freedom to copy and redistribute it, with or without modifying it, either commercially or noncommercially. Secondarily, this License preserves for the author and publisher a way to get credit for their work, while not being considered responsible for modifications made by others.

This License is a kind of copyleft, which means that derivative works of the document must themselves be free in the same sense. It complements the GNU General Public License, which is a copyleft license designed for free software.

We have designed this License in order to use it for manuals for free software, because free software needs free documentation: a free program should come with manuals providing the same freedoms that the software does. But this License is not limited to software manuals; it can be used for any textual work, regardless of subject matter or whether it is published as a printed book. We recommend this License principally for works whose purpose is instruction or reference.

** 1. APPLICABILITY AND DEFINITIONS

This License applies to any manual or other work, in any medium, that contains a notice placed by the copyright holder saying it can be distributed under the terms of this License. Such a notice grants a world-wide, royalty-free license, unlimited in duration, to use that work under the conditions stated herein. The Document, below, refers to any such manual or work. Any member of the public is a licensee, and is addressed as you. You accept the license if you copy, modify or distribute the work in a way requiring permission under copyright law.

A Modified Version of the Document means any work containing the Document or a portion of it, either copied verbatim, or with modifications and/or translated into another language.

A Secondary Section is a named appendix or a front-matter section of the Document that deals exclusively with the relationship of the publishers or authors of the Document to the Document's overall subject (or to related matters) and contains nothing that could fall directly within that overall subject. (Thus, if the Document is in part a textbook of mathematics, a Secondary Section may not explain any mathematics.) The relationship could be a matter of historical connection with the subject or with related matters, or of legal, commercial, philosophical, ethical or political position regarding them.

The Invariant Sections are certain Secondary Sections whose titles are designated, as being those of Invariant Sections, in the notice that says that the Document is released under this License. If a section does not fit the above definition of Secondary then it is not allowed to be designated as Invariant. The Document may contain zero Invariant Sections. If the Document does not identify any Invariant Sections then there are none.

The Cover Texts are certain short passages of text that are listed, as Front-Cover Texts or Back-Cover Texts, in the notice that says that the Document is released under this License. A Front-Cover Text may be at most 5 words, and a Back-Cover Text may be at most 25 words.

A Transparent copy of the Document means a machine-readable copy, represented in a format whose specification is available to the general public, that is suitable for revising the document straightforwardly with generic text editors or (for images composed of pixels) generic paint programs or (for drawings) some widely available drawing editor, and that is suitable for input to text formatters or for automatic translation to a variety of formats suitable for input to text formatters. A copy made in an otherwise Transparent file format whose markup, or absence of markup, has been arranged to thwart or discourage subsequent modification by readers is not Transparent. An image format is not Transparent if used for any substantial amount of text. A copy that is not Transparent is called Opaque.

Examples of suitable formats for Transparent copies include plain ASCII without markup, Texinfo input format, \LaTeX{} input format, SGML or XML using a publicly available DTD, and standard-conforming simple HTML, PostScript or PDF designed for human modification. Examples of transparent image formats include PNG, XCF and JPG. Opaque formats include proprietary formats that can be read and edited only by proprietary word processors, SGML or XML for which the DTD and/or processing tools are not generally available, and the machine-generated HTML, PostScript or PDF produced by some word processors for output purposes only.

The Title Page means, for a printed book, the title page itself, plus such following pages as are needed to hold, legibly, the material this License requires to appear in the title page. For works in formats which do not have any title page as such, Title Page means the text near the most prominent appearance of the work's title, preceding the beginning of the body of the text.

The publisher means any person or entity that distributes copies of the Document to the public.  A section Entitled XYZ means a named subunit of the Document whose title either is precisely XYZ or contains XYZ in parentheses following text that translates XYZ in another language. (Here XYZ stands for a specific section name mentioned below, such as Acknowledgements, Dedications, Endorsements, or History.) To Preserve the Title of such a section when you modify the Document means that it remains a section Entitled XYZ according to this definition.

The Document may include Warranty Disclaimers next to the notice which states that this License applies to the Document. These Warranty Disclaimers are considered to be included by reference in this License, but only as regards disclaiming warranties: any other implication that these Warranty Disclaimers may have is void and has no effect on the meaning of this License.

** 2. VERBATIM COPYING

You may copy and distribute the Document in any medium, either commercially or noncommercially, provided that this License, the copyright notices, and the license notice saying this License applies to the Document are reproduced in all copies, and that you add no other conditions whatsoever to those of this License. You may not use technical measures to obstruct or control the reading or further copying of the copies you make or distribute. However, you may accept compensation in exchange for copies. If you distribute a large enough number of copies you must also follow the conditions in section 3.

You may also lend copies, under the same conditions stated above, and you may publicly display copies.

** 3. COPYING IN QUANTITY

If you publish printed copies (or copies in media that commonly have printed covers) of the Document, numbering more than 100, and the Document's license notice requires Cover Texts, you must enclose the copies in covers that carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the front cover, and Back-Cover Texts on the back cover. Both covers must also clearly and legibly identify you as the publisher of these copies. The front cover must present the full title with all words of the title equally prominent and visible. You may add other material on the covers in addition. Copying with changes limited to the covers, as long as they preserve the title of the Document and satisfy these conditions, can be treated as verbatim copying in other respects.

If the required texts for either cover are too voluminous to fit legibly, you should put the first ones listed (as many as fit reasonably) on the actual cover, and continue the rest onto adjacent pages.

If you publish or distribute Opaque copies of the Document numbering more than 100, you must either include a machine-readable Transparent copy along with each Opaque copy, or state in or with each Opaque copy a computer-network location from which the general network-using public has access to download using public-standard network protocols a complete Transparent copy of the Document, free of added material. If you use the latter option, you must take reasonably prudent steps, when you begin distribution of Opaque copies in quantity, to ensure that this Transparent copy will remain thus accessible at the stated location until at least one year after the last time you distribute an Opaque copy (directly or through your agents or retailers) of that edition to the public.

It is requested, but not required, that you contact the authors of the Document well before redistributing any large number of copies, to give them a chance to provide you with an updated version of the Document.

** 4. MODIFICATIONS

You may copy and distribute a Modified Version of the Document under the conditions of sections 2 and 3 above, provided that you release the Modified Version under precisely this License, with the Modified Version filling the role of the Document, thus licensing distribution and modification of the Modified Version to whoever possesses a copy of it. In addition, you must do these things in the Modified Version:

A. Use in the Title Page (and on the covers, if any) a title distinct from that of the Document, and from those of previous versions (which should, if there were any, be listed in the History section of the Document). You may use the same title as a previous version if the original publisher of that version gives permission. 

B. List on the Title Page, as authors, one or more persons or entities responsible for authorship of the modifications in the Modified Version, together with at least five of the principal authors of the Document (all of its principal authors, if it has fewer than five), unless they release you from this requirement. 

C. State on the Title page the name of the publisher of the Modified Version, as the publisher. 

D. Preserve all the copyright notices of the Document. 

E. Add an appropriate copyright notice for your modifications adjacent to the other copyright notices. 

F. Include, immediately after the copyright notices, a license notice giving the public permission to use the Modified Version under the terms of this License, in the form shown in the Addendum below.  

G. Preserve in that license notice the full lists of Invariant Sections and required Cover Texts given in the Document's license notice. 

H. Include an unaltered copy of this License. 

I. Preserve the section Entitled History, Preserve its Title, and add to it an item stating at least the title, year, new authors, and publisher of the Modified Version as given on the Title Page. If there is no section Entitled History in the Document, create one stating the title, year, authors, and publisher of the Document as given on its Title Page, then add an item describing the Modified Version as stated in the previous sentence. 

J. Preserve the network location, if any, given in the Document for public access to a Transparent copy of the Document, and likewise the network locations given in the Document for previous versions it was based on. These may be placed in the History section. You may omit a network location for a work that was published at least four years before the Document itself, or if the original publisher of the version it refers to gives permission. 

K. For any section Entitled Acknowledgements or Dedications, Preserve the Title of the section, and preserve in the section all the substance and tone of each of the contributor acknowledgements and/or dedications given therein. 
L. Preserve all the Invariant Sections of the Document, unaltered in their text and in their titles. Section numbers or the equivalent are not considered part of the section titles. 

M. Delete any section Entitled Endorsements. Such a section may not be included in the Modified Version. 

N. Do not retitle any existing section to be Entitled Endorsements or to conflict in title with any Invariant Section. 

O. Preserve any Warranty Disclaimers.

If the Modified Version includes new front-matter sections or appendices that qualify as Secondary Sections and contain no material copied from the Document, you may at your option designate some or all of these sections as invariant. To do this, add their titles to the list of Invariant Sections in the Modified Version's license notice. These titles must be distinct from any other section titles.

You may add a section Entitled Endorsements, provided it contains nothing but endorsements of your Modified Version by various parties--for example, statements of peer review or that the text has been approved by an organization as the authoritative definition of a standard.

You may add a passage of up to five words as a Front-Cover Text, and a passage of up to 25 words as a Back-Cover Text, to the end of the list of Cover Texts in the Modified Version. Only one passage of Front-Cover Text and one of Back-Cover Text may be added by (or through arrangements made by) any one entity. If the Document already includes a cover text for the same cover, previously added by you or by arrangement made by the same entity you are acting on behalf of, you may not add another; but you may replace the old one, on explicit permission from the previous publisher that added the old one.

The author(s) and publisher(s) of the Document do not by this License give permission to use their names for publicity for or to assert or imply endorsement of any Modified Version.

** 5. COMBINING DOCUMENTS

You may combine the Document with other documents released under this License, under the terms defined in section 4 above for modified versions, provided that you include in the combination all of the Invariant Sections of all of the original documents, unmodified, and list them all as Invariant Sections of your combined work in its license notice, and that you preserve all their Warranty Disclaimers.

The combined work need only contain one copy of this License, and multiple identical Invariant Sections may be replaced with a single copy. If there are multiple Invariant Sections with the same name but different contents, make the title of each such section unique by adding at the end of it, in parentheses, the name of the original author or publisher of that section if known, or else a unique number. Make the same adjustment to the section titles in the list of Invariant Sections in the license notice of the combined work.

In the combination, you must combine any sections Entitled History in the various original documents, forming one section Entitled History;
likewise combine any sections Entitled Acknowledgements, and any sections Entitled Dedications. You must delete all sections Entitled Endorsements.

** 6. COLLECTIONS OF DOCUMENTS

You may make a collection consisting of the Document and other documents released under this License, and replace the individual copies of this License in the various documents with a single copy that is included in the collection, provided that you follow the rules of this License for verbatim copying of each of the documents in all other respects. 

You may extract a single document from such a collection, and distribute it individually under this License, provided you insert a copy of this License into the extracted document, and follow this License in all other respects regarding verbatim copying of that document.

** 7. AGGREGATION WITH INDEPENDENT WORKS

A compilation of the Document or its derivatives with other separate and independent documents or works, in or on a volume of a storage or distribution medium, is called an aggregate if the copyright resulting from the compilation is not used to limit the legal rights of the compilation's users beyond what the individual works permit. When the Document is included in an aggregate, this License does not apply to the other works in the aggregate which are not themselves derivative works of the Document.

If the Cover Text requirement of section 3 is applicable to these copies of the Document, then if the Document is less than one half of the entire aggregate, the Document's Cover Texts may be placed on covers that bracket the Document within the aggregate, or the electronic equivalent of covers if the Document is in electronic form. Otherwise they must appear on printed covers that bracket the whole aggregate.

** 8. TRANSLATION

Translation is considered a kind of modification, so you may distribute translations of the Document under the terms of section 4. Replacing Invariant Sections with translations requires special permission from their copyright holders, but you may include translations of some or all Invariant Sections in addition to the original versions of these Invariant Sections. You may include a translation of this License, and all the license notices in the Document, and any Warranty Disclaimers, provided that you also include the original English version of this License and the original versions of those notices and disclaimers. In case of a disagreement between the translation and the original version of this License or a notice or disclaimer, the original version will prevail.

If a section in the Document is Entitled Acknowledgements, Dedications, or History, the requirement (section 4) to Preserve its Title (section 1) will typically require changing the actual title.

** 9. TERMINATION

You may not copy, modify, sublicense, or distribute the Document except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense, or distribute it is void, and will automatically terminate your rights under this License.

However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.

Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice. 

Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, receipt of a copy of some or all of the same material does not give you any rights to use it.

** 10. FUTURE REVISIONS OF THIS LICENSE

The Free Software Foundation may publish new, revised versions of the GNU Free Documentation License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. See http://www.gnu.org/copyleft/.

Each version of the License is given a distinguishing version number. If the Document specifies that a particular numbered version of this License or any later version applies to it, you have the option of following the terms and conditions either of that specified version or of any later version that has been published (not as a draft) by the Free Software Foundation. If the Document does not specify a version number of this License, you may choose any version ever published (not as a draft) by the Free Software Foundation. If the Document specifies that a proxy can decide which future versions of this License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Document.

** 11. RELICENSING

Massive Multiauthor Collaboration Site (or MMC Site) means any World Wide Web server that publishes copyrightable works and also provides prominent facilities for anybody to edit those works. A public wiki that anybody can edit is an example of such a server. A Massive Multiauthor Collaboration (or MMC) contained in the site means any set of copyrightable works thus published on the MMC site.

CC-BY-SA means the Creative Commons Attribution-Share Alike 3.0 license published by Creative Commons Corporation, a not-for-profit corporation with a principal place of business in San Francisco, California, as well as future copyleft versions of that license published by that same organization.

Incorporate means to publish or republish a Document, in whole or in part, as part of another Document.

An MMC is eligible for relicensing if it is licensed under this License, and if all works that were first published under this License somewhere other than this MMC, and subsequently incorporated in whole or in part into the MMC, (1) had no cover texts or invariant sections, and (2) were thus incorporated prior to November 1, 2008.

The operator of an MMC Site may republish an MMC contained in the site under CC-BY-SA on the same site at any time before August 1, 2009, provided the MMC is eligible for relicensing.

** ADDENDUM: How to use this License for your documents

To use this License in a document you have written, include a copy of the License in the document and put the following copyright and license notices just after the title page: 

#+begin_quote
Copyright (c) YEAR YOUR NAME. Permission is granted to copy, distribute and/or modify this document under the terms of the GNU Free Documentation License, Version 1.3 or any later version published by the Free Software Foundation; with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. A copy of the license is included in the section entitled GNU Free Documentation License.
#+end_quote

If you have Invariant Sections, Front-Cover Texts and Back-Cover Texts, replace the with...Texts. line with this:

#+begin_quote
with the Invariant Sections being LIST THEIR TITLES, with the Front-Cover Texts being LIST, and with the Back-Cover Texts being LIST.
#+end_quote

If you have Invariant Sections without Cover Texts, or some other combination of the three, merge those two alternatives to suit the situation.

If your document contains nontrivial examples of program code, we recommend releasing these examples in parallel under your choice of free software license, such as the GNU General Public License, to permit their use in free software. 

* History                                                          :appendix:
:PROPERTIES:
:CUSTOM_ID: cha-History
:END:

| Title:     | Introduction to Probability and Statistics Using \(\mathsf{R}\), Second Edition |
| Year:      | 2011                                                                        |
| Authors:   | G. Jay Kerns                                                                |
| Publisher: | G. Jay Kerns                                                                |

| Title:     | Introduction to Probability and Statistics Using \(\mathsf{R}\), First Edition |
| Year:      | 2010                                                                       |
| Authors:   | G. Jay Kerns                                                               |
| Publisher: | G. Jay Kerns                                                               |

#+latex: \vfill{}

* Data                                                             :appendix:
:PROPERTIES:
:CUSTOM_ID: cha-data
:END:

This appendix is a reference of sorts regarding some of the data structures a statistician is likely to encounter. We discuss their salient features and idiosyncrasies.

** Data Structures
:PROPERTIES:
:CUSTOM_ID: sec-Data-Structures
:END: 

*** Vectors

See the ``Vectors and Assignment'' section of /An Introduction to/ \(\mathsf{R}\). A vector is an ordered sequence of elements, such as numbers, characters, or logical values, and there may be =NA='s present. We usually make vectors with the assignment operator =<-=.

#+begin_src R :exports both :results output pp 
x <- c(3, 5, 9)
#+end_src

Vectors are atomic in the sense that if you try to mix and match elements of different modes then all elements will be coerced to the most convenient common mode.

#+begin_src R :exports both :results output pp 
y <- c(3, "5", TRUE)
#+end_src

In the example all elements were coerced to /character/ mode. We can test whether a given object is a vector with =is.vector= and can coerce an object (if possible) to a vector with =as.vector=.

*** Matrices and Arrays

See the ``Arrays and Matrices'' section of /An Introduction to/ \(\mathsf{R}\). Loosely speaking, a matrix is a vector that has been reshaped into rectangular form, and an array is a multidimensional matrix. Strictly speaking, it is the other way around: an array is a data vector with a dimension attribute (=dim=), and a matrix is the special case of an array with only two dimensions. We can construct a matrix with the =matrix= function. 

#+begin_src R :exports both :results output pp 
matrix(letters[1:6], nrow = 2, ncol = 3)
#+end_src

Notice the order of the matrix entries, which shows how the matrix is populated by default. We can change this with the =byrow= argument:

#+begin_src R :exports both :results output pp 
matrix(letters[1:6], nrow = 2, ncol = 3, byrow = TRUE)
#+end_src

We can test whether a given object is a matrix with =is.matrix= and can coerce an object (if possible) to a matrix with =as.matrix=. As a final example watch what happens when we mix and match types in the first argument:

#+begin_src R :exports both :results output pp 
matrix(c(1,"2",NA, FALSE), nrow = 2, ncol = 3)
#+end_src

Notice how all of the entries were coerced to character for the final result (except =NA=). Also notice how the four values were /recycled/ to fill up the six entries of the matrix.

The standard arithmetic operations work element-wise with matrices.

#+begin_src R :exports both :results output pp 
A <- matrix(1:6, 2, 3)
B <- matrix(2:7, 2, 3)
A + B
A * B
#+end_src

If you want the standard definition of matrix multiplication then use the =%*%= function. If we were to try =A %*% B= we would get an error because the dimensions do not match correctly, but for fun, we could transpose =B= to get conformable matrices. The transpose function =t= only works for matrices (and data frames).

#+begin_src R :exports both :results output pp 
try(A * B)     # an error
A %*% t(B)     # this is alright
#+end_src

To get the ordinary matrix inverse use the =solve= function: 

#+begin_src R :exports both :results output pp 
solve(A %*% t(B))     # input matrix must be square
#+end_src

Arrays more general than matrices, and some functions (like transpose) do not work for the more general array. Here is what an array looks like: 

#+begin_src R :exports both :results output pp 
array(LETTERS[1:24], dim = c(3,4,2))
#+end_src

We can test with =is.array= and may coerce with =as.array=.

*** Data Frames

A data frame is a rectangular array of information with a special status in \(\mathsf{R}\). It is used as the fundamental data structure by many of the modeling functions. It is like a matrix in that all of the columns must be the same length, but it is more general than a matrix in that columns are allowed to have different modes.

#+begin_src R :exports both :results output pp 
x <- c(1.3, 5.2, 6)
y <- letters[1:3]
z <- c(TRUE, FALSE, TRUE)
A <- data.frame(x, y, z)
A
#+end_src

Notice the =names= on the columns of =A=. We can change those with the =names= function.

#+begin_src R :exports both :results output pp 
names(A) <- c("Fred","Mary","Sue")
A
#+end_src

Basic command is =data.frame=. You can test with =is.data.frame= and you can coerce with =as.data.frame=.

*** Lists
A list is more general than a data frame.

*** Tables
The word ``table'' has a special meaning in \(\mathsf{R}\). More precisely, a contingency table is an object of class =table= which is an array.

Suppose you have a contingency table and would like to do descriptive or inferential statistics on it. The default form of the table is usually inconvenient to use unless we are working with a function specially tailored for tables. Here is how to transform your data to a more manageable form, namely, the raw data used to make the table.

First, we coerce the table to a data frame with: 

#+begin_src R :exports both :results output pp 
A <- as.data.frame(Titanic)
head(A)
#+end_src

Note that there are as many preliminary columns of =A= as there are dimensions to the table. The rows of =A= contain every possible combination of levels from each of the dimensions. There is also a =Freq= column, which shows how many observations there were at that particular combination of levels. 

The form of =A= is often sufficient for our purposes, but more often we need to do more work: we would usually like to repeat each row of =A= exactly the number of times shown in the =Freq= column. The =reshape= package \cite{reshape} has the function =untable= designed for that very purpose: 

#+begin_src R :exports both :results output pp 
B <- with(A, untable(A, Freq))
head(B)
#+end_src

Now, this is more like it. Note that we slipped in a call to the =with= function, which was done to make the call to =untable= more pretty; we could just as easily have done
:  untable(TitanicDF, A$Freq)


The only fly in the ointment is the lingering =Freq= column which has repeated values that do not have any meaning any more. We could just ignore it, but it would be better to get rid of the meaningless column so that it does not cause trouble later. While we are at it, we could clean up the =rownames=, too.

#+begin_src R :exports both :results output pp 
C <- B[, -5]
rownames(C) <- 1:dim(C)[1]
head(C)
#+end_src

*** More about Tables
Suppose you want to make a table that looks like this:

There are at least two ways to do it.

- Using a matrix:
  #+begin_src R :exports both :results output pp 
  tab <- matrix(1:6, nrow = 2, ncol = 3)
  rownames(tab) <- c('first', 'second')
  colnames(tab) <- c('A', 'B', 'C')
  tab  # Counts
  #+end_src
   - note that the columns are filled in consecutively by default. If you want to fill the data in by rows then do =byrow = TRUE= in the =matrix= command.

- Using a dataframe
  #+begin_src R :exports both :results output pp 
  p <- c("milk","tea")
  g <- c("milk","tea")
  catgs <- expand.grid(poured = p, guessed = g)
  cnts <- c(3, 1, 1, 3)
  D <- cbind(catgs, count = cnts)
  xtabs(count ~ poured + guessed, data = D)
  #+end_src
   - again, the data are filled in column-wise.
   - the object is a dataframe
   - if you want to store it as a table then do =A <- xtabs(count ~ poured + guessed, data = D)=

** Importing Data
:PROPERTIES:
:CUSTOM_ID: sec-Importing-A-Data
:END: 

Statistics is the study of data, so the statistician's first step is usually to obtain data from somewhere or another and read them into \(\mathsf{R}\). In this section we describe some of the most common sources of data and how to get data from those sources into a running \(\mathsf{R}\) session.

For more information please refer to the \(\mathsf{R}\) /Data Import/Export Manual/, \cite{rstatenv} and /An Introduction to/ \(\mathsf{R}\), \cite{Venables2010}.

*** Data in Packages

There are many data sets stored in the =datasets= package \cite{datasets} of base \(\mathsf{R}\). To see a list of them all issue the command =data(package = "datasets")=. The output is omitted here because the list is so long. The names of the data sets are listed in the left column. Any data set in that list is already on the search path by default, which means that a user can use it immediately without any additional work. 

There are many other data sets available in the thousands of contributed packages. To see the data sets available in those packages that are currently loaded into memory issue the single command =data()=. If you would like to see all of the data sets that are available in all packages that are installed on your computer (but not necessarily loaded), issue the command 

:  data(package = .packages(all.available = TRUE))

To load the data set =foo= in the contributed package =bar= issue the commands =library("bar")= followed by =data(foo)=, or just the single command  

:  data(foo, package = "bar")

*** Text Files
Many sources of data are simple text files. The entries in the file are separated by delimeters such as TABS (tab-delimeted), commas (comma separated values, or =.csv=, for short) or even just white space (no special name). A lot of data on the Internet are stored with text files, and even if they are not, a person can copy-paste information from a web page to a text file, save it on the computer, and read it into \(\mathsf{R}\). 

*** Other Software Files
Often the data set of interest is stored in some other, proprietary, format by third-party software such as Minitab, SAS, or SPSS. The =foreign= package \cite{foreign} supports import/conversion from many of these formats. Please note, however, that data sets from other software sometimes have properties with no direct analogue in \(\mathsf{R}\). In those cases the conversion process may lose some information which will need to be reentered manually from within \(\mathsf{R}\). See the /Data Import/Export Manual/.

As an example, suppose the data are stored in the SPSS file =foo.sav= which the user has copied to the working directory; it can be imported with the commands

#+begin_src R :exports code :eval never
library("foreign")
read.spss("foo.sav")
#+end_src

See =?read.spss= for the available options to customize the file import. Note that the \(\mathsf{R}\) Commander will import many of the common file types with a menu driven interface.

*** Importing a Data Frame

The basic command is =read.table=.

** Creating New Data Sets
:PROPERTIES:
:CUSTOM_ID: sec-Creating-New-Data
:END: 

Using =c=
Using =scan=
Using the \(\mathsf{R}\) Commander.

** Editing Data
:PROPERTIES:
:CUSTOM_ID: sec-Editing-Data-Sets
:END: 

*** Editing Data Values
*** Inserting Rows and Columns
*** Deleting Rows and Columns
*** Sorting Data

We can sort a vector with the =sort= function. Normally we have a data frame of several columns (variables) and many, many rows (observations). The goal is to shuffle the rows so that they are ordered by the values of one or more columns. This is done with the =order= function. 

For example, we may sort all of the rows of the =Puromycin= data (in ascending order) by the variable =conc= with the following: 

#+begin_src R :exports both :results output pp 
Tmp <- Puromycin[order(Puromycin$conc), ]
head(Tmp)
#+end_src

We can accomplish the same thing with the command 

#+begin_src R :exports code :eval never
with(Puromycin, Puromycin[order(conc), ])
#+end_src

We can sort by more than one variable. To sort first by =state= and next by =conc= do 

#+begin_src R :exports code :eval never
with(Puromycin, Puromycin[order(state, conc), ])
#+end_src

If we would like to sort a numeric variable in descending order then we put a minus sign in front of it. 

#+begin_src R :exports both :results output pp 
Tmp <- with(Puromycin, Puromycin[order(-conc), ])
head(Tmp)
#+end_src

If we would like to sort by a character (or factor) in decreasing order then we can use the =xtfrm= function which produces a numeric vector in the same order as the character vector.

#+begin_src R :exports both :results output pp 
Tmp <- with(Puromycin, Puromycin[order(-xtfrm(state)), ])
head(Tmp)
#+end_src

** Exporting Data
:PROPERTIES:
:CUSTOM_ID: sec-Exporting-a-Data
:END: 

The basic function is =write.table=. The =MASS= package \cite{MASS} also has a =write.matrix= function.

** Reshaping Data
:PROPERTIES:
:CUSTOM_ID: sec-Reshaping-a-Data
:END: 

- Aggregation
- Convert Tables to data frames and back

=rbind=, =cbind=
=ab[order(ab[,1]),]=
=complete.cases=
=aggregate=
=stack=

* Mathematical Machinery                                           :appendix:
:PROPERTIES:
:CUSTOM_ID: cha-Mathematical-Machinery
:END:

This appendix houses many of the standard definitions and theorems that are used at some point during the narrative. It is targeted for someone reading the book who forgets the precise definition of something and would like a quick reminder of an exact statement. No proofs are given, and the interested reader should consult a good text on Calculus (say, Stewart \cite{Stewart2008} or Apostol \cite{Apostol1967,ApostolI1967}), Linear Algebra (say, Strang \cite{Strang1988} and Magnus \cite{Magnus1999}), Real Analysis (say, Folland \cite{Folland1999}, or Carothers \cite{Carothers2000}), or Measure Theory (Billingsley \cite{Billingsley1995}, Ash \cite{Ash2000}, Resnick \cite{Resnick1999}) for details. 

** Set Algebra
:PROPERTIES:
:CUSTOM_ID: sec-The-Algebra-of
:END: 


We denote sets by capital letters, \(A\), \(B\), \(C\), /etc/. The letter \(S\) is reserved for the sample space, also known as the universe or universal set, the set which contains all possible elements. The symbol \(\emptyset\) represents the empty set, the set with no elements. 

*** Set Union, Intersection, and Difference

Given subsets \(A\) and \(B\), we may manipulate them in an algebraic fashion. To this end, we have three set operations at our disposal: union, intersection, and difference. Below is a table summarizing the pertinent information about these operations.

#+CAPTION: [Set operations]{Set operations.}
#+LABEL: tab-Set-Operations
| Name         | Denoted             | Defined by elements   | \(\mathsf{R}\) syntax |
|--------------+---------------------+-----------------------+-------------------|
| Union        | \(A\cup B\)           | in \(A\) or \(B\) or both | =union(A, B)=     |
| Intersection | \(A\cap B\)           | in both \(A\) and \(B\)   | =intersect(A, B)= |
| Difference   | \(A\backslash B\)     | in \(A\) but not in \(B\) | =setdiff(A, B)=   |
| Complement   | \(A^{c}\)             | in \(S\) but not in \(A\) | =setdiff(S, A)=   |

*** Identities and Properties

1. \(A\cup\emptyset=A,\quad A\cap\emptyset=\emptyset\)
1. \(A\cup S=S,\quad A\cap S=A\)
1. \(A\cup A^{c}=S\), \(A\cap A^{c}=\emptyset\)
1. \((A{}^{c})^{c}=A\)
1. The Commutative Property: 
   \begin{equation}
   A \cup B = B\cup A,\quad A\cap B = B\cap A
   \end{equation}
1. The Associative Property: 
   \begin{equation}
   (A\cup B)\cup C=A\cup(B\cup C),\quad (A\cap B)\cap C=A\cap(B\cap C)
   \end{equation}
1. The Distributive Property: 
   \begin{equation}
   A\cup(B\cap C)=(A\cup B)\cap(A\cup B),\quad A\cap(B\cup C)=(A\cap B)\cup(A\cap B)
   \end{equation}
1. DeMorgan's Laws
   \begin{equation}
   (A\cup B)^{c}=A^{c}\cap B^{c}\quad \mbox{and}\quad (A\cap B)^{c}=A^{c}\cup B^{c},
   \end{equation}
   or more generally,
   \begin{equation}
   \left(\bigcup_{\alpha}A_{\alpha}\right)^{c}=\bigcap_{\alpha}A_{\alpha}^{c},\quad \mbox{and}\quad \left(\bigcap_{\alpha}A_{\alpha}\right)^{c}=\bigcup_{\alpha}A_{\alpha}^{c}
   \end{equation}

** Differential and Integral Calculus
:PROPERTIES:
:CUSTOM_ID: sec-Differential-and-Integral
:END: 

A function \(f\) of one variable is said to be one-to-one if no two distinct \(x\) values are mapped to the same \(y=f(x)\) value. To show that a function is one-to-one we can either use the horizontal line test or we may start with the equation \(f(x_{1}) = f(x_{2})\) and use algebra to show that it implies \(x_{1} = x_{2}\).

*** Limits and Continuity
#+begin_defn
Let \(f\) be a function defined on some open interval that contains the number \(a\), except possibly at \(a\) itself. Then we say the /limit of/ \(f(x)\) /as/ \(x\) /approaches/ \(a\) /is/ \(L\), and we write 
\begin{equation}
\lim_{x \to a}f(x) = L,
\end{equation}
if for every \(\epsilon > 0\) there exists a number \(\delta > 0\) such that \(0 < |x-a| < \delta\) implies \(|f(x) - L| < \epsilon\).
#+end_defn

#+begin_defn
A function \(f\) is /continuous at a number/ \(a\) if 
\begin{equation}
\lim_{x \to a} f(x) = f(a).
\end{equation}
The function \(f\) is /right-continuous at the number/ \(a\) if \(\lim_{x\to a^{+}}f(x)=f(a)\), and /left-continuous/ at \(a\) if \(\lim_{x\to a^{-}}f(x)=f(a)\). Finally, the function \(f\) is /continuous on an interval/ \(I\) if it is continuous at every number in the interval. 
#+end_defn

*** Differentiation
#+begin_defn
The /derivative of a function/ \(f\) /at a number/ \(a\), denoted by \(f'(a)\), is
\begin{equation}
f'(a)=\lim_{h\to0}\frac{f(a+h)-f(a)}{h},
\end{equation}
provided this limit exists.
A function is /differentiable at/ \(a\) if \(f'(a)\) exists. It is /differentiable on an open interval/ \((a,b)\) if it is differentiable at every number in the interval.
#+end_defn

**** Differentiation Rules
In the table that follows, \(f\) and \(g\) are differentiable functions and \(c\) is a constant.

#+CAPTION: [Differentiation rules]{Differentiation rules.}
#+LABEL: tab-Differentiation-Rules
| \(\frac{\mathrm{d}}{\mathrm{d} x}c=0\) | \(\frac{\mathrm{d}}{\mathrm{d} x}x^{n}=nx^{n-1}\) | \((cf)'=cf'\)                                       |
| \((f\pm g)'=f'\pm g'\)       | \((fg)'=f'g+fg'\)                       | \(\left(\frac{f}{g}\right)'=\frac{f'g-fg'}{g^{2}}\) |

#+begin_thm
Chain Rule: If \(f\) and \(g\) are both differentiable and \(F=f\circ g\) is the composite function defined by \(F(x)=f[g(x)]\), then \(F\) is differentiable and \(F'(x) = f'[ g(x) ] \cdot g'(x)\).  
#+end_thm

**** Useful Derivatives


#+CAPTION: [Some derivatives]{Some derivatives.}
#+LABEL: tab-Useful-Derivatives
| \(\frac{\mathrm{d}}{\mathrm{d} x}\mathrm{e}^{x}=\mathrm{e}^{x}\) | \(\frac{\mathrm{d}}{\mathrm{d} x}\ln x=x^{-1}\)     | \(\frac{\mathrm{d}}{\mathrm{d} x}\sin x=\cos x\)             |
| \(\frac{\mathrm{d}}{\mathrm{d} x}\cos x=-\sin x\)  | \(\frac{\mathrm{d}}{\mathrm{d} x}\tan x=\sec^{2}x\) | \(\frac{\mathrm{d}}{\mathrm{d} x}\tan^{-1}x=(1+x^{2})^{-1}\) |
|                                        |                                         |                                                  |

*** Optimization
#+begin_defn
A /critical number/ of the function \(f\) is a value \(x^{\ast}\) for which \(f'(x^{\ast})=0\) or for which \(f'(x^{\ast})\) does not exist.
#+end_defn

#+begin_thm
# <<thm-First-Derivative-Test>>
First Derivative Test. If \(f\) is differentiable and if \(x^{\ast}\) is a critical number of \(f\) and if \(f'(x)\geq0\) for \(x\leq x^{\ast}\) and \(f'(x)\leq0\) for \(x\geq x^{\ast}\), then \(x^{\ast}\) is a local maximum of \(f\). If \(f'(x)\leq0\) for \(x\leq x^{\ast}\) and \(f'(x)\geq0\) for \(x\geq x^{\ast}\) , then \(x^{\ast}\) is a local minimum of \(f\).
#+end_thm

#+begin_thm
Second Derivative Test. If \(f\) is twice differentiable and if \(x^{\ast}\) is a critical number of \(f\), then \(x^{\ast}\) is a local maximum of \(f\) if \(f''(x^{\ast})<0\) and \(x^{\ast}\) is a local minimum of \(f\) if \(f''(x^{\ast})>0\).
#+end_thm

*** Integration
As it turns out, there are all sorts of things called ``integrals'', each defined in its own idiosyncratic way. There are /Riemann/ integrals, /Lebesgue/ integrals, variants of these called /Stieltjes/ integrals, /Daniell/ integrals, /Ito/ integrals, and the list continues. Given that this is an introductory book, we will use the Riemannian integral with the caveat that the Riemann integral is /not/ the integral that will be used in more advanced study.

#+begin_defn
Let \(f\) be defined on \([a,b]\), a closed interval of the real line. For each \(n\), divide \([a,b]\) into subintervals \([x_{i},x_{i+1}]\), \(i=0,1,\ldots,n-1\), of length \(\Delta x_{i}=(b-a)/n\) where \(x_{0}=a\) and \(x_{n}=b\), and let \(x_{i}^{\ast}\) be any points chosen from the respective subintervals. Then the /definite integral/ of \(f\) from \(a\) to \(b\) is defined by
\begin{equation}
\int_{a}^{b}f(x)\,\mathrm{d} x=\lim_{n\to\infty}\sum_{i=0}^{n-1}f(x_{i}^{\ast})\,\Delta x_{i},
\end{equation}
provided the limit exists, and in that case, we say that \(f\) is /integrable/ from \(a\) to \(b\). 
#+end_defn

#+begin_thm
The Fundamental Theorem of Calculus. Suppose \(f\) is continuous on \([a,b]\). Then
1. the function \(g\) defined by \(g(x)=\int_{a}^{x}f(t)\:\mathrm{d} t\), \(a\leq x\leq b\), is continuous on \([a,b]\) and differentiable on \((a,b)\) with \(g'(x)=f(x)\).
1. \(\int_{a}^{b}f(x)\,\mathrm{d} x=F(b)-F(a)\), where \(F\) is any /antiderivative/ of \(f\), that is, any function \(F\) satisfying \(F'=f\).
#+end_thm

**** Change of Variables
#+begin_thm
If \(g\) is a differentiable function whose range is the interval \([a,b]\) and if both \(f\) and \(g'\) are continuous on the range of \(u = g(x)\), then
\begin{equation}
\int_{g(a)}^{g(b)}f(u)\:\mathrm{d} u=\int_{a}^{b}f[g(x)]\: g'(x)\:\mathrm{d} x.
\end{equation}
#+end_thm

**** Useful Integrals


#+CAPTION: [Some integrals (constants of integration omitted)]{Some integrals (constants of integration omitted).}
#+LABEL: tab-Useful-Integrals
| \(\int x^{n}\,\mathrm{d} x=x^{n+1}/(n+1),\ n\neq-1\) | \(\int\mathrm{e}^{x}\,\mathrm{d} x=\mathrm{e}^{x}\) | \(\int x^{-1}\,\mathrm{d} x=\ln \mathrm{abs}(x) \) |
| \(\int\tan x\:\mathrm{d} x=\ln \mathrm{abs}(\sec x)\) | \(\int a^{x}\,\mathrm{d} x=a^{x}/\ln a\)            | \(\int(x^{2}+1)^{-1}\,\mathrm{d} x=\tan^{-1}x\) |

**** Integration by Parts

\begin{equation}
\int u\:\mathrm{d} v=uv-\int v\:\mathrm{d} u
\end{equation}
#+begin_thm
L'H\^ opital's Rule. Suppose \(f\) and \(g\) are differentiable and \(g'(x)\neq0\) near \(a\), except possibly at \(a\). Suppose that the limit 
\begin{equation}
\lim_{x\to a}\frac{f(x)}{g(x)}
\end{equation}
is an indeterminate form of type \(\frac{0}{0}\) or \(\infty/\infty\). Then
\begin{equation}
\lim_{x\to a}\frac{f(x)}{g(x)}=\lim_{x\to a}\frac{f'(x)}{g'(x)},
\end{equation}
provided the limit on the right-hand side exists or is infinite.
#+end_thm

**** Improper Integrals

If \(\int_{a}^{t}f(x)\mathrm{d} x\) exists for every number \(t\geq a\), then we define 
\begin{equation}
\int_{a}^{\infty}f(x)\,\mathrm{d} x=\lim_{t\to\infty}\int_{a}^{t}f(x)\,\mathrm{d} x,
\end{equation}
provided this limit exists as a finite number, and in that case we say that \(\int_{a}^{\infty}f(x)\,\mathrm{d} x\) is /convergent/. Otherwise, we say that the improper integral is /divergent/.

If \(\int_{t}^{b}f(x)\,\mathrm{d} x\) exists for every number \(t\leq b\), then we define
\begin{equation}
\int_{-\infty}^{b}f(x)\,\mathrm{d} x=\lim_{t\to-\infty}\int_{t}^{b}f(x)\,\mathrm{d} x,
\end{equation}
provided this limit exists as a finite number, and in that case we say that \(\int_{-\infty}^{b}f(x)\,\mathrm{d} x\) is /convergent/. Otherwise, we say that the improper integral is /divergent/.

If both \(\int_{a}^{\infty}f(x)\,\mathrm{d} x\) and \(\int_{-\infty}^{a}f(x)\,\mathrm{d} x\) are convergent, then we define
\begin{equation}
\int_{-\infty}^{\infty}f(x)\,\mathrm{d} x=\int_{-\infty}^{a}f(x)\,\mathrm{d} x+\int_{a}^{\infty}f(x)\mathrm{d} x,
\end{equation}
and we say that \(\int_{-\infty}^{\infty}f(x)\,\mathrm{d} x\) is /convergent/. Otherwise, we say that the improper integral is /divergent/.

** Sequences and Series
:PROPERTIES:
:CUSTOM_ID: sec-Sequences-and-Series
:END: 

A /sequence/ is an ordered list of numbers, \(a_{1}\), \(a_{2}\), \(a_{3}\), ..., \(a_{n} =\left(a_{k}\right)_{k=1}^{n}\). A sequence may be finite or infinite. In the latter case we write \(a_{1}\), \(a_{2}\), \(a_{3}\), ... \( =\left(a_{k}\right)_{k=1}^{\infty}\). We say that /the infinite sequence/ \(\left(a_{k}\right)_{k=1}^{\infty}\) /converges to the finite limit/ L, and we write
\begin{equation}
\lim_{k\to\infty}a_{k} = L,
\end{equation}
if for every \(\epsilon > 0\) there exists an integer \(N \geq 1\) such that \(|a_{k} - L| < \epsilon\) for all \(k \geq N\). We say that /the infinite sequence/ \(\left(a_{k}\right)_{k=1}^{\infty}\) /diverges to/ \(+\infty\) (or \( -\infty\)) if for every \(M\geq0\) there exists an integer \(N\geq1\) such that \(a_{k} \geq M\) for all \(k \geq N\) (or \(a_{k} \leq - M\) for all \(k \geq N\)).

*** Finite Series

\begin{equation}
\label{eq-gauss-series}
\sum_{k=1}^{n}k=1+2+\cdots+n=\frac{n(n+1)}{2}
\end{equation}
\begin{equation}
\label{eq-gauss-series-sq}
\sum_{k=1}^{n}k^{2}=1^{2}+2^{2}+\cdots+n^{2}=\frac{n(n+1)(2n+3)}{6}
\end{equation}

**** The Binomial Series
\begin{equation}
\label{eq-binom-series}
\sum_{k=0}^{n}{n \choose k}\, a^{n-k}b^{k}=(a+b)^{n}
\end{equation}

*** Infinite Series

Given an infinite sequence of numbers \(a_{1}\), \(a_{2}\), \(a_{3}\), ...\(=\left(a_{k}\right)_{k=1}^{\infty}\), let \(s_{n}\) denote the /partial sum/ of the first \(n\) terms:
\begin{equation}
s_{n}=\sum_{k=1}^{n}a_{k}=a_{1}+a_{2}+\cdots+a_{n}.
\end{equation}
If the sequence \(\left(s_{n}\right)_{n=1}^{\infty}\) converges to a finite number \(S\) then we say that the infinite series \(\sum_{k}a_{k}\) is /convergent/ and write
\begin{equation}
\sum_{k=1}^{\infty}a_{k}=S.
\end{equation}
Otherwise we say the infinite series is /divergent/.

*** Rules for Series

Let \(\left(a_{k}\right)_{k=1}^{\infty}\) and \(\left(b_{k}\right)_{k=1}^{\infty}\) be infinite sequences and let \(c\) be a constant.

\begin{equation}
\sum_{k=1}^{\infty}ca_{k}=c\sum_{k=1}^{\infty}a_{k}
\end{equation}
\begin{equation}
\sum_{k=1}^{\infty}(a_{k}\pm b_{k})=\sum_{k=1}^{\infty}a_{k}\pm\sum_{k=1}^{\infty}b_{k}
\end{equation}

In both of the above the series on the left is convergent if the series on the right is (are) convergent.

**** The Geometric Series
\begin{equation}
\label{eq-geom-series}
\sum_{k=0}^{\infty} x^{k} = \frac{1}{1 - x},\quad |x| < 1.
\end{equation}

**** The Exponential Series
\begin{equation}
\label{eq-exp-series}
\sum_{k=0}^{\infty}\frac{x^{k}}{k!} = \mathrm{e}^{x},\quad -\infty < x < \infty.
\end{equation}

Other Series
\begin{equation}
\label{eq-negbin-series}
\sum_{k=0}^{\infty}{m+k-1 \choose m-1}x^{k}=\frac{1}{(1-x)^{m}},\quad |x|<1.
\end{equation}

\begin{equation}
\label{eq-log-series}
-\sum_{k=1}^{\infty}\frac{x^{n}}{n}=\ln(1-x),\quad |x| < 1.
\end{equation}
\begin{equation}
\label{eq-binom-series-infinite}
\sum_{k=0}^{\infty}{n \choose k}x^{k}=(1+x)^{n},\quad |x| < 1.
\end{equation}

*** Taylor Series

If the function \(f\) has a /power series/ representation at the point \(a\) with radius of convergence \(R>0\), that is, if
\begin{equation}
f(x)=\sum_{k=0}^{\infty}c_{k}(x-a)^{k},\quad |x - a| < R,
\end{equation}
for some constants \(\left(c_{k}\right)_{k=0}^{\infty}\), then \(c_{k}\) must be
\begin{equation}
c_{k}=\frac{f^{(k)}(a)}{k!},\quad k=0,1,2,\ldots
\end{equation}
Furthermore, the function \(f\) is differentiable on the open interval \((a-R,\, a+R)\) with
\begin{equation}
f'(x)=\sum_{k=1}^{\infty}kc_{k}(x-a)^{k-1},\quad |x-a| < R,
\end{equation}
\begin{equation}
\int f(x)\,\mathrm{d} x=C+\sum_{k=0}^{\infty}c_{k}\frac{(x-a)^{k+1}}{k+1},\quad |x-a| < R,
\end{equation}
in which case both of the above series have radius of convergence \(R\).

** The Gamma Function
:PROPERTIES:
:CUSTOM_ID: sec-The-Gamma-Function
:END: 

The /Gamma function/ \(\Gamma\) will be defined in this book according to the formula
\begin{equation}
\Gamma(\alpha)=\int_{0}^{\infty}x^{\alpha-1}\mathrm{e}^{-x}\:\mathrm{d} x,\quad \mbox{for }\alpha > 0.
\end{equation}

#+begin_fact
Properties of the Gamma Function:
- \(\Gamma(\alpha)=(\alpha - 1)\Gamma(\alpha - 1)\) for any \(\alpha > 1\), and so \(\Gamma(n)=(n-1)!\) for any positive integer \(n\).
- \(\Gamma(1/2)=\sqrt{\pi}\).
#+end_fact

** Linear Algebra
:PROPERTIES:
:CUSTOM_ID: sec-Linear-Algebra
:END: 

*** Matrices
A /matrix/ is an ordered array of numbers or expressions; typically we write \(\mathbf{A}=\begin{pmatrix}a_{ij}\end{pmatrix}\) or \(\mathbf{A}=\begin{bmatrix}a_{ij}\end{bmatrix}\). If \(\mathbf{A}\) has \(m\) rows and \(n\) columns then we write
\begin{equation}
\mathbf{A}_{\mathrm{m}\times\mathrm{n}}=\begin{bmatrix}a_{11} & a_{12} & \cdots & a_{1n}\\
a_{21} & a_{22} & \cdots & a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{m1} & a_{m2} & \cdots & a_{mn}\end{bmatrix}.
\end{equation}
The /identity matrix/ \(\mathbf{I}_{\mathrm{n}\times\mathrm{n}}\) is an \(\mathrm{n}\times\mathrm{n}\) matrix with zeros everywhere except for 1's along the main diagonal: 
\begin{equation}
\mathbf{I}_{\mathrm{n}\times\mathrm{n}}=\begin{bmatrix}1 & 0 & \cdots & 0\\
0 & 1 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & 1\end{bmatrix}.
\end{equation}
and the matrix with ones everywhere is denoted \(\mathbf{J}_{\mathrm{n}\times\mathrm{n}}\):
\begin{equation}
\mathbf{J}_{\mathrm{n}\times\mathrm{n}}=\begin{bmatrix}1 & 1 & \cdots & 1\\
1 & 1 & \cdots & 1\\
\vdots & \vdots & \ddots & \vdots\\
1 & 1 & \cdots & 1\end{bmatrix}.
\end{equation}

A /vector/ is a matrix with one of the dimensions equal to one, such as \(\mathbf{A}_{\mathrm{m}\times1}\) (a column vector) or \(\mathbf{A}_{\mathrm{1}\times\mathrm{n}}\) (a row vector). The /zero vector/ \(\mathbf{0}_{\mathrm{n}\times1}\)
is an \(\mathrm{n}\times1\) matrix of zeros:
\begin{equation}
\mathbf{0}_{\mathrm{n}\times1}=\begin{bmatrix}0 & 0 & \cdots & 0\end{bmatrix}^{\mathrm{T}}.
\end{equation}

The /transpose/ of a matrix \(\mathbf{A}=\begin{pmatrix}a_{ij}\end{pmatrix}\) is the matrix \(\mathbf{A}^{\mathrm{T}}=\begin{pmatrix}a_{ji}\end{pmatrix}\), which is just like \(\mathbf{A}\) except the rows are columns and the columns are rows. The matrix \(\mathbf{A}\) is said to be /symmetric/ if \(\mathbf{A}^{\mathrm{T}}=\mathbf{A}\). Note that \(\left(\mathbf{A}\mathbf{B}\right)^{\mathrm{T}}=\mathbf{B}^{\mathrm{T}}\mathbf{A}^{\mathrm{T}}\).

The /trace/ of a square matrix \(\mathbf{A}\) is the sum of its diagonal elements: \(\mathrm{tr}(\mathbf{A})=\sum_{i}a_{ii}\). 

The /inverse/ of a square matrix \(\mathbf{A}_{\mathrm{n}\times\mathrm{n}}\) (when it exists) is the unique matrix denoted \(\mathbf{A}^{-1}\) which satisfies \(\mathbf{A}\mathbf{A}^{-1}=\mathbf{A}^{-1}\mathbf{A}=\mathbf{I}_{\mathrm{n}\times\mathrm{n}}\). If \(\mathbf{A}^{-1}\) exists then we say \(\mathbf{A}\) is /invertible/, or /nonsingular/. Note that \(\left(\mathbf{A}^{\mathrm{T}}\right)^{-1}=\left(\mathbf{A}^{\mathrm{-1}}\right)^{\mathrm{T}}\).
#+begin_fact
The inverse of the \(2\times2\) matrix
\begin{equation}
\mathbf{A}=\begin{bmatrix}a & b\\
c & d\end{bmatrix}\quad \mbox{is}\quad \mathbf{A}^{-1}=\frac{1}{ad-bc}\begin{bmatrix}d & -b\\
-c & a\end{bmatrix},
\end{equation}
provided \(ad-bc\neq0\).
#+end_fact

*** Determinants
#+begin_defn
The /determinant/ of a square matrix \(\mathbf{A}_{\mathrm{n}\times n}\) is denoted \(\mathrm{det}(\mathbf{A})\) or \(|\mathbf{A}|\) and is defined recursively by
\begin{equation}
\mathrm{det}(\mathbf{A})=\sum_{i=1}^{n}(-1)^{i+j}a_{ij}\,\mathrm{det}(\mathbf{M}_{ij}),
\end{equation}
where \(\mathbf{M}_{ij}\) is the submatrix formed by deleting the \(i^{\mathrm{th}}\) row and \(j^{\mathrm{th}}\) column of \(\mathbf{A}\). We may choose any fixed \(1\leq j\leq n\) we wish to compute the determinant; the final result is independent of the \(j\) chosen.
#+end_defn
#+begin_fact
The determinant of the \(2\times2\) matrix
\begin{equation}
\mathbf{A}=\begin{bmatrix}a & b\\
c & d\end{bmatrix}\quad \mbox{is} \quad |\mathbf{A}|=ad-bc.
\end{equation}
#+end_fact

#+begin_fact
A square matrix \(\mathbf{A}\) is nonsingular if and only if \(\mathrm{det}(\mathbf{A})\neq0\).
#+end_fact

*** Positive (Semi)Definite
If the matrix \(\mathbf{A}\) satisfies \(\mathbf{x^{\mathrm{T}}}\mathbf{A}\mathbf{x}\geq0\) for all vectors \(\mathbf{x}\neq\mathbf{0}\), then we say that \(\mathbf{A}\) is /positive semidefinite/. If strict inequality holds for all \(\mathbf{x}\neq\mathbf{0}\), then \(\mathbf{A}\) is /positive definite/. The connection to statistics is that covariance matrices (see Chapter [[cha-Multivariable-Distributions][Multivariate Distributions]]) are always positive semidefinite, and many of them are even positive definite.

** Multivariable Calculus
:PROPERTIES:
:CUSTOM_ID: sec-Multivariable-Calculus
:END: 

*** Partial Derivatives
If \(f\) is a function of two variables, its /first-order partial derivatives/ are defined by
\begin{equation}
\frac{\partial f}{\partial x}=\frac{\partial}{\partial x}f(x,y)=\lim_{h\to0}\frac{f(x+h,\, y)-f(x,y)}{h}
\end{equation}
and
\begin{equation}
\frac{\partial f}{\partial y}=\frac{\partial}{\partial y}f(x,y)=\lim_{h\to0}\frac{f(x,\, y+h)-f(x,y)}{h},
\end{equation}
provided these limits exist. The /second-order partial derivatives/ of \(f\) are defined by
\begin{equation}
\frac{\partial^{2}f}{\partial x^{2}}=\frac{\partial}{\partial x}\left(\frac{\partial f}{\partial x}\right),\quad \frac{\partial^{2}f}{\partial y^{2}}=\frac{\partial}{\partial y}\left(\frac{\partial f}{\partial y}\right),\quad \frac{\partial^{2}f}{\partial x\partial y}=\frac{\partial}{\partial x}\left(\frac{\partial f}{\partial y}\right),\quad \frac{\partial^{2}f}{\partial y\partial x}=\frac{\partial}{\partial y}\left(\frac{\partial f}{\partial x}\right).
\end{equation}
In many cases (and for all cases in this book) it is true that
\begin{equation}
\frac{\partial^{2}f}{\partial x\partial y}=\frac{\partial^{2}f}{\partial y\partial x}.
\end{equation}

*** Optimization
An function \(f\) of two variables has a /local maximum/ at \((a,b)\) if \(f(x,y)\geq f(a,b)\) for all points \((x,y)\) near \((a,b)\), that is, for all points in an open disk centered at \((a,b)\). The number \(f(a,b)\) is then called a /local maximum value/ of \(f\). The function \(f\) has a /local minimum/ if the same thing happens with the inequality reversed.

Suppose the point \((a,b)\) is a /critical point/ of \(f\), that is, suppose \((a,b)\) satisfies 
\begin{equation}
\frac{\partial f}{\partial x}(a,b)=\frac{\partial f}{\partial y}(a,b)=0.
\end{equation}
Further suppose \(\frac{\partial^{2}f}{\partial x^{2}}\) and \(\frac{\partial^{2}f}{\partial y^{2}}\) are continuous near \((a,b)\). Let the /Hessian matrix/ \(H\) (not to be confused with the /hat matrix/ \(\mathbf{H}\) of Chapter [[cha-multiple-linear-regression][Multiple Linear Regression]]) be defined by
\begin{equation}
H = 
\begin{bmatrix}
\frac{\partial^{2}f}{\partial x^{2}} & \frac{\partial^{2}f}{\partial x\partial y}\\
\frac{\partial^{2}f}{\partial y\partial x} & \frac{\partial^{2}f}{\partial y^{2}}
\end{bmatrix}.
\end{equation}
We use the following rules to decide whether \((a,b)\) is an /extremum/ (that is, a local minimum or local maximum) of \(f\).
- If \(\mbox{det}(H)>0\) and \(\frac{\partial^{2}f}{\partial x^{2}}(a,b)>0\), then \((a,b)\) is a local minimum of \(f\).
- If \(\mbox{det}(H)>0\) and \(\frac{\partial^{2}f}{\partial x^{2}}(a,b)<0\), then \((a,b)\) is a local maximum of \(f\).
- If \(\mbox{det}(H)<0\), then \((a,b)\) is a /saddle point/ of \(f\) and so is not an extremum of \(f\).
- If \(\mbox{det}(H)=0\), then we do not know the status of \((a,b)\); it might be an extremum or it might not be.

*** Double and Multiple Integrals
Let \(f\) be defined on a rectangle \(R=[a,b]\times[c,d]\), and for each \(m\) and \(n\) divide \([a,b]\) (respectively \([c,d]\)) into subintervals \([x_{j},x_{j+1}]\), \(i=0,1,\ldots,m-1\) (respectively \([y_{i},y_{i+1}]\)) of length \(\Delta x_{j}=(b-a)/m\) (respectively \(\Delta y_{i}=(d-c)/n\)) where \(x_{0}=a\) and \(x_{m}=b\) (and \(y_{0}=c\) and \(y_{n}=d\) ), and let \(x_{j}^{\ast}\) (\(y_{i}^{\ast}\)) be any points chosen from their respective subintervals. Then the /double integral/ of \(f\) over the rectangle \(R\) is
\begin{equation}
\iintop_{R}f(x,y)\,\mathrm{d} A=\intop_{c}^{d}\!\!\!\intop_{a}^{b}f(x,y)\,\mathrm{d} x\mathrm{d} y=\lim_{m,n\to\infty}\sum_{i=1}^{n}\sum_{j=1}^{m}f(x_{j}^{\ast},y_{i}^{\ast})\Delta x_{j}\Delta y_{i},
\end{equation}
provided this limit exists. Multiple integrals are defined in the same way just with more letters and sums.

*** Bivariate and Multivariate Change of Variables
Suppose we have a transformation
#+latex: \footnote{For our purposes \(T\) is in fact the /inverse/ of a one-to-one transformation that we are initially given. We usually start with functions that map \((x,y) \longmapsto (u,v)\), and one of our first tasks is to solve for the inverse transformation that maps \((u,v)\longmapsto(x,y)\). It is this inverse transformation which we are calling \(T\).}
\(T\) that maps points \((u,v)\) in a set \(A\) to points \((x,y)\) in a set \(B\). We typically write \(x=x(u,v)\) and \(y=y(u,v)\), and we assume that \(x\) and \(y\) have continuous first-order partial derivatives. We say that \(T\) is /one-to-one/ if no two distinct \((u,v)\) pairs get mapped to the same \((x,y)\) pair; in this book, all of our multivariate transformations \(T\) are one-to-one.

The /Jacobian/ (pronounced ``yah-KOH-bee-uhn'') of \(T\) is denoted by \(\partial(x,y)/\partial(u,v)\) and is defined by the determinant of the following matrix of partial derivatives:
\begin{equation}
\frac{\partial(x,y)}{\partial(u,v)}=\left|
\begin{array}{cc}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v}\\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
\end{array}
\right|=\frac{\partial x}{\partial u}\frac{\partial y}{\partial v}-\frac{\partial x}{\partial v}\frac{\partial y}{\partial u}.
\end{equation}

If the function \(f\) is continuous on \(A\) and if the Jacobian of \(T\) is nonzero except perhaps on the boundary of \(A\), then 
\begin{equation}
\iint_{B}f(x,y)\,\mathrm{d} x\,\mathrm{d} y=\iint_{A}f\left[x(u,v),\, y(u,v)\right]\ \left|\frac{\partial(x,y)}{\partial(u,v)}\right|\mathrm{d} u\,\mathrm{d} v.
\end{equation} 

A multivariate change of variables is defined in an analogous way: the one-to-one transformation \(T\) maps points \((u_{1},u_{2},\ldots,u_{n})\) to points \((x_{1},x_{2},\ldots,x_{n})\), the Jacobian is the determinant of the \(\mathrm{n}\times\mathrm{n}\) matrix of first-order partial derivatives of \(T\) (lined up in the natural manner), and instead of a double integral we have a multiple integral over multidimensional sets \(A\) and \(B\).

* Writing Reports with \(\mathsf{R}\)                              :appendix:
:PROPERTIES:
:CUSTOM_ID: cha-Writing-Reports-with
:END:

Perhaps the most important part of a statistician's job once the analysis is complete is to communicate the results to others. This is usually done with some type of report that is delivered to the client, manager, or administrator. Other situations that call for reports include term papers, final projects, thesis work, /etc/. This chapter is designed to pass along some tips about writing reports once the work is completed with \(\mathsf{R}\).

** What to Write
:PROPERTIES:
:CUSTOM_ID: sec-What-to-Write
:END: 

It is possible to summarize this entire appendix with only one sentence: /the statistician's goal is to communicate with others/. To this end, there are some general guidelines that I give to students which are based on an outline originally written and shared with me by Dr. G. Andy Chang.

*** Basic Outline for a Statistical Report
1. Executive Summary (a one page description of the study and conclusion) 
1. Introduction 
   1. What is the question, and why is it important?
   1. Is the study observational or experimental?
   1. What are the hypotheses of interest to the researcher?
   1. What are the types of analyses employed? (one sample \(t\)-test, paired-sample \(t\)-test, ANOVA, chi-square test, regression, ...) 
1. Data Collection 
   1. Describe how the data were collected in detail.
   1. Identify all variable types: quantitative, qualitative, ordered or nominal (with levels), discrete, continuous.
   1. Discuss any limitations of the data collection procedure. Look carefully for any sources of bias.
1. Summary Information
   1. Give numeric summaries of all variables of interest.
      1. Discrete: (relative) frequencies, contingency tables, odds ratios, /etc/. 
      1. Continuous: measures of center, spread, shape.
   1. Give visual summaries of all variables of interest.
      1. Side-by-side boxplots, scatterplots, histograms, /etc/.
   1. Discuss any unusual features of the data (outliers, clusters, granularity, /etc/.)
   1. Report any missing data and identify any potential problems or bias.
1. Analysis 
   1. State any hypotheses employed, and check the assumptions. 
   1. Report test statistics, /p/-values, and confidence intervals. 
   1. Interpret the results in the context of the study.
   1. Attach (labeled) tables and/or graphs and make reference to them in the report as needed. 
1. Conclusion
   1. Summarize the results of the study. What did you learn? 
   1. Discuss any limitations of the study or inferences.
   1. Discuss avenues of future research suggested by the study. 

** How to Write It with R
:PROPERTIES:
:CUSTOM_ID: sec-How-to-Write
:END: 

Once the decision has been made what to write, the next task is to typeset the information to be shared. To do this the author will need to select software to use to write the documents. There are many options available, and choosing one over another is sometimes a matter of taste. But not all software were created equal, and \(\mathsf{R}\) plays better with some applications than it does with others. 
In short, \(\mathsf{R}\) does great with \LaTeX{} and there are many resources available to make writing a document with \(\mathsf{R}\) and \LaTeX{} easier. But \LaTeX{} is not for the beginner, and there are other word processors which may be acceptable depending on the circumstances.

*** Microsoft\(\circledR\) Word
It is a fact of life that Microsoft\(\circledR\) Windows is currently the most prevalent desktop operating system on the planet. Those who own Windows also typically own some version of Microsoft Office, thus Microsoft Word is the default word processor for many, many people.  

The standard way to write an \(\mathsf{R}\) report with Microsoft\(\circledR\) Word is to generate material with \(\mathsf{R}\) and then copy-paste the material at selected places in a Word document. An advantage to this approach is that Word is nicely designed to make it easy to copy-and-paste from =RGui= to the Word document.

A disadvantage to this approach is that the R input/output needs to be edited manually by the author to make it readable for others. Another disadvantage is that the approach does not work on all operating systems (not on Linux, in particular). Yet another disadvantage is that Microsoft\(\circledR\) Word is proprietary, and as a result, \(\mathsf{R}\) does not communicate with Microsoft\(\circledR\) Word as well as it does with other software as we shall soon see.

Nevertheless, if you are going to write a report with Word there are some steps that you can take to make the report more amenable to the reader. 

1. Copy and paste graphs into the document. You can do this by right clicking on the graph and selecting =Copy as bitmap=, or =Copy as metafile=, or one of the other options. Then move the cursor to the document where you want the picture, right-click, and select =Paste=.
1. Resize (most) pictures so that they take up no more than 1/2 page. You may want to put graphs side by side; do this by inserting a table and placing the graphs inside the cells.
1. Copy selected \(\mathsf{R}\) input and output to the Word document. All code should be separated from the rest of the writing, except when specifically mentioning a function or object in a sentence.
1. The font of \(\mathsf{R}\) input/output should be Courier New, or some other monowidth font (not Times New Roman or Calibri); the default font size of =12= is usually too big for \(\mathsf{R}\) code and should be reduced to, for example, =10pt=.

It is also possible to communicate with \(\mathsf{R}\) through OpenOffice.org, which can export to the proprietary (=.doc=) format.

*** OpenOffice.org and \texttt{odfWeave}
OpenOffice.org (OO.o) is an open source desktop productivity suite which mirrors Microsoft\(\circledR\) Office. It is especially nice because it works on all operating systems. OO.o can read most document formats, and in particular, it will read =.doc= files. The standard OO.o file extension for documents is =.odt=, which stands for ``open document text''.

The =odfWeave=  package \cite{odfWeave} provides a way to generate an =.odt= file with \(\mathsf{R}\) input and output code formatted correctly and inserted in the correct places, without any additional work. In this way, one does not need to worry about all of the trouble of typesetting \(\mathsf{R}\) output. Another advantage of =odfWeave= is that it allows you to generate the report dynamically; if the data underlying the report change or are updated, then a few clicks (or commands) will generate a brand new report.

One disadvantage is that the source =.odt=  file is not easy to read, because it is difficult to visually distinguish the noweb parts (where the \(\mathsf{R}\) code is) from the non-noweb parts. This can be fixed by manually changing the font of the noweb sections to, for instance, Courier font, size =10pt=. But it is extra work. It would be nice if a program would discriminate between the two different sections and automatically typeset the respective parts in their correct fonts. This is one of the advantages to \LyX{}.

Another advantage of OO.o is that even after you have generated the outfile, it is fully editable just like any other =.odt= document. If there are errors or formatting problems, they can be fixed at any time.

Here are the basic steps to typeset a statistical report with OO.o.

1. Write your report as an =.odt= document in OO.o just as you would any other document. Call this document =infile.odt=, and make sure that it is saved in your working directory. 
1. At the places you would like to insert \(\mathsf{R}\) code in the document, write the code chunks in the following format:
   \texttt{<\textcompwordmark{}<>\textcompwordmark{}>=}~\\
   \texttt{x <- rnorm(10)}~\\
   \texttt{mean(x)}~\\
   \texttt{@}
   or write whatever code you want between the symbols \texttt{<\textcompwordmark{}<>\textcompwordmark{}>=} and \texttt{@}.
1. Open \(\mathsf{R}\) and type the following:
   #+begin_src R :exports code :eval never
   library("odfWeave")
   odfWeave(file = "infile.odt", dest = "outfile.odt")
   #+end_src
1. The compiled (=.odt=) file, complete with all of the \(\mathsf{R}\) output automatically inserted in the correct places, will now be the file =outfile.odt= located in the working directory. Open =outfile.odt=, examine it, modify it, and repeat if desired. 

There are all sorts of extra things that you can do. For example, the \(\mathsf{R}\) commands can be suppressed with the tag \texttt{<\textcompwordmark{}<echo = FALSE>\textcompwordmark{}>=}, and the \(\mathsf{R}\) output may be hidden with \texttt{<\textcompwordmark{}<results = hide>\textcompwordmark{}>=}. See the =odfWeave= package documentation for details.

*** Sweave and \protect\LaTeX{}

This approach is nice because it works for all operating systems. One can quite literally typeset /anything/ with \LaTeX{}. All of this power comes at a price, however. The writer must learn the \LaTeX{} language which is a nontrivial enterprise. Even given the language, if there is a single syntax error, or a single delimeter missing in the entire document, then the whole thing breaks.

\LaTeX{} can do anything, but it is relatively difficult to learn  and very grumpy about syntax errors and delimiter matching. There are however programs useful for formatting LaTeX.

A disadvantage is that you cannot see the mathematical formulas until you run the whole file with \LaTeX{}.

A disadvantage is that figures and tables are relatively difficult.

There are programs to make the process easier: AUC\TeX{}.

dev.copy2eps, also dev.copy2pdf

[[http://www.stat.uni-muenchen.de/~leisch/Sweave/]]

*** Sweave and \protect\LyX{}
This approach is nice because it works for all operating  systems. It gives you everything from the last section and makes it easier to use \LaTeX{}. That being said, it is better to know \LaTeX{} already when migrating to \LyX{}, because you understand all of the machinery going on under the hood.

Program Listings and the \(\mathsf{R}\) language
[[http://gregor.gorjanc.googlepages.com/lyx-sweave]]

** Formatting Tables
:PROPERTIES:
:CUSTOM_ID: sec-Formatting-Tables
:END: 

The prettyR package \cite{prettyR}.

The Hmisc package \cite{Hmisc}.

#+begin_src R :exports both :results output pp 
summary(cbind(Sepal.Length, Sepal.Width) ~ Species, data = iris)
#+end_src

There is a =method= argument to =summary=, which is set to \texttt{method = "response"} by default. There are two other methods for summarizing data: =reverse= and =cross=. See =?summary.formula= or the [[http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/StatReport][following document]] from Frank Harrell for more details.

** Other Formats
:PROPERTIES:
:CUSTOM_ID: sec-Other-Formats
:END: 

HTML and prettyR
R2HTML

* Instructions for Instructors                                     :appendix:
:PROPERTIES:
:CUSTOM_ID: cha-Instructions-for-Instructors
:END:

\noun{WARNING: this appendix is not applicable until the exercises
have been written.} 
#+begin_src R :exports none :results silent
set.seed(095259)
#+end_src


Probably this /book/ could more accurately be described as /software/. The reason is that the document is one big random variable, one observation realized out of millions. It is electronically distributed under the GNU FDL, and ``free'' in both senses: speech and beer.  

There are four components to \IPSUR: the Document, the Program used to generate it, the \(\mathsf{R}\) package that holds the Program, and the Ancillaries that accompany it.

The majority of the data and exercises have been designed to be randomly generated. Different realizations of this book will have different graphs and exercises throughout. The advantage of this approach is that a teacher, say, can generate a unique version to be used in his/her class. Students can do the exercises and the teacher will have the answers to all of the problems in their own, unique solutions manual. Students may download a different solutions manual online somewhere else, but none of the answers will match the teacher's copy. 

Then next semester, the teacher can generate a /new/ book and the problems will be more or less identical, except the numbers will be changed. This means that students from different sections of the same class will not be able to copy from one another quite so easily. The same will be true for similar classes at different institutions. Indeed, as long as the instructor protects his/her /key/ used to generate the book, it will be difficult for students to crack the code. And if they are industrious enough at this level to find a way to (a) download and decipher my version's source code, (b) hack the teacher's password somehow, and (c) generate the teacher's book with all of the answers, then they probably should be testing out of an ``Introduction to Probability and Statistics'' course, anyway. 

The book that you are reading was created with a random seed which was set at the beginning. The original seed is 42. You can choose your own seed, and generate a new book with brand new data for the text and exercises, complete with updated manuals. A method I recommend for finding a seed is to look down at your watch at this very moment and record the 6 digit hour, minute, and second (say, 9:52:59am): choose that for a seed
#+latex: \footnote{In fact, this is essentially the method used by \(\mathsf{R}\) to select an initial random seed, see =?set.seed=. However, the instructor should set the seed manually so that the book can be regenerated at a later time, if necessary.}.
This method already provides for over 43,000 books, without taking military time into account. An alternative would be to go to \(\mathsf{R}\) and type 

#+begin_src R
options(digits = 16)
runif(1)
#+end_src

Now choose \( SRC_R{set.seed(095259); options(digits = 16); runif(1)*10^16} \)as your secret seed... write it down in a safe place and do not share it with anyone. Next generate the book with your seed using \LyX{}-Sweave or Sweave-\LaTeX{}. You may wish to also generate Student and Instructor Solution Manuals. Guidance regarding this is given below in the How to Use This Document section.

** Generating This Document
:PROPERTIES:
:CUSTOM_ID: sec-Generating-This-Document
:END: 

You will need three (3) things to generate this document for yourself, in addition to a current \(\mathsf{R}\) distribution which at the time of this writing is \texttt{ SRC_R{version$version.string} } :
1. a LaTeX distribution,
2. Sweave (which comes with \(\mathsf{R}\) automatically), and 
3. \LyX (optional, but recommended).

We will discuss each of these in turn.

- LaTeX: ::  The distribution used by the present author was \TeX{} Live (\url{http://www.tug.org/texlive/}). There are plenty of other perfectly suitable LaTeX distributions depending on your operating system, one such alternative being [[http://miktex.org/][MikTeX]] for Microsoft Windows. 
- Sweave: :: If you have \(\mathsf{R}\) installed, then the required Sweave files are already on your system... somewhere. The only problems that you may have are likely associated with making sure that your LaTeX distribution knows where to find the =Sweave.sty= file. See the Sweave Homepage (\url{http://www.statistik.lmu.de/~leisch/Sweave/}) for guidance on how to get it working on your particular operating system.
- LyX: :: Strictly speaking, LyX is not needed to generate this document. But this document was written stem to stern with LyX, taking full advantage of all of the bells and whistles that LyX has to offer over plain \LaTeX{} editors. And it's free. See the LyX homepage (\url{http://www.lyx.org/}) for additional information. If you decide to give LyX a try, then you will need to complete some extra steps to coordinate Sweave and LyX with each other. Luckily, Gregor Gorjanc has a website and an \(\mathsf{R}\) News article \cite{Gorjanc2008} to help you do exactly that. See the LyX-Sweave homepage (\url{http://gregor.gorjanc.googlepages.com/lyx-sweave}) for details.

An attempt was made to not be extravagant with fonts or packages so that a person would not need the entire =CTAN= (or =CRAN=) installed on their personal computer to generate the book. Nevertheless, there are a few extra packages required. These packages are listed in the =preamble= of =IPSUR.Rnw=, =IPSUR.tex=, and =IPSUR.lyx=.

** How to Use This Document
:PROPERTIES:
:CUSTOM_ID: sec-How-to-Use-Document
:END: 

The easiest way to use this document is to install the =IPSUR= package from =CRAN= and be all done. This way would be acceptable if there is another, primary, text being used for the course and \IPSUR\ is only meant to play a supplementary role.

If you plan for \IPSUR\ to serve as the primary text for your course, then it would be wise to generate your own version of the document. You will need the source code for the Program which can be downloaded from =CRAN= or the \IPSUR\ website. Once the source is obtained there are four (4) basic steps to generating your own copy.

1. Randomly select a secret ``seed'' of integers and replace my seed of 42 with your own seed. 
1. Make sure that the =maintext= branch is turned =ON= and also make sure that both the =solutions= branch and the =answers= branch are turned =OFF=. Use LyX or your LaTeX editor with Sweave to generate your unique PDF copy of the book and distribute this copy to your students. (See the LyX User's Guide to learn more about branches; the ones referenced above can be found under =Document= \(\triangleright\) =Settings= \(\triangleright\) =Branches=.)
1. Turn the =maintext= branch =OFF= and the =solutions= branch =ON=. Generate a ``Student Solutions Manual'' which has complete solutions to selected exercises and distribute the PDF to the students. 
1. Leave the =solutions= branch =ON= and also turn the =answers= branch =ON= and generate an ``Instructor Solutions and Answers Manual'' with full solutions to some of the exercises and just answers to the remaining exercises. Do NOT distribute this to the students -- unless of course you want them to have the answers to all of the problems.  

To make it easier for those people who do not want to use LyX (or for whatever reason cannot get it working), I have included three (3) Sweave files corresponding to the main text, student solutions, and instructor answers, that are included in the \IPSUR\ source package in the =/tex= subdirectory. In principle it is possible to change the seed and generate the three parts separately with only Sweave and LaTeX. This method is not recommended by me, but is perhaps desirable for some people.

*** Generating Quizzes and Exams
- You can copy-paste selected exercises from the text, put them together, and you have a quiz. Since the numbers are randomly generated you do not need to worry about different semesters. And you will have answer keys already for all of your QUIZZES and EXAMS, too. 

** Ancillary Materials
:PROPERTIES:
:CUSTOM_ID: sec-Ancillary-Materials
:END: 

In addition to the main text, student manual, and instructor manual, there are two other ancillaries. IPSUR.R, and IPSUR.RData.

** Modifying This Document
:PROPERTIES:
:CUSTOM_ID: sec-Modifying-This-Document
:END: 

Since this document is released under the GNU-FDL, you are free to modify this document however you wish (in accordance with the license -- see Appendix [[cha-GNU-Free-Documentation][GNU FDL]]). The immediate benefit of this is that you can generate the book, with brand new problem sets, and distribute it to your students simply as a PDF (in an email, for instance). As long as you distribute less than 100 such /Opaque/ copies, you are not even required by the GNU-FDL to share your /Transparent/ copy (the source code with the secret key) that you used to generate them. Next semester, choose a new key and generate a new copy to be distributed to the new class. 

#+begin_quote
But more generally, if you are not keen on the way I explained (or failed to explain) something, then you are _free_ to rewrite it. If you would like to cover more (or less) material, then you are _free_ to add (or delete) whatever Chapters/Sections/Paragraphs that you wish. And since you have the source code, you do not need to retype the wheel. 
#+end_quote

Some individuals will argue that the nature of a statistics textbook like this one, many of the exercises being randomly generated /by design/, does a disservice to the students because the exercises do not use real-world data. That is a valid criticism... but in my case the benefits outweighed the detriments and I moved forward to incorporate static data sets whenever it was feasible and effective. Frankly, and most humbly, the only response I have for those individuals is: ``Please refer to the preceding paragraph.''

* =RcmdrTestDrive= Story                                           :appendix:
:PROPERTIES:
:CUSTOM_ID: cha-RcmdrTestDrive-Story
:END:

The goal of =RcmdrTestDrive= was to have a data set sufficiently rich in the types of data represented such that a person could load it into the \(\mathsf{R}\) Commander and be able to explore all of =Rcmdr='s menu options at once. I decided early-on that an efficient way to do this would be to generate the data set randomly, and later add to the list of variables as more =Rcmdr= menu options became available. Generating the data was easy, but generating a story that related all of the respective variables proved to be less so.

In the Summer of 2006 I gave a version of the raw data and variable names to my STAT 3743 Probability and Statistics class and invited each of them to write a short story linking all of the variables together in a coherent narrative. No further direction was given. 

The most colorful of those I received was written by Jeffery Cornfield, submitted July 12, 2006, and is included below with his permission. It was edited slightly by the present author and updated to respond dynamically to the random generation of =RcmdrTestDrive=; otherwise, the story has been unchanged. 

** Case File: ALU-179 ``Murder Madness in Toon Town”
#+begin_quote
*WARNING* 
This file is not for the faint of heart, dear reader, because it is filled with horrible images that will haunt your nightmares. If you are weak of stomach, have irritable bowel syndrome, or are simply paranoid, DO NOT READ FURTHER! Otherwise, read at your own risk.
#+end_quote

One fine sunny day, Police Chief R. Runner called up the forensics department at Acme-Looney University. There had been \( SRC_R{dim(RcmdrTestDrive)[ 1 ] - 2} \) murders in the past \( SRC_R{round((dim(RcmdrTestDrive)[ 1 ] - 2)/24)} \) days, approximately one murder every hour, of many of the local Human workers, shop keepers, and residents of Toon Town. These alarming rates threatened to destroy the fragile balance of Toon and Human camaraderie that had developed in Toon Town. 

Professor Twee T. Bird, a world-renowned forensics specialist and a Czechoslovakian native, received the call. “Professor, we need your expertise in this field to identify the pattern of the killer or killers,” Chief Runner exclaimed. “We need to establish a link between these people to stop this massacre.” 

“Yes, Chief Runner, please give me the details of the case,” Professor Bird declared with a heavy native accent, (though, for the sake of the case file, reader, I have decided to leave out the accent due to the fact that it would obviously drive you -- if you will forgive the pun -- looney!) 

“All prints are wiped clean and there are no identifiable marks on the bodies of the victims. All we are able to come up with is the possibility that perhaps there is some kind of alternative method of which we are unaware. We have sent a secure e-mail with a listing of all of the victims’ =races=, =genders=, locations of the bodies, and the sequential =order= in which they were killed. We have also included other information that might be helpful,” said Chief Runner.

“Thank you very much. Perhaps I will contact my colleague in the Statistics Department here, Dr. Elmer Fudd-Einstein,” exclaimed Professor Bird. “He might be able to identify a pattern of attack with mathematics and statistics.”

“Good luck trying to find him, Professor. Last I heard, he had a bottle of scotch and was in the Hundred Acre Woods hunting rabbits,” Chief Runner declared in a manner that questioned the beloved doctor’s credibility. 

“Perhaps I will take a drive to find him. The fresh air will do me good.”

#+begin_quote
I will skip ahead, dear reader, for much occurred during this time. Needless to say, after a fierce battle with a mountain cat that the Toon-ology Department tagged earlier in the year as “Sylvester,” Professor Bird found Dr. Fudd-Einstein and brought him back, with much bribery of alcohol and the promise of the future slaying of those “wascally wabbits” (it would help to explain that Dr. Fudd-Einstein had a speech impediment which was only worsened during the consumption of alcohol.)
#+end_quote

Once our two heroes returned to the beautiful Acme-Looney University, and once Dr. Fudd-Einstein became sober and coherent, they set off to examine the case and begin solving these mysterious murders.

“First off,” Dr. Fudd-Einstein explained, “these people all worked at the University at some point or another. Also, there also seems to be a trend in the fact that they all had a =salary= between \( SRC_R{round(min(RcmdrTestDrive$salary))} \) and \( SRC_R{round(max(RcmdrTestDrive$salary))} \) when they retired.” 

“That’s not really a lot to live off of,” explained Professor Bird. 

“Yes, but you forget that the Looney Currency System works differently than the rest of the American Currency System. One Looney is equivalent to Ten American Dollars. Also, these faculty members are the ones who faced a cut in their salary, as denoted by =reduction=. Some of them dropped quite substantially when the University had to fix that little /faux pas/ in the Chemistry Department. You remember: when Dr. D. Duck tried to create that ‘Everlasting Elixir?’ As a result, these faculty left the university. Speaking of which, when is his memorial service?” inquired Dr. Fudd-Einstein. 

“This coming Monday. But if there were all of these killings, how in the world could one person do it? It just doesn’t seem to be possible; stay up \( SRC_R{round((dim(RcmdrTestDrive)[ 1 ] - 2)/24)} \) days straight and be able to kill all of these people and have the energy to continue on,” Professor Bird exclaimed, doubting the guilt of only one person. 

“Perhaps then, it was a group of people, perhaps there was more than one killer placed throughout Toon Town to commit these crimes. If I feed in these variables, along with any others that might have a pattern, the Acme Computer will give us an accurate reading of suspects, with a scant probability of error. As you know, the Acme Computer was developed entirely in house here at Acme-Looney University,” Dr. Fudd-Einstein said as he began feeding the numbers into the massive server. 

“Hey, look at this,” Professor Bird exclaimed, “What’s with this =before= and =after= information?” 

“Scroll down; it shows it as a note from the coroner’s office. Apparently Toon Town Coroner Marvin -- that strange fellow from Mars, Pennsylvania -- feels, in his opinion, that given the fact that the cadavers were either =smokers= or non-smokers, and given their personal health, and family medical history, that this was their life expectancy before contact with cigarettes or second-hand smoke and after,” Dr. Fudd-Einstein declared matter-of-factly. 

“Well, would race or gender have something to do with it, Elmer?” inquired Professor Bird.

“Maybe, but I would bet my money on somebody was trying to quiet these faculty before they made a big ruckus about the secret money-laundering of Old Man Acme. You know, most people think that is how the University receives most of its funds, through the mob families out of Chicago. And I would be willing to bet that these faculty figured out the connection and were ready to tell the Looney Police.” Dr. Fudd-Einstein spoke lower, fearing that somebody would overhear their conversation.  

Dr. Fudd-Einstein then pressed =Enter= on the keyboard and waited for the results. The massive computer roared to life... and when I say roared, I mean it literally /roared/. All the hidden bells, whistles, and alarm clocks in its secret compartments came out and created such a loud racket that classes across the university had to come to a stand-still until it finished computing. 

Once it was completed, the computer listed 4 names:

#+begin_example
****************************SUSPECTS****************************
- Yosemite Sam (“Looney” Insane Asylum) 
- Wile E. Coyote (deceased) 
- Foghorn Leghorn (whereabouts unknown) 
- Granny (1313 Mockingbird Lane, Toon Town USA)
#+end_example

Dr. Fudd-Einstein and Professor Bird looked on in silence. They could not believe their eyes. The greatest computer on the Gulf of Mexico seaboard just released the most obscure results imaginable.

“There seems to be a mistake. Perhaps something is off,” Professor Bird asked, still unable to believe the results.

“Not possible; the Acme Computer takes into account every kind of connection available. It considers affiliations to groups, and affiliations those groups have to other groups. It checks the FBI, CIA, British intelligence, NAACP, AARP, NSA, JAG, TWA, EPA, FDA, USWA, \(\mathsf{R}\), MAPLE, SPSS, SAS, and Ben & Jerry’s files to identify possible links, creating the most powerful computer in the world... with a tweak of Toon fanaticism,” Dr. Fudd-Einstein proclaimed, being a proud co-founder of the Acme Computer Technology.

“Wait a minute, Ben & Jerry? What would eating ice cream have to do with anything?” Professor Bird inquired.

“It is in the works now, but a few of my fellow statistician colleagues are trying to find a mathematical model to link the type of ice cream consumed to the type of person they might become. Assassins always ate vanilla with chocolate sprinkles, a little known fact they would tell you about Oswald and Booth,” Dr. Fudd-Einstein declared.

“I’ve heard about this. My forensics graduate students are trying to identify car thieves with either rocky road or mint chocolate chip… so far, the pattern is showing a clear trend with chocolate chip,” Professor Bird declared. 
“Well, what do we know about these suspects, Twee?” Dr. Fudd-Einstein asked.

“Yosemite Sam was locked up after trying to rob that bank in the West Borough. Apparently his guns were switched and he was sent the Acme Kids Joke Gun and they blew up in his face. The containers of peroxide they contained turned all of his facial hair red. Some little child is running around Toon Town with a pair of .38’s to this day. 

“Wile E. Coyote was that psychopath working for the Yahtzee - the fanatics who believed that Toons were superior to Humans. He strapped sticks of Acme Dynamite to his chest to be a martyr for the cause, but before he got to the middle of Toon Town, this defective TNT blew him up. Not a single other person -- Toon or Human -- was even close.

“Foghorn Leghorn is the most infamous Dog Kidnapper of all times. He goes to the homes of prominent Dog citizens and holds one of their relatives for ransom. If they refuse to pay, he sends them to the pound. Either way, they’re sure stuck in the dog house,” Professor Bird laughed. Dr. Fudd-Einstein didn’t seem amused, so Professor Bird continued. 

“Granny is the most beloved alumnus of Acme-Looney University. She was in the first graduating class and gives graciously each year to the university. Without her continued financial support, we wouldn’t have the jobs we do. She worked as a parking attendant at the University lots... wait a minute, take a look at this,” Professor Bird said as he scrolled down in the police information. “Granny’s signature is on each of these faculty members’ =parking= tickets. Kind of odd, considering the Chief-of-Parking signed each personally. The deceased had from as few as \( SRC_R{min(RcmdrTestDrive$parking)} \) ticket to as many as \( SRC_R{max(RcmdrTestDrive$parking)} \). All tickets were unpaid.

“And look at this, Granny married Old Man Acme after graduation. He was a resident of Chicago and rumored to be a consigliere to one of the most prominent crime families in Chicago, the Chuck Jones/Warner Crime Family,” Professor Bird read from the screen as a cold feeling of terror rose from the pit of his stomach. 

“Say, don’t you live at her house? Wow, you’re living under the same roof as one of the greatest criminals/murderers of all time!” Dr. Fudd-Einstein said in awe and sarcasm.

“I would never have suspected her, but I guess it makes sense. She is older, so she doesn’t need near the amount of sleep as a younger person. She has access to all of the vehicles so she can copy license plate numbers and follow them to their houses. She has the finances to pay for this kind of massive campaign on behalf of the Mob, and she hates anyone that even remotely smells like smoke,” Professor Bird explained, wishing to have his hit of nicotine at this time.

“Well, I guess there is nothing left to do but to call Police Chief Runner and have him arrest her,” Dr. Fudd-Einstein explained as he began dialing. “What I can’t understand is how in the world the Police Chief sent me all of this information and somehow seemed to screw it up.”

“What do you mean?” inquired Professor Bird.

“Well, look here. The data file from the Chief's email shows \( SRC_R{dim(RcmdrTestDrive)[ 1 ]} \) murders, but there have only been \( SRC_R{dim(RcmdrTestDrive)[ 1 ] - 2} \). This doesn’t make any sense. I’ll have to straighten it out. Hey, wait a minute. Look at this, Person number \( SRC_R{dim(RcmdrTestDrive)[ 1 ] - 1} \) and Person number \( SRC_R{dim(RcmdrTestDrive)[ 1 ]} \) seem to match our stats. But how can that be?”

It was at this moment that our two heroes were shot from behind and fell over the computer, dead. The killer hit =Delete= on the computer and walked out slowly (considering they had arthritis) and cackling loudly in the now quiet computer lab.  

And so, I guess my question to you the reader is, did Granny murder \( SRC_R{dim(RcmdrTestDrive)[ 1 ]} \) people, or did the murderer slip through the cracks of justice? You be the statistician and come to your own conclusion. 

Detective Pyork E. Pig 

: ***End File***

#+latex: \vfill{}

#+begin_latex
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{\bibname}
\bibliographystyle{plainurl}
\nocite{*}
\bibliography{IPSUR,Rpackages-2.14.1}
\vfill{}
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{\indexname} 
\printindex{}
#+end_latex

#+begin_src R :eval never 
rm(.Random.seed)
try(dir.create("../../data"), silent = TRUE)
save.image(file = "../../data/IPSUR.RData")
tools::resaveRdaFiles('../../data', compress = 'xz')
Stangle(file="IPSUR.Rnw", output="../IPSUR.R", annotate=TRUE)
#+end_src






