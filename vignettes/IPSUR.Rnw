% \VignetteIndexEntry{Introduction to Probability and Statistics Using R}
% \VignetteDepends{}
% \VignetteKeywords{}
% \VignettePackage{IPSUR}
% Package

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                                                                  %%%
%%%  IPSUR.Rnw - Introduction to Probability and Statistics Using R  %%%
%%%  Copyright (C) 2021  G. Jay Kerns, <gkerns@ysu.edu>              %%%
%%%  This program is free software: you can redistribute it and/or   %%%
%%%  modify it under the terms of the GNU General Public License as  %%%
%%%  published by the Free Software Foundation, either version 3     %%%
%%%  of the License, or (at your option) any later version.  This    %%%
%%%  program is distributed in the hope that it will be useful,      %%%
%%%  but WITHOUT ANY WARRANTY; without even the implied warranty     %%%
%%%  of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.         %%%
%%%  See the GNU General Public License for more details.  You       %%%
%%%  should have received a copy of the GNU General Public License   %%%
%%%  along with this program.  If not, see                           %%%
%%%  <http://www.gnu.org/licenses/>                                  %%%
%%%                                                                  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[10pt,english,nogin]{book}

\usepackage{lmodern}
\renewcommand{\sfdefault}{lmss}
\renewcommand{\ttdefault}{lmtt}

% needed packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
%\usepackage{array}
\usepackage[english]{babel}
\usepackage{epsfig}
\usepackage{exercise}
\usepackage{fancyvrb}
\usepackage{fixltx2e}
\usepackage{float}
\usepackage[T1]{fontenc}
\usepackage{footnote}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{latexsym}
\usepackage{longtable}
\usepackage{makeidx}
\usepackage{marvosym}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{soul}
\usepackage{srcltx}
\usepackage{stmaryrd}
\usepackage{subfig}
\usepackage[subfigure]{tocloft}
\usepackage{textcomp}
\usepackage{txfonts}
\usepackage{upgreek}
\usepackage{url}
\usepackage{varioref}
\usepackage{verbatim}
\usepackage{wrapfig}

% Page setup
%\usepackage[paperwidth=7.44in,paperheight=9.69in]{geometry}
%\geometry{verbose,tmargin=1in,bmargin=1in,outer=1.25in,inner=0.75in}
%\geometry{paperwidth=6in,paperheight=9in, margin=0.55in}
%\geometry{paperwidth=6in,paperheight=9in, margin=0.75in}
%\usepackage[paperwidth=6in,paperheight=9in,tmargin=0.75in,bmargin=0.8in,outer=0.5in,inner=0.75in]{geometry}
%\geometry{paperwidth=6in,paperheight=9in,tmargin=0.75in,bmargin=0.8in,outer=0.5in,inner=0.75in}
\usepackage[paperwidth=7.25in,paperheight=10.25in,tmargin=0.75in,bmargin=0.8in,outer=0.5in,inner=0.75in]{geometry}
%\geometry{paperwidth=7in,paperheight=10in,tmargin=0.75in,bmargin=0.75in,outer=0.5in,inner=0.75in}
\pagestyle{headings}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{1}

\makeindex

% PDF settings
\usepackage[hyperref,x11names]{xcolor}
\usepackage{hyperref}
\hypersetup{pdftitle={Introduction to Probability and Statistics Using R},
 		pdfauthor={G. Jay Kerns},
		linkcolor=Firebrick4,
		citecolor=black,
		urlcolor=SteelBlue4}

% special logos
\providecommand{\IPSUR}
{\textsc{I\kern 0ex\lower-0.3ex\hbox{\small P}\kern -0.5ex\lower0.4ex\hbox{\footnotesize S}\kern -0.25exU}\kern -0.1ex\lower 0.15ex\hbox{\textsf{\large R}}\@}

%  user defined commands
% special operators
\renewcommand{\vec}[1]{\mbox{\boldmath$#1$}}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.

\numberwithin{equation}{chapter}
\numberwithin{figure}{chapter}

\theoremstyle{plain}
  \newtheorem{thm}{Theorem}[chapter]
  \newtheorem{fact}[thm]{Fact}
  \newtheorem{axiom}[thm]{Axiom}
   \newtheorem{cor}[thm]{Corollary}
   \newtheorem{prop}[thm]{Proposition}
  \newtheorem{assumption}[thm]{Assumption}

\theoremstyle{definition}
  \newtheorem{defn}[thm]{Definition}
\newtheorem{example}[thm]{Example}

\theoremstyle{remark}
  \newtheorem{note}[thm]{Note}
  \newtheorem{rem}[thm]{Remark}


\setlength{\cftfignumwidth}{1.5cm}

\@ifundefined{showcaptionsetup}{}{%
 \PassOptionsToPackage{caption=false}{subfig}}
\usepackage{subfig}
\AtBeginDocument{
  \def\labelitemii{\(\circ\)}
}

\makeatother

\begin{document}
\SweaveOpts{concordance=TRUE}

<<echo=FALSE>>=
# Preliminary code to load before start
# clear everything to start
rm(list = ls(all = TRUE))

# initial customizations
seed <- 42
set.seed(seed)
options(width = 55)
cexlab <- 1.5
@

\title{\fontsize{30}{35}\selectfont Introduction to Probability\\ and
Statistics Using \textsf{R}}

\date{\fontsize{24}{28}\selectfont \textsc{Fourth Edition}}

 \author{\fontsize{24}{28}\selectfont G.~Jay Kerns}

\maketitle

\pagenumbering{roman}
\setcounter{page}{2}

\noindent \IPSUR: Introduction to Probability and Statistics Using \textsf{R}

\noindent Copyright \textcopyright~2021 G.~Jay Kerns \\
\noindent ISBN-13: 978-1726343909\\
\noindent ISBN-10: 1726343901\\
\medskip{}

\noindent Permission is granted to copy, distribute and/or modify this
document under the terms of the GNU Free Documentation License,
Version 1.3 or any later version published by the Free Software
Foundation; with no Invariant Sections, no Front-Cover Texts, and no
Back-Cover Texts. A copy of the license is included in the section
entitled ``GNU Free Documentation License''.

\vspace{0.25in}
\noindent Date: \today
\noindent \vfill{}

\cleardoublepage
\phantomsection
\pdfbookmark[1]{Contents}{table}

\tableofcontents{}

%    IPSUR: Introduction to Probability and Statistics Using R
%    Copyright (C)  2021 G. Jay Kerns
%
%    This file is part of IPSUR.
%
%    Permission is granted to copy, distribute and/or modify this document
%    under the terms of the GNU Free Documentation License, Version 1.3
%    or any later version published by the Free Software Foundation;
%    with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.
%    A copy of the license is contained in the LICENSE file in this
%    directory.

\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Preface to the Fourth Edition}

\chapter*{Preface to the Fourth Edition}

What is new in the Fourth Edition? Not much beyond the Third Edition. The Third Edition was a return to that which works from the testing grounds of the Second Edition. The Fourth Edition fills in many BLANK's that had lingered from earlier editions, in particular, some examples from the one and two sample confidence interval and hypothesis testing sections.

\section*{Acknowledgments}

The successes of the Fourth Edition (if any) will be due in no small
part to the successes of all earlier editions, so it would be
apropos to copy-paste the acknowledgments from all earlier Prefaces
here.

I think, though, that the failures of earlier editions have played an
important role as well, if not more so. I would like to extend
gracious thanks to Mr. P.J.C. Dwarshuis (Hans), Statistician, from The
Hague, Netherlands, and Jesus Juan, who, both armed with sharp eyes, have pointed out mistakes, misstatements, and places where better
discussion is warranted. It is the selfless contributions of people
just like these gentlemen which make the hours spent polishing a FREE
book all the more worthwhile.

\vfill{}
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{List of Figures}
\listoffigures

\vfill{}
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{List of Tables}
\listoftables


\chapter{An Introduction to Probability and Statistics}  \label{cha:introps}

\pagenumbering{arabic}

<<echo=FALSE, eval=FALSE>>=
#    IPSUR: Introduction to Probability and Statistics Using R
#    Copyright (C) 2021 G. Jay Kerns
#
#    Chapter: An Introduction to R
#
#    This file is part of IPSUR.
#
#    IPSUR is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    IPSUR is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with IPSUR.  If not, see <http://www.gnu.org/licenses/>.
@


This chapter has proved to be the hardest to write, by far. The
trouble is that there is so much to say -- and so many people have
already said it so much better than I could. When I get something I
like I will release it here.

In the meantime, there is a lot of information already available to a
person with an Internet connection. I recommend to start at Wikipedia,
which is not a flawless resource but it has the main ideas with links
to reputable sources.

\section{Probability}

Probability concerns randomness; its behavior, properties, and consequences.
The common folklore is that probability has been around for millennia
but did not gain the attention of mathematicians until approximately
1654 when the Chevalier de Mere had a question regarding the fair
division of a game's payoff to the two players, supposing the game had
to end prematurely.

\section{Statistics}

Statistics concerns data; their collection, analysis, and
interpretation. In this book we distinguish between two types of
statistics: descriptive and inferential.

Descriptive statistics concerns the summarization of data. We have a
data set and we would like to describe the data set in multiple
ways. Usually this entails calculating numbers from the data, called
descriptive measures, such as percentages, sums, averages, and so
forth.

Inferential statistics does more. There is an inference associated
with the data set, a conclusion drawn about the population from which
the data originated.



\chapter{An Introduction to \textsf{R}} \label{cha:introduction-to-r}

Every \textsf{R} book I have ever seen has had a section/chapter
that is an introduction to \textsf{R}, and so does this one.  The
goal of this chapter is for a person to get up and running, ready for
the material that follows.  See Section~\ref{sec:external-resources} for links
to other material which the reader may find useful.

\paragraph{What do I want them to know?}

\begin{itemize}
\item Where to find \textsf{R} to install on a home computer, and a
  few comments to help with the usual hiccups that occur when
  installing something.
\item Abbreviated remarks about the available options to interact with
  \textsf{R}.
\item Basic operations (arithmetic, entering data, vectors) at the command
  prompt.
\item How and where to find help when they get in trouble.
\item Other little shortcuts I am usually asked when introducing
  \textsf{R}.
\end{itemize}

\section{Downloading and Installing \textsf{R}} \label{sec:download-install-r}

The instructions for obtaining \textsf{R} largely depend on the
user's hardware and operating system. The \textsf{R} Project has
written an \textsf{R} Installation and Administration manual with
complete, precise instructions about what to do, together with all
sorts of additional information. The following is just a primer to get
a person started.

\subsection{Installing \textsf{R}}

Visit one of the links below to download the latest version of \textsf{R}
for your operating system:

\begin{itemize}
\item Microsoft Windows: \url{http://cran.r-project.org/bin/windows/base/}
\item MacOS: \url{http://cran.r-project.org/bin/macosx/}
\item Linux:  \url{http://cran.r-project.org/bin/linux/}
\end{itemize}

On Microsoft Windows, click the \texttt{R-x.y.z.exe} installer to start
installation. When it asks for ``Customized startup options'', specify
\texttt{Yes}. In the next window, be sure to select the SDI (single document
interface) option; this is useful later when we discuss three
dimensional plots with the \texttt{rgl} package \cite{rgl}.

\subsubsection{Installing \textsf{R} on a USB drive (Windows)}

With this option you can use \textsf{R} portably and without
administrative privileges. There is an entry in the \textsf{R} for
Windows FAQ about this. Here is the procedure I use:

\begin{enumerate}
\item Download the Windows installer above and start installation as
   usual. When it asks \textit{where} to install, navigate to the top-level
   directory of the USB drive instead of the default \texttt{C} drive.
\item When it asks whether to modify the Windows registry, uncheck the
   box; we do NOT want to tamper with the registry.
\end{enumerate}

After installation, navigate to the \texttt{R-x.y.z/bin} directory
to double-click \texttt{Rgui.exe} to launch the program. Note that it is useless
to create your own shortcut to \texttt{Rgui.exe}. Windows does not
allow shortcuts to have relative paths; they always have a drive
letter associated with them. So if you make your own shortcut and
plug your USB drive into some \textit{other} machine that happens to
assign your drive a different letter, then
your shortcut will no longer be pointing to the right place.

\subsection{Installing and Loading Add-on Packages} \label{sub:installing-loading-packages}

There are \textit{base} packages (which come with \textsf{R}
automatically), and \textit{contributed} packages (which must be downloaded
for installation). For example, on the version of \textsf{R} being
used for this document the default base packages loaded at startup are

<<echo=TRUE>>=
getOption("defaultPackages")
@

The base packages are maintained by a select group of volunteers,
called \textsf{R} Core. In addition to the base packages, there
are over ten thousand additional contributed packages written by
individuals all over the world. These are stored worldwide on mirrors
of the Comprehensive \textsf{R} Archive Network, or \texttt{CRAN} for
short. Given an active Internet connection, anybody is free to
download and install these packages and even inspect the source code.

To install a package named \texttt{foo}, open up \textsf{R} and type
\texttt{install.packages("foo")}
\index{install.packages@\texttt{install.packages}}. To
install \texttt{foo} and additionally install all of the other packages on
which \texttt{foo} depends, instead type \texttt{install.packages("foo", depends =
TRUE)}.

The general command \texttt{install.packages()} will (on most operating
systems) open a window containing a huge list of available packages;
simply choose one or more to install.

No matter how many packages are installed onto the system, each one
must first be loaded for use with the
\texttt{library} \index{library@\texttt{library}} function. For instance, the
\texttt{foreign} package \cite{foreign} contains all sorts of functions
needed to import data sets into \textsf{R} from other software
such as SPSS, SAS, \textit{etc}. But none of those functions will be
available until the command \texttt{library("foreign")} is issued.

Type \texttt{library()} at the command prompt (described below) to see a list
of all available packages in your library.

For complete, precise information regarding installation of
R and add-on packages, see the \textsf{R} Installation and Administration
manual, \url{http://cran.r-project.org/manuals.html}.

\section{Communicating with \textsf{R}} \label{sec:communicating-with-r}

\subsection{One line at a time}

This is the most basic method and is the first one that beginners will use.

\begin{itemize}
\item RGui (Microsoft \(\circledR\) Windows)
\item RStudio
\item Terminal
\item Emacs/ESS, XEmacs
\end{itemize}

\subsection{Multiple lines at a time}

For longer programs (called \textit{scripts}) there is too much code to write
all at once at the command prompt. Furthermore, for longer scripts it
is convenient to be able to only modify a certain piece of the script
and run it again in \textsf{R}. Programs called \textit{script editors}
are specially designed to aid the communication and code writing
process. They have all sorts of helpful features including
R syntax highlighting, automatic code completion,
delimiter matching, and dynamic help on the \textsf{R} functions
as they are being written. Even more, they often have all of the text
editing features of programs like Microsoft\(\circledR\)Word. Lastly,
most script editors are fully customizable in the sense that the user
can customize the appearance of the interface to choose what colors to
display, when to display them, and how to display them.

\begin{description}
\item [{R Editor (Windows):}] In Microsoft\(\circledR\) Windows,
     \textsf{R} Gui has its own built-in script editor, called
     \textsf{R} Editor. From the console window, select \texttt{File}
     \(\triangleright\) \texttt{New Script}. A script window opens, and the
     lines of code can be written in the window. When satisfied with
     the code, the user highlights all of the commands and presses
     \textsf{Ctrl+R}. The commands are automatically run at once in
     \textsf{R} and the output is shown. To save the script for
     later, click \texttt{File} \(\triangleright\) \texttt{Save as\ldots} in
     \textsf{R} Editor. The script can be reopened later with
     \texttt{File} \(\triangleright\) \texttt{Open Script\ldots} in \texttt{RGui}. Note that
     \textsf{R} Editor does not have the fancy syntax highlighting
     that the others do.
\item [{RStudio:}]
\item [{Emacs/ESS:}] Emacs is an all purpose text editor. It can do
  absolutely anything with respect to modifying, searching, editing,
  and manipulating, text. And if Emacs can't do it, then you can write
  a program that extends Emacs to do it. One such extension is called
  \texttt{ESS}, which stands for \textit{E}macs \textit{S}peaks
  \textit{S}tatistics. With ESS a person can speak to \textsf{R}, do all of
  the tricks that the other script editors offer, and much, much,
  more. Please see the following for installation details,
  documentation, reference cards, and a whole lot more:
  \url{http://ess.r-project.org}.  \textit{Fair warning:} if you want
  to try Emacs and if you grew up with Microsoft\(\circledR\) Windows
  or Macintosh, then you are going to need to relearn everything you
  thought you knew about computers your whole life. (Or, since Emacs
  is completely customizable, you can reconfigure Emacs to behave the
  way you want.) I have personally experienced this transformation and
  I will never go back.
\end{description}

\subsection{Graphical User Interfaces (GUIs)}

By the word ``GUI'' we mean an interface in which the user communicates
with \textsf{R} by way of points-and-clicks in a menu of some
sort. Again, there are many, many options and I only mention one that
I have used and enjoyed.

\begin{description}
\item [{R Commander}] provides a point-and-click interface to
     many basic statistical tasks. It is called the ``Commander''
     because every time one makes a selection from the menus, the code
     corresponding to the task is listed in the output window. One can
     take this code, copy-and-paste it to a text file, then re-run it
     again at a later time without the \textsf{R} Commander's
     assistance. It is well suited for the introductory level. \texttt{Rcmdr}
     \cite{Rcmdr} also allows for user-contributed ``Plugins'' which are
     separate packages on \texttt{CRAN} that add extra functionality to the
     \texttt{Rcmdr} package. The plugins are typically named with the prefix
     \texttt{RcmdrPlugin} to make them easy to identify in the \texttt{CRAN} package
     list. One such plugin is the \texttt{RcmdrPlugin.IPSUR} package
     \cite{RcmdrPlugin.IPSUR} which accompanies this text.
\end{description}

\section{Basic \textsf{R} Operations and Concepts} \label{sec:basic-r-operations}

The \textsf{R} developers have written an introductory document
entitled ``An Introduction to \textsf{R}''. There is a sample
session included which shows what basic interaction with
R looks like. I recommend that all new users of
R read that document, but bear in mind that there are
concepts mentioned which will be unfamiliar to the beginner.

Below are some of the most basic operations that can be done with
R. Almost every book about \textsf{R} begins with a
section like the one below; look around to see all sorts of things
that can be done at this most basic level.

\subsection{Arithmetic} \label{sub:arithmetic}

<<echo=TRUE>>=
2 + 3       # add
4 * 5 / 6   # multiply and divide
7^8         # 7 to the 8th power
@

Notice the comment character \texttt{\#}. Anything typed
after a \texttt{\#} symbol is ignored by \textsf{R}. We know that \(20/6\)
is a repeating decimal, but the above example shows only 7 digits. We
can change the number of digits displayed with
\texttt{options}:

<<echo=TRUE>>=
options(digits = 16)
10/3                 # see more digits
sqrt(2)              # square root
exp(1)               # Euler's constant, e
pi
options(digits = 7)  # back to default
@

Note that it is possible to set \texttt{digits}
 up to 22, but setting them
over 16 is not recommended (the extra significant digits are not
necessarily reliable). Above notice the \texttt{sqrt}
 function for square roots and the
\texttt{exp} \index{exp@\texttt{exp}} function for powers of
\(\mathrm{e}\), Euler's number.

\subsection{Assignment, Object names, and Data types} \label{sub:assignment-object-names}

It is often convenient to assign numbers and values to variables
(objects) to be used later. The proper way to assign values to a
variable is with the \texttt{<-} operator (with a space on either side). The
\texttt{=} symbol works too, but it is recommended by the \textsf{R}
masters to reserve \texttt{=} for specifying arguments to functions
(discussed later). In this book we will follow their advice and use
\texttt{<-} for assignment. Once a variable is assigned, its value can be
printed by simply entering the variable name by itself.

<<echo=TRUE>>=
x <- 7*41/pi   # don't see the calculated value
x              # take a look
@

When choosing a variable name you can use letters, numbers, dots
``\texttt{.}'', or underscore ``\texttt{\_}'' characters. You cannot
use mathematical operators, and a leading dot may not be followed by a
number. Examples of valid names are: \texttt{x}, \texttt{x1},
\texttt{y.value}, and \texttt{y\_hat}. (More precisely, the set of
allowable characters in object names depends on one's particular
system and locale; see An Introduction to \textsf{R} for more discussion on
this.)

Objects can be of many \textit{types}, \textit{modes}, and
\textit{classes}. At this level, it is not necessary to investigate
all of the intricacies of the respective types, but there are some
with which you need to become familiar:

\begin{itemize}
\item integer: the values \(0\), \(\pm1\), \(\pm2\), \ldots; these are
  represented exactly by \textsf{R}.
\item double: real numbers (rational and irrational); these numbers
  are not represented exactly (save integers or fractions with a
  denominator that is a power of 2, see \cite{Venables2010}).
\item character: elements that are wrapped with pairs of \texttt{"} or
  \texttt{'};
\item logical: includes \texttt{TRUE}, \texttt{FALSE}, and \texttt{NA}
  (which are reserved words); the \texttt{NA} \index{NA@\texttt{NA}}
  stands for ``not available'', \textit{i.e.}, a missing value.
\end{itemize}

You can determine an object's type with the \texttt{typeof}
\index{typeof@\texttt{typeof}} function. In addition to the above,
there is the \texttt{complex} \index{complex@\texttt{complex}}
\index{as.complex@\texttt{as.complex}} data type:

<<echo=TRUE>>=
sqrt(-1)              # isn't defined
sqrt(-1+0i)           # is defined
sqrt(as.complex(-1))  # same thing
(0 + 1i)^2            # should be -1
typeof((0 + 1i)^2)
@

Note that you can just type \texttt{(1i)\^2} to get the same answer. The
\texttt{NaN} \index{NaN@\texttt{NaN}} stands for ``not a number''; it is
represented internally as \texttt{double} \index{double}.

\subsection{Vectors} \label{sub:vectors}

All of this time we have been manipulating vectors of length 1. Now
let us move to vectors with multiple entries.

\subsubsection{Entering data vectors}

\textbf{The long way:} \index{c@\texttt{c}} If you would like to enter the
data \texttt{74,31,95,61,76,34,23,54,96} into \textsf{R}, you may create
a data vector with the \texttt{c} function (which is short for
\textit{concatenate}).

<<echo=TRUE>>=
x <- c(74, 31, 95, 61, 76, 34, 23, 54, 96)
x
@

The elements of a vector are usually coerced by \textsf{R} to the
the most general type of any of the elements, so if you do \texttt{c(1, "2")}
then the result will be \texttt{c("1", "2")}.

\textbf{A shorter way:} \index{scan@\texttt{scan}} The \texttt{scan} method is
useful when the data are stored somewhere else. For instance, you may
type \texttt{x <- scan()} at the command prompt and \textsf{R} will
display \texttt{1:} to indicate that it is waiting for the first data
value. Type a value and press \texttt{Enter}, at which point \textsf{R}
will display \texttt{2:}, and so forth. Note that entering an empty line
stops the scan. This method is especially handy when you have a column
of values, say, stored in a text file or spreadsheet. You may copy and
paste them all at the \texttt{1:} prompt, and \textsf{R} will store all
of the values instantly in the vector \texttt{x}.

\textbf{Repeated data; regular patterns:} the \texttt{seq} \index{seq@\texttt{seq}}
function will generate all sorts of sequences of numbers. It has the
arguments \texttt{from}, \texttt{to}, \texttt{by}, and \texttt{length.out} which can be set in
concert with one another. We will do a couple of examples to show you
how it works.

<<echo=TRUE>>=
seq(from = 1, to = 5)
seq(from = 2, by = -0.1, length.out = 4)
@

Note that we can get the first line much quicker with the colon
operator.

<<echo=TRUE>>=
1:5
@

The vector \texttt{LETTERS} \index{LETTERS@\texttt{LETTERS}} has the 26
letters of the English alphabet in uppercase and
\texttt{letters} \index{letters@\texttt{letters}} has all of them in
lowercase.

\subsubsection{Indexing data vectors}

Sometimes we do not want the whole vector, but just a piece of it. We
can access the intermediate parts with the \texttt{[]} operator. Observe
(with \texttt{x} defined above)

<<echo=TRUE>>=
x[1]
x[2:4]
x[c(1,3,4,8)]
x[-c(1,3,4,8)]
@

Notice that we used the minus sign to specify those elements that we
do \textit{not} want.

<<echo=TRUE>>=
LETTERS[1:5]
letters[-(6:24)]
@

\subsection{Functions and Expressions} \label{sub:functions-and-expressions}

A function takes arguments as input and returns an object as
output. There are functions to do all sorts of things. We show some
examples below.

<<echo=TRUE>>=
x <- 1:5
sum(x)
length(x)
min(x)
mean(x)      # sample mean
sd(x)        # sample standard deviation
@

It will not be long before the user starts to wonder how a particular
function is doing its job, and since \textsf{R} is open-source,
anybody is free to look under the hood of a function to see how things
are calculated. For detailed instructions see the article ``Accessing
the Sources'' by Uwe Ligges \cite{Ligges2006}. In short:

\textbf{Type the name of the function} without any parentheses or
arguments. If you are lucky then the code for the entire function will
be printed, right there looking at you. For instance, suppose that we
would like to see how the \texttt{intersect}
\index{intersect@\texttt{intersect}} function works:

<<echo=TRUE>>=
intersect
@

If instead it shows \texttt{UseMethod(something)}
\index{UseMethod@\texttt{UseMethod}} then you will need to choose the
\textit{class} of the object to be inputted and next look at the \textit{method}
that will be \textit{dispatched} to the object. For instance, typing \texttt{rev}
\index{rev@\texttt{rev}} says

<<echo=TRUE>>=
rev
@

The output is telling us that there are multiple methods associated
with the \texttt{rev} function. To see what these are, type

<<echo=TRUE>>=
methods(rev)
@

Now we learn that there are two different \texttt{rev(x)} functions, only one
of which being chosen at each call depending on what \texttt{x} is. There is
one for \texttt{dendrogram} objects and a \texttt{default} method for everything
else. Simply type the name to see what each method does. For example,
the \texttt{default} method can be viewed with

<<echo=TRUE>>=
rev.default
@

\textbf{Some functions are hidden by a namespace} (see An Introduction to
R @Venables2010), and are not visible on the first
try. For example, if we try to look at the code for \texttt{wilcox.test}
\index{wilcox.test@\texttt{wilcox.test}} (see Chapter~\ref{cha:nonparametric-statistics}) we get the following:

<<echo=TRUE>>=
wilcox.test
methods(wilcox.test)
@

If we were to try \texttt{wilcox.test.default} we would get a ``not found''
error, because it is hidden behind the namespace for the package
\texttt{stats} \cite{stats} (shown in the last line when we tried
\texttt{wilcox.test}). In cases like these we prefix the package name to the
front of the function name with three colons; the command
\texttt{stats:::wilcox.test.default} will show the source code, omitted here
for brevity.

If it shows \texttt{.Internal(something)}
\index{.Internal@\texttt{.Internal}} or \texttt{.Primitive(something)}
\index{.Primitive@\texttt{.Primitive}}, then it will be necessary to
download the source code of \textsf{R} (which is \textit{not} a binary
version with an \texttt{.exe} extension) and search inside the code
there. See Ligges \cite{Ligges2006} for more discussion on this. An
example is \texttt{exp}:

<<echo=TRUE>>=
exp
@


Be warned that most of the \texttt{.Internal} functions are written in other
computer languages which the beginner may not understand, at least
initially.

\section{Getting Help} \label{sec:getting-help}

When you are using \textsf{R}, it will not take long before you
find yourself needing help. Fortunately, \textsf{R} has extensive
help resources and you should immediately become familiar with
them. Begin by clicking \texttt{Help} on \texttt{RGui}. The following options are
available.

\begin{itemize}
\item Console: gives useful shortcuts, for instance, \texttt{Ctrl+L},
  to clear the \textsf{R} console screen.
\item FAQ on \textsf{R}: frequently asked questions concerning general \textsf{R}
  operation.
\item FAQ on \textsf{R} for Windows: frequently asked questions about \textsf{R},
  tailored to the Microsoft Windows operating system.
\item Manuals: technical manuals about all features of the \textsf{R} system
  including installation, the complete language definition, and add-on
  packages.
\item \textsf{R} functions (text)\ldots: use this if you know the \textit{exact}
  name of the function you want to know more about, for example,
  \texttt{mean} or \texttt{plot}. Typing \texttt{mean} in the window
  is equivalent to typing \texttt{help("mean")}
  \index{help@\texttt{help}} at the command line, or more simply,
  \texttt{?mean} \index{?@\texttt{?}}. Note that this method only
  works if the function of interest is contained in a package that is
  already loaded into the search path with \texttt{library}.
\item HTML Help: use this to browse the manuals with point-and-click
  links. It also has a Search Engine \& Keywords for searching the
  help page titles, with point-and-click links for the search
  results. This is possibly the best help method for beginners. It can
  be started from the command line with the command
  \texttt{help.start()} \index{help.start@\texttt{help.start}}.
\item Search help\ldots: use this if you do not know the exact name of
  the function of interest, or if the function is in a package that
  has not been loaded yet. For example, you may enter \texttt{plo} and
  a text window will return listing all the help files with an alias,
  concept, or title matching \texttt{plo} using regular expression
  matching; it is equivalent to typing \texttt{help.search("plo")}
  \index{help.search@\texttt{help.search}} at the command line. The
  advantage is that you do not need to know the exact name of the
  function; the disadvantage is that you cannot point-and-click the
  results. Therefore, one may wish to use the HTML Help search engine
  instead. An equivalent way is \texttt{??plo} \index{??@\texttt{??}}
  at the command line.
\item search.r-project.org\ldots: this will search for words in help
  lists and email archives of the \textsf{R} Project. It can be very useful for
  finding other questions that other users have asked.
\item Apropos\ldots: use this for more sophisticated partial name
  matching of functions. See \texttt{?apropos}
  \index{apropos@\texttt{apropos}} for details.
\end{itemize}

On the help pages for a function there are sometimes ``Examples''
listed at the bottom of the page, which will work if copy-pasted at
the command line (unless marked otherwise). The \texttt{example}
\index{example@\texttt{example}} function will run the code
automatically, skipping the intermediate step. For instance, we may
try \texttt{example(mean)} to see a few examples of how the
\texttt{mean} function works.

\subsection{R Help Mailing Lists}

There are several mailing lists associated with \textsf{R}, and there is a huge
community of people that read and answer questions related to \textsf{R}. See
\url{http://www.r-project.org/mail.html} for an idea of what is
available. Particularly pay attention to the bottom of the page which
lists several special interest groups (SIGs) related to \textsf{R}.

Bear in mind that \textsf{R} is free software, which means that it was written
by volunteers, and the people that frequent the mailing lists are also
volunteers who are not paid by customer support fees. Consequently, if
you want to use the mailing lists for free advice then you must adhere
to some basic etiquette, or else you may not get a reply, or even
worse, you may receive a reply which is a bit less cordial than you
are used to. Below are a few considerations:

\begin{enumerate}
\item Read the FAQ, \url{http://cran.r-project.org/faqs.html}. Note
  that there are different FAQs for different operating systems. You
  should read these now, even without a question at the moment, to
  learn a lot about the idiosyncrasies of \textsf{R}.
\item Search the archives. Even if your question is not a FAQ, there
  is a very high likelihood that your question has been asked before
  on the mailing list. If you want to know about topic \texttt{foo},
  then you can do \texttt{RSiteSearch("foo")}
  \index{RSiteSearch@\texttt{RSiteSearch}} to search the mailing list
  archives (and the online help) for it.
\item Do a Google search and an \texttt{RSeek.org} search.
\end{enumerate}

If your question is not a FAQ, has not been asked on R-help before,
and does not yield to a Google (or alternative) search, then, and only
then, should you even consider writing to R-help. Below are a few
additional considerations.

\begin{itemize}
\item Read the posting guide,
  \url{http://www.r-project.org/posting-guide.html} before
  posting. This will save you a lot of trouble and pain.
\item Get rid of the command prompts (\texttt{>}) from output. Readers
  of your message will take the text from your mail and copy-paste
  into an \textsf{R} session. If you make the readers' job easier then it will
  increase the likelihood of a response.
\item Questions are often related to a specific data set, and the best
  way to communicate the data is with a \texttt{dump}
  \index{dump@\texttt{dump}} command. For instance, if your question
  involves data stored in a vector \texttt{x}, you can type
  \texttt{dump("x","")} at the command prompt and copy-paste the
  output into the body of your email message. Then the reader may
  easily copy-paste the message from your email into \textsf{R} and \texttt{x}
  will be available to him/her.
\item Sometimes the answer the question is related to the operating
  system used, the attached packages, or the exact version of \textsf{R} being
  used. The \texttt{sessionInfo()}
  \index{sessionInfo@\texttt{sessionInfo}} command collects all of
  this information to be copy-pasted into an email (and the Posting
  Guide requests this information). See Appendix~\ref{cha:r-session-information} for an example.
\end{itemize}

\section{External Resources} \label{sec:external-resources}

There is a mountain of information on the Internet about
R. Below are a few of the important ones.

\begin{itemize}
\item The \textsf{R}-Project for Statistical Computing,
  \url{http://www.r-project.org/} \index{The R-Project@The
    \textsf{R}-Project}. Go there first.
\item The Comprehensive \textsf{R} Archive Network, \url{http://cran.r-project.org/}
  \index{CRAN}. That is where \textsf{R} is stored along with thousands of contributed
  packages. There are also loads of contributed information (books,
  tutorials, \textit{etc}.). There are mirrors all over the world with
  duplicate information.
\item R-Forge, \url{http://r-forge.r-project.org/} \index{R-Forge@\textsf{R}-Forge}. This is
  another location where \textsf{R} packages are stored. Here you
  can find development code which has not yet been released to \texttt{CRAN}.
\item \textsf{R} Seek, \url{http://www.rseek.org} is a
  search engine based on Google specifically tailored for
  \textsf{R} queries.
\end{itemize}

\section{Other Tips}

It is unnecessary to retype commands repeatedly, since \textsf{R}
remembers what you have recently entered on the command line. On the
Microsoft\(\circledR\) Windows \textsf{R} Gui, to cycle through
the previous commands just push the \(\uparrow\) (up arrow) key. On
Emacs/ESS the command is \texttt{M-p} (which means hold down the \texttt{Alt} button
and press ``p''). More generally, the command \texttt{history()}
\index{history@\texttt{history}} will show a whole list of recently
entered commands.

\begin{itemize}
\item To find out what all variables are in the current work environment,
  use the commands \texttt{objects()} \index{objects@\texttt{objects}} or
  \texttt{ls()} \index{ls@\texttt{ls}}. These list all available objects in
  the workspace. If you wish to remove one or more variables, use
  \texttt{remove(var1, var2, var3)} \index{remove@\texttt{remove}}, or more
  simply use \texttt{rm(var1, var2, var3)}, and to remove all objects use
  \texttt{rm(list = ls(all = TRUE))}.
\item Another use of \texttt{scan} is when you have a long list of numbers
  (separated by spaces or on different lines) already typed somewhere
  else, say in a text file To enter all the data in one fell swoop,
  first highlight and copy the list of numbers to the Clipboard with
  \texttt{Edit} \(\triangleright\) \texttt{Copy} (or by right-clicking and selecting
  \texttt{Copy}). Next type the \texttt{x <- scan()} command in the \textsf{R}
  console, and paste the numbers at the \texttt{1:} prompt with \texttt{Edit}
  \(\triangleright\) \texttt{Paste}. All of the numbers will automatically be
  entered into the vector \texttt{x}.
\item The command \texttt{Ctrl+l} clears the display in the
  Microsoft\(\circledR\) Windows \textsf{R} Gui. In Emacs/ESS,
  press \texttt{Ctrl+l} repeatedly to cycle point (the place where the cursor
  is) to the bottom, middle, and top of the display.
\item Once you use \textsf{R} for awhile there may be some commands
  that you wish to run automatically whenever \textsf{R}
  starts. These commands may be saved in a file called \texttt{Rprofile.site}
  \index{Rprofile.site@\texttt{Rprofile.site}} which is
  usually in the \texttt{etc} folder, which lives in the \textsf{R} home
  directory (which on Microsoft\(\circledR\) Windows usually is in the
  \texttt{Program Files} folder). Alternatively, you can make a file
  \texttt{.Rprofile} \index{.Rprofile@\texttt{.Rprofile}} to be
  stored in the user's home directory, or anywhere \textsf{R} is
  invoked. This allows for multiple configurations for different
  projects or users. See ``Customizing the Environment'' of \textit{An
  Introduction to \textsf{R}} for more details.
\item When exiting \textsf{R} the user is given the option to ``save
  the workspace''. I recommend that beginners DO NOT save the
  workspace when quitting. If \texttt{Yes} is selected, then all of the
  objects and data currently in \textsf{R}'s memory is saved in a
  file located in the working directory called
  \texttt{.RData} \index{.RData@\texttt{.RData}}. This file is then
  automatically loaded the next time \textsf{R} starts (in which
  case \textsf{R} will say \texttt{[previously saved workspace restored]}).
  This is a valuable feature for experienced users of
  \textsf{R}, but I find that it causes more trouble than it saves
  with beginners.
\end{itemize}


\section{Chapter Exercises}



\chapter{Data Description} 
\label{cha:describing-data-distributions}

<<echo=FALSE, include=FALSE>>=
# Preliminary code to load before start
# This chapter's package dependencies
library(aplpack)
library(e1071)
library(lattice)
library(qcc)
@

In this chapter we introduce the different types of data that a
statistician is likely to encounter, and in each subsection we give
some examples of how to display the data of that particular type. Once
we see how to display data distributions, we next introduce the basic
properties of data distributions. We qualitatively explore several
data sets. Once that we have intuitive properties of data sets, we
next discuss how we may numerically measure and describe those
properties with descriptive statistics.

\paragraph{What do I want them to know?}

\begin{itemize}
\item different data types, such as quantitative versus qualitative,
  nominal versus ordinal, and discrete versus continuous
\item basic graphical displays for assorted data types, and some of their
  (dis)advantages
\item fundamental properties of data distributions, including center,
  spread, shape, and crazy observations
\item methods to describe data (visually/numerically) with respect to the
  properties, and how the methods differ depending on the data type
\item all of the above in the context of grouped data, and in particular,
  the concept of a factor
\end{itemize}

\section{Types of Data} 
\label{sec:types-of-data}

Loosely speaking, a datum is any piece of collected information, and a
data set is a collection of data related to each other in some way. We
will categorize data into five types and describe each in turn:

\begin{description}
\item [{Quantitative:}] data associated with a measurement of some quantity
                  on an observational unit,
\item [{Qualitative:}] data associated with some quality or property of an
                 observational unit,
\item [{Logical:}] data which represent true or false and play an important
             role later,
\item [{Missing:}] data which should be there but are not, and
\item [{Other types:}] everything else under the sun.
\end{description}

In each subsection we look at some examples of the type in question
and introduce methods to display them.

\subsection{Quantitative data} \label{sub:quantitative-data}

Quantitative data are any data that measure or are associated with a
measurement of the quantity of something. They invariably assume
numerical values. Quantitative data can be further subdivided into two
categories.

\begin{itemize}
\item \textit{Discrete data} take values in a finite or countably infinite set of
  numbers, that is, all possible values could (at least in principle)
  be written down in an ordered list. Examples include: counts, number
  of arrivals, or number of successes. They are often represented by
  integers, say, 0, 1, 2, \textit{etc}.
\item \textit{Continuous data} take values in an interval of
  numbers. These are also known as scale data, interval data, or
  measurement data. Examples include: height, weight, length,
  time. Continuous data are often characterized by fractions or
  decimals: 3.82, 7.0001, 4 \(\frac{5}{8}\), \textit{etc}.
\end{itemize}

Note that the distinction between discrete and continuous data is not
always clear-cut. Sometimes it is convenient to treat data as if they
were continuous, even though strictly speaking they are not
continuous. See the examples.



\begin{example}[Annual Precipitation in US Cities]
The vector \texttt{precip} \index{Data sets!precip@\texttt{precip}}
contains average amount of rainfall (in inches) for each of 70 cities
in the United States and Puerto Rico. Let us take a look at the data:
\end{example}

<<echo=TRUE>>=
str(precip)
@

<<echo=TRUE>>=
precip[1:4]
@

The output shows that \texttt{precip} is a numeric vector which has been
\textit{named}, that is, each value has a name associated with it (which can
be set with the \texttt{names} \index{names@\texttt{names}} function). These
are quantitative continuous data.



\begin{example}[Lengths of Major North American Rivers]
The U.S. Geological Survey recorded the lengths (in miles) of several
rivers in North America. They are stored in the vector \texttt{rivers}
\index{Data sets!rivers@\texttt{rivers}} in the \texttt{datasets}
package \cite{datasets} (which ships with base \textsf{R}). See
\texttt{?rivers}. Let us take a look at the data with the \texttt{str}
\index{str@\texttt{str}} function.
\end{example}

<<echo=TRUE>>=
str(rivers)
@

The output says that \texttt{rivers} is a numeric vector of length 141, and
the first few values are 735, 320, 325, \textit{etc}. These data are
definitely quantitative and it appears that the measurements have been
rounded to the nearest mile. Thus, strictly speaking, these are
discrete data. But we will find it convenient later to take data like
these to be continuous for some of our statistical procedures.



\begin{example}[Yearly Numbers of Important Discoveries]
The vector \texttt{discoveries} \index{Data
sets!discoveries@\texttt{discoveries}} contains numbers of ``great''
inventions/discoveries in each year from 1860 to 1959, as reported by
the 1975 World Almanac. Let us take a look at the data:
\end{example}

<<echo=TRUE>>=
str(discoveries)
@

The output is telling us that \texttt{discoveries} is a \textit{time series} (see Section~\ref{sub:other-data-types} for more) of length 100. The entries are integers, and since they represent counts this is a good example of
discrete quantitative data. We will take a closer look in the following sections.

\subsection{Displaying Quantitative Data} \label{sub:displaying-quantitative-data}

One of the first things to do when confronted by quantitative data (or
any data, for that matter) is to make some sort of visual display to
gain some insight into the data's structure. There are almost as many
display types from which to choose as there are data sets to plot. We
describe some of the more popular alternatives.

\subsubsection{Strip charts, also known as Dot plots} 
\label{par:strip-charts}

\index{strip chart} \index{dot plot| see\{strip chart\}}

These can be used for discrete or continuous data, and usually look
best when the data set is not too large. Along the horizontal axis is
a numerical scale above which the data values are plotted. We can do
it in \textsf{R} with a call to the \texttt{stripchart}
\index{stripchart@\texttt{stripchart}} function. There are three
available methods.

\begin{itemize}
\item \texttt{overplot}: plots ties covering each other. This method is good to
              display only the distinct values assumed by the data
              set.
\item \texttt{jitter}: adds some noise to the data in the \(y\) direction in
            which case the data values are not covered up by ties.
\item \texttt{stack}: plots repeated values stacked on top of one another. This
           method is best used for discrete data with a lot of ties;
           if there are no repeats then this method is identical to
           overplot.
\end{itemize}

See Figure~\ref{fig:stripcharts}, which was produced by the following code.

<<echo=TRUE, eval=FALSE>>=
stripchart(precip, xlab="rainfall")
stripchart(rivers, method="jitter", xlab="length")
stripchart(discoveries, method="stack", xlab="number")
@

The leftmost graph is a strip chart of the \texttt{precip} data. The
graph shows tightly clustered values in the middle with some others
falling balanced on either side, with perhaps slightly more falling to
the left. Later we will call this a symmetric distribution, see
Section~\ref{sub:shape}. The middle graph is of the \texttt{rivers}
data, a vector of length 141. There are several repeated values in the
rivers data, and if we were to use the overplot method we would lose
some of them in the display. This plot shows a what we will later call
a right-skewed shape with perhaps some extreme values on the far right
of the display. The third graph strip charts \texttt{discoveries} data
which are literally a textbook example of a right skewed distribution.

<<stripcharts, echo=FALSE, fig=TRUE, include=FALSE, width=5>>=
par(mfrow = c(3,1)) # 3 plots: 3 rows, 1 column
stripchart(precip, xlab="rainfall", cex.lab = cexlab)
stripchart(rivers, method="jitter", xlab="length", cex.lab = cexlab)
stripchart(discoveries, method="stack", xlab="number", ylim = c(0,3), cex.lab = cexlab)
par(mfrow = c(1,1)) # back to normal
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-stripcharts}
\end{center}
\caption[Three stripcharts of three data sets.]{\small Three stripcharts of three data sets.  The first graph uses the \texttt{overplot} method, the second the \texttt{jitter} method, and the third the \texttt{stack} method.}
\label{fig:stripcharts}
\end{figure}


The \texttt{DOTplot} \index{DOTplot@\texttt{DOTplot}} function in the
\texttt{UsingR} \index{R packages!UsingR@\texttt{UsingR}} package
\cite{UsingR} is another alternative.

\subsubsection{Histogram} \index{Histogram}

These are typically used for continuous data. A histogram is
constructed by first deciding on a set of classes, or bins, which
partition the real line into a set of boxes into which the data values
fall. Then vertical bars are drawn over the bins with height
proportional to the number of observations that fell into the bin.

These are one of the most common summary displays, and they are often
misidentified as ``Bar Graphs'' (see below.) The scale on the \(y\)
axis can be frequency, percentage, or density (relative
frequency). The term histogram was coined by Karl Pearson in 1891, see
\cite{Miller}.



\begin{example}[Annual Precipitation in US Cities]
  \label{exm:annual}
We are going to take another look at the \texttt{precip}
\index{Data sets!precip@\texttt{precip}} data that we
investigated earlier. The strip chart in Figure~\ref{fig:stripcharts}
suggested a loosely balanced distribution; let us now look to see what
a histogram says.
\end{example}

There are many ways to plot histograms in \textsf{R}, and one of
the easiest is with the \texttt{hist} \index{hist@\texttt{hist}}
function. The following code produces the plots in Figure~\ref{fig:histograms}.

<<echo=TRUE, eval=FALSE>>=
hist(precip, main = "")
hist(precip, freq = FALSE, main = "")
@

Notice the argument \(\mathtt{main = ""}\) which suppresses the main
title from being displayed -- it would have said ``Histogram of
\texttt{precip}'' otherwise. The plot on the left is a frequency histogram
(the default), and the plot on the right is a relative frequency
histogram (\texttt{freq = FALSE}).

<<histograms, echo=FALSE, fig=TRUE, include=FALSE, width=5, height=4>>=
par(mfrow = c(1,2))
hist(precip, main = "", cex.lab = cexlab)
hist(precip, freq = FALSE, main = "", cex.lab = cexlab)
par(mfrow = c(1,1))
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-histograms}
\end{center}
\caption[(Relative) frequency histograms of the \texttt{precip} data.]{\small (Relative) frequency histograms of the \texttt{precip} data.}
\label{fig:histograms}
\end{figure}


Please mind the biggest weakness of histograms: the graph obtained
strongly depends on the bins chosen. Choose another set of bins, and
you will get a different histogram. Moreover, there are not any
definitive criteria by which bins should be defined; the best choice
for a given data set is the one which illuminates the data set's
underlying structure (if any). Luckily for us there are algorithms to
automatically choose bins that are likely to display well, and more
often than not the default bins do a good job. This is not always the
case, however, and a responsible statistician will investigate many
bin choices to test the stability of the display.

Recall that the strip chart in Figure~\ref{fig:stripcharts} suggested
a relatively balanced shape to the \texttt{precip} data
distribution. Watch what happens when we change the bins slightly
(with the \texttt{breaks} argument to \texttt{hist}). See
Figure~\ref{fig:histograms-bins} which was produced by the following
code.

<<echo=TRUE, eval=FALSE>>=
hist(precip, breaks = 10)
hist(precip, breaks = 25)
hist(precip, breaks = 50)
@


<<histograms-bins, echo=FALSE, fig=TRUE, include=FALSE, width=5, height=4>>=
par(mfrow = c(1,3))
hist(precip, breaks = 10, main = "", cex.lab = cexlab)
hist(precip, breaks = 25, main = "", cex.lab = cexlab)
hist(precip, breaks = 50, main = "", cex.lab = cexlab)
par(mfrow = c(1,1))
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-histograms-bins}
\end{center}
\caption[More histograms of the \texttt{precip} data.]{{\small More histograms of the \texttt{precip} data.}}
\label{fig:histograms-bins}
\end{figure}

The leftmost graph (with \texttt{breaks = 10}) shows that the distribution is
not balanced at all. There are two humps: a big one in the middle and
a smaller one to the left. Graphs like this often indicate some
underlying group structure to the data; we could now investigate
whether the cities for which rainfall was measured were similar in
some way, with respect to geographic region, for example.

The rightmost graph in Figure~\ref{fig:histograms-bins} shows what happens when
the number of bins is too large: the histogram is too grainy and hides
the rounded appearance of the earlier histograms. If we were to
continue increasing the number of bins we would eventually get all
observed bins to have exactly one element, which is nothing more than
a glorified strip chart.

\subsubsection{Stem-and-leaf displays (more to be said in Section~\ref{sec:exploratory-data-analysis})}

Stem-and-leaf displays (also known as stemplots) have two basic parts:
\textit{stems} and \textit{leaves}. The final digit of the data values is taken to
be a \textit{leaf}, and the leading digit(s) is (are) taken to be \textit{stems}. We
draw a vertical line, and to the left of the line we list the
stems. To the right of the line, we list the leaves beside their
corresponding stem. There will typically be several leaves for each
stem, in which case the leaves accumulate to the right. It is
sometimes necessary to round the data values, especially for larger
data sets.



\begin{example}[Driver Deaths in the United Kingdom]
\label{exm:ukdriverdeaths-first}
\texttt{UKDriverDeaths} \index{Data
sets!UKDriverDeaths@\texttt{UKDriverDeaths}} is a time series that
contains the total car drivers killed or seriously injured in Great
Britain monthly from Jan 1969 to Dec 1984. See
\texttt{?UKDriverDeaths}. Compulsory seat belt use was introduced on January
31, 1983. We construct a stem and leaf diagram in \textsf{R} with the
\texttt{stem.leaf} \index{stem.leaf@\texttt{stem.leaf}} function from the
\texttt{aplpack} \index{R packages@\textsf{R}
packages!aplpack@\texttt{aplpack}} package \cite{aplpack}.
\end{example}

<<echo=TRUE>>=
stem.leaf(UKDriverDeaths, depth = FALSE)
@

The display shows a more or less balanced mound-shaped distribution,
with one or maybe two humps, a big one and a smaller one just to its
right. Note that the data have been rounded to the tens place so that
each datum gets only one leaf to the right of the dividing line.

Notice that the \texttt{depth}s \index{depths} have been suppressed. To learn
more about this option and many others, see Section~\ref{sec:exploratory-data-analysis}. Unlike a histogram, the original
data values may be recovered from the stem-and-leaf display -- modulo
the rounding -- that is, starting from the top and working down we can
read off the data values 1050, 1070, 1110, 1130, and so forth.


\subsubsection{Index plots}

Done with the \texttt{plot} \index{plot@\texttt{plot}} function. These are
good for plotting data which are ordered, for example, when the data
are measured over time. That is, the first observation was measured at
time 1, the second at time 2, \textit{etc}. It is a two dimensional plot, in
which the index (or time) is the \(x\) variable and the measured value
is the \(y\) variable. There are several plotting methods for index
plots, and we mention two of them:

\begin{itemize}
\item \texttt{spikes}: draws a vertical line from the \(x\)-axis to
  the observation height.
\item \texttt{points}: plots a simple point at the observation height.
\end{itemize}




\begin{example}[Level of Lake Huron 1875-1972] Brockwell and Davis
  \cite{Brockwell1991} give the annual measurements of the level (in
  feet) of Lake Huron from 1875--1972. The data are stored in the time
  series \texttt{LakeHuron}. \index{Data
    sets!LakeHuron@\texttt{LakeHuron}} See
  \texttt{?LakeHuron}. Figure~\ref{fig:indpl-lakehuron} was produced
  with the following code:
\end{example}

<<echo=TRUE, eval=FALSE>>=
plot(LakeHuron)
plot(LakeHuron, type = "p")
plot(LakeHuron, type = "h")
@

The plots show an overall decreasing trend to the observations, and
there appears to be some seasonal variation that increases over time.

<<indpl-lakehuron, echo=FALSE, fig=TRUE, include=FALSE, width=5, height=7>>=
par(mfrow = c(3,1))
plot(LakeHuron, cex.lab = cexlab)
plot(LakeHuron, type = "p", cex.lab = cexlab)
plot(LakeHuron, type = "h", cex.lab = cexlab)
par(mfrow = c(1,1))
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-indpl-lakehuron}
\end{center}
\caption[Index plots of the \texttt{LakeHuron} data.]{{\small Index plots of the \texttt{LakeHuron} data.}}
\label{fig:indpl-lakehuron}
\end{figure}



\subsubsection{Density estimates}

The default method uses a Gaussian kernel density estimate.

<<echo = TRUE, eval = FALSE>>=
# The Old Faithful geyser data
d <- with(faithful, density(eruptions, bw = "sj"))
d
plot(d)
hist(precip, freq = FALSE)
lines(density(precip))
@

\subsection{Qualitative Data, Categorical Data, and Factors} \label{sub:qualitative-data}

Qualitative data are simply any type of data that are not numerical,
or do not represent numerical quantities. Examples of qualitative
variables include a subject's name, gender, race/ethnicity, political
party, socioeconomic status, class rank, driver's license number, and
social security number (SSN).

Please bear in mind that some data \textit{look} to be quantitative but are
\textit{not}, because they do not represent numerical quantities and do not
obey mathematical rules. For example, a person's shoe size is
typically written with numbers: 8, or 9, or 12, or
\(12\,\frac{1}{2}\). Shoe size is not quantitative, however, because
if we take a size 8 and combine with a size 9 we do not get a size 17.

Some qualitative data serve merely to \textit{identify} the observation (such
a subject's name, driver's license number, or SSN). This type of data
does not usually play much of a role in statistics. But other
qualitative variables serve to \textit{subdivide} the data set into
categories; we call these \textit{factors}. In the above examples, gender,
race, political party, and socioeconomic status would be considered
factors (shoe size would be another one). The possible values of a
factor are called its \textit{levels}. For instance, the factor of gender
would have two levels, namely, male and female. Socioeconomic status
typically has three levels: high, middle, and low.

Factors may be of two types: \textit{nominal} \index{nominal data} and
\textit{ordinal}. \index{ordinal data} Nominal factors have levels that
correspond to names of the categories, with no implied
ordering. Examples of nominal factors would be hair color, gender,
race, or political party. There is no natural ordering to ``Democrat''
and ``Republican''; the categories are just names associated with
different groups of people.

In contrast, ordinal factors have some sort of ordered structure to
the underlying factor levels. For instance, socioeconomic status would
be an ordinal categorical variable because the levels correspond to
ranks associated with income, education, and occupation. Another
example of ordinal categorical data would be class rank.

Factors have special status in \textsf{R}. They are represented
internally by numbers, but even when they are written numerically
their values do not convey any numeric meaning or obey any
mathematical rules (that is, Stage III cancer is not Stage I cancer +
Stage II cancer).



\begin{example}[]
The \texttt{state.abb} \index{Data sets!state.abb@\texttt{state.abb}} vector
gives the two letter postal abbreviations for all 50 states.
\end{example}

<<echo=TRUE>>=
str(state.abb)
@

These would be ID data. The \texttt{state.name} \index{Data
sets!state.name@\texttt{state.name}} vector lists all of the complete
names and those data would also be ID.



\begin{example}[U.S.~State Facts and Features]
The U.S. Department of Commerce of the U.S. Census Bureau releases all
sorts of information in the \textit{Statistical Abstract of the United
States}, and the \texttt{state.region} \index{Data
sets!state.region@\texttt{state.region}} data lists each of the 50
states and the region to which it belongs, be it Northeast, South,
North Central, or West. See \texttt{?state.region}.
\end{example}

<<echo=TRUE>>=
str(state.region)
state.region[1:5]
@

The \texttt{str} \index{str@\texttt{str}} output shows that \texttt{state.region} is
already stored internally as a factor and it lists a couple of the
factor levels. To see all of the levels we printed the first five
entries of the vector in the second line.

\subsection{Displaying Qualitative Data} \label{sub:displaying-qualitative-data}

\subsubsection{Tables} \label{par:tables}

One of the best ways to summarize qualitative data is with a table of
the data values. We may count frequencies with the \texttt{table} function or
list proportions with the \texttt{prop.table}
\index{prop.table@\texttt{prop.table}} function (whose input is a
frequency table). In the \textsf{R} Commander you can do it with
\texttt{Statistics} \(\triangleright\) \texttt{Frequency Distribution\ldots}
Alternatively, to look at tables for all factors in the \texttt{Active data
set} \index{Active data set@\texttt{Active data set}} you can do
\texttt{Statistics} \(\triangleright\) \texttt{Summaries} \(\triangleright\) \texttt{Active
Dataset}.

<<echo=TRUE>>=
Tbl <- table(state.division)
Tbl
@

<<echo=TRUE>>=
Tbl/sum(Tbl)      # relative frequencies
@

<<echo=TRUE>>=
prop.table(Tbl)   # same thing
@

\subsubsection{Bar Graphs} \label{par:bar-graphs}

A bar graph is the analogue of a histogram for categorical data. A bar
is displayed for each level of a factor, with the heights of the bars
proportional to the frequencies of observations falling in the
respective categories. A disadvantage of bar graphs is that the levels
are ordered alphabetically (by default), which may sometimes obscure
patterns in the display.



\begin{example}[U.S.~State Facts and Features]
The \texttt{state.region} data lists each of the 50 states and the region to
which it belongs, be it Northeast, South, North Central, or West. See
\texttt{?state.region}. It is already stored internally as a factor. We make
a bar graph with the \texttt{barplot}
\index{barplot@\texttt{barplot}} function:
\end{example}

<<echo=TRUE, eval=FALSE>>=
barplot(table(state.region), ylab="frequency")
barplot(prop.table(table(state.region)), ylab="rel frequency")
@

See Figure~\ref{fig:bar-gr-stateregion}. The display on the left is a
frequency bar graph because the \(y\) axis shows counts, while the
display on the left is a relative frequency bar graph. The only
difference between the two is the scale. Looking at the graph we see
that the majority of the fifty states are in the South, followed by
West, North Central, and finally Northeast. Over 30\% of the states are
in the South.

Notice the \texttt{cex.names} \index{cex.names@\texttt{cex.names}}
argument that we used, above. It expands the names on the \(x\) axis
by 20\% which makes them easier to read. See \texttt{?par}
\index{par@\texttt{par}} for a detailed list of additional
plot parameters.


<<bar-gr-stateregion, echo=FALSE, fig=TRUE, include=FALSE, width=5, height=7>>=
par(mfrow = c(2,1)) # 2 plots: 2 rows, 1 column
barplot(table(state.region), ylab="frequency")
barplot(prop.table(table(state.region)), ylab="rel frequency")
par(mfrow = c(1,1)) # back to normal
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-bar-gr-stateregion}
\end{center}
\caption[Bar graphs of the U.S. State Facts and Features.]{{\small Bar
    graphs of the U.S. State Facts and Features. The top graph is a
    frequency barplot made with \texttt{table} and the bottom is a
    relative frequency barplot made with \texttt{prop.table}.}}
\label{fig:bar-gr-stateregion}
\end{figure}


\subsubsection{Pareto Diagrams} \label{par:pareto-diagrams}

A pareto diagram is a lot like a bar graph except the bars are
rearranged such that they decrease in height going from left to
right. The rearrangement is handy because it can visually reveal
structure (if any) in how fast the bars decrease -- this is much more
difficult when the bars are jumbled.



\begin{example}[U.S.~State Facts and Features]
The \texttt{state.division} \index{Data
sets!state.division@\texttt{state.division}} data record the
division (New England, Middle Atlantic, South Atlantic, East South
Central, West South Central, East North Central, West North Central,
Mountain, and Pacific) of the fifty states. We can make a pareto
diagram with either the \texttt{RcmdrPlugin.IPSUR} \index{R
packages@\textsf{R}
packages!RcmdrPlugin.IPSUR@\texttt{RcmdrPlugin.IPSUR}} package
\cite{RcmdrPlugin.IPSUR} or with the \texttt{pareto.chart}
\index{pareto.chart@\texttt{pareto.chart}} function from the
\texttt{qcc} \index{R packages@\textsf{R}
packages!qcc@\texttt{qcc}} package \cite{qcc}. See Figure~\ref{fig:pareto-chart}. The code follows.
\end{example}


<<eval = FALSE>>=
pareto.chart(table(state.division), ylab="Frequency")
@

<<pareto-chart, echo=FALSE, fig=TRUE, include=FALSE, width=5>>=
pareto.chart(table(state.division), ylab="Frequency", cex.lab = cexlab)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-pareto-chart}
\end{center}
\caption[Pareto chart of the \texttt{state.division} data.]{{\small Pareto chart of the \texttt{state.division} data.}}
\label{fig:pareto-chart}
\end{figure}


\subsubsection{Dot Charts} \label{par:dotcharts}

These are a lot like a bar graph that has been turned on its side with
the bars replaced by dots on horizontal lines. They do not convey any
more (or less) information than the associated bar graph, but the
strength lies in the economy of the display. Dot charts are so compact
that it is easy to graph very complicated multi-variable interactions
together in one graph. See Section~\ref{sec:comparing-data-sets}. We
will give an example here using the same data as above for
comparison. The graph was produced by the following code.

<<dotchart, echo=TRUE, fig=TRUE, include=FALSE, width=5>>=
x <- table(state.region)
dotchart(as.vector(x), labels = names(x), cex.lab = cexlab)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-dotchart}
\end{center}
\caption[Dot chart of the \texttt{state.region} data.]{{\small Dot chart of the \texttt{state.region} data.}}
\label{fig:dotchart}
\end{figure}

See Figure~\ref{fig:dotchart}. Compare it to Figure~\ref{fig:bar-gr-stateregion}.


\subsubsection{Pie Graphs} \label{par:pie-graphs}

These can be done with \textsf{R} and the \textsf{R} Commander, but they fallen out of
favor in recent years because researchers have determined that while
the human eye is good at judging linear measures, it is notoriously
bad at judging relative areas (such as those displayed by a pie
graph). Pie charts are consequently a very bad way of displaying
information unless the number of categories is two or three. A bar
chart or dot chart is a preferable way of displaying qualitative
data. See \texttt{?pie} \index{pie@\texttt{pie}} for more information.

We are not going to do any examples of a pie graph and discourage
their use elsewhere.

\subsection{Logical Data} \label{sub:logical-data}

There is another type of information recognized by \textsf{R}
which does not fall into the above categories. The value is either
\texttt{TRUE} or \texttt{FALSE} (note that equivalently you can use \texttt{1 = TRUE}, \texttt{0 =
FALSE}). Here is an example of a logical vector:

<<echo=TRUE>>=
x <- 5:9
y <- (x < 7.3)
y
@

Many functions in \textsf{R} have options that the user may or may
not want to activate in the function call. For example, the
\texttt{stem.leaf} function has the \texttt{depths} argument which is \texttt{TRUE} by
default. We saw in Section~\ref{sub:quantitative-data} how to turn the option
off, simply enter \texttt{stem.leaf(x, depths = FALSE)} and they will not be
shown on the display.

We can swap \texttt{TRUE} with \texttt{FALSE} with the exclamation point \texttt{!}.

<<echo=TRUE>>=
!y
@


\subsection{Missing Data} \label{sub:missing-data}

Missing data are a persistent and prevalent problem in many
statistical analyses, especially those associated with the social
sciences. \textsf{R} reserves the special symbol \texttt{NA} to
representing missing data.

Ordinary arithmetic with \texttt{NA} values give \texttt{NA}'s (addition,
subtraction, \textit{etc}.) and applying a function to a vector that has an
\texttt{NA} in it will usually give an \texttt{NA}.

<<echo=TRUE>>=
x <- c(3, 7, NA, 4, 7)
y <- c(5, NA, 1, 2, 2)
x + y
@

Some functions have a \texttt{na.rm} argument which when \texttt{TRUE} will ignore
missing data as if they were not there (such as \texttt{mean}, \texttt{var}, \texttt{sd},
\texttt{IQR}, \texttt{mad}, \ldots).

<<echo=TRUE>>=
sum(x)
sum(x, na.rm = TRUE)
@

Other functions do not have a \texttt{na.rm} argument and will return \texttt{NA} or
an error if the argument has \texttt{NA}s. In those cases we can find
the locations of any \texttt{NA}s with the \texttt{is.na} function and remove
those cases with the \texttt{[]} operator.

<<echo=TRUE>>=
is.na(x)
z <- x[!is.na(x)]
sum(z)
@

The analogue of \texttt{is.na} for rectangular data sets (or data frames) is
the \texttt{complete.cases} function. See Appendix~\ref{sec:editing-data-sets}.

\subsection{Other Data Types} \label{sub:other-data-types}

\section{Features of Data Distributions} \label{sec:features-of-data}

Given that the data have been appropriately displayed, the next step
is to try to identify salient features represented in the graph. The
acronym to remember is \textit{C}-enter, \textit{U}-nusual features, \textit{S}-pread, and
\textit{S}-hape. (CUSS).

\subsection{Center} \label{sub:center}

One of the most basic features of a data set is its
\textit{center}. Loosely speaking, the center of a data set is
associated with a number that represents a middle or general tendency
of the data. Of course, there are usually several values that would
serve as a center, and our later tasks will be focused on choosing an
appropriate one for the data at hand. Judging from the histogram that
we saw in Figure~\ref{fig:histograms-bins}, a measure of center would
be about \Sexpr{round(mean(precip))}.

\subsection{Spread} \label{sub:spread}

The \textit{spread} of a data set is associated with its variability;
data sets with a large spread tend to cover a large interval of
values, while data sets with small spread tend to cluster tightly
around a central value.

\subsection{Shape} \label{sub:shape}

When we speak of the \textit{shape} of a data set, we are usually referring
to the shape exhibited by an associated graphical display, such as a
histogram. The shape can tell us a lot about any underlying structure
to the data, and can help us decide which statistical procedure we
should use to analyze them.

\subsubsection{Symmetry and Skewness}

A distribution is said to be \textit{right-skewed} (or
\textit{positively skewed}) if the right tail seems to be stretched
from the center. A \textit{left-skewed} (or \textit{negatively
  skewed}) distribution is stretched to the left side. A symmetric
distribution has a graph that is balanced about its center, in the
sense that half of the graph may be reflected about a central line of
symmetry to match the other half.

We have already encountered skewed distributions: both the discoveries
data in Figure~\ref{fig:stripcharts} and the \texttt{precip} data in
Figure~\ref{fig:histograms-bins} appear right-skewed. The \texttt{UKDriverDeaths}
data in Example~\ref{exm:ukdriverdeaths-first} is relatively
symmetric (but note the one extreme value 2654 identified at the
bottom of the stem-and-leaf display).

\subsubsection{Kurtosis}

Another component to the shape of a distribution is how ``peaked'' it
is. Some distributions tend to have a flat shape with thin
tails. These are called \textit{platykurtic}, and an example of a platykurtic
distribution is the uniform distribution; see Section~\ref{sec:the-continuous-uniform}. On the other end of the spectrum
are distributions with a steep peak, or spike, accompanied by heavy
tails; these are called \textit{leptokurtic}. Examples of leptokurtic
distributions are the Laplace distribution and the logistic
distribution. See Section~\ref{sec:other-continuous-distributions}. In
between are distributions (called \textit{mesokurtic}) with a rounded peak
and moderately sized tails. The standard example of a mesokurtic
distribution is the famous bell-shaped curve, also known as the
Gaussian, or normal, distribution, and the binomial distribution can
be mesokurtic for specific choices of \(p\). See Sections~\ref{sec:binom-dist} and~\ref{sec:the-normal-distribution}.

\subsection{Clusters and Gaps} \label{sub:clusters-and-gaps}

Clusters or gaps are sometimes observed in quantitative data
distributions. They indicate clumping of the data about distinct
values, and gaps may exist between clusters. Clusters often suggest an
underlying grouping to the data. For example, take a look at the
\texttt{faithful} data which contains the duration of \texttt{eruptions} and the
\texttt{waiting} time between eruptions of the Old Faithful geyser in
Yellowstone National Park. Do not be frightened by the complicated
information at the left of the display for now; we will learn how to
interpret it in Section~\ref{sec:exploratory-data-analysis}.

<<echo=TRUE>>=
with(faithful, stem.leaf(eruptions))
@
There are definitely two clusters of data here; an upper cluster and a
lower cluster.


\subsection{Extreme Observations and other Unusual Features} \label{sub:extreme-observations}

Extreme observations fall far from the rest of the data. Such
observations are troublesome to many statistical procedures; they
cause exaggerated estimates and instability. It is important to
identify extreme observations and examine the source of the data more
closely. There are many possible reasons underlying an extreme
observation:

\begin{itemize}
\item \textit{Maybe the value is a typographical error.} Especially with large
  data sets becoming more prevalent, many of which being recorded by
  hand, mistakes are a common problem. After closer scrutiny, these
  can often be fixed.
\item \textit{Maybe the observation was not meant for the study}, because it does
  not belong to the population of interest. For example, in medical
  research some subjects may have relevant complications in their
  genealogical history that would rule out their participation in the
  experiment. Or when a manufacturing company investigates the
  properties of one of its devices, perhaps a particular product is
  malfunctioning and is not representative of the majority of the
  items.
\item \textit{Maybe it indicates a deeper trend or phenomenon.} Many of the most
  influential scientific discoveries were made when the investigator
  noticed an unexpected result, a value that was not predicted by the
  classical theory. Albert Einstein, Louis Pasteur, and others built
  their careers on exactly this circumstance.
\end{itemize}

\section{Descriptive Statistics} \label{sec:descriptive-statistics}

One of my favorite professors would repeatedly harp, ``You cannot do
statistics without data.''

\paragraph{What do I want them to know?}

\begin{itemize}
\item The fundamental data types we encounter most often, how to classify
  given data into a likely type, and that sometimes the distinction is
  blurry.
\end{itemize}

\subsection{Frequencies and Relative Frequencies} \label{sub:frequencies-and-relative}

These are used for categorical data. The idea is that there are a
number of different categories, and we would like to get some idea
about how the categories are represented in the population.

\subsection{Measures of Center} \label{sub:measures-of-center}

The \textit{sample mean} is denoted \(\overline{x}\) (read ``\(x\)-bar'') and
is simply the arithmetic average of the observations:
\begin{equation}
\overline{x}=\frac{x_{1}+x_{2}+\cdots+x_{n}}{n}=\frac{1}{n}\sum_{i=1}^{n}x_{i}.
\end{equation}

\begin{itemize}
\item Good: natural, easy to compute, has nice mathematical properties
\item Bad: sensitive to extreme values
\end{itemize}

It is appropriate for use with data sets that are not highly skewed
without extreme observations.

The \textit{sample median} is another popular measure of center and is
denoted \(\tilde{x}\). To calculate its value, first sort the data
into an increasing sequence of numbers. If the data set has an odd
number of observations then \(\tilde{x}\) is the value of the middle
observation, which lies in position \((n+1)/2\); otherwise, there are
two middle observations and \(\tilde{x}\) is the average of those
middle values.

\begin{itemize}
\item Good: resistant to extreme values, easy to describe
\item Bad: not as mathematically tractable, need to sort the data to calculate
\end{itemize}

One desirable property of the sample median is that it is \textit{resistant}
to extreme observations, in the sense that the value of \(\tilde{x}\)
depends only on those data values in the middle, and is quite
unaffected by the actual values of the outer observations in the
ordered list. The same cannot be said for the sample mean. Any
significant changes in the magnitude of an observation \(x_{k}\)
results in a corresponding change in the value of the
mean. Consequently, the sample mean is said to be \textit{sensitive} to
extreme observations.

The \textit{trimmed mean} is a measure designed to address the sensitivity of
the sample mean to extreme observations. The idea is to ``trim'' a
fraction (less than 1/2) of the observations off each end of the
ordered list, and then calculate the sample mean of what remains. We
will denote it by \(\overline{x}_{t=0.05}\).
\begin{itemize}
\item Good: resistant to extreme values, shares nice statistical properties
\item Bad: need to sort the data
\end{itemize}

\subsubsection{How to do it with \textsf{R}}

\begin{itemize}
\item You can calculate frequencies or relative frequencies with the
  \texttt{table} function, and relative frequencies with
  \texttt{prop.table(table())}.
\item You can calculate the sample mean of a data vector \texttt{x} with the
  command \texttt{mean(x)}.
\item You can calculate the sample median of \texttt{x} with the command \texttt{median(x)}.
\item You can calculate the trimmed mean with the \texttt{trim} argument;
  \texttt{mean(x, trim = 0.05)}.
\end{itemize}

\subsection{Order Statistics and the Sample Quantiles} \label{sub:order-statistics}

A common first step in an analysis of a data set is to sort the
values. Given a data set \(x_{1}\), \(x_{2}\), \ldots, \(x_{n}\), we may
sort the values to obtain an increasing sequence
\begin{equation}
x_{(1)}\leq x_{(2)}\leq x_{(3)}\leq\cdots\leq x_{(n)}
\end{equation}
and the resulting values are called the \textit{order statistics}. The
\(k^{\mathrm{th}}\) entry in the list, \(x_{(k)}\), is the
\(k^{\mathrm{th}}\) order statistic, and approximately \(100(k/n)\)\%
of the observations fall below \(x_{(k)}\). The order statistics give
an indication of the shape of the data distribution, in the sense that
a person can look at the order statistics and have an idea about where
the data are concentrated, and where they are sparse.

The \textit{sample quantiles} are related to the order
statistics. Unfortunately, there is not a universally accepted
definition of them. Indeed, \textsf{R} is equipped to calculate
quantiles using nine distinct definitions! We will describe the
default method (\texttt{type = 7}), but the interested reader can see the
details for the other methods with \texttt{?quantile}.

Suppose the data set has \(n\) observations. Find the sample quantile
of order \(p\) (\(0<p<1\)), denoted \(\tilde{q}_{p}\) , as follows:

\begin{description}
\item [{First step:}] sort the data to obtain the order statistics
                 \(x_{(1)}\), \(x_{(2)}\), \ldots,\(x_{(n)}\).
\item [{Second step:}] calculate \((n-1)p+1\) and write it in the form
                  \(k.d\), where \(k\) is an integer and \(d\) is a
                  decimal.
\item [{Third step:}] The sample quantile \(\tilde{q}_{p}\) is
   \begin{equation}
      \tilde{q}_{p}=x_{(k)}+d(x_{(k+1)}-x_{(k)}).
   \end{equation}
\end{description}

The interpretation of \(\tilde{q}_{p}\) is that approximately \(100p\)
\% of the data fall below the value \(\tilde{q}_{p}\).

Keep in mind that there is not a unique definition of percentiles,
quartiles, \textit{etc}. Open a different book, and you'll find a different
definition. The difference is small and seldom plays a role except in
small data sets with repeated values. In fact, most people do not even
notice in common use.

Clearly, the most popular sample quantile is \(\tilde{q}_{0.50}\),
also known as the sample median, \(\tilde{x}\). The closest runners-up
are the \textit{first quartile} \(\tilde{q}_{0.25}\) and the \textit{third quartile}
\(\tilde{q}_{0.75}\) (the \textit{second quartile} is the median).


\subsubsection{How to do it with \textsf{R}}

\textbf{At the command prompt} We can find the order statistics of a data set
stored in a vector \texttt{x} with the command \texttt{sort(x)}.

We can calculate the sample quantiles of any order \(p\) where
\(0<p<1\) for a data set stored in a data vector \texttt{x} with the
\texttt{quantile} function, for instance, the command \texttt{quantile(x, probs =
c(0, 0.25, 0.37))} will return the smallest observation, the first
quartile, \(\tilde{q}_{0.25}\), and the 37th sample quantile,
\(\tilde{q}_{0.37}\). For \(\tilde{q}_{p}\) simply change the values
in the \texttt{probs} argument to the value \(p\).

\textbf{With the \textsf{R} Commander} we can find the order statistics of a variable
in the \texttt{Active data set} by doing \texttt{Data} \(\triangleright\) \texttt{Manage
variables in Active data set\ldots} \(\triangleright\) \texttt{Compute new
variable\ldots} In the \texttt{Expression to compute} dialog simply type
\texttt{sort(varname)}, where \texttt{varname} is the variable that it is desired to
sort.

In \texttt{Rcmdr}, we can calculate the sample quantiles for a particular
variable with the sequence \texttt{Statistics} \(\triangleright\) \texttt{Summaries}
\(\triangleright\) \texttt{Numerical Summaries\ldots} We can automatically
calculate the quartiles for all variables in the \texttt{Active data set}
with the sequence \texttt{Statistics} \(\triangleright\) \texttt{Summaries}
\(\triangleright\) \texttt{Active Dataset}.

\subsection{Measures of Spread} \label{sub:measures-of-spread}

\subsubsection{Sample Variance and Standard Deviation}

The \textit{sample variance} is denoted \(s^{2}\) and is calculated with the
formula
\begin{equation}
s^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}.
\end{equation}
The \textit{sample standard deviation} is \(s=\sqrt{s^{2}}\). Intuitively,
the sample variance is approximately the average squared distance of
the observations from the sample mean. The sample standard deviation
is used to scale the estimate back to the measurement units of the
original data.

\begin{itemize}
\item Good: tractable, has nice mathematical/statistical properties
\item Bad: sensitive to extreme values
\end{itemize}

We will spend a lot of time with the variance and standard deviation
in the coming chapters. In the meantime, the following two rules give
some meaning to the standard deviation, in that there are bounds on
how much of the data can fall past a certain distance from the mean.

\begin{fact}[Chebychev's Rule]
The proportion of observations within \(k\) standard
deviations of the mean is at least \(1-1/k^{2}\), \textit{i.e.}, at least
75\%, 89\%, and 94\% of the data are within 2, 3, and 4 standard
deviations of the mean, respectively.
\end{fact}

Note that Chebychev's Rule does not say anything about when \(k=1\),
because \(1-1/1^{2}=0\), which states that at least 0\% of the
observations are within one standard deviation of the mean (which is
not saying much).

Chebychev's Rule applies to any data distribution, \textit{any} list of
numbers, no matter where it came from or what the histogram looks
like. The price for such generality is that the bounds are not very
tight; if we know more about how the data are shaped then we can say
more about how much of the data can fall a given distance from the
mean.

\begin{fact}[Empirical Rule]
If data follow a bell-shaped curve, then approximately 68\%, 95\%, and
99.7\% of the data are within 1, 2, and 3 standard deviations of the
mean, respectively.
\label{fac:empirical-rule}
\end{fact}


\subsubsection{Interquartile Range}

Just as the sample mean is sensitive to extreme values, so the
associated measure of spread is similarly sensitive to
extremes. Further, the problem is exacerbated by the fact that the
extreme distances are squared. We know that the sample quartiles are
resistant to extremes, and a measure of spread associated with them is
the \textit{interquartile range} (\(IQR\)) defined by
\(IQR=q_{0.75}-q_{0.25}\).

\begin{itemize}
\item Good: stable, resistant to outliers, robust to nonnormality, easy to
  explain
\item Bad: not as tractable, need to sort the data, only involves the
  middle 50\% of the data.
\end{itemize}

\subsubsection{Median Absolute Deviation}

A measure even more robust than the \(IQR\) is the \textit{median absolute
deviation} (\(MAD\)). To calculate it we first get the median
\(\widetilde{x}\), next the \textit{absolute deviations}
\(|x_{1}-\tilde{x}|\), \(|x_{2}-\tilde{x}|\), \ldots,
\(|x_{n}-\tilde{x}|\), and the \(MAD\) is proportional to the median
of those deviations:
\begin{equation}
MAD\propto\mbox{median}(|x_{1}-\tilde{x}|,\ |x_{2}-\tilde{x}|,\ldots,|x_{n}-\tilde{x}|).
\end{equation}
That is, the
\(MAD=c\cdot\mbox{median}(|x_{1}-\tilde{x}|,\ |x_{2}-\tilde{x}|,\ldots,|x_{n}-\tilde{x}|)\),
where \(c\) is a constant chosen so that the \(MAD\) has nice
properties. The value of \(c\) in \textsf{R} is by default
\(c=1.4286\). This value is chosen to ensure that the estimator of
\(\sigma\) is correct, on the average, under suitable sampling
assumptions (see Section~\ref{sec:point-estimation}).

\begin{itemize}
\item Good: stable, very robust, even more so than the \(IQR\).
\item Bad: not tractable, not well known and less easy to explain.
\end{itemize}

\subsubsection{Comparing Apples to Apples}

We have seen three different measures of spread which, for a given
data set, will give three different answers. Which one should we use?
It depends on the data set. If the data are well behaved, with an
approximate bell-shaped distribution, then the sample mean and sample
standard deviation are natural choices with nice mathematical
properties. However, if the data have an unusual or skewed shape with
several extreme values, perhaps the more resistant choices among the
\(IQR\) or \(MAD\) would be more appropriate.

However, once we are looking at the three numbers it is important to
understand that the estimators are not all measuring the same
quantity, on the average. In particular, it can be shown that when the
data follow an approximately bell-shaped distribution, then on the
average, the sample standard deviation \(s\) and the \(MAD\) will be
the approximately the same value, namely, \(\sigma\), but the \(IQR\)
will be on the average 1.349 times larger than \(s\) and the
\(MAD\). See~\ref{cha:sampling-distributions} for more details.

\subsubsection{How to do it with \textsf{R}}

\textbf{At the command prompt} we may compute the sample range with
\texttt{range(x)} and the sample variance with \texttt{var(x)}, where \texttt{x} is a
numeric vector. The sample standard deviation is \texttt{sqrt(var(x))} or
just \texttt{sd(x)}. The \(IQR\) is \texttt{IQR(x)} and the median absolute
deviation is \texttt{mad(x)}.

\textbf{With the \textsf{R} Commander} we can calculate the sample standard deviation
with the \texttt{Statistics} \(\triangleright\) \texttt{Summaries}
\(\triangleright\) \texttt{Numerical Summaries\ldots}
combination. \textsf{R} Commander does not calculate the \(IQR\)
or \(MAD\) in any of the menu selections, by default.

\subsection{Measures of Shape} \label{sub:measures-of-shape}

\subsubsection{Sample Skewness}

The \textit{sample skewness}, denoted by \(g_{1}\), is defined by the formula
\begin{equation}
g_{1}=\frac{1}{n}\frac{\sum_{i=1}^{n}(x_{i}-\overline{x})^{3}}{s^{3}}.
\end{equation}
The sample skewness can be any value \(-\infty<g_{1}<\infty\). The
sign of \(g_{1}\) indicates the direction of skewness of the
distribution. Samples that have \(g_{1}>0\) indicate right-skewed
distributions (or positively skewed), and samples with \(g_{1}<0\)
indicate left-skewed distributions (or negatively skewed). Values of
\(g_{1}\) near zero indicate a symmetric distribution. These are not
hard and fast rules, however. The value of \(g_{1}\) is subject to
sampling variability and thus only provides a suggestion to the
skewness of the underlying distribution.

We still need to know how big is ``big'', that is, how do we judge
whether an observed value of \(g_{1}\) is far enough away from zero
for the data set to be considered skewed to the right or left? A good
rule of thumb is that data sets with skewness larger than
\(2\sqrt{6/n}\) in magnitude are substantially skewed, in the
direction of the sign of \(g_{1}\). See Tabachnick \& Fidell
\cite{Tabachnick2006} for details.

\subsubsection{Sample Excess Kurtosis}

The \textit{sample excess kurtosis}, denoted by \(g_{2}\), is given by the
formula
\begin{equation}
g_{2}=\frac{1}{n}\frac{\sum_{i=1}^{n}(x_{i}-\overline{x})^{4}}{s^{4}}-3.
\end{equation}
The sample excess kurtosis takes values \(-2\leq g_{2}<\infty\). The
subtraction of 3 may seem mysterious but it is done so that mound
shaped samples have values of \(g_{2}\) near zero. Samples with
\(g_{2}>0\) are called \textit{leptokurtic}, and samples with \(g_{2}<0\) are
called \textit{platykurtic}. Samples with \(g_{2}\approx0\) are called
\textit{mesokurtic}.

As a rule of thumb, if \(|g_{2}|>4\sqrt{6/n}\) then the sample excess
kurtosis is substantially different from zero in the direction of the
sign of \(g_{2}\). See Tabachnick \& Fidell \cite{Tabachnick2006} for
details.

Notice that both the sample skewness and the sample kurtosis are
invariant with respect to location and scale, that is, the values of
\(g_{1}\) and \(g_{2}\) do not depend on the measurement units of the
data.

\subsubsection{How to do it with \textsf{R}}

The \texttt{e1071} package \cite{e1071} has the \texttt{skewness} function for the
sample skewness and the \texttt{kurtosis} function for the sample excess
kurtosis. Both functions have a \texttt{na.rm} argument which is \texttt{FALSE} by
default.



\begin{example}[]
We said earlier that the \texttt{discoveries} data looked positively skewed;
let's see what the statistics say:
\end{example}

<<echo=TRUE>>=
e1071::skewness(discoveries)
2*sqrt(6/length(discoveries))
@

The data are definitely skewed to the right. Let us check the sample
excess kurtosis of the \texttt{UKDriverDeaths} data:

<<echo=TRUE>>=
kurtosis(UKDriverDeaths)
4*sqrt(6/length(UKDriverDeaths))
@

so that the \texttt{UKDriverDeaths} data appear to be mesokurtic, or at least
not substantially leptokurtic.

\section{Exploratory Data Analysis} \label{sec:exploratory-data-analysis}

This field was founded (mostly) by John Tukey (1915-2000). Its tools
are useful when not much is known regarding the underlying causes
associated with the data set, and are often used for checking
assumptions. For example, suppose we perform an experiment and collect
some data\ldots now what? We look at the data using exploratory visual
tools.

\subsection{More About Stem-and-leaf Displays}

There are many bells and whistles associated with stemplots, and the
\texttt{stem.leaf} function can do many of them.

\begin{description}
\item [{Trim Outliers:}] Some data sets have observations that fall far
                    from the bulk of the other data (in a sense made
                    more precise in Section~\ref{sub:outliers}). These extreme
                    observations often obscure the underlying
                    structure to the data and are best left out of the
                    data display. The \texttt{trim.outliers} argument (which
                    is \texttt{TRUE} by default) will separate the extreme
                    observations from the others and graph the
                    stemplot without them; they are listed at the
                    bottom (respectively, top) of the stemplot with
                    the label \texttt{HI} (respectively \texttt{LO}).
\item [{Split Stems:}] The standard stemplot has only one line per stem,
                  which means that all observations with first digit
                  \texttt{3} are plotted on the same line, regardless of the
                  value of the second digit. But this gives some
                  stemplots a ``skyscraper'' appearance, with too many
                  observations stacked onto the same stem. We can
                  often fix the display by increasing the number of
                  lines available for a given stem. For example, we
                  could make two lines per stem, say, \texttt{3*} and
                  \texttt{3.}. Observations with second digit 0 through 4
                  would go on the upper line, while observations with
                  second digit 5 through 9 would go on the lower
                  line. (We could do a similar thing with five lines
                  per stem, or even ten lines per stem.) The end
                  result is a more spread out stemplot which often
                  looks better. A good example of this was shown in Section~\ref{sub:clusters-and-gaps}.
\item [{Depths:}] these are used to give insight into the balance of the
             observations as they accumulate toward the median. In a
             column beside the standard stemplot, the frequency of the
             stem containing the sample median is shown in
             parentheses. Next, frequencies are accumulated from the
             outside inward, including the outliers. Distributions
             that are more symmetric will have better balanced depths
             on either side of the sample median.
\end{description}

\subsubsection{How to do it with \textsf{R}}

The basic command is \texttt{stem(x)} or a more sophisticated version written
by Peter Wolf called \texttt{stem.leaf(x)} in the \textsf{R}
Commander. We will describe \texttt{stem.leaf} since that is the one used by
R Commander.

WARNING: Sometimes when making a stem-and-leaf display the result will
not be what you expected. There are several reasons for this:

\begin{itemize}
\item Stemplots by default will trim extreme observations (defined in
  Section~\ref{sub:outliers}) from the display. This in some cases will result
  in stemplots that are not as wide as expected.
\item The leafs digit is chosen automatically by \texttt{stem.leaf} according to
  an algorithm that the computer believes will represent the data
  well. Depending on the choice of the digit, \texttt{stem.leaf} may drop
  digits from the data or round the values in unexpected ways.
\end{itemize}

Let us take a look at the \texttt{rivers} data set.

<<echo=TRUE>>=
stem.leaf(rivers)
@

The stem-and-leaf display shows a right-skewed shape to the \texttt{rivers}
data distribution. Notice that the last digit of each of the data
values were dropped from the display. Notice also that there were
eight extreme observations identified by the computer, and their exact
values are listed at the bottom of the stemplot. Look at the scale on
the left of the stemplot and try to imagine how ridiculous the graph
would have looked had we tried to include enough stems to include
these other eight observations; the stemplot would have stretched over
several pages. Notice finally that we can use the depths to
approximate the sample median for these data. The median lies in the
row identified by \texttt{(18)}, which means that the median is the average
of the ninth and tenth observation on that row. Those two values
correspond to \texttt{43} and \texttt{43}, so a good guess for the median would
be 430. (For the record, the sample median is
\(\widetilde{x}=425\). Recall that stemplots round the data to the
nearest stem-leaf pair.)

Next let us see what the \texttt{precip} data look like.

<<echo=TRUE>>=
stem.leaf(precip)
@

Here is an example of split stems, with two lines per stem. The final
digit of each datum has been dropped for the display. The data appear
to be left skewed with four extreme values to the left and one extreme
value to the right. The sample median is approximately 37 (it turns
out to be 36.6).

\subsection{Hinges and the Five Number Summary} \label{sub:hinges-and-5ns}

Given a data set \(x_{1}\), \(x_{2}\), \ldots, \(x_{n}\), the hinges are
found by the following method:

\begin{itemize}
\item Find the order statistics \(x_{(1)}\), \(x_{(2)}\), \ldots,
  \(x_{(n)}\).

\item The \textit{lower hinge} \(h_{L}\) is in position
  \(L=\left\lfloor (n+3)/2\right\rfloor / 2\), where the symbol
  $\left\lfloor x\right\rfloor$ denotes the largest integer less than
  or equal to \(x\). If the position \(L\) is not an integer, then the
  hinge \(h_{L}\) is the average of the adjacent order statistics.

\item The \textit{upper hinge} \(h_{U}\) is in position \(n+1-L\).
\end{itemize}

Given the hinges, the \textit{five number summary} (\(5NS\)) is
\begin{equation}
5NS=(x_{(1)},\ h_{L},\ \tilde{x},\ h_{U},\ x_{(n)}).
\end{equation}
An advantage of the \(5NS\) is that it reduces a potentially large
data set to a shorter list of only five numbers, and further, these
numbers give insight regarding the shape of the data distribution
similar to the sample quantiles in Section~\ref{sub:order-statistics}.

\subsubsection{How to do it with \textsf{R}}

If the data are stored in a vector \texttt{x}, then you can compute the
\(5NS\) with the \texttt{fivenum} function.

\subsection{Boxplots} \label{sub:boxplots}

A boxplot is essentially a graphical representation of the \(5NS\). It
can be a handy alternative to a stripchart when the sample size is
large.

A boxplot is constructed by drawing a box alongside the data axis with
sides located at the upper and lower hinges. A line is drawn parallel
to the sides to denote the sample median. Lastly, whiskers are
extended from the sides of the box to the maximum and minimum data
values (more precisely, to the most extreme values that are not
potential outliers, defined below).

Boxplots are good for quick visual summaries of data sets, and the
relative positions of the values in the \(5NS\) are good at indicating
the underlying shape of the data distribution, although perhaps not as
effectively as a histogram. Perhaps the greatest advantage of a
boxplot is that it can help to objectively identify extreme
observations in the data set as described in the next section.

Boxplots are also good because one can visually assess multiple
features of the data set simultaneously:

\begin{description}
\item [{Center:}] can be estimated by the sample median, \(\tilde{x}\).
\item [{Spread:}] can be judged by the width of the box,
\(h_{U}-h_{L}\). We know that this will be close to the \(IQR\), which
can be compared to \(s\) and the \(MAD\), perhaps after rescaling if
appropriate.
\item [{Shape:}] is indicated by the relative lengths of the whiskers, and
the position of the median inside the box. Boxes with unbalanced
whiskers indicate skewness in the direction of the long
whisker. Skewed distributions often have the median tending in the
opposite direction of skewness. Kurtosis can be assessed using the box
and whiskers. A wide box with short whiskers will tend to be
platykurtic, while a skinny box with wide whiskers indicates
leptokurtic distributions.
\item [{Extreme observations:}] are identified with open circles (see
  below).
\end{description}

\subsection{Outliers} \label{sub:outliers}

A \textit{potential outlier} is any observation that falls beyond 1.5 times
the width of the box on either side, that is, any observation less
than \(h_{L}-1.5(h_{U}-h_{L})\) or greater than
\(h_{U}+1.5(h_{U}-h_{L})\). A \textit{suspected outlier} is any observation
that falls beyond 3 times the width of the box on either side. In
R, both potential and suspected outliers (if present) are
denoted by open circles; there is no distinction between the two.

When potential outliers are present, the whiskers of the boxplot are
then shortened to extend to the most extreme observation that is not a
potential outlier. If an outlier is displayed in a boxplot, the index
of the observation may be identified in a subsequent plot in \texttt{Rcmdr}
by clicking the \texttt{Identify outliers with mouse} option in the \texttt{Boxplot}
dialog.

What do we do about outliers? They merit further investigation. The
primary goal is to determine why the observation is outlying, if
possible. If the observation is a typographical error, then it should
be corrected before continuing. If the observation is from a subject
that does not belong to the population of interest, then perhaps the
datum should be removed. Otherwise, perhaps the value is hinting at
some hidden structure to the data.

\subsubsection{How to do it with \textsf{R}}

The quickest way to visually identify outliers is with a boxplot, described above. Another way is with the \texttt{boxplot.stats} function.

\begin{example}[Lengths of Major North American Rivers]
We will look for potential outliers in the \texttt{rivers} data.
\end{example}

<<echo=TRUE>>=
boxplot.stats(rivers)$out
@

We may change the \texttt{coef} argument to 3 (it is 1.5 by default) to
identify suspected outliers.

<<echo=TRUE>>=
boxplot.stats(rivers, coef = 3)$out
@

\subsection{Standardizing variables}

It is sometimes useful to compare data sets with each other on a scale
that is independent of the measurement units. Given a set of observed
data \(x_{1}\), \(x_{2}\), \ldots, \(x_{n}\) we get \(z\) scores, denoted
\(z_{1}\), \(z_{2}\), \ldots, \(z_{n}\), by means of the following
formula \[ z_{i}=\frac{x_{i}-\overline{x}}{s},\quad
i=1,\,2,\,\ldots,\, n.  \]

\subsubsection{How to do it with \textsf{R}}

The \texttt{scale} function will rescale a numeric vector (or data frame) by
subtracting the sample mean from each value (column) and/or by
dividing each observation by the sample standard deviation.

\section{Multivariate Data and Data Frames} \label{sec:multivariate-data}

We have had experience with vectors of data, which are long lists of
numbers. Typically, each entry in the vector is a single measurement
on a subject or experimental unit in the study. We saw in Section~\ref{sub:vectors} how to form vectors with the \texttt{c} function or the
\texttt{scan} function.

However, statistical studies often involve experiments where there are
two (or more) measurements associated with each subject. We display
the measured information in a rectangular array in which each row
corresponds to a subject, and the columns contain the measurements for
each respective variable. For instance, if one were to measure the
height and weight and hair color of each of 11 persons in a research
study, the information could be represented with a rectangular
array. There would be 11 rows. Each row would have the person's height
in the first column and hair color in the second column.

The corresponding objects in \textsf{R} are called \textit{data frames},
and they can be constructed with the \texttt{data.frame} function. Each row
is an observation, and each column is a variable.



\begin{example}[]
Suppose we have two vectors \texttt{x} and \texttt{y} and we want to make a data
frame out of them.
\end{example}

<<>>=
x <- 5:8
y <- letters[3:6]
A <- data.frame(v1 = x, v2 = y)
@

Notice that \texttt{x} and \texttt{y} are the same length. This is \textit{necessary}. Also
notice that \texttt{x} is a numeric vector and \texttt{y} is a character vector. We
may choose numeric and character vectors (or even factors) for the
columns of the data frame, but each column must be of exactly one
type. That is, we can have a column for \texttt{height} and a column for
\texttt{gender}, but we will get an error if we try to mix function \texttt{height}
(numeric) and \texttt{gender} (character or factor) information in the same
column.

Indexing of data frames is similar to indexing of vectors. To get the
entry in row \(i\) and column \(j\) do \texttt{A[i,j]}. We can get entire
rows and columns by omitting the other index.

<<echo=TRUE>>=
A[3, ]
A[ , 1]
A[ , 2]
@

There are several things happening above. Notice that \texttt{A[3, ]} gave a
data frame (with the same entries as the third row of \texttt{A}) yet \texttt{A[ ,
1]} is a numeric vector. \texttt{A[ ,2]} is a factor vector because the
default setting for \texttt{data.frame} is \texttt{stringsAsFactors = TRUE}.

Data frames have a \texttt{names} attribute and the names may be extracted
with the \texttt{names} function. Once we have the names we may extract given
columns by way of the dollar sign.

<<echo=TRUE>>=
names(A)
A['v1']
@

The above is identical to \texttt{A[ ,1]}.

\subsection{Bivariate Data} \label{sub:bivariate-data}

Here we take two variables at a time and explore how they are related.  Each one of the variables will be either quantitative or qualitative.

\subsubsection{Qualitative versus Qualitative}

In this case we will usually begin with a two-way contingency table.

Two-Way Tables. Done with \texttt{table}, or in the \textsf{R}
Commander by following \texttt{Statistics} \(\triangleright\) \texttt{Contingency Tables} \(\triangleright\) \texttt{Two-way Tables}. You can also enter and analyze a two-way table in the \textsf{R} Commander.

\begin{example}[Survival of passengers on the Titanic]
  These data set provide information on the fate of passengers on the
  fatal maiden voyage of the ocean liner Titanic, summarized according
  to economic status (class), sex, age and survival.
\end{example}

We start by making a table with the \texttt{xtabs} function.

<<>>=
A <- xtabs(Freq ~ Survived + Class, data = Titanic)
addmargins(A)
@

The \texttt{addmargins} function simply adds row/column sums to the
table made by \texttt{xtabs}.  We can read the numbers from the
output, of course, but it was Farquar and Farquar in who rightly said
that ``Getting information from a table is like extracting sunbeams
from a cucumber.''  We try visual displays to learn about the data.

<<echo=TRUE, eval=FALSE>>=
barplot(A, legend.text = TRUE, args.legend = list(x="topleft"))
barplot(A, legend.text = TRUE, beside = TRUE, args.legend = list(x="topleft"))
@


<<barplots-Titanic, echo=FALSE, fig=TRUE, include=FALSE, height=4,width=6.5>>=
par(mfrow=c(1,2))
barplot(A, legend.text = TRUE, args.legend = list(x="topleft"))
barplot(A, legend.text = TRUE, beside = TRUE, args.legend = list(x="topleft"))
par(mfrow=c(1,1))
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-barplots-Titanic}
\end{center}
\vspace{-0.5in}
\caption[Bar plots of the \texttt{Titanic} data.]{{\small Bar plots of the \texttt{Titanic} data.}}
\label{fig:barplots-Titanic}
\end{figure}

<<echo=TRUE, eval=FALSE>>=
spineplot(A)
mosaicplot(A)
@


<<spineplot-Titanic, echo=FALSE, fig=TRUE, include=FALSE, height=4,width=6.5>>=
par(mfrow=c(1,2))
spineplot(A)
mosaicplot(A)
par(mfrow=c(1,1))
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-spineplot-Titanic}
\end{center}
\vspace{-0.5in}
\caption[Spine and Mosaic plots of the \texttt{Titanic} data.]{{\small Spine and Mosaic plots of the \texttt{Titanic} data.}}
\label{fig:spineplot-Titanic}
\end{figure}


\subsubsection{Quantitative versus Quantitative}

Two quantitative variables are usually displayed with some sort of scatter plot.

\begin{example}[Reaction Velocity of an Enzymatic Reaction]
The \texttt{Puromycin} data records the reaction velocity (\texttt{rate}) versus substrate concentration (\texttt{conc}) in an enzymatic reaction involving untreated cells or cells treated with Puromycin.  A scatterplot of the two variables is in Figure~ref{}.  We see that rate increases as concentration increases, and in a nonlinear fashion.
\end{example}

\begin{example}[The JoynerBoore Attenuation Data]
The data record peak accelerations measured at various observation stations for 23 earthquakes in California.
\end{example}

<<>>=
par(mfrow=c(1,2))
plot(rate ~ conc, data = Puromycin)
library(lattice)
xyplot(accel ~ dist, data = attenu)
par(mfrow=c(1,1))
@

\begin{example}[Old Faithful Geyser Data]
These data record the waiting time between eruptions and the duration of the eruption for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA.
\end{example}

\begin{example}[Vapor Pressure of Mercury as a Function of Temperature]
These are data on the relation between temperature in degrees Celsius and vapor pressure of mercury in millimeters (of mercury).
\end{example}


<<>>=
par(mfrow=c(1,2))
xyplot(eruptions ~ waiting, data = faithful)
xyplot(pressure ~ temperature, data = pressure)
par(mfrow=c(1,1))
@

The \textit{sample Pearson product-moment correlation coefficient}:
\[
r=\frac{\sum_{i=1}^{n}(x_{i}-\overline{x})(y_{i}-\overline{y})}{\sqrt{\sum_{i=1}^{n}(x_{i}-\overline{x})}\sqrt{\sum_{i=1}^{n}(y_{i}-\overline{y})}}
\]
\begin{itemize}
\item independent of scale
\item \(-1< r <1\)
\item measures \textit{strength} and \textit{direction} of linear association
\end{itemize}

\begin{itemize}
\item Rules of thumb:
  \begin{itemize}
  \item \(0 < \vert r\vert  < 0.3\), weak linear association
  \item \(0.3 < \vert r\vert <0.7\), moderate linear association
  \item \(0.7 < \vert r\vert  < 1\), strong linear association
  \end{itemize}
\item Just because \(r \approx 0\) doesn't mean there isn't any association
\end{itemize}

<<>>=
par(mfrow=c(1,2))
plot( carb ~ optden, data = Formaldehyde)
plot(weight ~ height, data = women)
par(mfrow=c(1,1))
@

\subsubsection{Qualitative versus Qualitative}

We will talk about this more in Section~\ref{sec:comparing-data-sets}.

\subsection{Multivariate Data} \label{sub:multivariate-data}

Multivariate Data Display

\begin{itemize}
\item Multi-Way Tables. You can do this with \texttt{table}, or in \textsf{R}
Commander by following \texttt{Statistics} \(\triangleright\) \texttt{Contingency Tables} \(\triangleright\) \texttt{Multi-way Tables}.

\item Scatterplot matrix. used for displaying pairwise scatterplots simultaneously. Again, look for linear association and correlation.

\item 3D Scatterplot. See Figure~\ref{fig:3D-scatterplot-trees}

\item \texttt{plot(state.region, state.division)}

\item \texttt{barplot(table(state.division,state.region), legend.text=TRUE)}
\end{itemize}

<<eval=FALSE>>=
require(graphics)
mosaicplot(HairEyeColor)
x <- apply(HairEyeColor, c(1, 2), sum)
x
mosaicplot(x, main = "Relation between hair and eye color")
y <- apply(HairEyeColor, c(1, 3), sum)
y
mosaicplot(y, main = "Relation between hair color and sex")
z <- apply(HairEyeColor, c(2, 3), sum)
z
mosaicplot(z, main = "Relation between eye color and sex")
@


\section{Comparing Populations} \label{sec:comparing-data-sets}

Sometimes we have data from two or more groups (or populations) and we
would like to compare them and draw conclusions. Some issues that we
would like to address:

\begin{itemize}
\item Comparing centers and spreads: variation within versus between groups
\item Comparing clusters and gaps
\item Comparing outliers and unusual features
\item Comparing shapes.
\end{itemize}

\subsection{Numerically}

\subsubsection{Data Description}

We provide basic descriptive statistics (mean, standard deviation, and sample size) for each cell (that is, factor level). These are displayed below.

\textbf{Cell means}

<<>>=
with(chickwts, tapply(weight, list(feed = feed), mean))
@

\textbf{Cell standard deviations}

<<>>=
with(chickwts, tapply(weight, list(feed = feed), sd))
@

\textbf{Cell counts}
<<>>=
with(chickwts, tapply(weight, list(feed = feed), length))
@





I am thinking here about the \texttt{Statistics} \(\triangleright\)
\texttt{Numerical Summaries} \(\triangleright\) \texttt{Summarize by groups} option
or the \texttt{Statistics} \(\triangleright\) \texttt{Summaries} \(\triangleright\)
\texttt{Table of Statistics} option.

\subsection{Graphically}

\begin{itemize}
\item Boxplots
  \begin{itemize}
  \item Variable width: the width of the drawn boxplots are proportional
    to \(\sqrt{n_{i}}\), where \(n_{i}\) is the size of the
    \(i^{\mathrm{th}}\) group. Why? Because many statistics have
    variability proportional to the reciprocal of the square root of
    the sample size.
  \item Notches: extend to \(1.58\cdot(h_{U}-h_{L})/\sqrt{n}\). The idea
    is to give roughly a 95\% confidence interval for the difference in
    two medians. See Chapter~\ref{cha:hypothesis-testing}.
    \end{itemize}
\item Stripcharts
  \begin{itemize}
  \item stripchart(weight ~ feed, method= "stack", data=chickwts)
  \end{itemize}
\item Bar Graphs
  \begin{itemize}
  \item barplot(xtabs(Freq ~ Admit + Gender, data = UCBAdmissions))
    stacked bar chart
  \item barplot(xtabs(Freq ~ Admit, data = UCBAdmissions))
  \item barplot(xtabs(Freq ~ Gender + Admit, data = UCBAdmissions, legend = TRUE, beside = TRUE)  oops, discrimination.
  \item barplot(xtabs(Freq ~ Admit+Dept, data = UCBAdmissions), legend = TRUE, beside = TRUE) different departments have different standards
  \item barplot(xtabs(Freq ~ Gender+Dept, data = UCBAdmissions), legend = TRUE, beside = TRUE) men mostly applied to easy departments, women mostly applied to difficult departments
  \item barplot(xtabs(Freq ~ Gender+Dept, data = UCBAdmissions), legend = TRUE, beside = TRUE)
  \item barchart(Admit ~ Freq, data = C)
  \item barchart(Admit ~ Freq|Gender, data = C)
  \item barchart(Admit ~ Freq | Dept, groups = Gender, data = C)
  \item barchart(Admit ~ Freq | Dept, groups = Gender, data = C,
    auto.key = TRUE)
    \end{itemize}
\item Histograms
  \begin{itemize}
  \item ~ breaks | wool * tension, data = warpbreaks
  \item ~ weight | feed, data = chickwts
  \item ~ weight | group, data = PlantGrowth
  \item ~ count | spray, data = InsectSprays
  \item ~ len | dose, data = ToothGrowth
  \item ~ decrease | treatment, data = OrchardSprays (or rowpos or colpos)
\item Scatterplots
  \end{itemize}

<<echo=TRUE, eval=FALSE>>=
xyplot(Petal.Width ~ Petal.Length, data = iris, group = Species)
@

<<xyplot, echo=FALSE, fig=TRUE, include=FALSE, width=5>>=
print(xyplot(Petal.Width ~ Petal.Length, data = iris, group = Species))
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-xyplot}
\end{center}
\caption[Scatterplot of petal width versus length in the \texttt{iris} data.]{{\small Scatterplot of petal width versus length in the \texttt{iris} data.}}
\label{fig:xyplot}
\end{figure}



\item Scatterplot matrices

<<eval=FALSE>>=
splom( ~ cbind(GNP.deflator,GNP,Unemployed,Armed.Forces,
             Population,Year,Employed),  data = longley)
splom( ~ cbind(pop15,pop75,dpi), data = LifeCycleSavings)
splom( ~ cbind(Murder, Assault, Rape), data = USArrests)
splom( ~ cbind(CONT, INTG, DMNR), data = USJudgeRatings)
splom( ~ cbind(area,peri,shape,perm), data = rock)
splom( ~ cbind(Air.Flow, Water.Temp, Acid.Conc.,
               stack.loss), data = stackloss)
#splom( ~ cbind(Fertility,Agriculture,Examination,Education,Catholic,Infant.Mortality), data = swiss)
#splom(~ cbind(Fertility,Agriculture,Examination), data = swiss) #(positive and negative)
@

\item Dot charts

<<eval=FALSE>>=
#dotchart(USPersonalExpenditure)
#dotchart(t(USPersonalExpenditure))
dotchart(WorldPhones)
dotplot(Survived ~ Freq | Class, groups = Sex, data = B)
dotplot(Admit ~ Freq | Dept, groups = Gender, data = C)
@

\item Mosaic plot
<<eval=FALSE>>=
mosaic( ~ Survived + Class + Age + Sex, data = Titanic)
    #  (or just mosaic(Titanic))
mosaic( ~ Admit + Dept + Gender, data = UCBAdmissions)
@

\item Spine plots

<<eval=FALSE>>=
spineplot(xtabs(Freq ~ Admit + Gender, data = UCBAdmissions))
# rescaled barplot
@

\item Quantile-quantile plots: There are two ways to do this. One way is
  to compare two independent samples (of the same
  size). qqplot(x,y). Another way is to compare the sample quantiles
  of one variable to the theoretical quantiles of another
  distribution.

Given two samples \(x_{1}\), \(x_{2}\), \ldots, \(x_{n}\), and \(y_{1}\),
\(y_{2}\), \ldots, \(y_{n}\), we may find the order statistics
\(x_{(1)}\leq x_{(2)}\leq\cdots\leq x_{(n)}\) and \(y_{(1)}\leq
y_{(2)}\leq\cdots\leq y_{(n)}\). Next, plot the \(n\) points
\((x_{(1)},y_{(1)})\), \((x_{(2)},y_{(2)})\), \ldots,
\((x_{(n)},y_{(n)})\).

It is clear that if \(x_{(k)}=y_{(k)}\) for all \(k=1,2,\ldots,n\),
then we will have a straight line. It is also clear that in the real
world, a straight line is NEVER observed, and instead we have a
scatterplot that hopefully had a general linear trend. What do the
rules tell us?

\item If the \(y\)-intercept of the line is greater (less) than zero, then
  the center of the \(Y\) data is greater (less) than the center of
  the \(X\) data.
\item If the slope of the line is greater (less) than one, then the spread
  of the \(Y\) data is greater (less) than the spread of the \(X\) data.

\end{itemize}


\subsection{Lattice Graphics} \label{sub:lattice-graphics}

The following types of plots are useful when there is one variable of
interest and there is a factor in the data set by which the variable
is categorized.

It is sometimes nice to set \texttt{lattice.options(default.theme = "col.whitebg")}

\subsubsection{Side by side boxplots}

<<echo=TRUE, eval=FALSE>>=
bwplot(~weight | feed, data = chickwts)
@

<<bwplot, echo=FALSE, fig=TRUE, include=FALSE, width=5, height=4>>=
print(bwplot(~weight | feed, data = chickwts))
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-bwplot}
\end{center}
\caption[Boxplots of \texttt{weight} by \texttt{feed} type in the \texttt{chickwts} data.]{{\small Boxplots of \texttt{weight} by \texttt{feed} type in the \texttt{chickwts} data.}}
\label{fig:bwplot}
\end{figure}


\subsubsection{Histograms}

<<echo=TRUE, eval=FALSE>>=
histogram(~age | education, data = infert)
@

<<histg, echo=FALSE, fig=TRUE, include=FALSE, width=5, height=4>>=
print(histogram(~age | education, data = infert))
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-histg}
\end{center}
\caption[Histograms of \texttt{age} by \texttt{education} level from the \texttt{infert} data.]{{\small Histograms of \texttt{age} by \texttt{education} level from the \texttt{infert} data.}}
\label{fig:histg}
\end{figure}



\subsubsection{Scatterplots}

<<echo=TRUE, eval=FALSE>>=
xyplot(Petal.Length ~ Petal.Width | Species, data = iris)
@

<<xyplot-by, echo=FALSE, fig=TRUE, include=FALSE, width=5, height=4>>=
print(xyplot(Petal.Length ~ Petal.Width | Species, data = iris))
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-xyplot-by}
\end{center}
\caption[An \texttt{xyplot} of the \texttt{iris} data by \texttt{Species}.]{{\small An \texttt{xyplot} of \texttt{Petal.Length} versus \texttt{Petal.Width} by \texttt{Species} in the \texttt{iris} data.}}
\label{fig:xyplot-by}
\end{figure}



\subsubsection{Coplots}

<<echo=TRUE, eval=FALSE>>=
coplot(conc ~ uptake | Type * Treatment, data = CO2)
@

<<coplot, echo=FALSE, fig=TRUE, include=FALSE, width=5>>=
print(coplot(conc ~ uptake | Type * Treatment, data = CO2))
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-coplot}
\end{center}
\caption[A \texttt{coplot} of \texttt{conc} versus \texttt{uptake} by \texttt{Type} and \texttt{Treatment}.]{{\small A \texttt{coplot} of \texttt{conc} versus \texttt{uptake} by \texttt{Type} and \texttt{Treatment}.}}
\label{fig:coplot}
\end{figure}



\section{Bad Description}

All of the above discussion was about how to describe data, with the
goal of finding a description, visual or otherwise, that tells the
true story of the data, whatever that turns out to be.

A human that exists today finds itself constantly bombarded with data
descriptions, at all hours of the day and night, and unfortunately,
not all of them are good.  Some are bad.  And every now and then one of them
is \textit{very} bad.  And the onus is on the student of statistics
to become competent in proper description of data, so (s)he can be a
critical consumer of statistical information, and not be so easily
manipulated by negligent or malevolent actors.

Case in point: in 2015, the Planned Parenthood organization was the
target of a highly publicized investigation after a series of videos
were released to the media that allegedly showed Planned Parenthood
executives discussing potential illicit sale of fetal tissues.  The President of Planned Parenthood, Cecile Richards, testified before a House
Oversight Committee as part of the investigation. At the time, Rep. Jason Chaffetz
(R-UT) was the chair of the committee, and near the end of his
inquiry, he presented the following graph to Richards and asked her
questions about it. (See Figure~\ref{fig:PPb}.)

<<PPb, echo=FALSE, fig=TRUE, include=FALSE, width=5, height=5>>=
plot(x = c(74, 74, 774, 774),
     y = c(-541, -213, -305, -572),
     main = "PLANNED PARENTHOOD FEDERATION OF AMERICA:",
     xlab = "", ylab = "", asp = 1, axes = FALSE, cex.main=0.9,
     xlim = c(74, 774), ylim = c(-679, -150),
     type = "n")
mtext("ABORTIONS UP - LIFE SAVING PROCEDURES DOWN", side = 3, cex=0.9)
arrows(x0 = 74, y0 = -541, x1 = 774, y1 = -305,
       lwd = 3, col = "red")
arrows(x0 = 74, y0 = -213, x1 = 774, y1 = -572,
       lwd = 3, col = "purple")
text(x = 176, y = -480, labels = "ABORTIONS", pos = 4, srt = 18, col = "red", cex=0.9)
text(x = 300, y = -230, col = "purple",
     labels = "CANCER SCREENING &\nPREVENTION SERVICES", pos = 1, srt = -27, cex=0.9)
axis(side = 1,
     at = 100*(0:7) + 74,
     labels = 2006:2013)
segments(x0=74, y0=-150, x1=774, y1=-150)
text(100, -265, labels = "2,007,371\nIN 2006", pos = 1, cex = 0.75)
text(94, -557, labels = "289,750\nIN 2006", pos = 1, cex = 0.75)
text(734, -292, labels = "327,653\nIN 2013", pos = 3, cex = 0.75)
text(748, -594, labels = "935,573\nIN 2013", pos = 1, cex = 0.75)
mtext("SOURCE: AMERICANS UNITED FOR LIFE", side = 1, line = 3, cex = 0.75, adj = 1)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-PPb}
\end{center}
\caption[Bad graph of the Planned Parenthood data.]{{\small Bad graph of the Planned Parenthood data.  Notice there is mismatched scale on the vertical axis.  Also notice the graph only plots two points in time, 2013 and 2016, and connects the dots with straight lines, whose slopes the author selects arbitrarily. What should perhaps shock us most about the graph is what it does \textbf{not} display, that is, all of the other ``life saving procedures'' that Planned Parenthood provides, in the context of relative magnitudes.  The subtitle makes clear the misleading impression this graph was meant to evoke among uncritical consumers of statistical information.}}
\label{fig:PPb}
\end{figure}

The reader should take a hard look at Figure~\ref{fig:PPb}.  What is the graph
trying to communicate?  Does it communicate its message well?

Next, let us take a look at the data from which the graph was
constructed, but in the context of all the other services that Planned
Parenthood offers, and with more years included to get a better
picture of trends over time, than can be gleaned by restricting
attention to years 2006, 2013 only.  Here are the raw data.

<<echo=FALSE>>=
PP <- structure(c(3989474L, 3889980L, NA, 3868901L, 3685437L, 3436813L,
                  3724558L, 3577348L, 3018853L, 3363222L, NA, 4034264L, 4179053L,
                  4475013L, 4469308L, 4470597L, 2007371L, 1900850L, NA, 1830811L,
                  1596741L, 1307570L, 1121580L, 935573L, 1119977L, 1207340L, NA,
                  1070310L, 1144558L, 1179263L, 1167755L, 1147467L, 289750L, 305310L,
                  NA, 331796L, 329445L, 333964L, 327116L, 327653L, 162935L, 255123L,
                  NA, 102332L, 68132L, 132036L, 123308L, 131795L), .Dim = c(8L, 6L), .Dimnames = list(NULL, c("Contraception", "STD.Test.Treat", "Cancer.Screen", "Preg.Prenat.Serv", "Abortion.Serv", "Other.Serv")), .Tsp = c(2006, 2013, 1),
                class = c("mts", "ts"))

PPcpct = t(PP)
colnames(PPcpct) = 2006:2013
@


<<echo = FALSE>>=
PPcpct
@

Now let's make a proper graph of the data. Note that the 2008 annual report is difficult to track down so data from that year are missing.

<<PPg, echo=FALSE, fig=TRUE, include=FALSE, width=5>>=
ts.plot(PP/1000000, lty=1:6, lwd=3, col = c("black", "black","purple","black","red","black"), ylab="Services (in millions)", xlab="Year")
legend(x=2007,y=3.2, colnames(PP), lty=1:6, cex=.65, col = c("black", "black","purple","black","red","black"), lwd=3)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-PPg}
\end{center}
\caption[Better graph of the Planned Parenthood data.]{{\small Better graph of the Planned Parenthood data.  Notice how this time a more complete picture of services offered by the PP organization is now shown, all on the same vertical scale. Notice that the legend is ordered to match the initial values of the line plots.  From here we see that the bulk of 2013 services were related to contraception and STD treatment/testing, with same true in 2016 but trading places. We do observe a decline in cancer screening over the period 2013-16, but even at its lowest level cancer screenings still dwarf abortion services, which we now can see was mostly flat over this time period, with only a slight increase relative to the movement in other categories.}}
\label{fig:PPg}
\end{figure}


\section{Chapter Exercises}

Open \textsf{R} and issue the following commands at the command line to get started. Note that you need to have the \texttt{RcmdrPlugin.IPSUR} package \cite{RcmdrPlugin.IPSUR} installed, and for some exercises you need the \texttt{e1071} package \cite{e1071}.

<<echo=FALSE>>=
#library("RcmdrPlugin.IPSUR")
data(RcmdrTestDrive)
attach(RcmdrTestDrive)
names(RcmdrTestDrive)
@

To load the data in the \textsf{R} Commander (\texttt{Rcmdr}), click the
\texttt{Data Set} button, and select \texttt{RcmdrTestDrive} as the active data set. To learn more about the data set and where it comes from, type
\texttt{?RcmdrTestDrive} at the command line.



\begin{Exercise}[label=exr:summary-RcmdrTestDrive]
Perform a summary of all variables in
\texttt{RcmdrTestDrive}. You can do this with the command
\texttt{summary(RcmdrTestDrive)}.

Alternatively, you can do this in the \texttt{Rcmdr} with the sequence
\texttt{Statistics} \(\triangleright\) \texttt{Summaries} \(\triangleright\) \texttt{Active
Data Set}. Report the values of the summary statistics for each
variable.
\end{Exercise}




\begin{Exercise}[]
Make a table of the \texttt{race} variable. Do this with \texttt{Statistics}
\(\triangleright\) \texttt{Summaries} \(\triangleright\) \texttt{Frequency
Distributions - IPSUR\ldots}

\begin{enumerate}
\item Which ethnicity has the highest frequency?
\item Which ethnicity has the lowest frequency?
\item Include a bar graph of \texttt{race}. Do this with \texttt{Graphs}
   \(\triangleright\) \texttt{IPSUR - Bar Graph\ldots}
\end{enumerate}

\end{Exercise}




\begin{Exercise}[]
Calculate the average \texttt{salary} by the factor \texttt{gender}. Do this with
\texttt{Statistics} \(\triangleright\) \texttt{Summaries} \(\triangleright\) \texttt{Table
of Statistics\ldots}

\begin{enumerate}
\item Which \texttt{gender} has the highest mean \texttt{salary}?
\item Report the highest mean \texttt{salary}.
\item Compare the spreads for the genders by calculating the standard
   deviation of \texttt{salary} by \texttt{gender}. Which \texttt{gender} has the biggest
   standard deviation?
\item Make boxplots of \texttt{salary} by \texttt{gender} with the following method:
       \begin{enumerate}
       \item On the \texttt{Rcmdr}, click \texttt{Graphs} \(\triangleright\) \texttt{IPSUR - Boxplot\ldots}
       \item In the \texttt{Variable} box, select \texttt{salary}.
       \item Click the \texttt{Plot by groups\ldots} box and select \texttt{gender}. Click \texttt{OK}.
       \item Click \texttt{OK} to graph the boxplot.
       \end{enumerate}
How does the boxplot compare to your answers to (1) and (3)?
\end{enumerate}

\end{Exercise}




\begin{Exercise}[]
For this problem we will study the variable \texttt{reduction}.

\begin{enumerate}
\item Find the order statistics and store them in a vector \texttt{x}. \textit{Hint:}
   \texttt{x <- sort(reduction)}
\item Find \(x_{(137)}\), the 137\(^{\mathrm{th}}\) order statistic.
\item Find the IQR.
\item Find the Five Number Summary (5NS).
\item Use the 5NS to calculate what the width of a boxplot of \texttt{reduction}
   would be.
\item Compare your answers (3) and (5). Are they the same? If not, are
   they close?
\item Make a boxplot of \texttt{reduction}, and include the boxplot in your
   report. You can do this with the \texttt{boxplot} function, or in \texttt{Rcmdr}
   with \texttt{Graphs} \(\triangleright\) \texttt{IPSUR - Boxplot\ldots}
\item Are there any potential/suspected outliers? If so, list their
   values. \textit{Hint:} use your answer to (a).
\item Using the rules discussed in the text, classify answers to (8), if
   any, as \textit{potential} or \textit{suspected} outliers.
\end{enumerate}

\end{Exercise}




\begin{Exercise}[]
In this problem we will compare the variables \texttt{before} and
\texttt{after}. Do not forget \texttt{library("e1071")}.

\begin{enumerate}
\item Examine the two measures of center for both variables. Judging from
   these measures, which variable has a higher center?
\item Which measure of center is more appropriate for \texttt{before}? (You may
   want to look at a boxplot.) Which measure of center is more
   appropriate for \texttt{after}?
\item Based on your answer to (2), choose an appropriate measure of
   spread for each variable, calculate it, and report its value. Which
   variable has the biggest spread? (Note that you need to make sure
   that your measures are on the same scale.)
\item Calculate and report the skewness and kurtosis for \texttt{before}. Based
   on these values, how would you describe the shape of \texttt{before}?
\item Calculate and report the skewness and kurtosis for \texttt{after}. Based
   on these values, how would you describe the shape of \texttt{after}?
\item Plot histograms of \texttt{before} and \texttt{after} and compare them to your
   answers to (4) and (5).
\end{enumerate}

\end{Exercise}




\begin{Exercise}[]
Describe the following data sets just as if you were communicating
with an alien, but one who has had a statistics class. Mention the
salient features (data type, important properties, anything
special). Support your answers with the appropriate visual displays
and descriptive statistics.

\begin{enumerate}
\item Conversion rates of Euro currencies stored in \texttt{euro}.
\item State abbreviations stored in \texttt{state.abb}.
\item Areas of the world's landmasses stored in \texttt{islands}.
\item Areas of the 50 United States stored in \texttt{state.area}.
\item Region of the 50 United States stored in \texttt{state.region}.
\end{enumerate}

\end{Exercise}



<<histexr, echo=FALSE, fig=TRUE, include=FALSE, height=3.5,width=5>>=
m = sample(25:95, size = 1)
x = rexp(500, rate = m)
hist(x, main = "", breaks = 15, xlab="")
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-histexr}
\end{center}
\caption{{\small Some data of interest to a researcher.}}
\label{fig:histexr}
\end{figure}


<<echo = FALSE>>=
@

\begin{Exercise}[]
The data graphed in Figure~\ref{fig:histexr} represent measurements on some quantitative continuous
variable of interest to a researcher.

\begin{enumerate}
\item Among the measures of center discussed, what would be an appropriate
choice to measure the center of these data? (You need not actually
report the center.)
\item Among the measures of spread we discussed, what would be an appropriate
choice to measure the spread of these data? (You need not actually
report the spread.)
\end{enumerate}

\end{Exercise}





\begin{Exercise}[]
Suppose Fred takes a statistics exam and earns a score 85 out of 100 points. He asks the instructor
who reports that the average score was 77 points with a standard deviation of 24 points, and the instructor also relates that a histogram of the students' scores on the exam was approximately mound shaped.

\begin{enumerate}
\item Between what two values did approximately 95\% of the students score?

\item Should Fred be happy or sad (or not care) with his performance? Explain.

\item The instructor says that (s)he made a mistake, and the standard
  deviation was actually 2.4 points. How should Fred feel now? Explain.
\end{enumerate}

\end{Exercise}





\begin{Exercise}[]
  Investigate the relationship between \texttt{Petal.Width} and
  \texttt{Petal.Length} in the \texttt{iris} data.  Point out any
  unusual features.
\end{Exercise}




\begin{Exercise}[]
  Consruct side-by-side boxplots of the \texttt{chickwts} data, with
  horizontal axis are the numerical \texttt{weight} of chicks, and
  with vertical axis showing the different \texttt{feed} supplement
  types. Compare and contrast the distributions of weights. You will
  want to address the four dimensions of data sets. If ``high weight''
  were a goal of the chicken farmer, which feed supplement(s) should
  (s)he use, and why?
\end{Exercise}



\chapter{Probability} \label{cha:probability}

<<echo=FALSE, include=FALSE>>=
# This chapter's package dependencies
library(diagram)
library(ggplot2)
library(prob)
#library(RcmdrPlugin.IPSUR)
@

In this chapter we define the basic terminology associated with
probability and derive some of its properties. We discuss three
interpretations of probability. We discuss conditional probability and
independent events, along with Bayes' Theorem. We finish the chapter
with an introduction to random variables, which paves the way for the
next two chapters.

In this book we distinguish between two types of experiments:
\textit{deterministic} and \textit{random}. A \textit{deterministic} experiment is one
whose outcome may be predicted with certainty beforehand, such as
combining Hydrogen and Oxygen, or adding two numbers such as
\(2+3\). A \textit{random} experiment is one whose outcome is determined by
chance. We posit that the outcome of a random experiment may not be
predicted with certainty beforehand, even in principle. Examples of
random experiments include tossing a coin, rolling a die, and throwing
a dart on a board, how many red lights you encounter on the drive
home, how many ants traverse a certain patch of sidewalk over a short
period, \textit{etc}.

\paragraph{What do I want them to know?}

\begin{itemize}
\item that there are multiple interpretations of probability, and the
  methods used depend somewhat on the philosophy chosen
\item nuts and bolts of basic probability jargon: sample spaces, events,
  probability functions, \textit{etc}.
\item how to count
\item conditional probability and its relationship with independence
\item Bayes' Rule and how it relates to the subjective view of probability
\item what we mean by 'random variables', and where they come from
\end{itemize}


<<diagram, echo=FALSE, fig=TRUE, include=FALSE, width=5>>=
require(diagram)
par(mex = 0.2, cex = 0.5)
openplotmat(frame.plot=TRUE)
straightarrow(from = c(0.46,0.74), to = c(0.53,0.71), arr.pos = 1)
straightarrow(from = c(0.3,0.65), to = c(0.3,0.51), arr.pos = 1)
textellipse(mid = c(0.74,0.55), box.col = grey(0.95),
  radx = 0.24, rady = 0.22,
  lab = c(expression(bold(underline(DETERMINISTIC))),
          expression(2*H[2]+O[2] %->% H[2]*O), "3 + 4 = 7"), cex = 2 )
textrect(mid = c(0.3, 0.75), radx = 0.15, rady = 0.1,
  lab = c(expression(bold(Experiments))), cex = 2 )
textellipse(mid = c(0.29,0.25), box.col = grey(0.95),
  radx = 0.27, rady = 0.22, lab = c(expression(bold(underline(RANDOM))),
  "toss coin, roll die", "count ants on sidewalk", "measure rainfall" ),
  cex = 2 )
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-diagram}
\end{center}
\caption[Two types of experiments.]{{\small Two types of experiments.}}
\label{fig:diagram}
\end{figure}



\section{Sample Spaces} \label{sec:sample-spaces}

For a random experiment \(E\), the set of all possible outcomes of
\(E\) is called the \textit{sample space} \index{sample space} and is
denoted by the letter \(S\). For a coin-toss experiment, \(S\) would
be the results ``Head'' and ``Tail'', which we may represent by
\(S = \{H,T \}\). Formally, the performance of a random experiment is
the unpredictable selection of an outcome in \(S\).

\subsection{How to do it with \textsf{R}}

Most of the probability work in this book is done with the \texttt{prob}
package \cite{prob}. A sample space is (usually) represented by a
\textit{data frame}, that is, a rectangular collection of variables
(see Section~\ref{sub:multivariate-data}). Each row of the data frame
corresponds to an outcome of the experiment. The data frame choice is
convenient both for its simplicity and its compatibility with the \textsf{R}
Commander. Data frames alone are, however, not sufficient to describe
some of the more interesting probabilistic applications we will study
later; to handle those we will need to consider a more general
\textit{list} data structure. See Section~\ref{sub:howto-ps-objects}
for details.



\begin{example}[]
  Consider the random experiment of dropping a Styrofoam cup onto the
  floor from a height of four feet. The cup hits the ground and
  eventually comes to rest. It could land upside down, right side up,
  or it could land on its side. We represent these possible outcomes
  of the random experiment by the following.
\end{example}

<<echo=TRUE>>=
S <- data.frame(lands = c("down","up","side"))
S
@

The sample space \texttt{S} contains the column \texttt{lands} which stores the
outcomes \texttt{down}, \texttt{up}, and \texttt{side}.


Some sample spaces are so common that convenience wrappers were
written to set them up with minimal effort. The underlying machinery
that does the work includes the \texttt{expand.grid} function in the \texttt{base}
package \cite{base}, \texttt{combn} in the \texttt{combinat} package \cite{combinat}, and
\texttt{permsn} in the \texttt{prob} package \cite{prob}\footnote{The seasoned \textsf{R} user
  can get the job done without the convenience wrappers. I encourage
  the beginner to use them to get started, but I also recommend that
  introductory students wean themselves as soon as possible. The
  wrappers were designed for ease and intuitive use, not for speed or
  efficiency.}.

Consider the random experiment of tossing a coin. The outcomes are
\(H\) and \(T\). We can set up the sample space quickly with the
\texttt{tosscoin} function:


<<echo=TRUE>>=
tosscoin(1)
@


The number \texttt{1} tells \texttt{tosscoin} that we only want to toss the coin
once. We could toss it three times:

<<echo=TRUE>>=
tosscoin(3)
@

Alternatively we could roll a fair die:

<<echo=TRUE>>=
rolldie(1)
@


The \texttt{rolldie} function defaults to a 6-sided die, but we can specify
others with the \texttt{nsides} argument. The command \texttt{rolldie(3, nsides = 4)} would be used to roll a 4-sided die three times.

Perhaps we would like to draw one card from a standard set of playing
cards (it is a long data frame):

<<echo=TRUE>>=
head(cards())
@


The \texttt{cards} function that we just used has optional arguments \texttt{jokers}
(if you would like Jokers to be in the deck) and \texttt{makespace} which we
will discuss later. There is also a \texttt{roulette} function which returns
the sample space associated with one spin on a roulette wheel. There
are EU and USA versions available. Interested readers may contribute
any other game or sample spaces that may be of general interest.

\subsection{Sampling from Urns} \label{sub:sampling-from-urns}

This is perhaps the most fundamental type of random experiment. We
have an urn that contains a bunch of distinguishable objects (balls)
inside. We shake up the urn, reach inside, grab a ball, and take a
look. That's all.

But there are all sorts of variations on this theme. Maybe we would
like to grab more than one ball -- say, two balls. What are all of the
possible outcomes of the experiment now? It depends on how we
sample. We could select a ball, take a look, put it back, and sample
again. Another way would be to select a ball, take a look -- but do
not put it back -- and sample again (equivalently, just reach in and
grab two balls). There are certainly more possible outcomes of the
experiment in the former case than in the latter. In the first
(second) case we say that sampling is done \textit{with (without)
  replacement}.

There is more. Suppose we do not actually keep track of which ball
came first. All we observe are the two balls, and we have no idea
about the order in which they were selected. We call this \textit{unordered
sampling} (in contrast to \textit{ordered}) because the order of the
selections does not matter with respect to what we observe. We might
as well have selected the balls and put them in a bag before looking.

Note that this one general class of random experiments contains as a
special case all of the common elementary random experiments. Tossing
a coin twice is equivalent to selecting two balls labeled \(H\) and
\(T\) from an urn, with replacement. The die-roll experiment is
equivalent to selecting a ball from an urn with six elements, labeled
1 through 6.

\subsubsection{How to do it with \textsf{R}}

The \texttt{prob} package \cite{prob} accomplishes sampling from urns with
the \texttt{urnsamples} \index{urnsamples@\texttt{urnsamples}}
function, which has arguments \texttt{x}, \texttt{size}, \texttt{replace}, and
\texttt{ordered}. The argument \texttt{x} represents the urn from which sampling is
to be done. The \texttt{size} argument tells how large the sample will
be. The \texttt{ordered} and \texttt{replace} arguments are logical and specify how
sampling will be performed. We will discuss each in turn.



\begin{example}[]
Let our urn simply contain three
balls, labeled 1, 2, and 3, respectively. We are going to take a
sample of size 2 from the urn.
\label{exm:sample-urn-two-from-three}
\end{example}


\subsubsection{Ordered, With Replacement}

If sampling is with replacement, then we can get any outcome 1, 2, or
3 on any draw. Further, by ``ordered'' we mean that we shall keep
track of the order of the draws that we observe. We can accomplish
this in \textsf{R} with

<<echo=TRUE>>=
urnsamples(1:3, size = 2, replace = TRUE, ordered = TRUE)
@


Notice that rows 2 and 4 are identical, save for the order in which
the numbers are shown. Further, note that every possible pair of the
numbers 1 through 3 are listed. This experiment is equivalent to
rolling a 3-sided die twice, which we could have accomplished with
\texttt{rolldie(2, nsides = 3)}.

\subsubsection{Ordered, Without Replacement}

Here sampling is without replacement, so we may not observe the same
number twice in any row. Order is still important, however, so we
expect to see the outcomes \texttt{1,2} and \texttt{2,1} somewhere in our data
frame.

<<echo=TRUE>>=
urnsamples(1:3, size = 2, replace = FALSE, ordered = TRUE)
@


This is just as we expected. Notice that there are less rows in this
answer due to the more restrictive sampling procedure. If the numbers
1, 2, and 3 represented ``Fred'', ``Mary'', and ``Sue'', respectively,
then this experiment would be equivalent to selecting two people of
the three to serve as president and vice-president of a company,
respectively, and the sample space shown above lists all possible ways
that this could be done.

\subsubsection{Unordered, Without Replacement}

Again, we may not observe the same outcome twice, but in this case, we
will only retain those outcomes which (when jumbled) would not
duplicate earlier ones.

<<echo=TRUE>>=
urnsamples(1:3, size = 2, replace = FALSE, ordered = FALSE)
@

This experiment is equivalent to reaching in the urn, picking a pair,
and looking to see what they are. This is the default setting of
\texttt{urnsamples}, so we would have received the same output by simply
typing \texttt{urnsamples(1:3, 2)}.

\subsubsection{Unordered, With Replacement}

The last possibility is perhaps the most interesting. We replace the
balls after every draw, but we do not remember the order in which the
draws came.

<<echo=TRUE>>=
urnsamples(1:3, size = 2, replace = TRUE, ordered = FALSE)
@


We may interpret this experiment in a number of alternative ways. One
way is to consider this as simply putting two 3-sided dice in a cup,
shaking the cup, and looking inside -- as in a game of \textit{Liar's
  Dice}, for instance. Each row of the sample space is a potential
pair we could observe. Another way is to view each outcome as a
separate method to distribute two identical golf balls into three
boxes labeled 1, 2, and 3. Regardless of the interpretation,
\texttt{urnsamples} lists every possible way that the experiment can
conclude.



Note that the urn does not need to contain numbers; we could have just
as easily taken our urn to be \texttt{x = c("Red","Blue","Green")}. But,
there is an \textit{important} point to mention before proceeding. Astute
readers will notice that in our example, the balls in the urn were
\textit{distinguishable} in the sense that each had a unique label to
distinguish it from the others in the urn. A natural question would
be, ``What happens if your urn has indistinguishable elements, for
example, what if \texttt{x = c("Red","Red","Blue")}?'' The answer is that
\texttt{urnsamples} behaves as if each ball in the urn is distinguishable,
regardless of its actual contents. We may thus imagine that while
there are two red balls in the urn, the balls are such that we can
tell them apart (in principle) by looking closely enough at the
imperfections on their surface.

In this way, when the \texttt{x} argument of \texttt{urnsamples} has repeated
elements, the resulting sample space may appear to be \texttt{ordered = TRUE}
even when, in fact, the call to the function was \texttt{urnsamples(\ldots, ordered = FALSE)}. Similar remarks apply for the \texttt{replace} argument.

\section{Events} \label{sec:events}

An \textit{event} \index{event} \(A\) is merely a collection of outcomes, or
in other words, a subset of the sample space\footnote{This na\"{\i}ve definition works for finite or countably
infinite sample spaces, but is inadequate for sample spaces in
general. In this book, we will not address the subtleties that arise,
but will refer the interested reader to any text on advanced
probability or measure theory.}. After the
performance of a random experiment \(E\) we say that the event \(A\)
\textit{occurred} if the experiment's outcome belongs to \(A\). We say that a
bunch of events \(A_{1}\), \(A_{2}\), \(A_{3}\), \ldots are \textit{mutually exclusive} \index{mutually exclusive} or \textit{disjoint} if \(A_{i}\cap
A_{j}=\emptyset\) for any distinct pair \(A_{i}\neq A_{j}\). For
instance, in the coin-toss experiment the events \(A = \{
\mbox{Heads} \}\) and \(B = \{ \mbox{Tails} \}\) would be mutually
exclusive. Now would be a good time to review the algebra of sets in
Appendix~\ref{sec:the-algebra-of}.



\subsection{How to do it with \textsf{R}}

Given a data frame sample/probability space \texttt{S}, we may extract rows
using the \texttt{[]} operator:

<<echo=TRUE>>=
S <- tosscoin(2, makespace = TRUE)
S[1:3, ]
@


<<echo=TRUE>>=
S[c(2,4), ]
@

and so forth. We may also extract rows that satisfy a logical
expression using the \texttt{subset} function, for instance

<<echo=TRUE>>=
S <- cards()
@

<<echo=TRUE>>=
subset(S, suit == "Heart")
@


<<echo=TRUE>>=
subset(S, rank %in% 7:9)
@

We could continue indefinitely. Also note that mathematical
expressions are allowed:

<<echo=TRUE>>=
subset(rolldie(3), X1+X2+X3 > 16)
@


\subsection{Functions for Finding Subsets}

It does not take long before the subsets of interest become
complicated to specify. Yet the main idea remains: we have a
particular logical condition to apply to each row. If the row
satisfies the condition, then it should be in the subset. It should
not be in the subset otherwise. The ease with which the condition may
be coded depends of course on the question being asked. Here are a few
functions to get started.

\subsubsection{The \texttt{\%in\%} function}

The function \texttt{\%in\%} helps to learn whether each value of one vector
lies somewhere inside another vector.

<<echo=TRUE>>=
x <- 1:10
y <- 8:12
y %in% x
@


Notice that the returned value is a vector of length 5 which tests
whether each element of \texttt{y} is in \texttt{x}, in turn.

\subsubsection{The \texttt{isin} function}

It is more common to want to know whether the \textit{whole} vector \texttt{y} is in
\texttt{x}. We can do this with the \texttt{isin} function.

<<echo=TRUE>>=
isin(x,y)
@

Of course, one may ask why we did not try something like \texttt{all(y \%in\% x)}, which would give a single result, \texttt{TRUE}. The reason is that the
answers are different in the case that \texttt{y} has repeated
values. Compare:

<<echo=TRUE>>=
x <- 1:10
y <- c(3,3,7)
@

<<echo=TRUE>>=
all(y %in% x)
isin(x,y)
@


The reason for the above is of course that \texttt{x} contains the value 3,
but \texttt{x} does not have \textit{two} 3's. The difference is important when
rolling multiple dice, playing cards, \textit{etc}. Note that there is an
optional argument \texttt{ordered} which tests whether the elements of \texttt{y}
appear in \texttt{x} in the order in which they are appear in \texttt{y}. The
consequences are

<<echo=TRUE>>=
isin(x, c(3,4,5), ordered = TRUE)
isin(x, c(3,5,4), ordered = TRUE)
@

The connection to probability is that have a data frame sample space
and we would like to find a subset of that space. A \texttt{data.frame}
method was written for \texttt{isin} that simply applies the function to each
row of the data frame. We can see the method in action with the
following:

<<echo=TRUE>>=
S <- rolldie(4)
subset(S, isin(S, c(2,2,6), ordered = TRUE))
@


There are a few other functions written to find useful subsets,
namely, \texttt{countrep} and \texttt{isrep}. Essentially these were written to test
for (or count) a specific number of designated values in outcomes. See
the documentation for details.

\subsection{Set Union, Intersection, and Difference}

Given subsets \(A\) and \(B\), it is often useful to manipulate them
in an algebraic fashion. To this end, we have three set operations at
our disposal: union, intersection, and
difference. Table~\ref{tab:set-oper} summarizes the pertinent
information about these basic operations.

\begin{table}
\begin{tabular}{lcll}
% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
Name  & Denoted  & Defined by elements  & Code \tabularnewline
\hline
Union  & $A\cup B$  & in $A$ or $B$ or both  & \texttt{union(A,B)} \tabularnewline
Intersection  & $A\cap B$  & in both $A$ and $B$  & \texttt{intersect(A,B)} \tabularnewline
Difference  & $A$\textbackslash $B$  & in $A$ but not in $B$  & \texttt{setdiff(A,B)} \tabularnewline
\end{tabular}
\caption[Basic set operations.]{{\small Basic set operations.  The first column lists the name, the second shows the typical notation, the third describes set membership, and the fourth shows how to accomplish it with \textsf{R}.}}
\label{tab:set-oper}
\end{table}

Some examples follow.

<<echo=TRUE>>=
S <- cards()
A <- subset(S, suit == "Heart")
B <- subset(S, rank %in% 7:9)
@

We can now do some set algebra:

<<echo=TRUE>>=
union(A,B)
@


<<echo=TRUE>>=
intersect(A,B)
@

<<echo=TRUE>>=
setdiff(A,B)
@

<<echo=TRUE>>=
setdiff(B,A)
@

Notice that \texttt{setdiff} is not symmetric. Further, note that we can
calculate the \textit{complement} of a set \(A\), denoted \(A^{c}\) and
defined to be the elements of \(S\) that are not in \(A\) simply with
\texttt{setdiff(S,A)}. There have been methods written for \texttt{intersect},
\texttt{setdiff}, \texttt{subset}, and \texttt{union} in the case that the input objects
are of class \texttt{ps}. See Section~\ref{sub:howto-ps-objects}.



\begin{note}[]
When the \texttt{prob} package \cite{prob} loads you will notice a message:
\texttt{The following object(s) are masked from package:base: intersect, setdiff}. The reason for this message is that there already exist
methods for the functions \texttt{intersect}, \texttt{setdiff}, \texttt{subset}, and
\texttt{union} in the \texttt{base} package which ships with
R. However, these methods were designed for when the
arguments are vectors of the same mode. Since we are manipulating
sample spaces which are data frames and lists, it was necessary to
write methods to handle those cases as well. When the \texttt{prob} package
is loaded, \textsf{R} recognizes that there are multiple versions
of the same function in the search path and acts to shield the new
definitions from the existing ones. But there is no cause for alarm,
thankfully, because the \texttt{prob} functions have been carefully defined
to match the usual \texttt{base} package definition in the case that the
arguments are vectors.
\end{note}


\section{Model Assignment} \label{sec:interpreting-probabilities}

Let us take a look at the coin-toss experiment more closely. What do
we mean when we say ``the probability of Heads'' or write
\(\mathbb{P}(\mbox{Heads})\)? Given a coin and an itchy thumb, how do
we go about finding what \(\mathbb{P}(\mbox{Heads})\) should be?

\subsection{The Measure Theory Approach}

This approach states that the way to handle
\(\mathbb{P}(\mbox{Heads})\) is to define a mathematical function,
called a \textit{probability measure}, on the sample space. Probability
measures satisfy certain axioms (to be introduced later) and have
special mathematical properties, so not just any mathematical function
will do. But in any given physical circumstance there are typically
all sorts of probability measures from which to choose, and it is left
to the experimenter to make a reasonable choice -- one usually based
on considerations of objectivity. For the tossing coin example, a
valid probability measure assigns probability \(p\) to the event \(\{
\mbox{Heads} \}\), where \(p\) is some number \(0\leq p\leq1\). An
experimenter that wishes to incorporate the symmetry of the coin would
choose \(p=1/2\) to balance the likelihood of \(\{\mbox{Heads} \}\)
and \(\{ \mbox{Tails} \}\).

Once the probability measure is chosen (or determined), there is not
much left to do. All assignments of probability are made by the
probability function, and the experimenter needs only to plug the
event \(\{ \mbox{Heads} \}\) into to the probability function to find
\(\mathbb{P}(\mbox{Heads})\). In this way, the probability of an event
is simply a calculated value, nothing more, nothing less. Of course
this is not the whole story; there are many theorems and consequences
associated with this approach that will keep us occupied for the
remainder of this book. The approach is called \textit{measure theory}
because the measure (probability) of a set (event) is associated with
how big it is (how likely it is to occur).

The measure theory approach is well suited for situations where there
is symmetry to the experiment, such as flipping a balanced coin or
spinning an arrow around a circle with well-defined pie slices. It is
also handy because of its mathematical simplicity, elegance, and
flexibility. There are literally volumes of information that one can
prove about probability measures, and the cold rules of mathematics
allow us to analyze intricate probabilistic problems with vigor.

The large degree of flexibility is also a disadvantage, however. When
symmetry fails it is not always obvious what an ``objective'' choice
of probability measure should be; for instance, what probability
should we assign to \(\{ \mbox{Heads} \}\) if we spin the coin
rather than flip it? (It is not \(1/2\).) Furthermore, the
mathematical rules are restrictive when we wish to incorporate
subjective knowledge into the model, knowledge which changes over time
and depends on the experimenter, such as personal knowledge about the
properties of the specific coin being flipped, or of the person doing
the flipping.

The mathematician who revolutionized this way to do probability theory
was Andrey Kolmogorov, who published a landmark monograph in 1933. See
\url{http://www-history.mcs.st-andrews.ac.uk/Mathematicians/Kolmogorov.html} for more information.

\subsection{Relative Frequency Approach}

This approach states that the way to determine
\(\mathbb{P}(\mbox{Heads})\) is to flip the coin repeatedly, in
exactly the same way each time. Keep a tally of the number of flips
and the number of Heads observed. Then a good approximation to
\(\mathbb{P}(\mbox{Heads})\) will be
\begin{equation}
\mathbb{P}(\mbox{Heads})\approx\frac{\mbox{number of observed Heads}}{\mbox{total number of flips}}.
\end{equation}
The mathematical underpinning of this approach is the celebrated
\textit{Law of Large Numbers} which may be loosely described as follows. Let \(E\)
be a random experiment in which the event \(A\) either does or does
not occur. Perform the experiment repeatedly, in an identical manner,
in such a way that the successive experiments do not influence each
other. After each experiment, keep a running tally of whether or not
the event \(A\) occurred. Let \(S_{n}\) count the number of times that
\(A\) occurred in the \(n\) experiments. Then the law of large numbers
says that
\begin{equation}
\frac{S_{n}}{n}\to\mathbb{P}(A)\mbox{ as }n\to\infty.
\end{equation}
As the reasoning goes, to learn about the probability of an event
\(A\) we need only repeat the random experiment to get a reasonable
estimate of the probability's value, and if we are not satisfied with
our estimate then we may simply repeat the experiment more times all
the while confident that with more and more experiments our estimate
will stabilize to the true value.

The frequentist approach is good because it is relatively light on
assumptions and does not worry about symmetry or claims of objectivity
like the measure-theoretic approach does. It is perfect for the
spinning coin experiment. One drawback to the method is that one can
never know the exact value of a probability, only a long-run
approximation. It also does not work well with experiments that can
not be repeated indefinitely, say, the probability that it will rain
today, the chances that you get will get an A in your Statistics
class, or the probability that the world is destroyed by nuclear war.

This approach was espoused by Richard von Mises in the early twentieth
century, and some of his main ideas were incorporated into the measure
theory approach. See \url{http://www-history.mcs.st-andrews.ac.uk/Biographies/Mises.html} for more.

\subsection{The Subjective Approach}

The subjective approach interprets probability as the experimenter's
\textit{degree of belief} that the event will occur. The estimate of the
probability of an event is based on the totality of the individual's
knowledge at the time. As new information becomes available, the
estimate is modified accordingly to best reflect his/her current
knowledge. The method by which the probabilities are updated is
commonly done with Bayes' Rule, discussed in Section~\ref{sec:bayes-rule}.

So for the coin toss example, a person may have
\(\mathbb{P}(\mbox{Heads})=1/2\) in the absence of additional
information. But perhaps the observer knows additional information
about the coin or the thrower that would shift the probability in a
certain direction. For instance, parlor magicians may be trained to be
quite skilled at tossing coins, and some are so skilled that they may
toss a fair coin and get nothing but Heads, indefinitely. I have
\textit{seen} this. It was similarly claimed in \textit{Bringing Down
  the House} \cite{Mezrich2003} that MIT students were accomplished enough
with cards to be able to cut a deck to the same location, every single
time. In such cases, one clearly should use the additional information
to assign \(\mathbb{P}(\mbox{Heads})\) away from the symmetry value of
\(1/2\).

This approach works well in situations that cannot be repeated
indefinitely, for example, to assign your probability that you will
get an A in this class, the chances of a devastating nuclear war, or
the likelihood that a cure for the common cold will be discovered.

The roots of subjective probability reach back a long time. See
\url{http://en.wikipedia.org/wiki/Subjective\_probability} for a short
discussion and links to references about the subjective approach.

\subsection{Equally Likely Model (ELM)}

We have seen several approaches to the assignment of a probability
model to a given random experiment and they are very different in
their underlying interpretation. But they all cross paths when it
comes to the equally likely model which assigns equal probability to
all elementary outcomes of the experiment.

The ELM appears in the measure theory approach when the experiment
boasts symmetry of some kind. If symmetry guarantees that all outcomes
have equal ``size'', and if outcomes with equal ``size'' should get
the same probability, then the ELM is a logical objective choice for
the experimenter. Consider the balanced 6-sided die, the fair coin, or
the dart board with equal-sized wedges.

The ELM appears in the subjective approach when the experimenter
resorts to indifference or ignorance with respect to his/her knowledge
of the outcome of the experiment. If the experimenter has no prior
knowledge to suggest that (s)he prefer Heads over Tails, then it is
reasonable for the him/her to assign equal subjective probability to
both possible outcomes.

The ELM appears in the relative frequency approach as a fascinating
fact of Nature: when we flip balanced coins over and over again, we
observe that the proportion of times that the coin comes up Heads
tends to \(1/2\). Of course if we assume that the measure theory
applies then we can prove that the sample proportion must tend to 1/2
as expected, but that is putting the cart before the horse, in a
manner of speaking.

The ELM is only available when there are finitely many elements in the
sample space.

\subsubsection{How to do it with \textsf{R}}

In the \texttt{prob} package \cite{prob}, a probability space is an
object of outcomes \texttt{S} and a vector of probabilities (called
\texttt{probs}) with entries that correspond to each outcome in
\texttt{S}. When \texttt{S} is a data frame, we may simply add a
column called \texttt{probs} to \texttt{S} and we will be finished;
the probability space will simply be a data frame which we may call
\texttt{S}. In the case that S is a list, we may combine the
\texttt{outcomes} and \texttt{probs} into a larger list,
\texttt{space}; it will have two components: \texttt{outcomes} and
\texttt{probs}. The only requirements we need are for the entries of
\texttt{probs} to be nonnegative and \texttt{sum(probs)} to be one.

To accomplish this in \textsf{R}, we may use the \texttt{probspace}
function. The general syntax is \texttt{probspace(x, probs)}, where
\texttt{x} is a sample space of outcomes and \texttt{probs} is a
vector (of the same length as the number of outcomes in
\texttt{x}). The specific choice of \texttt{probs} depends on the
context of the problem, and some examples follow to demonstrate some
of the more common choices.



\begin{example}[]
The Equally Likely Model asserts that every outcome of the sample
space has the same probability, thus, if a sample space has \(n\)
outcomes, then \texttt{probs} would be a vector of length \(n\) with
identical entries \(1/n\). The quickest way to generate \texttt{probs} is
with the \texttt{rep} function. We will start with the experiment of rolling
a die, so that \(n=6\). We will construct the sample space, generate
the \texttt{probs} vector, and put them together with \texttt{probspace}.
\end{example}

<<echo=TRUE>>=
outcomes <- rolldie(1)
p <- rep(1/6, times = 6)
probspace(outcomes, probs = p)
@


The \texttt{probspace} function is designed to save us some time in the most
common situations. For example, due to the especial simplicity of the
sample space in this case, we could have achieved the same result with
only (note the name change for the first column)

<<echo=TRUE>>=
probspace(1:6, probs = p)
@


Further, since the equally likely model plays such a fundamental role
in the study of probability the \texttt{probspace} function will assume that
the equally model is desired if no \texttt{probs} are specified. Thus, we get
the same answer with only

<<echo=TRUE>>=
probspace(1:6)
@


And finally, since rolling dice is such a common experiment in
probability classes, the \texttt{rolldie} function has an additional logical
argument \texttt{makespace} that will add a column of equally likely \texttt{probs}
to the generated sample space:

<<echo=TRUE>>=
rolldie(1, makespace = TRUE)
@


or just \texttt{rolldie(1, TRUE)}. Many of the other sample space functions
(\texttt{tosscoin}, \texttt{cards}, \texttt{roulette}, \textit{etc}.) have similar \texttt{makespace}
arguments. Check the documentation for details.

One sample space function that does NOT have a \texttt{makespace} option is
the \texttt{urnsamples} function. This was intentional. The reason is that
under the varied sampling assumptions the outcomes in the respective
sample spaces are NOT, in general, equally likely. It is important for
the user to carefully consider the experiment to decide whether or not
the outcomes are equally likely and then use \texttt{probspace} to assign the
model.



\begin{example}[An unbalanced coin]
While the \texttt{makespace}
argument to \texttt{tosscoin} is useful to represent the tossing of a \textit{fair}
coin, it is not always appropriate. For example, suppose our coin is
not perfectly balanced, for instance, maybe the \(H\) side is somewhat
heavier such that the chances of a \(H\) appearing in a single toss is
0.70 instead of 0.5. We may set up the probability space with
\label{exm:unbalanced-coin}
\end{example}

<<echo=TRUE>>=
probspace(tosscoin(1), probs = c(0.70, 0.30))
@

The same procedure can be used to represent an unbalanced die,
roulette wheel, \textit{etc}.

\subsection{Words of Warning}

It should be mentioned that while the splendour of \textsf{R} is
uncontested, it, like everything else, has limits both with respect to
the sample/probability spaces it can manage and with respect to the
finite accuracy of the representation of most numbers (see the
R FAQ 7.31). When playing around with probability, one
may be tempted to set up a probability space for tossing 100 coins or
rolling 50 dice in an attempt to answer some scintillating
question. (Bear in mind: rolling a die just 9 times has a sample space
with over \textit{10 million} outcomes.)

Alas, even if there were enough RAM to barely hold the sample space
(and there were enough time to wait for it to be generated), the
infinitesimal probabilities that are associated with \textit{so many}
outcomes make it difficult for the underlying machinery to handle
reliably. In some cases, special algorithms need to be called just to
give something that holds asymptotically. User beware.

\section{Properties of Probability} \label{sec:properties-of-probability}

\subsection{Probability Functions} \label{sub:probability-functions}

A \textit{probability function} is a rule that associates with each event
\(A\) of the sample space a single number \(\mathbb{P}(A)=p\), called
the \textit{probability of} \(A\). Any probability function \(\mathbb{P}\)
satisfies the following three Kolmogorov Axioms:

\begin{axiom}[]
\(\mathbb{P}(A)\geq0\) for any event \(A\subset S\).
\label{ax:prob-nonnegative}
\end{axiom}



\begin{axiom}[]
\(\mathbb{P}(S)=1\).
\label{ax:total-mass-one}
\end{axiom}



\begin{axiom}[]
If the events \(A_{1}\), \(A_{2}\),
\(A_{3}\)\ldots are disjoint then
\begin{equation}
\mathbb{P}\left(\bigcup_{i=1}^{n}A_{i}\right)=\sum_{i=1}^{n}\mathbb{P}(A_{i})\mbox{ for every }n,
\end{equation}
and furthermore,
\begin{equation}
\mathbb{P}\left(\bigcup_{i=1}^{\infty}A_{i}\right)=\sum_{i=1}^{\infty}\mathbb{P}(A_{i}).
\end{equation}
\label{ax:countable-additivity}
\end{axiom}

The intuition behind the axioms goes like this: first, the probability
of an event should never be negative. Second, since the sample space
contains all possible outcomes, its probability should be one, or
100\%. The last axiom may look intimidating but it simply means that in
a sequence of disjoint events (in other words, sets that do not
overlap), the total probability (measure) should equal the sum of its
parts. For example, the chance of rolling a 1 or a 2 on a die should
be the chance of rolling a 1 plus the chance of rolling a 2.

\subsection{Properties}

For any events \(A\) and \(B\),

\begin{enumerate}
\item \label{enu:prop-prob-complement} \(\mathbb{P}(A^{c})=1-\mathbb{P}(A)\).
   \textbf{Proof:}
   Since \(A\cup A^{c}=S\) and \(A\cap A^{c}=\emptyset\), we have
   \[
   1=\mathbb{P}(S)=\mathbb{P}(A\cup A^{c})=\mathbb{P}(A)+\mathbb{P}(A^{c}).
   \]
\item \(\mathbb{P}(\emptyset)=0\).
   \textbf{Proof:}
   Note that \(\emptyset=S^{c}\), and use Property 1.
\item If \(A\subset B\) , then \(\mathbb{P}(A)\leq\mathbb{P}(B)\).
   \textbf{Proof:}
   Write \(B=A\cup\left(B\cap A^{c}\right)\), and notice that \(A\cap\left(B\cap A^{c}\right)=\emptyset\); thus
   \[
   \mathbb{P}(B)=\mathbb{P}(A\cup\left(B\cap A^{c}\right))=\mathbb{P}(A)+\mathbb{P}\left(B\cap A^{c}\right)\geq\mathbb{P}(A),
   \]
   since \(\mathbb{P}\left(B\cap A^{c}\right)\ge0\).
\item \(0\leq\mathbb{P}(A)\leq1\).
   \textbf{Proof:}
   The left inequality is immediate from Axiom~\ref{ax:prob-nonnegative}, and the second inequality follows from Property 3 since \(A\subset S\).
\item \textbf{The General Addition Rule.}
   \begin{equation}
   \label{eq:general-addition-rule-1}
   \mathbb{P}(A\cup B)=\mathbb{P}(A)+\mathbb{P}(B)-\mathbb{P}(A\cap B).
   \end{equation}
   More generally, for events \(A_{1}\), \(A_{2}\), \(A_{3}\),\ldots, \(A_{n}\),
   \begin{equation}
   \mathbb{P}\left(\bigcup_{i=1}^{n}A_{i}\right)=\sum_{i=1}^{n}\mathbb{P}(A_{i})-\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}\mathbb{P}(A_{i}\cap A_{j})+\cdots+(-1)^{n-1}\mathbb{P}\left(\bigcap_{i=1}^{n}A_{i}\right)
   \end{equation}
\item \textbf{The Theorem of Total Probability.} Let \(B_{1}\), \(B_{2}\), \ldots,
   \(B_{n}\) be mutually exclusive and exhaustive. Then
   \begin{equation}
   \label{eq:theorem-total-probability}
   \mathbb{P}(A)=\mathbb{P}(A\cap B_{1})+\mathbb{P}(A\cap B_{2})+\cdots+\mathbb{P}(A\cap B_{n}).
   \end{equation}
\end{enumerate}

\subsection{Assigning Probabilities}

A model of particular interest is the \textit{equally likely model}. The idea
is to divide the sample space \(S\) into a finite collection of
elementary events \(\{ a_{1},\ a_{2}, \ldots, a_{N} \}\) that are
equally likely in the sense that each \(a_{i}\) has equal chances of
occurring. The probability function associated with this model must
satisfy \(\mathbb{P}(S)=1\), by Axiom 2. On the other hand, it must
also satisfy \[ \mathbb{P}(S)=\mathbb{P}( \{ a_{1},\
a_{2},\ldots,a_{N} \} )=\mathbb{P}(a_{1}\cup a_{2}\cup\cdots\cup
a_{N})=\sum_{i=1}^{N}\mathbb{P}(a_{i}), \] by Axiom 3. Since
\(\mathbb{P}(a_{i})\) is the same for all \(i\), each one necessarily
equals \(1/N\).

For an event \(A\subset S\), we write \(A\) as a collection of
elementary outcomes: if \(A = \{ a_{i_{1}}, a_{i_{2}}, \ldots,
a_{i_{k}} \}\) then \(A\) has \(k\) elements and
\begin{align*}
\mathbb{P}(A) & =\mathbb{P}(a_{i_{1}})+\mathbb{P}(a_{i_{2}})+\cdots+\mathbb{P}(a_{i_{k}}),\\
 & =\frac{1}{N}+\frac{1}{N}+\cdots+\frac{1}{N},\\
 & =\frac{k}{N}=\frac{\#(A)}{\#(S)}.
\end{align*}
In other words, under the equally likely model, the probability of an
event \(A\) is determined by the number of elementary events that
\(A\) contains.



\begin{example}[]
Consider the random experiment \(E\) of tossing a coin. Then the
sample space is \(S=\{H,T\}\), and under the equally likely model,
these two outcomes have \(\mathbb{P}(H)=\mathbb{P}(T)=1/2\). This
model is taken when it is reasonable to assume that the coin is fair.
\end{example}



\begin{example}[]
Suppose the experiment \(E\) consists of tossing a fair coin
twice. The sample space may be represented by \(S=\{HH,\, HT,\, TH,\,
TT\}\). Given that the coin is fair and that the coin is tossed in an
independent and identical manner, it is reasonable to apply the
equally likely model.
\end{example}

What is \(\mathbb{P}(\mbox{at least 1 Head})\)? Looking at the sample
space we see the elements \(HH\), \(HT\), and \(TH\) have at least one
Head; thus, \(\mathbb{P}(\mbox{at least 1 Head})=3/4\).

What is \(\mathbb{P}(\mbox{no Heads})\)? Notice that the event \(\{
\mbox{no Heads} \} = \{ \mbox{at least one Head} \} ^{c}\), which by
Property~\ref{enu:prop-prob-complement} means \(\mathbb{P}(\mbox{no
Heads})=1-\mathbb{P}(\mbox{at least one head})=1-3/4=1/4\). It is
obvious in this simple example that the only outcome with no Heads is
\(TT\), however, this complementation trick can be handy in more
complicated problems.



\begin{example}[]
Imagine a three child family, each child
being either Boy (\(B\)) or Girl (\(G\)). An example sequence of
siblings would be \(BGB\). The sample space may be written \[ S =
\left\{ BBB,\ BGB,\ GBB,\ GGB,\ BBG,\ BGG,\ GBG,\ GGG,\ \right\}.\]
\label{exm:three-child-family}
\end{example}

Note that for many reasons (for instance, it turns out that girls are
slightly more likely to be born than boys), this sample space is
\textit{not} equally likely. For the sake of argument, however, we
will assume that the elementary outcomes each have probability
\(1/8\).

What is \(\mathbb{P}(\mbox{exactly 2 Boys})\)? Inspecting the sample
space reveals three outcomes with exactly two boys: \(\{ BBG,\,
BGB,\, GBB \}\).  Therefore \(\mathbb{P}(\mbox{exactly 2 Boys}) =
3/8\).

What is \(\mathbb{P}(\mbox{at most 2 Boys})\)? One way to solve the
problem would be to count the outcomes that have 2 or less Boys, but a
quicker way would be to recognize that the only way that the event
\(\{ \mbox{at most 2 Boys} \}\) does \textit{not} occur is the event \(\{
\mbox{all Boys} \}\).

Thus \[ \mathbb{P}(\mbox{at most 2 Boys}) = 1 - \mathbb{P}(BBB) = 1 -
1/8 = 7/8. \]



\begin{example}[]
Consider the experiment of rolling a six-sided die, and let the
outcome be the face showing up when the die comes to rest. Then \(S =
\{ 1,\,2,\,3,\,4,\,5,\,6 \}\). It is usually reasonable to suppose
that the die is fair, so that the six outcomes are equally likely.
\end{example}



\begin{example}[]
Consider a standard deck of 52 cards. These are usually labeled with
the four \textit{suits}: Clubs, Diamonds, Hearts, and Spades, and the 13
\textit{ranks}: 2, 3, 4, \ldots, 10, Jack (J), Queen (Q), King (K), and Ace
(A). Depending on the game played, the Ace may be ranked below 2 or
above King.
\end{example}

Let the random experiment \(E\) consist of drawing exactly one card
from a well-shuffled deck, and let the outcome be the face of the
card. Define the events \(A = \{ \mbox{draw an Ace} \}\) and \(B =
\{ \mbox{draw a Club} \}\). Bear in mind: we are only drawing one
card.

Immediately we have \(\mathbb{P}(A) = 4/52\) since there are four Aces
in the deck; similarly, there are \(13\) Clubs which implies
\(\mathbb{P}(B) = 13/52\).

What is \(\mathbb{P}(A\cap B)\)? We realize that there is only one
card of the 52 which is an Ace and a Club at the same time, namely,
the Ace of Clubs. Therefore \(\mathbb{P}(A\cap B)=1/52\).

To find \(\mathbb{P}(A\cup B)\) we may use the above with the General
Addition Rule to get
\begin{eqnarray*}
\mathbb{P}(A\cup B) & = & \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B),\\
 & = & 4/52 + 13/52 - 1/52,\\
 & = & 16/52.
\end{eqnarray*}



\begin{example}[]
Staying with the deck of cards, let another random experiment be the
selection of a five card stud poker hand, where ``five card stud''
means that we draw exactly five cards from the deck without
replacement, no more, and no less. It turns out that the sample space
\(S\) is so large and complicated that we will be obliged to settle
for the trivial description \(S = \{ \mbox{all possible 5 card hands}
\}\) for the time being. We will have a more precise description
later.  What is \(\mathbb{P}(\mbox{Royal Flush})\), or in other words,
\(\mathbb{P}(\mbox{A, K, Q, J, 10 all in the same suit})\)?

It should be clear that there are only four possible royal
flushes. Thus, if we could only count the number of outcomes in \(S\)
then we could simply divide four by that number and we would have our
answer under the equally likely model. This is the subject of Section~\ref{sec:methods-of-counting}.
\end{example}


\subsubsection{How to do it with \textsf{R}}

Probabilities are calculated in the \texttt{prob} package \cite{prob}
with the \texttt{Prob} function.

Consider the experiment of drawing a card from a standard deck of
playing cards. Let's denote the probability space associated with the
experiment as \texttt{S}, and let the subsets \texttt{A} and \texttt{B} be defined by the
following:

<<echo=TRUE>>=
S <- cards(makespace = TRUE)
A <- subset(S, suit == "Heart")
B <- subset(S, rank %in% 7:9)
@

Now it is easy to calculate

<<echo=TRUE>>=
Prob(A)
@

Note that we can get the same answer with

<<echo=TRUE>>=
Prob(S, suit == "Heart")
@

We also find \texttt{Prob(B) = 0.23} (listed here approximately, but
12/52 actually). Internally, the \texttt{Prob} function operates by summing
the \texttt{probs} column of its argument. It will find subsets on-the-fly if
desired.

We have as yet glossed over the details. More specifically, \texttt{Prob} has
three arguments: \texttt{x}, which is a probability space (or a subset of
one), \texttt{event}, which is a logical expression used to define a subset,
and \texttt{given}, which is described in Section~\ref{sec:conditional-probability}.

\textbf{WARNING.} The \texttt{event} argument is used to define a subset of \texttt{x},
that is, the only outcomes used in the probability calculation will be
those that are elements of \texttt{x} and satisfy \texttt{event} simultaneously. In
other words, \texttt{Prob(x, event)} calculates \texttt{Prob(intersect(x, subset(x, event)))}.

Consequently, \texttt{x} should be the entire probability space in the case
that \texttt{event} is non-null.

\section{Counting Methods} \label{sec:methods-of-counting}

The equally-likely model is a convenient and popular way to analyze
random experiments. And when the equally likely model applies, finding
the probability of an event \(A\) amounts to nothing more than
counting the number of outcomes that \(A\) contains (together with the
number of events in \(S\)). Hence, to be a master of probability one
must be skilled at counting outcomes in events of all kinds.



\begin{prop}[The Multiplication Principle]
Suppose that an experiment is composed of two successive
steps. Further suppose that the first step may be performed in
\(n_{1}\) distinct ways while the second step may be performed in
\(n_{2}\) distinct ways. Then the experiment may be performed in
\(n_{1}n_{2}\) distinct ways.
More generally, if the experiment is composed of \(k\) successive
steps which may be performed in \(n_{1}\), \(n_{2}\), \ldots, \(n_{k}\)
distinct ways, respectively, then the experiment may be performed in
\(n_{1} n_{2} \cdots n_{k}\) distinct ways.
\end{prop}



\begin{example}[]
We would like to order a pizza. It will be sure to have cheese (and
marinara sauce), but we may elect to add one or more of the following
five (5) available toppings: \[ \mbox{pepperoni, sausage, anchovies,
olives, and green peppers.}  \] How many distinct pizzas are possible?
\end{example}

There are many ways to approach the problem, but the quickest avenue
employs the Multiplication Principle directly. We will separate the
action of ordering the pizza into a series of stages. At the first
stage, we will decide whether or not to include pepperoni on the pizza
(two possibilities). At the next stage, we will decide whether or not
to include sausage on the pizza (again, two possibilities). We will
continue in this fashion until at last we will decide whether or not
to include green peppers on the pizza.

At each stage we will have had two options, or ways, to select a pizza
to be made. The Multiplication Principle says that we should multiply
the 2's to find the total number of possible pizzas: \(2 \cdot 2 \cdot
2 \cdot 2 \cdot 2 = 2^{5} = 32\).



\begin{example}[]
We would like to buy a desktop computer to study statistics. We go to
a website to build our computer our way. Given a line of products we
have many options to customize our computer. In particular, there are
2 choices for a processor, 3 different operating systems, 4 levels of
memory, 4 hard drives of differing sizes, and 10 choices for a
monitor. How many possible types of computer must the company be
prepared to build? \textbf{Answer:} \(2 \cdot 3 \cdot 4 \cdot 4 \cdot 10 = 960\)
\end{example}

\subsection{Ordered Samples}

Imagine a bag with \(n\) distinguishable balls inside. Now shake up
the bag and select \(k\) balls at random. How many possible sequences
might we observe?



\begin{prop}[]
The number of ways in which one may select an ordered sample of \(k\)
subjects from a population that has \(n\) distinguishable members is

\begin{itemize}
\item \(n^{k}\) if sampling is done with replacement,
\item \(n(n-1)(n-2)\cdots(n-k+1)\) if sampling is done without
  replacement.
\end{itemize}
\end{prop}

Recall from calculus the notation for \textit{factorials}:
\begin{eqnarray*}
1! & = & 1,\\
2! & = & 2 \cdot 1 = 2,\\
3! & = & 3 \cdot 2 \cdot 1 = 6,\\
 & \vdots\\
n! & = & n(n - 1)(n - 2) \cdots 3 \cdot 2 \cdot 1.
\end{eqnarray*}

\textbf{Fact:}
The number of permutations of \(n\) elements is \(n!\).



\begin{example}[]
Take a coin and flip it 7 times. How many sequences of Heads and Tails
are possible? \textbf{Answer:} \(2^{7}=128\).
\end{example}



\begin{example}[]
In a class of 20 students, we randomly select a class president, a
class vice-president, and a treasurer. How many ways can this be
done? \textbf{Answer:} \(20\cdot19\cdot18=6840\).
\end{example}



\begin{example}[]
We rent five movies to watch over the span of two nights. We wish to
watch 3 movies on the first night. How many distinct sequences of 3
movies could we possibly watch? \textit{Answer:} \(5\cdot4\cdot3=60\).
\end{example}


\subsection{Unordered Samples}

\begin{prop}[]
The number of ways in which one may select an unordered sample of
\(k\) subjects from a population that has \(n\) distinguishable
members is

\begin{itemize}
\item \((n-1+k)!/[(n-1)!k!]\) if sampling is done with replacement,
\item \(n!/[k!(n-k)!]\) if sampling is done without replacement.
\end{itemize}
\end{prop}

The quantity \(n!/[k!(n-k)!]\) is called a \textit{binomial coefficient} and
plays a special role in mathematics; it is denoted
\begin{equation}
\label{eq:binomial-coefficient}
{n \choose k}=\frac{n!}{k!(n-k)!}
\end{equation}
and is read ``\(n\) choose \(k\)''.



\begin{example}[]
You rent five movies to watch over the span of two nights, but only
wish to watch 3 movies the first night. Your friend, Fred, wishes to
borrow some movies to watch at his house on the first night. You owe
Fred a favor, and allow him to select 2 movies from the set of 5. How
many choices does Fred have? \textbf{Answer:} \({5 \choose 2}=10\).
\end{example}



\begin{example}[]
Place 3 six-sided dice into a cup. Next, shake the cup well and pour
out the dice. How many distinct rolls are possible? \textbf{Answer:}
\((6-1+3)!/[(6-1)!3!]={8 \choose 5}=56\).
\end{example}

\subsubsection{How to do it with \textsf{R}}

The factorial \(n!\) is computed with the command \texttt{factorial(n)} and
the binomial coefficient \({n \choose k}\) with the command
\texttt{choose(n,k)}.

The sample spaces we have computed so far have been relatively small,
and we can visually study them without much trouble. However, it is
\textit{very} easy to generate sample spaces that are prohibitively
large. And while \textsf{R} is wonderful and powerful and does almost
everything except wash windows, even \textsf{R} has limits of which we should
be mindful.

But we often do not need to actually generate the sample space; it
suffices to count the number of outcomes. The \texttt{nsamp} function
will calculate the number of rows in a sample space made by
\texttt{urnsamples} without actually devoting the memory resources
necessary to generate the space. The arguments are \texttt{n}, the
number of (distinguishable) objects in the urn, \texttt{k}, the sample
size, and \texttt{replace}, \texttt{ordered}, as above.

\begin{table}
\begin{centering}
\begin{tabular}{l|cc}
 & \texttt{ordered = TRUE}  & \texttt{ordered = FALSE} \tabularnewline
\hline
 &  & \tabularnewline
 \texttt{replace = TRUE} & \(n^{k}\) & \(\frac{(n-1+k)!}{(n-1)!k!}\) \tabularnewline
 &  & \tabularnewline
\texttt{replace = FALSE} & \(\frac{n!}{(n-k)!}\) & \({n \choose k} \)\tabularnewline
\multicolumn{1}{l}{} &  & \tabularnewline
\end{tabular}
\par\end{centering}
\caption[Sampling \(k\) from \(n\) objects with \texttt{urnsamples}.]{{\small Sampling \(k\) from \(n\) objects with \texttt{urnsamples}.}}
\label{tab:sampling-k-from-n}
\end{table}



\begin{example}[]
We will compute the number of outcomes for each of the four
\texttt{urnsamples} examples that we saw in Example~\ref{exm:sample-urn-two-from-three}. Recall that we took a sample of size two from an
urn with three distinguishable elements.
\end{example}

<<echo=TRUE>>=
nsamp(n=3, k=2, replace = TRUE, ordered = TRUE)
nsamp(n=3, k=2, replace = FALSE, ordered = TRUE)
nsamp(n=3, k=2, replace = FALSE, ordered = FALSE)
nsamp(n=3, k=2, replace = TRUE, ordered = FALSE)
@


Compare these answers with the length of the data frames generated above.

\subsubsection{The Multiplication Principle}

A benefit of \texttt{nsamp} is that it is \textit{vectorized} so that entering
vectors instead of numbers for \texttt{n}, \texttt{k}, \texttt{replace}, and \texttt{ordered}
results in a vector of corresponding answers. This becomes
particularly convenient for combinatorics problems.



\begin{example}[]
There are 11 artists who each submit a portfolio containing 7
paintings for competition in an art exhibition. Unfortunately, the
gallery director only has space in the winners' section to accommodate
12 paintings in a row equally spread over three consecutive walls. The
director decides to give the first, second, and third place winners
each a wall to display the work of their choice. The walls boast 31
separate lighting options apiece. How many displays are possible?
\end{example}

\textbf{Answer:} The judges will pick 3 (ranked) winners out of 11
(with \texttt{rep = FALSE}, \texttt{ord = TRUE}). Each artist will select 4 of his/her
paintings from 7 for display in a row (\texttt{rep = FALSE}, \texttt{ord = TRUE}),
and lastly, each of the 3 walls has 31 lighting possibilities
(\texttt{rep = TRUE}, \texttt{ord = TRUE}). These three numbers can be calculated quickly
with

<<echo=TRUE>>=
n <- c(11,7,31)
k <- c(3,4,3)
r <- c(FALSE,FALSE,TRUE)
@

<<echo=TRUE>>=
x <- nsamp(n, k, rep = r, ord = TRUE)
@

(Notice that \texttt{ordered} is always \texttt{TRUE}; \texttt{nsamp} will recycle
\texttt{ordered} and \texttt{replace} to the appropriate length.) By the
Multiplication Principle, the number of ways to complete the
experiment is the product of the entries of \texttt{x}:

<<echo=TRUE>>=
prod(x)
@


Compare this with the some other ways to compute the same thing:

<<echo=TRUE>>=
(11*10*9)*(7*6*5*4)*31^3
@


or alternatively

<<echo=TRUE>>=
prod(9:11)*prod(4:7)*31^3
@


or even

<<echo=TRUE>>=
prod(factorial(c(11,7))/factorial(c(8,3)))*31^3
@

As one can guess, in many of the standard counting problems there
aren't substantial savings in the amount of typing; it is about the
same using \texttt{nsamp} versus \texttt{factorial} and \texttt{choose}. But the virtue of
\texttt{nsamp} lies in its collecting the relevant counting formulas in a
one-stop shop. Ultimately, it is up to the user to choose the method
that works best for him/herself.



\begin{example}[The Birthday Problem]
Suppose that there are \(n\) people together
in a room. Each person announces the date of his/her birthday in
turn. The question is: what is the probability of at least one match?
If we let the event \(A\) represent \[ A = \{ \mbox{there is at least
one match}\}, \] then we are looking for \(\mathbb{P}(A)\), but as we
soon will see, it will be more convenient for us to calculate
\(\mathbb{P}(A^{c})\).
\end{example}

For starters we will ignore leap years and assume that there are only
365 days in a year. Second, we will assume that births are equally
distributed over the course of a year (which is not true due to all
sorts of complications such as hospital delivery schedules). See \url{http://en.wikipedia.org/wiki/Birthday\_problem}
for more.

Let us next think about the sample space. There are 365 possibilities
for the first person's birthday, 365 possibilities for the second, and
so forth. The total number of possible birthday sequences is therefore
\(\#(S)=365^{n}\).

Now we will use the complementation trick we saw in Example~\ref{exm:three-child-family}. We realize that the only situation in
  which \(A\) does \textit{not} occur is if there are \textit{no}
  matches among all people in the room, that is, only when everybody's
  birthday is different, so
  \[ \mathbb{P}(A)=1-\mathbb{P}(A^{c})=1-\frac{\#(A^{c})}{\#(S)}, \]
  since the outcomes are equally likely. Let us then suppose that
  there are no matches. The first person has one of 365 possible
  birthdays. The second person must not match the first, thus, the
  second person has only 364 available birthdays from which to
  choose. Similarly, the third person has only 363 possible birthdays,
  and so forth, until we reach the \(n^{\mathrm{th}}\) person, who has
  only \(365-n+1\) remaining possible days for a birthday. By the
  Multiplication Principle, we have
  \(\#(A^{c})=365\cdot364\cdots(365-n+1)\), and
\begin{equation}
\mathbb{P}(A)=1-\frac{365\cdot364\cdots(365-n+1)}{365^{n}}=1-\frac{364}{365}\cdot\frac{363}{365}\cdots\frac{(365-n+1)}{365}.
\end{equation}
As a surprising consequence, consider this: how many people does it
take to be in the room so that the probability of at least one match
is at least 0.50? Clearly, if there is only \(n=1\) person in the room
then the probability of a match is zero, and when there are \(n=366\)
people in the room there is a 100\% chance of a match (recall that we
are ignoring leap years). So how many people does it take so that
there is an equal chance of a match and no match?

When I have asked this question to students, the usual response is
``somewhere around \(n=180\) people'' in the room. The reasoning seems
to be that in order to get a 50\% chance of a match, there should be
50\% of the available days to be occupied. The number of students in a
typical classroom is 25, so as a companion question I ask students to
estimate the probability of a match when there are \(n=25\) students
in the room. Common estimates are a 1\%, or 0.5\%, or even 0.1\% chance
of a match. After they have given their estimates, we go around the
room and each student announces their birthday. More often than not,
we observe a match in the class, to the students' disbelief.

Students are usually surprised to hear that, using the formula above,
one needs only \(n=23\) students to have a greater than 50\% chance of
at least one match. Figure~\ref{fig:birthday} shows a graph of the birthday
probabilities:

<<birthday, echo=FALSE, fig=TRUE, include=FALSE, height=3.1,width=5>>=
g <- Vectorize(pbirthday.ipsur)
plot(1:50, g(1:50), xlab = "Number of people in room", ylab = "Prob(at least one match)")
remove(g)
abline(h = 0.5, lty = 2)
abline(v = 23, lty = 2)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-birthday}
\end{center}
\caption[The birthday problem.]{{\small The birthday problem. The horizontal line is at \(p=0.50\) and the vertical line is at \(n=23\).}}
\label{fig:birthday}
\end{figure}

\subsubsection{How to do it with \textsf{R}}

We can make the plot in Figure~\ref{fig:birthday} with the following
sequence of commands.
<<echo=TRUE, eval=FALSE>>=
#library(RcmdrPlugin.IPSUR)
g <- Vectorize(pbirthday.ipsur)
plot(1:50, g(1:50), xlab = "Number of people in room",
  ylab = "Prob(at least one match)" )
abline(h = 0.5)
abline(v = 23, lty = 2)
remove(g)
@
There is a \texttt{Birthday problem} item in the \texttt{Probability} menu \newline of
\texttt{RcmdrPlugin.IPSUR}. In the base \textsf{R} version, one can
compute approximate probabilities for the more general case of
probabilities other than 1/2, for differing total number of days in
the year, and even for more than two matches.

\section{Conditional Probability} \label{sec:conditional-probability}

Consider a full deck of 52 standard playing cards. Now select two cards from the deck, in succession. Let \[A = \{ \mbox{first card drawn is an Ace} \}\] and \[B = \{ \mbox{second card drawn is an Ace} \}\]. Since there are four Aces in the deck, it is natural to assign \(\mathbb{P}(A) = 4/52\). Suppose we look at the first card. What now is the probability of \(B\)? Of course, the answer depends on the value of the first card. If the first card is an Ace, then the probability that the second also is an Ace should be \(3/51\), but if the first card is not an Ace, then the probability that the second is an Ace should be \(4/51\). As notation for these two situations we write
\[
\mathbb{P}(B\vert A)=3/51,\quad
\mathbb{P}(B\vert A^{c})=4/51.
\]

\begin{defn}[Conditional probability.]
The \textit{conditional probability} of \(B\) given \(A\), denoted
\(\mathbb{P}(B|A)\), is defined by
\begin{equation}
\mathbb{P}(B|A)=\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(A)},\quad \mbox{if }\mathbb{P}(A)>0.
\end{equation}
\end{defn}

We will not be discussing a conditional probability of \(B\) given
\(A\) when \(\mathbb{P}(A)=0\), even though this theory exists, is
well developed, and forms the foundation for the study of stochastic
processes\footnote{Conditional probability in this case is defined by
means of \textit{conditional expectation}, a topic that is well beyond the
scope of this text. The interested reader should consult an advanced
text on probability theory, such as Billingsley, Resnick, or Ash
Dooleans-Dade.}.

\begin{example}[Toss a coin twice.] 
The sample space is given by \[S=\{ HH,\ HT,\ TH,\
TT \}.\] Let \(A= \{ \mbox{a head occurs} \}\) and \(B= \{ \mbox{a
head and tail occur} \}\). It should be clear that
\(\mathbb{P}(A)=3/4\), \(\mathbb{P}(B)=2/4\), and \(\mathbb{P}(A\cap
B)=2/4\). What now are the probabilities \(\mathbb{P}(A|B)\) and
\(\mathbb{P}(B|A)\)?

\[ \mathbb{P}(A|B)=\frac{\mathbb{P}(A\cap
B)}{\mathbb{P}(B)}=\frac{2/4}{2/4}=1. \]
In other words, once we know that a Head and Tail occur, we may be
certain that a Head occurs. Next
\[
\mathbb{P}(B|A)=\frac{\mathbb{P}(A\cap
B)}{\mathbb{P}(A)}=\frac{2/4}{3/4}=\frac{2}{3},
\]
which means that given the information that a Head has occurred, we no longer need to account for the outcome \(TT\), and the remaining three outcomes are equally likely with exactly two outcomes lying in the set \(B\).
\end{example}


\begin{example}[Rolling a die twice.]
Toss a six-sided die twice. The sample space consists of all ordered pairs \((i,j)\) of the numbers \(1,2,\ldots,6\), that is, \(S = \{ (1,1),\ (1,2),\ldots,(6,6) \}
\). We know from Section~\ref{sec:methods-of-counting} that \(\# (S) =
6^{2} = 36\). Let \(A = \{ \mbox{outcomes match} \}\) and \(B = \{
\mbox{sum of outcomes at least 8} \}\). The sample space may be
represented by a matrix:
\label{exm:toss-a-six-sided-die-twice}
\end{example}

\begin{table}
\begin{centering}
\begin{tabular}{c}
\begin{sideways}
First roll%
\end{sideways}\tabularnewline
\end{tabular}\begin{tabular}{c|cccccc|}
\multicolumn{1}{c}{} & \multicolumn{6}{c}{Second Roll}\tabularnewline
\multicolumn{1}{c}{} & 1 & 2 & 3 & 4 & 5 & \multicolumn{1}{c}{6}\tabularnewline
\cline{2-7} 
1 & $\varprod$ &  &  &  &  & \tabularnewline
2 &  & $\varprod$ &  &  &  & {\Large $\bigcirc$}\tabularnewline
3 &  &  & $\varprod$ &  & {\Large $\bigcirc$} & {\Large $\bigcirc$}\tabularnewline
4 &  &  &  & {\huge $\otimes$} & {\Large $\bigcirc$} & {\Large $\bigcirc$}\tabularnewline
5 &  &  & {\Large $\bigcirc$} & {\Large $\bigcirc$} & {\huge $\otimes$} & {\Large $\bigcirc$}\tabularnewline
6 &  & {\Large $\bigcirc$} & {\Large $\bigcirc$} & {\Large $\bigcirc$} & {\Large $\bigcirc$} & {\huge $\otimes$}\tabularnewline
\cline{2-7} 
\end{tabular}
\par\end{centering}
\caption[Rolling two dice.]{{\small Rolling two dice. The outcomes in \(A\) are marked with X, the outcomes in \(B\) are marked with O.}}
\label{tab:twodiceAB}
\end{table}

The outcomes lying in the event $A$ are marked with the symbol {}``$\varprod$'',
the outcomes falling in $B$ are marked with {}``$\bigcirc$'',
and those in both $A$ and $B$ are marked {}``$\otimes$''. Now
it is clear that \(\mathbb{P}(A)=6/36\), \(\mathbb{P}(B)=15/36\), and
\(\mathbb{P}(A\cap B)=3/36\).
Finally,
\[
\mathbb{P}(A|B)=\frac{3/36}{15/36}=\frac{1}{5},\quad
\mathbb{P}(B|A)=\frac{3/36}{6/36}=\frac{1}{2}.
\]
Again, we see that given the knowledge that \(B\) occurred (the 15
outcomes in the upper right triangle), there are 3 of the 15 that fall
into the set \(A\), thus the probability is \(3/15\). Similarly, given
that \(A\) occurred (we are on the diagonal), there are 3 out of 6
outcomes that also fall in \(B\), thus, the probability of \(B\) given
\(A\) is 1/2.


\subsection{How to do it with \textsf{R}}

Continuing with Example~\ref{exm:toss-a-six-sided-die-twice}, the
first thing to do is set up the probability space with the
\texttt{rolldie} function.

<<echo=TRUE>>=
S <- rolldie(2, makespace = TRUE)  # assumes ELM
head(S)                            #  first few rows
@


Next we define the events

<<echo=TRUE>>=
A <- subset(S, X1 == X2)
B <- subset(S, X1 + X2 >= 8)
@

And now we are ready to calculate probabilities. To do conditional
probability, we use the \texttt{given} argument of the \texttt{Prob} function:

<<echo=TRUE>>=
Prob(A, given = B)
Prob(B, given = A)
@

Note that we do not actually need to define the events \(A\) and \(B\)
separately as long as we reference the original probability space
\(S\) as the first argument of the \texttt{Prob} calculation:

<<echo=TRUE>>=
Prob(S, X1==X2, given = (X1 + X2 >= 8) )
Prob(S, X1+X2 >= 8, given = (X1==X2) )
@

\subsection{Properties and Rules}

The following theorem establishes that conditional probabilities
behave just like regular probabilities when the conditioned event is
fixed.



\begin{thm}
For any fixed event \(A\) with \(\mathbb{P}(A)>0\),
\begin{enumerate}
\item \(\mathbb{P} (B|A)\geq 0\), for all events \(B \subset S\),
\item \(\mathbb{P} (S|A) = 1\), and
\item If \(B_{1}\), \(B_{2}\), \(B_{3}\),\ldots are disjoint events, then
  \begin{equation}
  \mathbb{P}\left(\left.\bigcup_{k=1}^{\infty}B_{k}\:\right|A\right)=\sum_{k=1}^{\infty}\mathbb{P}(B_{k}|A).
  \end{equation}
\end{enumerate}
\end{thm}

In other words, \(\mathbb{P}(\cdot|A)\) is a legitimate probability
function. With this fact in mind, the following properties are
immediate:



\begin{prop}[]
For any events \(A\), \(B\), and \(C\) with \(\mathbb{P}(A)>0\),

\begin{enumerate}
\item \(\mathbb{P} ( B^{c} | A ) = 1 - \mathbb{P} (B|A).\)
\item If \(B\subset C\) then \(\mathbb{P}(B|A)\leq\mathbb{P}(C|A)\).
\item \(\mathbb{P} [ ( B\cup C ) | A ] = \mathbb{P} (B|A) +
   \mathbb{P}(C|A) - \mathbb{P} [ (B \cap C|A) ].\)
\item \textbf{The Multiplication Rule.} For any two events \(A\) and \(B\),
   \begin{equation}
   \label{eq:multiplication-rule-short}
   \mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B|A).
   \end{equation}
   And more generally, for events \(A_{1}\), \(A_{2}\), \(A_{3}\),\ldots,
   \(A_{n}\),
   \begin{equation}
   \label{eq:multiplication-rule-long}
   \mathbb{P}(A_{1}\cap A_{2}\cap\cdots\cap A_{n})=\mathbb{P}(A_{1})\mathbb{P}(A_{2}|A_{1})\cdots\mathbb{P}(A_{n}|A_{1}\cap A_{2}\cap\cdots\cap A_{n-1}).
   \end{equation}
\end{enumerate}
\end{prop}

The Multiplication Rule is very important because it allows us to find
probabilities in random experiments that have a sequential structure,
as the next example shows.



\begin{example}[]
At the beginning of the section we drew
two cards from a standard playing deck. Now we may answer our original
question, what is \(\mathbb{P}(\mbox{both Aces})\)?  \[
\mathbb{P}(\mbox{both Aces})=\mathbb{P}(A\cap
B)=\mathbb{P}(A)\mathbb{P}(B|A)=\frac{4}{52}\cdot\frac{3}{51}\approx0.00452.
\]
\label{exm:two-cards-both-aces}
\end{example}

\subsubsection{How to do it with \textsf{R}} \label{sub:howto-ps-objects}

Continuing Example~\ref{exm:two-cards-both-aces}, we set up the probability
space by way of a three step process. First we employ the \texttt{cards}
function to get a data frame \texttt{L} with two columns: \texttt{rank} and
\texttt{suit}. Both columns are stored internally as factors with 13 and 4
levels, respectively.

Next we sample two cards randomly from the \texttt{L} data frame by way of
the \texttt{urnsamples} function. It returns a list \texttt{M} which contains all
possible pairs of rows from \texttt{L} (there are \texttt{choose(52,2)} of
them). The sample space for this experiment is exactly the list \texttt{M}.

At long last we associate a probability model with the sample
space. This is right down the \texttt{probspace} function's alley. It assumes
the equally likely model by default. We call this result \texttt{N} which is
an object of class \texttt{ps} -- short for ``probability space''.

But do not be intimidated. The object \texttt{N} is nothing more than a list
with two elements: \texttt{outcomes} and \texttt{probs}. The \texttt{outcomes} element is
itself just another list, with \texttt{choose(52,2)} entries, each one a data
frame with two rows which correspond to the pair of cards chosen. The
\texttt{probs} element is just a vector with \texttt{choose(52,2)} entries all the
same: \texttt{1/choose(52,2)}.

Putting all of this together we do

<<>>=
L <- cards()
M <- urnsamples(L, size = 2)
N <- probspace(M)
@

Now that we have the probability space \texttt{N} we are ready to do some
probability. We use the \texttt{Prob} function, just like before. The only
trick is to specify the event of interest correctly, and recall that
we were interested in \(\mathbb{P}(\mbox{both Aces})\). But if the
cards are both Aces then the \texttt{rank} of both cards should be \texttt{A}, which
sounds like a job for the \texttt{all} function:

<<echo=TRUE>>=
Prob(N, all(rank == "A"))
@

Note that this value matches what we found in Example~\ref{exm:two-cards-both-aces}, above. We could calculate all sorts of
probabilities at this point; we are limited only by the complexity of
the event's computer representation.




\begin{example}[]
Consider an urn with 10 balls inside, 7 of
which are red and 3 of which are green. Select 3 balls successively
from the urn. Let \(A = \{ 1^{\mathrm{st}} \mbox{ ball is red} \}\),
\(B = \{ 2^{\mathrm{nd}} \mbox{ ball is red} \}\), and \(C = \{
3^{\mathrm{rd}} \mbox{ ball is red} \}\). Then \[
\mathbb{P}(\mbox{all 3 balls are red})=\mathbb{P}(A\cap B\cap
C)=\frac{7}{10}\cdot\frac{6}{9}\cdot\frac{5}{8}\approx 0.2917.  \]
\label{exm:urn-7-red-3-green}
\end{example}

\subsubsection{How to do it with \textsf{R}}

Example~\ref{exm:urn-7-red-3-green} is similar to Example~\ref{exm:two-cards-both-aces}, but it is even easier. We need to set up
an urn (vector \texttt{L}) to hold the balls, we sample from \texttt{L} to get the
sample space (data frame \texttt{M}), and we associate a probability vector
(column \texttt{probs}) with the outcomes (rows of \texttt{M}) of the sample
space. The final result is a probability space (an ordinary data frame
\texttt{N}).

It is easier for us this time because our urn is a vector instead of a
\texttt{cards()} data frame. Before there were two dimensions of information
associated with the outcomes (rank and suit) but presently we have
only one dimension (color).

<<echo=TRUE>>=
L <- rep(c("red","green"), times = c(7,3))
M <- urnsamples(L, size = 3, replace = FALSE, ordered = TRUE)
N <- probspace(M)
@

Now let us think about how to set up the event
\(\{ \mbox{all 3 balls are red}\}\). Rows of \texttt{N} that satisfy
this condition have \texttt{X1=="red" \& X2=="red" \& X3=="red"}, but
there must be an easier way. Indeed, there is. The \texttt{isrep}
function (short for ``is repeated'') in the \texttt{prob} package was
written for this purpose. The command \texttt{isrep(N,"red",3)} will
test each row of \texttt{N} to see whether the value \texttt{red}
appears \texttt{3} times. The result is exactly what we need to define
an event with the \texttt{Prob} function. Observe

<<echo=TRUE>>=
Prob(N, isrep(N, "red", 3))
@

Note that this answer matches what we found in Example~\ref{exm:urn-7-red-3-green}. Now let us try some other probability
questions. What is the probability of getting two \texttt{red}'s?

<<echo=TRUE>>=
Prob(N, isrep(N, "red", 2))
@


Note that the exact value is \(21/40\); we will learn a quick way to
compute this in Section~\ref{sec:other-discrete-distributions}. What is the
probability of observing \texttt{red}, then \texttt{green}, then \texttt{red}?

<<echo=TRUE>>=
Prob(N, isin(N, c("red","green","red"), ordered = TRUE))
@

Note that the exact value is \(7/40\) (do it with the Multiplication
Rule). What is the probability of observing \texttt{red}, \texttt{green}, and \texttt{red},
in no particular order?

<<echo=TRUE>>=
Prob(N, isin(N, c("red","green","red")))
@

We already knew this. It is the probability of observing two \texttt{red}'s,
above.



\begin{example}[]
Consider two urns, the first with 5 red balls and 3 green balls, and
the second with 2 red balls and 6 green balls. Your friend randomly
selects one ball from the first urn and transfers it to the second
urn, without disclosing the color of the ball. You select one ball
from the second urn. What is the probability that the selected ball is
red?
\end{example}

Let \(A = \{ \mbox{transferred ball is red} \}\) and \(B = \{
\mbox{selected ball is red} \}\). Write
\begin{align*}
B & =S\cap B\\
 & =(A\cup A^{c})\cap B\\
 & =(A\cap B)\cup(A^{c}\cap B)
\end{align*}
and notice that \(A\cap B\) and \(A^{c}\cap B\) are disjoint. Therefore
\begin{align*}
\mathbb{P}(B) & =\mathbb{P}(A\cap B)+\mathbb{P}(A^{c}\cap B)\\
 & =\mathbb{P}(A)\mathbb{P}(B|A)+\mathbb{P}(A^{c})\mathbb{P}(B|A^{c})\\
 & =\frac{5}{8}\cdot\frac{3}{9}+\frac{3}{8}\cdot\frac{2}{9}\\
 & =\frac{21}{72}\
\end{align*}
(which is 7/24 in lowest terms).



\begin{example}[]
  We saw the \texttt{RcmdrTestDrive} data set in Chapter~\ref{cha:introduction-to-r} in which a two-way table of the smoking
  status versus the gender was
\end{example}

<<echo=TRUE>>=
#library(RcmdrPlugin.IPSUR)
data(RcmdrTestDrive)
.Table <- xtabs( ~ smoking + gender, data = RcmdrTestDrive)
addmargins(.Table) # Table with marginal distributions
@


If one person were selected at random from the data set, then we see
from the two-way table that \(\mathbb{P}(\mbox{Female})=70/168\) and
\(\mathbb{P}(\mbox{Smoker})=32/168\). Now suppose that one of the
subjects quits smoking, but we do not know the person's gender. If we
now select one nonsmoker at random, what would be
\(\mathbb{P}(\mbox{Female})\)? This example is just like the last
example, but with different labels. Let \(A = \{ \mbox{the quitter is
a female} \}\) and \(B = \{ \mbox{selected nonsmoker is a female} \}
\). Write

\begin{align*}
B & =S\cap B\\
 & =(A\cup A^{c})\cap B\\
 & =(A\cap B)\cup(A^{c}\cap B)
\end{align*}
and notice that \(A\cap B\) and \(A^{c}\cap B\) are disjoint. Therefore
\begin{align*}
\mathbb{P}(B) & =\mathbb{P}(A\cap B)+\mathbb{P}(A^{c}\cap B),\\
 & =\mathbb{P}(A)\mathbb{P}(B|A)+\mathbb{P}(A^{c})\mathbb{P}(B|A^{c}),\\
 & =\frac{9}{32}\cdot\frac{62}{137}+\frac{23}{32}\cdot\frac{76}{137},\\
 & =\frac{2306}{4384},
\end{align*}
(which is 1153/2192 in lowest terms).

Using the same reasoning, we can return to the example from the
beginning of the section and show that
\[ \mathbb{P}(\{ \mbox{second
card is an Ace} \} )=4/52.
\]

\section{Independent Events} \label{sec:independent-events}

Toss a coin twice. The sample space is \(S= \{ HH,\ HT,\ TH,\ TT \}
\). We know that \(\mathbb{P}(1^{\mathrm{st}}\mbox{ toss is }H)=2/4\),
\(\mathbb{P}(2^{\mathrm{nd}}\mbox{ toss is }H)=2/4\), and
\(\mathbb{P}(\mbox{both }H)=1/4\). Then

\begin{align*}
\mathbb{P}(2^{\mathrm{nd}}\mbox{ toss is }H\ \vert \ 1^{\mathrm{st}}\mbox{ toss is }H) & =\frac{\mathbb{P}(\mbox{both }H)}{\mathbb{P}(1^{\mathrm{st}}\mbox{ toss is }H)}, \\
 & =\frac{1/4}{2/4},\\
 & =\mathbb{P}(2^{\mathrm{nd}}\mbox{ toss is }H).
\end{align*}

Intuitively, this means that the information that the first toss is
\(H\) has no bearing on the probability that the second toss is
\(H\). The coin does not remember the result of the first toss.



\begin{defn}[Independent events.]
Events \(A\) and \(B\) are said to be \textit{independent} if
\begin{equation}
\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B).
\end{equation}
Otherwise, the events are said to be \textit{dependent}.
\end{defn}

The connection with the above example stems from the following. We
know from Section~\ref{sec:conditional-probability} that when
\(\mathbb{P}(B)>0\) we may write
\begin{equation}
\mathbb{P}(A|B)=\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}.
\end{equation}

In the case that \(A\) and \(B\) are independent, the numerator of the
fraction factors so that \(\mathbb{P}(B)\) cancels with the result:
\begin{equation}
\mathbb{P}(A|B)=\mathbb{P}(A)\mbox{ when \(A\), \(B\) are independent.}
\end{equation}

The interpretation in the case of independence is that the information
that the event \(B\) occurred does not influence the probability of
the event \(A\) occurring. Similarly,
\(\mathbb{P}(B|A)=\mathbb{P}(B)\), and so the occurrence of the event
\(A\) likewise does not affect the probability of event \(B\). It may
seem more natural to define \(A\) and \(B\) to be independent when
\(\mathbb{P}(A|B)=\mathbb{P}(A)\); however, the conditional
probability \(\mathbb{P}(A|B)\) is only defined when
\(\mathbb{P}(B)>0\). Our definition is not limited by this
restriction. It can be shown that when \(\mathbb{P}(A),\
\mathbb{P}(B)>0\) the two notions of independence are equivalent.



\begin{prop}[]
If the events \(A\) and \(B\) are independent then

\begin{itemize}
\item \(A\) and \(B^{c}\) are independent,
\item \(A^{c}\) and \(B\) are independent,
\item \(A^{c}\) and \(B^{c}\) are independent.
\end{itemize}
\end{prop}

\begin{proof}
Suppose that \(A\) and \(B\) are independent. We will show the second
one; the others are similar. We need to show that \[
\mathbb{P}(A^{c}\cap B)=\mathbb{P}(A^{c})\mathbb{P}(B).  \] To this
end, note that the Multiplication Rule, Equation
\eqref{eq:multiplication-rule-short} implies
\begin{eqnarray*}
\mathbb{P}(A^{c}\cap B) & = & \mathbb{P}(B)\mathbb{P}(A^{c}|B),\\
 & = & \mathbb{P}(B)[1-\mathbb{P}(A|B)],\\
 & = & \mathbb{P}(B)\mathbb{P}(A^{c}).
\end{eqnarray*}
\end{proof}



\begin{defn}[Mutually independent events.]
The events \(A\), \(B\), and \(C\) are \textit{mutually independent} if the
following four conditions are met:
\begin{eqnarray*}
\mathbb{P}(A\cap B) & = & \mathbb{P}(A)\mathbb{P}(B),\\
\mathbb{P}(A\cap C) & = & \mathbb{P}(A)\mathbb{P}(C),\\
\mathbb{P}(B\cap C) & = & \mathbb{P}(B)\mathbb{P}(C),
\end{eqnarray*}
and
\[
\mathbb{P}(A\cap B\cap C)=\mathbb{P}(A)\mathbb{P}(B)\mathbb{P}(C).
\]
If only the first three conditions hold then \(A\), \(B\), and \(C\)
are said to be independent \textit{pairwise}. Note that pairwise independence
is not the same as mutual independence when the number of events is
larger than two.
\end{defn}

We can now deduce the pattern for \(n\) events, \(n>3\). The events
will be mutually independent only if they satisfy the product equality
pairwise, then in groups of three, in groups of four, and so forth, up
to all \(n\) events at once. For \(n\) events, there will be
\(2^{n}-n-1\) equations that must be satisfied (see Exercise~\ref{exr:numb-cond-indep}). Although these requirements for a set of
events to be mutually independent may seem stringent, the good news is
that for most of the situations considered in this book the conditions
will all be met (or at least we will suppose that they are).



\begin{example}[]
Toss ten coins. What is the probability of
observing at least one Head?
\label{exm:toss-ten-coins}
\end{example}

\textbf{Answer:} Let \(A_{i}= \{ \mbox{the
}i^{\mathrm{th}}\mbox{ coin shows }H \} ,\
i=1,2,\ldots,10\). Supposing that we toss the coins in such a way that
they do not interfere with each other, this is one of the situations
where all of the \(A_{i}\) may be considered mutually independent due
to the nature of the tossing. Of course, the only way that there will
not be at least one Head showing is if all tosses are
Tails. Therefore,
\begin{align*}
\mathbb{P}(\mbox{at least one }H) & =1-\mathbb{P}(\mbox{all }T),\\
 & =1-\mathbb{P}(A_{1}^{c}\cap A_{2}^{c}\cap\cdots\cap A_{10}^{c}),\\
 & =1-\mathbb{P}(A_{1}^{c})\mathbb{P}(A_{2}^{c})\cdots\mathbb{P}(A_{10}^{c}),\\
 & =1-\left(\frac{1}{2}\right)^{10},
\end{align*}
which is approximately \(0.9990234\).


\subsection{How to do it with \textsf{R}}


Toss ten coins. What is the probability of observing at least one
Head?
@

<<echo=TRUE>>=
S <- tosscoin(10, makespace = TRUE)
A <- subset(S, isrep(S, vals = "T", nrep = 10))
1 - Prob(A)
@


Compare this answer to what we got in Example~\ref{exm:toss-ten-coins}.


\subsection{Independent, Repeated Experiments}

Generalizing from above it is common to repeat a certain experiment
multiple times under identical conditions and in an independent
manner. We have seen many examples of this already: tossing a coin
repeatedly, rolling a die or dice, \textit{etc}.

The \texttt{iidspace} function was designed specifically for this
situation. It has three arguments: \texttt{x}, which is a vector of outcomes,
\texttt{ntrials}, which is an integer telling how many times to repeat the
experiment, and \texttt{probs} to specify the probabilities of the outcomes
of \texttt{x} in a single trial.



\begin{example}[An unbalanced coin]
(Continued, see Example~\ref{exm:unbalanced-coin}). It was
easy enough to set up the probability space for one unbalanced toss,
however, the situation becomes more complicated when there are many
tosses involved. Clearly, the outcome \(HHH\) should not have the same
probability as \(TTT\), which should again not have the same
probability as \(HTH\). At the same time, there is symmetry in the
experiment in that the coin does not remember the face it shows from
toss to toss, and it is easy enough to toss the coin in a similar way
repeatedly.
\end{example}

We may represent tossing our unbalanced coin three times with the following:

<<echo=TRUE>>=
iidspace(c("H","T"), ntrials = 3, probs = c(0.7, 0.3))
@

As expected, the outcome \(HHH\) has the largest probability, while
\(TTT\) has the smallest. (Since the trials are independent,
\(\mathbb{P}(HHH)=0.7^{3}\) and \(\mathbb{P}(TTT)=0.3^{3}\), \textit{etc}.)
Note that the result of the function call is a probability space, not
a sample space (which we could construct already with the \texttt{tosscoin}
or \texttt{urnsamples} functions). The same procedure could be used to model
an unbalanced die or any other experiment that may be represented with
a vector of possible outcomes.


Note that \texttt{iidspace} will assume \texttt{x} has equally likely outcomes if no
\texttt{probs} argument is specified. Also note that the argument \texttt{x} is a
\textit{vector}, not a data frame. Something like
: iidspace(tosscoin(1),\ldots)
would give an error.

\section{Bayes' Rule} \label{sec:bayes-rule}

We mentioned the subjective view of probability in Section~\ref{sec:interpreting-probabilities}. In this section we introduce a
rule that allows us to update our probabilities when new information
becomes available.



\begin{thm}[Bayes' Rule]
Let \(B_{1}\), \(B_{2}\), \ldots, \(B_{n}\) be mutually exclusive and
exhaustive and let \(A\) be an event with \(\mathbb{P}(A)>0\). Then
\begin{equation}
\label{eq:bayes-rule}
\mathbb{P}(B_{k}|A)=\frac{\mathbb{P}(B_{k})\mathbb{P}(A|B_{k})}{\sum_{i=1}^{n}\mathbb{P}(B_{i})\mathbb{P}(A|B_{i})},\quad k=1,2,\ldots,n.
\end{equation}
\end{thm}

\begin{proof}
The proof follows from looking at \(\mathbb{P}(B_{k}\cap A)\) in two
different ways. For simplicity, suppose that \(P(B_{k})>0\) for all
\(k\). Then \[ \mathbb{P}(A)\mathbb{P}(B_{k}|A)=\mathbb{P}(B_{k}\cap
A)=\mathbb{P}(B_{k})\mathbb{P}(A|B_{k}).  \] Since \(\mathbb{P}(A)>0\)
we may divide through to obtain \[
\mathbb{P}(B_{k}|A)=\frac{\mathbb{P}(B_{k})\mathbb{P}(A|B_{k})}{\mathbb{P}(A)}.
\] Now remembering that \(\{ B_{k} \}\) is a partition, the Theorem of
Total Probability (Equation \eqref{eq:theorem-total-probability} )
gives the denominator of the last expression to be \[
\mathbb{P}(A)=\sum_{k=1}^{n}\mathbb{P}(B_{k}\cap
A)=\sum_{k=1}^{n}\mathbb{P}(B_{k})\mathbb{P}(A|B_{k}).  \]
\end{proof}


What does it mean? Usually in applications we are given (or know)
\textit{a priori} probabilities \(\mathbb{P}(B_{k})\). We go out and
collect some data, which we represent by the event \(A\). We want to
know: how do we \textit{update} \(\mathbb{P}(B_{k})\) to
\(\mathbb{P}(B_{k}|A)\)? The answer: Bayes' Rule.



\begin{example}[Misfiling Assistants]
In this problem,
there are three assistants working at a company: Moe, Larry, and
Curly. Their primary job duty is to file paperwork in the filing
cabinet when papers become available. The three assistants have
different work schedules:
\label{exm:misfiling-assistants}
\end{example}

\begin{table}[H]
\begin{tabular}{c|ccc}
 & Moe & Larry & Curly\tabularnewline
\hline
Workload & 60\% & 30\% & 10\%\tabularnewline
\end{tabular}
\caption[Misfiling assistants: workload.]{Misfiling assistants: workload.}
\end{table}

That is, Moe works 60\% of the time, Larry works 30\% of the time, and
Curly does the remaining 10\%, and they file documents at approximately
the same speed. Suppose a person were to select one of the documents
from the cabinet at random. Let \(M\) be the event \[ M= \{ \mbox{Moe
filed the document} \} \] and let \(L\) and \(C\) be the events that
Larry and Curly, respectively, filed the document. What are these
events' respective probabilities? In the absence of additional
information, reasonable prior probabilities would just be

\begin{table}[H]
\begin{tabular}{c|ccc}
 & Moe & Larry & Curly\tabularnewline
\hline
Prior Probability & $\mathbb{P}(M)=0.60$ & $\mathbb{P}(L)=0.30$ & $\mathbb{P}(C)=0.10$\tabularnewline
\end{tabular}
\caption{Misfiling assistants: prior.}
\end{table}

Now, the boss comes in one day, opens up the file cabinet, and selects
a file at random. The boss discovers that the file has been
misplaced. The boss is so angry at the mistake that (s)he threatens to
fire the one who erred. The question is: who misplaced the file?

The boss decides to use probability to decide, and walks straight to
the workload schedule. (S)he reasons that, since the three employees
work at the same speed, the probability that a randomly selected file
would have been filed by each one would be proportional to his
workload. The boss notifies \textit{Moe} that he has until the end of the day
to empty his desk.

But Moe argues in his defense that the boss has ignored additional
information. Moe's likelihood of having misfiled a document is smaller
than Larry's and Curly's, since he is a diligent worker who pays close
attention to his work. Moe admits that he works longer than the
others, but he doesn't make as many mistakes as they do. Thus, Moe
recommends that -- before making a decision -- the boss should update
the probability (initially based on workload alone) to incorporate the
likelihood of having observed a misfiled document.

And, as it turns out, the boss has information about Moe, Larry, and
Curly's filing accuracy in the past (due to historical performance
evaluations). The performance information may be represented by the
following table:

\begin{table}[H]
\begin{tabular}{c|ccc}
 & Moe & Larry & Curly\tabularnewline
\hline
Misfile Rate & 0.003 & 0.007 & 0.010\tabularnewline
\end{tabular}
\caption{Misfiling assistants: misfile rate.}
\end{table}


In other words, on the average, Moe misfiles 0.3\% of the documents he
is supposed to file. Notice that Moe was correct: he is the most
accurate filer, followed by Larry, and lastly Curly. If the boss were
to make a decision based only on the worker's overall accuracy, then
\textit{Curly} should get the axe. But Curly hears this and interjects
that he only works a short period during the day, and consequently
makes mistakes only very rarely; there is only the tiniest chance that
he misfiled this particular document.

The boss would like to use this updated information to update the
probabilities for the three assistants, that is, (s)he wants to use
the additional likelihood that the document was misfiled to update
his/her beliefs about the likely culprit. Let \(A\) be the event that
a document is misfiled. What the boss would like to know are the three
probabilities
\[
\mathbb{P}(M|A),\mbox{ }\mathbb{P}(L|A),\mbox{ and }\mathbb{P}(C|A).
\]
We will show the calculation for \(\mathbb{P}(M|A)\), the other two
cases being similar. We use Bayes' Rule in the form
\[
\mathbb{P}(M|A)=\frac{\mathbb{P}(M\cap A)}{\mathbb{P}(A)}.
\]
Let's try to find \(\mathbb{P}(M\cap A)\), which is just
\(\mathbb{P}(M)\cdot\mathbb{P}(A|M)\) by the Multiplication Rule. We
already know \(\mathbb{P}(M)=0.6\) and \(\mathbb{P}(A|M)\) is nothing
more than Moe's misfile rate, given above to be
\(\mathbb{P}(A|M)=0.003\). Thus, we compute
\[
\mathbb{P}(M\cap A)=(0.6)(0.003)=0.0018.
\]
Using the same procedure we may calculate
\[
\mathbb{P}(L \cap A)=0.0021\mbox{ and }\mathbb{P}(C \cap A)=0.0010.
\]

Now let's find the denominator, \(\mathbb{P}(A)\). The key here is the
notion that if a file is misplaced, then either Moe or Larry or Curly
must have filed it; there is no one else around to do the
misfiling. Further, these possibilities are mutually exclusive. We may
use the Theorem of Total Probability
\eqref{eq:theorem-total-probability} to write \[
\mathbb{P}(A)=\mathbb{P}(A\cap M)+\mathbb{P}(A\cap L)+\mathbb{P}(A\cap
C).  \] Luckily, we have computed these above. Thus \[
\mathbb{P}(A)=0.0018+0.0021+0.0010=0.0049.  \] Therefore, Bayes' Rule
yields \[ \mathbb{P}(M|A)=\frac{0.0018}{0.0049}\approx0.37.  \] This
last quantity is called the posterior probability that Moe misfiled
the document, since it incorporates the observed data that a randomly
selected file was misplaced (which is governed by the misfile
rate). We can use the same argument to calculate

\begin{table}[H]
\begin{tabular}{c|ccc}
 & Moe & Larry & Curly\tabularnewline
\hline
Posterior Probability & $\mathbb{P}(M|A)\approx0.37$ & $\mathbb{P}(L|A)\approx0.43$ & $\mathbb{P}(C|A)\approx0.20$\tabularnewline
\end{tabular}
\caption{Misfiling assistants: posterior.}
\end{table}

The conclusion: \textbf{Larry} gets the axe. What is happening is an
intricate interplay between the time on the job and the misfile
rate. It is not obvious who the winner (or in this case, loser) will
be, and the statistician needs to consult Bayes' Rule to determine the
best course of action.



\begin{example}[]
  Suppose the boss gets a change of heart and does not fire
  anybody. But the next day (s)he randomly selects another file and
  again finds it to be misplaced. To decide whom to fire now, the boss
  would use the same procedure, with one small change. (S)he would not
  use the prior probabilities 60\%, 30\%, and 10\%; those are old
  news. Instead, she would replace the prior probabilities with the
  posterior probabilities just calculated. After the math she will
  have new posterior probabilities, updated even more from the day
  before.
\label{exm:misfiling-assistants-multiple}
\end{example}

In this way, probabilities found by Bayes' rule are always on the
cutting edge, always updated with respect to the best information
available at the time.


\subsection{How to do it with \textsf{R}}

There are not any special functions for Bayes' Rule in the
\texttt{prob} package \cite{prob}, but problems like the ones above are
easy enough to do by hand.




\begin{example}[Misfiling Assistants, continued]
Continued from Example~\ref{exm:misfiling-assistants}. We store the
prior probabilities and the likelihoods in vectors and go to town.
\end{example}

<<echo=TRUE>>=
prior <- c(0.6, 0.3, 0.1)
like <- c(0.003, 0.007, 0.010)
post <- prior # like
post / sum(post)
@

Compare these answers with what we got in
Example~\ref{exm:misfiling-assistants}. We would replace
\texttt{prior} with \texttt{post} in a future calculation. We could
raise \texttt{like} to a power to see how the posterior is affected by
future document mistakes. (Do you see why? Think back to
Section~\ref{sec:independent-events}.)



\begin{example}[]
  Let us incorporate the posterior probability (\texttt{post})
  information from the last example and suppose that the assistants
  misfile seven more documents. Using Bayes' Rule, what would the new
  posterior probabilities be?
\end{example}

<<echo=TRUE>>=
newprior <- post
post <- newprior * like^7
post / sum(post)
@


We see that the individual with the highest probability of having
misfiled all eight documents given the observed data is no longer
Larry, but Curly.


There are two important points. First, we did not divide \texttt{post} by the
sum of its entries until the very last step; we do not need to
calculate it, and it will save us computing time to postpone
normalization until absolutely necessary, namely, until we finally
want to interpret them as probabilities.

Second, the reader might be wondering what the boss would get if (s)he
skipped the intermediate step of calculating the posterior after only
one misfiled document. What if she started from the \textit{original} prior,
then observed eight misfiled documents, and calculated the posterior?
What would she get? It must be the same answer, of course.

<<echo=TRUE>>=
fastpost <- prior * like^8
fastpost / sum(fastpost)
@


Compare this to what we got in Example~\ref{exm:misfiling-assistants-multiple}.

\section{Random Variables} \label{sec:random-variables}

We already know about experiments, sample spaces, and events. In this
section, we are interested in a \textit{number} that is associated with the
experiment. We conduct a random experiment \(E\) and after learning
the outcome \(\omega\) in \(S\) we calculate a number \(X\). That is,
to each outcome \(\omega\) in the sample space we associate a number
\(X(\omega)=x\).



\begin{defn}[Random variables.]
A \textit{random variable} \(X\) is a function \(X:S\to\mathbb{R}\) that
associates to each outcome \(\omega\in S\) exactly one number
\(X(\omega)=x\).
\end{defn}

We usually denote random variables by uppercase letters such as \(X\),
\(Y\), and \(Z\), and we denote their observed values by lowercase
letters \(x\), \(y\), and \(z\). Just as \(S\) is the set of all
possible outcomes of \(E\), we call the set of all possible values of
\(X\) the \textit{support} of \(X\) and denote it by \(S_{X}\).



\begin{example}[]
Let \(E\) be the experiment of flipping a coin twice. We have seen
that the sample space is \(S = \{ HH,\ HT,\ TH,\ TT \}\). Now define
the random variable \(X = \mbox{the number of heads}\). That is, for
example, \(X(HH)=2\), while \(X(HT)=1\). We may make a table of the
possibilities:
\end{example}

\begin{table}[H]
  \begin{center}
\begin{tabular}{c|cccc}
$\omega\in S$ & $HH$ & $HT$ & $TH$ & $TT$\tabularnewline
\hline
$X(\omega)=x$ & 2 & 1 & 1 & 0\tabularnewline
\end{tabular}
\caption{Flipping a coin twice.}
\label{tab:flip-coin-twice}
\end{center}
\end{table}

Taking a look at the second row of the table, we see that the support
of \(X\) -- the set of all numbers that \(X\) assumes -- would be
\(S_{X}= \{ 0,1,2 \}\).



\begin{example}[]
Let \(E\) be the experiment of flipping a coin repeatedly until
observing a Head. The sample space would be \(S= \{ H,\ TH,\ TTH,\
TTTH,\ \ldots \}\). Now define the random variable \(Y=\mbox{the
number of Tails before the first head}\). Then the support of \(Y\)
would be \(S_{Y}= \{ 0,1,2,\ldots \}\).
\end{example}



\begin{example}[]
  Let \(E\) be the experiment of tossing a coin in the air, and define
  the random variable
  \(Z = \mbox{the time (in seconds) until the coin hits the
    ground}\). In this case, the sample space is inconvenient to
  describe. Yet the support of \(Z\) would be \((0,\infty)\). Of
  course, it is reasonable to suppose that the coin will return to
  Earth in a short amount of time; in practice, the set \((0,\infty)\)
  is admittedly too large. However, we will find that in many
  circumstances it is mathematically convenient to study the extended
  set rather than a restricted one.
\end{example}

There are important differences between the supports of \(X\), \(Y\),
and \(Z\). The support of \(X\) is a finite collection of elements
that can be inspected all at once. And while the support of \(Y\)
cannot be exhaustively written down, its elements can nevertheless be
listed in a naturally ordered sequence. Random variables with supports
similar to those of \(X\) and \(Y\) are called \textit{discrete random
  variables}.  We study these in Chapter~\ref{cha:discrete-distributions}.

In contrast, the support of \(Z\) is a continuous interval, containing
all rational and irrational positive real numbers. For this
reason\footnote{This isn't really the reason, but it serves as an
  effective litmus test at the introductory level. See Billingsley
  \cite{Billingsley1995} or Resnick \cite{Resnick1999}.}, random variables
with supports like \(Z\) are called \textit{continuous random
  variables}, to be studied in Chapter~\ref{cha:continuous-distributions}.


\subsection{How to do it with \textsf{R}}

The primary vessel for this task is the \texttt{addrv} function. There are
two ways to use it, and we will describe both.

\subsubsection{Supply a Defining Formula}

The first method is based on the \texttt{transform} function. See
\texttt{?transform}. The idea is to write a formula defining the random
variable inside the function, and it will be added as a column to the
data frame. As an example, let us roll a 4-sided die three times, and
let us define the random variable \(U=X1-X2+X3\).

<<echo=TRUE>>=
S <- rolldie(3, nsides = 4, makespace = TRUE)
S <- addrv(S, U = X1-X2+X3)
@

Now let's take a look at the values of \(U\). In the interest of
space, we will only reproduce the first few rows of \(S\) (there are
\(4^{3}=64\) rows in total).

<<echo=TRUE>>=
head(S)
@


We see from the \(U\) column it is operating just like it should. We
can now answer questions like

<<echo=TRUE>>=
Prob(S, U > 6)
@

\subsubsection{Supply a Function}

Sometimes we have a function laying around that we would like to apply
to some of the outcome variables, but it is unfortunately tedious to
write out the formula defining what the new variable would be. The
\texttt{addrv} function has an argument \texttt{FUN} specifically for this case. Its
value should be a legitimate function from \textsf{R}, such as
\texttt{sum}, \texttt{mean}, \texttt{median}, and so forth. Or, you can define your own
function. Continuing the previous example, let's define
\(V=\max(X1,X2,X3)\) and \(W=X1+X2+X3\).

<<echo=TRUE>>=
S <- addrv(S, FUN = max, invars = c("X1","X2","X3"), name = "V")
S <- addrv(S, FUN = sum, invars = c("X1","X2","X3"), name = "W")
head(S)
@


Notice that \texttt{addrv} has an \texttt{invars} argument to specify exactly to
which columns one would like to apply the function \texttt{FUN}. If no input
variables are specified, then \texttt{addrv} will apply \texttt{FUN} to all
non-\texttt{probs} columns. Further, \texttt{addrv} has an optional argument \texttt{name}
to give the new variable; this can be useful when adding several
random variables to a probability space (as above). If not specified,
the default name is \texttt{X}.

\subsection{Marginal Distributions}

As we can see above, often after adding a random variable \(V\) to a
probability space one will find that \(V\) has values that are
repeated, so that it becomes difficult to understand what the ultimate
behavior of \(V\) actually is. We can use the \texttt{marginal} function to
aggregate the rows of the sample space by values of \(V\), all the
while accumulating the probability associated with \(V\)'s distinct
values. Continuing our example from above, suppose we would like to
focus entirely on the values and probabilities of
\(V=\max(X1,X2,X3)\).

<<echo=TRUE>>=
marginal(S, vars = "V")
@


We could save the probability space of \(V\) in a data frame and study
it further, if we wish. As a final remark, we can calculate the
marginal distributions of multiple variables desired using the \texttt{vars}
argument. For example, suppose we would like to examine the joint
distribution of \(V\) and \(W\).

<<echo=TRUE>>=
marginal(S, vars = c("V", "W"))
@


Note that the default value of \texttt{vars} is the names of all columns
except \texttt{probs}. This can be useful if there are duplicated rows in the
probability space.


\section{Chapter Exercises}



\begin{Exercise}[label=exr:numb-cond-indep]
Prove the assertion given in the text: the
number of conditions that the events \(A_{1}\), \(A_{2}\), \ldots,
\(A_{n}\) must satisfy in order to be mutually independent is
\(2^{n} - n - 1\). (\textit{Hint}: think about Pascal's triangle.)
\end{Exercise}




\begin{Exercise}[]
  The following table categorizes a group of people based on the
  flavor of Kool-Aid they drink and whether or not they like President
  Donald Trump. (No political statements here, it's just an absurd
  probability exercise.)

\begin{table}[H]
  \begin{center}
\begin{tabular}{l|cccc}
 & grape & cherry & orange & Total \tabularnewline
\hline
does like Trump & 92 &  & 29 & 214 \tabularnewline
doesn't like Trump &  & 64 &  &  \tabularnewline
Total & 176 &  &  &  413 \tabularnewline
\end{tabular}
\end{center}
\end{table}

The random experiment is to select one (1) person from the table out
of the 413 people, at random.

\begin{enumerate}
\item Fill in the table.
\item What is the probability that the person doesn't like Trump?
\item What is the probability that the person likes orange Kool-Aid?
\item What is the conditional probability that the person doesn't like Trump, given that the person likes cherry Kool-Aid?
\item What is the conditional probability that the person likes grape Kool-Aid, given that the person does like Trump?
\end{enumerate}
\end{Exercise}




\begin{Exercise}[]
  A bazooka for blowing up zombies has 7 identical gas vents. The
  probability that any particular vent opens during a firing event is
  0.91. Assume independent operation of the vents. Calculate the
  probability

\begin{enumerate}
\item All vents open.
\item At least one vent fails to open.
\item All vents open except one.
\end{enumerate}

\end{Exercise}



\chapter{Discrete Distributions} \label{cha:discrete-distributions}


<<echo=FALSE, include=FALSE>>=
# This chapter's package dependencies
library(distrEx)
distroptions("WarningSim" = FALSE)
         # switches off warnings as to (In)accuracy due to simulations
distroptions("WarningArith" = FALSE)
         # switches off warnings as to arithmetics
@

In this chapter we introduce discrete random variables, those who take
values in a finite or countably infinite support set. We discuss
probability mass functions and some special expectations, namely, the
mean, variance and standard deviation. Some of the more important
discrete distributions are explored in detail, and the more general
concept of expectation is defined, which paves the way for moment
generating functions.

We give special attention to the empirical distribution since it plays
such a fundamental role with respect to resampling and Chapter
\ref{cha:resampling-methods}; it will also be needed in Section~\ref{sub:kolmogorov-smirnov-goodness-of-fit-test} where we discuss
the Kolmogorov-Smirnov test. Following this is a section in which we
introduce a catalogue of discrete random variables that can be used to
model experiments.

There are some comments on simulation, and we mention transformations
of random variables in the discrete case. The interested reader who
would like to learn more about any of the assorted discrete
distributions mentioned here should take a look at \textit{Univariate
Discrete Distributions} by Johnson \textit{et al} \cite{Johnson1993}.

\paragraph{What do I want them to know?}

\begin{itemize}
\item how to choose a reasonable discrete model under a variety of
  physical circumstances
\item item the notion of mathematical expectation, how to calculate it,
  and basic properties, moment generating functions (yes, I want them
  to hear about those)
\item the general tools of the trade for manipulation of continuous random
  variables, integration, \textit{etc}.
\item some details on a couple of discrete models, and exposure to a bunch
  of other ones
\item how to make new discrete random variables from old ones
\end{itemize}

\section{Discrete Random Variables} \label{sec:discrete-random-variables}


\subsection{Probability Mass Functions} \label{sub:probability-mass-functions}


Discrete random variables are characterized by their supports which
take the form
\begin{equation}
S_{X}=\{u_{1},u_{2},\ldots,u_{k}\}\mbox{ or }S_{X}=\{u_{1},u_{2},u_{3}\ldots\}.
\end{equation}

Every discrete random variable \(X\) has associated with it a
probability mass function (PMF) \(f_{X}:S_{X}\to[0,1]\) defined by
\begin{equation}
f_{X}(x)=\mathbb{P}(X=x),\quad x\in S_{X}.
\end{equation}

Since values of the PMF represent probabilities, we know from Chapter~\ref{cha:probability} that PMFs enjoy certain properties. In particular, all
PMFs satisfy

\begin{enumerate}
\item \(f_{X}(x)>0\) for \(x\in S\),
\item \(\sum_{x\in S}f_{X}(x)=1\), and
\item \(\mathbb{P}(X\in A)=\sum_{x\in A}f_{X}(x)\), for any event
   \(A\subset S\).
\end{enumerate}



\begin{example}[]
Toss a coin 3 times. The sample space would be \[
S=\{ HHH,\ HTH,\ THH,\ TTH,\ HHT,\ HTT,\ THT,\ TTT\}.  \]
\label{exm:toss-a-coin}
\end{example}

Now let \(X\) be the number of Heads observed. Then \(X\) has support
\(S_{X}=\{ 0,1,2,3\}\). Assuming that the coin is fair and was tossed
in exactly the same way each time, it is not unreasonable to suppose
that the outcomes in the sample space are all equally likely.

What is the PMF of \(X\)? Notice that \(X\) is zero exactly when the
outcome \(TTT\) occurs, and this event has probability
\(1/8\). Therefore, \(f_{X}(0)=1/8\), and the same reasoning shows
that \(f_{X}(3)=1/8\). Exactly three outcomes result in \(X=1\), thus,
\(f_{X}(1)=3/8\) and \(f_{X}(3)\) holds the remaining \(3/8\)
probability (the total is 1). We can represent the PMF with a table:

\begin{table}[H]
  \begin{center}
\begin{tabular}{c|cccc|c}
$x\in S_{X}$ & 0 & 1 & 2 & 3 & Total\tabularnewline
\hline
$f_{X}(x)=\mathbb{P}(X=x)$ & 1/8 & 3/8 & 3/8 & 1/8 & 1\tabularnewline
\end{tabular}
\caption{Flipping a coin three times: the PMF.}
\label{tab:pmf-flip-coin-three}
\end{center}
\end{table}

\subsection{Mean, Variance, and Standard Deviation} \label{sub:mean-variance-sd}

There are numbers associated with PMFs. One important example is the
mean \(\mu\), also known as \(\mathbb{E} X\) (which we will discuss
later):
\begin{equation}
\mu=\mathbb{E} X=\sum_{x\in S}xf_{X}(x),
\end{equation}
provided the (potentially infinite) series \(\sum|x|f_{X}(x)\) is convergent. Another important number is the variance:
\begin{equation}
\sigma^{2}=\sum_{x\in S}(x-\mu)^{2}f_{X}(x),
\end{equation}
which can be computed (see Exercise~\ref{exr:variance-shortcut}) with the
alternate formula \(\sigma^{2}=\sum
x{}^{2}f_{X}(x)-\mu^{2}\). Directly defined from the variance is the
standard deviation \(\sigma=\sqrt{\sigma^{2}}\).



\begin{example}[]
  We will calculate the mean of \(X\) in
  Example~\ref{exm:toss-a-coin}.
  \[ \mu = \sum_{x = 0}^{3}xf_{X}(x) = 0 \cdot \frac{1}{8} + 1 \cdot
    \frac{3}{8} + 2 \cdot \frac{3}{8}+3\cdot\frac{1}{8} = 1.5. \] We
  interpret \(\mu = 1.5\) by reasoning that if we were to repeat the
  random experiment many times, independently each time, observe many
  corresponding outcomes of the random variable \(X\), and take the
  sample mean of the observations, then the calculated value would
  fall close to 1.5. The approximation would get better as we observe
  more and more values of \(X\) (another form of the Law of Large
  Numbers; see Section~\ref{sec:interpreting-probabilities}). Another
  way it is commonly stated is that \(X\) is 1.5 ``on the average'' or
  ``in the long run''.
\label{exm:disc-pmf-mean}
\end{example}



\begin{rem}[]
Note that although we say \(X\) is 3.5 on the average, we must keep in
mind that our \(X\) never actually equals 3.5 (in fact, it is
impossible for \(X\) to equal 3.5).
\end{rem}

Related to the probability mass function \(f_{X}(x)=\mathbb{P}(X=x)\)
is another important function called the \textit{cumulative distribution
function} (CDF), \(F_{X}\). It is defined by the formula
\begin{equation}
F_{X}(t)=\mathbb{P}(X\leq t),\quad -\infty < t < \infty.
\end{equation}

We know that all PMFs satisfy certain properties, and a similar
statement may be made for CDFs. In particular, any CDF \(F_{X}\)
satisfies

\begin{itemize}
\item \(F_{X}\) is nondecreasing (\(t_{1}\leq t_{2}\) implies
  \(F_{X}(t_{1})\leq F_{X}(t_{2})\)).
\item \(F_{X}\) is right-continuous (\(\lim_{t\to
  a^{+}}F_{X}(t)=F_{X}(a)\) for all \(a\in\mathbb{R}\)).
\item \(\lim_{t\to-\infty}F_{X}(t)=0\) and
  \(\lim_{t\to\infty}F_{X}(t)=1\).
\end{itemize}

We say that \(X\) has the distribution \(F_{X}\) and we write \(X\sim
F_{X}\). In an abuse of notation we will also write \(X\sim f_{X}\)
and for the named distributions the PMF or CDF will be identified by
the family name instead of the defining formula.

\subsubsection{How to do it with \textsf{R}} \label{sub:disc-rv-how-r}


The mean and variance of a discrete random variable is easy to compute
at the console. Let's return to Example~\ref{exm:disc-pmf-mean}. We will start
by defining a vector \texttt{x} containing the support of \(X\), and a vector
\texttt{f} to contain the values of \(f_{X}\) at the respective outcomes in
\texttt{x}:

<<echo=TRUE>>=
x <- c(0,1,2,3)
f <- c(1/8, 3/8, 3/8, 1/8)
@

To calculate the mean \(\mu\), we need to multiply the corresponding
values of \texttt{x} and \texttt{f} and add them. This is easily
accomplished in \textsf{R} since operations on vectors are performed
\textit{element-wise} (see Section~\ref{sub:functions-and-expressions}):

<<echo=TRUE>>=
mu <- sum(x * f)
mu
@


To compute the variance \(\sigma^{2}\), we subtract the value of \texttt{mu}
from each entry in \texttt{x}, square the answers, multiply by \texttt{f},and
\texttt{sum}. The standard deviation \(\sigma\) is simply the square root of
\(\sigma^{2}\).

<<echo=TRUE>>=
sigma2 <- sum((x-mu)^2 * f)
sigma2
@


<<echo=TRUE>>=
sigma <- sqrt(sigma2)
sigma
@

Finally, we may find the values of the CDF \(F_{X}\) on the support by
accumulating the probabilities in \(f_{X}\) with the \texttt{cumsum}
function.

<<echo=TRUE>>=
F <- cumsum(f)
F
@


As easy as this is, it is even easier to do with the \texttt{distrEx} package
\cite{distrEx}. We define a random variable \texttt{X} as an object, then
compute things from the object such as mean, variance, and standard
deviation with the functions \texttt{E}, \texttt{var}, and \texttt{sd}:

<<echo=TRUE>>=
library(distrEx)
X <- DiscreteDistribution(supp = 0:3, prob = c(1,3,3,1)/8)
E(X); var(X); sd(X)
@

\section{The Discrete Uniform Distribution} \label{sec:disc-uniform-dist}


We have seen the basic building blocks of discrete distributions and
we now study particular models that statisticians often encounter in
the field. Perhaps the most fundamental of all is the \textit{discrete
uniform} distribution.

A random variable \(X\) with the discrete uniform distribution on the
integers \(1,2,\ldots,m\) has PMF
\begin{equation}
f_{X}(x)=\frac{1}{m},\quad x=1,2,\ldots,m.
\end{equation}

We write \(X\sim\mathsf{disunif}(m)\). A random experiment where this
distribution occurs is the choice of an integer at random between 1
and 100, inclusive. Let \(X\) be the number chosen. Then
\(X\sim\mathsf{disunif}(m=100)\) and
\[
\mathbb{P}(X=x)=\frac{1}{100},\quad x=1,\ldots,100.
\]
We find a direct formula for the mean of \(X\sim\mathsf{disunif}(m)\):
\begin{equation}
\mu = \sum_{x = 1}^{m}xf_{X}(x) = \sum_{x = 1}^{m}x \cdot \frac{1}{m} = \frac{1}{m}(1 + 2 + \cdots + m) = \frac{m + 1}{2},
\end{equation}
where we have used the famous identity \(1 + 2 + \cdots + m = m(m +
1)/2\). That is, if we repeatedly choose integers at random from 1 to
\(m\) then, on the average, we expect to get \((m+1)/2\). To get the
variance we first calculate \[ \sum_{x = 1}^{m} x^{2} f_{X}(x) =
\frac{1}{m} \sum_{x = 1}^{m} x^{2} = \frac{1}{m}\frac{m(m + 1)(2m +
1)}{6} = \frac{(m + 1)(2m + 1)}{6}, \] and finally,
\begin{equation}
\sigma^{2} = \sum_{x = 1}^{m} x^{2} f_{X}(x) - \mu^{2} = \frac{(m + 1)(2m + 1)}{6} - \left(\frac{m + 1}{2}\right)^{2} = \cdots = \frac{m^{2} - 1}{12}.
\end{equation}



\begin{example}[]
Roll a die and  let \(X\) be the upward face showing.  Then \(m = 6\),
\(\mu = 7/2 = 3.5\), and \(\sigma^{2} = (6^{2} - 1)/12 = 35/12\).
\end{example}

\subsubsection{How to do it with \textsf{R}}

One can choose an integer at random with the \texttt{sample} function. The
general syntax to simulate a discrete uniform random variable is
\texttt{sample(x, size, replace = TRUE)}.

The argument \texttt{x} identifies the numbers from which to randomly
sample. If \texttt{x} is a number, then sampling is done from 1 to \texttt{x}. The
argument \texttt{size} tells how big the sample size should be, and \texttt{replace}
tells whether or not numbers should be replaced in the urn after
having been sampled. The default option is \texttt{replace = FALSE} but for
discrete uniforms the sampled values should be replaced. Some examples
follow.

\textbf{Examples.}
To roll a fair die 3000 times, do \texttt{sample(6, size = 3000, replace = TRUE)}. To choose 27 random numbers from 30 to 70, do \texttt{sample(30:70, size = 27, replace = TRUE)}. To flip a fair coin 1000 times, do \texttt{sample(c("H","T"), size = 1000, replace = TRUE)}.

\section{The Binomial Distribution} \label{sec:binom-dist}


The binomial distribution is based on a \textit{Bernoulli trial}, which is a
random experiment in which there are only two possible outcomes:
success (\(S\)) and failure (\(F\)). We conduct the Bernoulli trial
and let \begin{equation} X = \begin{cases} 1 & \mbox{if the outcome is $S$},\\ 0 & \mbox{if the outcome is $F$}. \end{cases} \end{equation}
If the probability of success is \(p\) then the probability of failure
must be \(1-p=q\) and the PMF of \(X\) is
\begin{equation}
f_{X}(x)=p^{x}(1-p)^{1-x},\quad x=0,1.
\end{equation}
It is easy to calculate \(\mu=\mathbb{E} X=p\) and \(\mathbb{E}
X^{2}=p\) so that \(\sigma^{2}=p-p^{2}=p(1-p)\).

\subsection{The Binomial Model} \label{sub:the-binomial-model}


The Binomial model has three defining properties:

\begin{itemize}
\item Bernoulli trials are conducted \(n\) times,
\item the trials are independent,
\item the probability of success \(p\) does not change between trials.
\end{itemize}

If \(X\) counts the number of successes in the \(n\) independent
trials, then the PMF of \(X\) is
\begin{equation}
f_{X}(x)={n \choose x}p^{x}(1-p)^{n-x},\quad x=0,1,2,\ldots,n.
\end{equation}

We say that \(X\) has a \textit{binomial distribution} and we write
\(X\sim\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p)\). It is
clear that \(f_{X}(x)\geq0\) for all \(x\) in the support because the
value is the product of nonnegative numbers. We next check that \(\sum
f(x)=1\): \[ \sum_{x = 0}^{n}{n \choose x} p^{x} (1 - p)^{n - x} =
[p + (1 - p)]^{n} = 1^{n} = 1.  \] We next find the mean:
\begin{alignat*}{1}
\mu= & \sum_{x=0}^{n}x\,{n \choose x}p^{x}(1-p)^{n-x},\\
= & \sum_{x=1}^{n}x\,\frac{n!}{x!(n-x)!}p^{x}q^{n-x},\\
= & n\cdot p\sum_{x=1}^{n}\frac{(n-1)!}{(x-1)!(n-x)!}p^{x-1}q^{n-x},\\
= & np\,\sum_{x-1=0}^{n-1}{n-1 \choose x-1}p^{(x-1)}(1-p)^{(n-1)-(x-1)},\\
= & np.
\end{alignat*}
A similar argument shows that \(\mathbb{E} X(X - 1) = n(n - 1)p^{2}\) (see
Exercise~\ref{exr:binom-factorial-expectation}). Therefore
\begin{alignat*}{1}
\sigma^{2}= & \mathbb{E} X(X-1)+\mathbb{E} X-[\mathbb{E} X]^{2},\\
= & n(n-1)p^{2}+np-(np)^{2},\\
= & n^{2}p^{2}-np^{2}+np-n^{2}p^{2},\\
= & np-np^{2}=np(1-p).
\end{alignat*}


\begin{example}[]
A four-child family. Each child may be either a boy (\(B\)) or a girl
(\(G\)). For simplicity we suppose that
\(\mathbb{P}(B)=\mathbb{P}(G)=1/2\) and that the genders of the
children are determined independently. If we let \(X\) count the
number of \(B\)'s, then
\(X\sim\mathsf{binom}(\mathtt{size}=4,\,\mathtt{prob}=1/2)\). Further,
\(\mathbb{P}(X=2)\) is
\[
f_{X}(2)={4 \choose 2}(1/2)^{2}(1/2)^{2}=\frac{6}{2^{4}}.
\]
The mean number of boys is \(4(1/2)=2\) and the variance of \(X\) is
\(4(1/2)(1/2)=1\).
\end{example}

\subsubsection{How to do it with \textsf{R}}

The corresponding \textsf{R} function for the PMF and CDF are
\texttt{dbinom} and \texttt{pbinom}, respectively. We demonstrate their use in the
following examples.



\begin{example}[]
We can calculate it in \textsf{R} Commander under the \texttt{Binomial
Distribution} menu with the \texttt{Binomial probabilities} menu item.
\end{example}

<<>>=
A <- data.frame(Pr=dbinom(0:4, size = 4, prob = 0.5))
rownames(A) <- 0:4
A
@


We know that the
\(\mathsf{binom}(\mathtt{size}=4,\,\mathtt{prob}=1/2)\) distribution
is supported on the integers 0, 1, 2, 3, and 4; thus the table is
complete. We can read off the answer to be \(\mathbb{P}(X=2)=0.3750\).




\begin{example}[]
Roll 12 dice simultaneously, and let \(X\) denote the number of 6's
that appear. We wish to find the probability of getting seven, eight,
or nine 6's. If we let \(S=\{ \mbox{get a 6 on one roll} \}\), then
\(\mathbb{P}(S)=1/6\) and the rolls constitute Bernoulli trials; thus
\(X\sim\mathsf{binom}(\mathtt{size}=12,\ \mathtt{prob}=1/6)\) and our
task is to find \(\mathbb{P}(7\leq X\leq9)\). This is just
\[
\mathbb{P}(7\leq X\leq9)=\sum_{x=7}^{9}{12 \choose x}(1/6)^{x}(5/6)^{12-x}.
\]
Again, one method to solve this problem would be to generate a
probability mass table and add up the relevant rows. However, an
alternative method is to notice that \(\mathbb{P}(7\leq
X\leq9)=\mathbb{P}(X\leq9)-\mathbb{P}(X\leq6)=F_{X}(9)-F_{X}(6)\), so
we could get the same answer by using the \texttt{Binomial tail
probabilities\ldots} menu in the \textsf{R} Commander or the
following from the command line:
\end{example}

<<echo=TRUE>>=
pbinom(9, size=12, prob=1/6) - pbinom(6, size=12, prob=1/6)
diff(pbinom(c(6,9), size = 12, prob = 1/6))  # same thing
@



\begin{example}[]
\label{exm:toss-coin-3-withr}
Toss a coin three times and let \(X\) be the
number of Heads observed. We know from before that
\(X\sim\mathsf{binom}(\mathtt{size}=3,\,\mathtt{prob}=1/2)\) which
implies the following PMF:
\end{example}

\begin{table}[H]
  \begin{center}
\begin{tabular}{c|cccc}
$x=\mbox{\#of Heads}$ & 0 & 1 & 2 & 3\tabularnewline
\hline
$f(x)=\mathbb{P}(X=x)$ & 1/8 & 3/8 & 3/8 & 1/8\tabularnewline
\end{tabular}
\caption{Flipping a coin three times: the PMF (again).}
\label{tab:flip-coin-thrice}
\end{center}
\end{table}

Our next goal is to write down the CDF of \(X\) explicitly. The first
case is easy: it is impossible for \(X\) to be negative, so if \(x<0\)
then we should have \(\mathbb{P}(X\leq x)=0\). Now choose a value
\(x\) satisfying \(0\leq x<1\), say, \(x=0.3\). The only way that
\(X\leq x\) could happen would be if \(X=0\), therefore,
\(\mathbb{P}(X\leq x)\) should equal \(\mathbb{P}(X=0)\), and the same
is true for any \(0\leq x<1\). Similarly, for any \(1\leq x<2\), say,
\(x=1.73\), the event \(\{ X\leq x \}\) is exactly the event \(\{
X=0\mbox{ or }X=1 \}\). Consequently, \(\mathbb{P}(X\leq x)\) should
equal \(\mathbb{P}(X=0\mbox{ or
}X=1)=\mathbb{P}(X=0)+\mathbb{P}(X=1)\). Continuing in this fashion,
we may figure out the values of \(F_{X}(x)\) for all possible inputs
\(-\infty<x<\infty\), and we may summarize our observations with the
following piecewise defined function: \[ F_{X}(x)=\mathbb{P}(X\leq x) = \begin{cases} 0, & x < 0,\\ \frac{1}{8}, & 0\leq x < 1,\\ \frac{1}{8} + \frac{3}{8} = \frac{4}{8}, & 1\leq x < 2,\\ \frac{4}{8} + \frac{3}{8} = \frac{7}{8}, & 2\leq x < 3,\\ 1, & x \geq 3. \end{cases} \]
In particular, the CDF of \(X\) is defined for the entire real line,
\(\mathbb{R}\). The CDF is right continuous and nondecreasing. A graph
of the \(\mathsf{binom}(\mathtt{size}=3,\,\mathtt{prob}=1/2)\) CDF is
shown in Figure~\ref{fig:binom-cdf-base}.

<<binom-cdf-base, echo=FALSE, fig=TRUE, include=FALSE, height=3.25,width=5>>=
plot(0, xlim = c(-1.2, 4.2), ylim = c(-0.04, 1.04), type = "n", xlab = "number of successes", ylab = "cumulative probability")
abline(h = c(0,1), lty = 2, col = "grey")
lines(stepfun(0:3, pbinom(-1:3, size = 3, prob = 0.5)), verticals = FALSE, do.p = FALSE)
points(0:3, pbinom(0:3, size = 3, prob = 0.5), pch = 16, cex = 1.2)
points(0:3, pbinom(-1:2, size = 3, prob = 0.5), pch = 1, cex = 1.2)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-binom-cdf-base}
\end{center}
\caption{{\small A graph of the \(\mathsf{binom}(\mathtt{size}=3,\,\mathtt{prob}=1/2)\) CDF.}}
\label{fig:binom-cdf-base}
\end{figure}




\begin{example}[]
Another way to do Example~\ref{exm:toss-coin-3-withr} is with the \texttt{distr}
family of packages. They use an object oriented approach
to random variables, that is, a random variable is stored in an object
\texttt{X}, and then questions about the random variable translate to
functions on and involving \texttt{X}. Random variables with distributions
from the \texttt{base} package are specified by capitalizing the
name of the distribution.
\end{example}

<<echo=TRUE>>=
X <- Binom(size = 3, prob = 1/2)
X
@


The analogue of the \texttt{dbinom} function for \texttt{X} is the \texttt{d(X)} function,
and the analogue of the \texttt{pbinom} function is the \texttt{p(X)}
function. Compare the following:

<<echo=TRUE>>=
d(X)(1)   # pmf of X evaluated at x = 1
p(X)(2)   # cdf of X evaluated at x = 2
@



Random variables defined via the \texttt{distr} package \cite{distr} may be
\textit{plotted}, which will return graphs of the PMF, CDF, and quantile
function (introduced in Section~\ref{sub:normal-quantiles-qf}). See
Figure~\ref{fig:binom-plot-distr} for an example.

<<binom-plot-distr, echo=FALSE, fig=TRUE, include=FALSE, width=5, height=3>>=
X <- Binom(size = 3, prob = 1/2)
plot(X, cex = 0.2, cex.main=0.7)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-binom-plot-distr}
\end{center}
\caption{{\small The \textsf{binom}(\texttt{size} = 3, \texttt{prob} = 0.5) distribution from \texttt{distr}.}}
\label{fig:binom-plot-distr}
\end{figure}


\begin{table}
\begin{centering}
\begin{tabular}{lllll}
\multicolumn{5}{l}{Given $X\sim\mathsf{dbinom}(\mathtt{size}=n,\,\mathtt{prob}=p)$.}\tabularnewline
 &  &  &  & \tabularnewline
How to do: &  & with $\mathtt{stats}$ (default)  &  & with $\mathtt{distr}$\tabularnewline
\hline
PMF: $\mathbb{P}(X=x)$ &  & $\mathtt{dbinom(x,size=n,prob=p)}$ &  & $\mathtt{d(X)(x)}$\tabularnewline
CDF: $\mathbb{P}(X\leq x)$ &  & $\mathtt{pbinom(x,size=n,prob=p)}$ &  & $\mathtt{p(X)(x)}$\tabularnewline
Simulate $k$ variates &  & $\mathtt{rbinom(k,size=n,prob=p)}$ &  & $\mathtt{r(X)(k)}$\tabularnewline
\hline
 &  &  &  & \tabularnewline
\multicolumn{5}{r}{For $\mathtt{distr}$ need \texttt{X <-} $\mathtt{Binom(size=}n\mathtt{,\ prob=}p\mathtt{)}$}\tabularnewline
\end{tabular}
\par\end{centering}
\caption[Correspondence between \texttt{stats} and
\texttt{distr}]{Correspondence between \texttt{stats} and
  \texttt{distr}. We are given
  \(X\sim\mathsf{dbinom}(\mathtt{size}=n,\,\mathtt{prob}=p)\).  For
  the \texttt{distr} package we must first set
  \(\mathtt{X\ <-\ Binom(size=}n\mathtt{,\ prob=}p\mathtt{)}\)}
\end{table}


\section{Expectation and Moment Generating Functions} 
\label{sec:expectation-and-mgfs}

\subsection{The Expectation Operator}
We next generalize some of the concepts from Section~\ref{sub:mean-variance-sd}. There we saw that every\footnote{Not every, only those PMFs for which the (potentially infinite) series converges.} PMF has two important numbers associated with it:

\begin{equation}
\mu = \sum_{x \in S}x f_{X}(x),\quad \sigma^{2} = \sum_{x \in S}(x - \mu)^{2} f_{X}(x).
\end{equation}

Intuitively, for repeated observations of \(X\) we would expect the sample mean to closely approximate \(\mu\) as the sample size increases without bound. For this reason we call \(\mu\) the \textit{expected value} of \(X\) and we write \(\mu=\mathbb{E} X\), where \(\mathbb{E}\) is an \textit{expectation operator}.



\begin{defn}[Expected value.]
More generally, given a function \(g\) we define the \textit{expected value of} \(g(X)\) by
\begin{equation}
\mathbb{E}\, g(X)=\sum_{x\in S}g(x)f_{X}(x),
\end{equation}
provided the (potentially infinite) series \(\sum_{x} \vert g(x) \vert f(x)\) is convergent. We say that \(\mathbb{E} g(X)\) \textit{exists}.
\end{defn}

In this notation the variance is \(\sigma^{2} = \mathbb{E}(X - \mu)^{2}\) and we prove the identity
\begin{equation}
\mathbb{E}(X - \mu)^{2} = \mathbb{E} X^{2} - (\mathbb{E} X)^{2}
\end{equation}
in Exercise~\ref{exr:variance-shortcut}. Intuitively, for repeated observations of \(X\) we would expect the sample mean of the \(g(X)\) values to closely approximate \(\mathbb{E}\, g(X)\) as the sample size increases without bound.

Let us take the analogy further. If we expect \(g(X)\) to be close to \(\mathbb{E} g(X)\) on the average, where would we expect \(3g(X)\) to be on the average? It could only be \(3\mathbb{E} g(X)\). The following theorem makes this idea precise.



\begin{prop}[]
\label{prp:expectation-properties}
For any functions \(g\) and \(h\), any random variable \(X\), and any constant \(c\):

\begin{enumerate}
\item \(\mathbb{E}\: c=c\),
\item \(\mathbb{E}[c\cdot g(X)]=c\mathbb{E} g(X)\)
\item \(\mathbb{E}[g(X)+h(X)]=\mathbb{E} g(X)+\mathbb{E} h(X)\), provided
   \(\mathbb{E} g(X)\) and \(\mathbb{E} h(X)\) exist.
\end{enumerate}
\end{prop}

\begin{proof}
Go directly from the definition. For example, \[ \mathbb{E}[c \cdot
g(X)] = \sum_{x \in S} c \cdot g(x) f_{X}(x) = c \cdot \sum_{x \in S}
g(x) f_{X}(x) = c \mathbb{E} g(X).  \]
\end{proof}

\subsection{Moment Generating Functions} \label{sub:mgfs}

\begin{defn}[Moment generating function.]
Given a random variable \(X\), its \textit{moment generating function}
(abbreviated MGF) is defined by the formula
\begin{equation}
M_{X}(t)=\mathbb{E}\mathrm{e}^{tX}=\sum_{x\in S}\mathrm{e}^{tx}f_{X}(x),
\end{equation}
provided the (potentially infinite) series is convergent for all \(t\)
in a neighborhood of zero (that is, for all \(-\epsilon < t <
\epsilon\), for some \(\epsilon > 0\)).
\end{defn}

Note that for any MGF \(M_{X}\),
\begin{equation}
M_{X}(0) = \mathbb{E} \mathrm{e}^{0 \cdot X} = \mathbb{E} 1 = 1.
\end{equation}
We will calculate the MGF for the two distributions introduced above.



\begin{example}[]
Find the MGF for \(X\sim\mathsf{disunif}(m)\). Since \(f(x) = 1/m\),
the MGF takes the form \[ M(t) = \sum_{x = 1}^{m} \mathrm{e}^{tx}
\frac{1}{m} = \frac{1}{m}(\mathrm{e}^{t} + \mathrm{e}^{2t} + \cdots +
\mathrm{e}^{mt}),\quad \mbox{for any $t$.}  \]
\end{example}



\begin{example}[]
Find the MGF for
\(X\sim\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p)\).
\end{example}

\begin{alignat*}{1}
M_{X}(t)= & \sum_{x=0}^{n}\mathrm{e}^{tx}\,{n \choose x}\, p^{x}(1-p)^{n-x},\\
= & \sum_{x=0}^{n}{n \choose x}\,(p\mathrm{e}^{t})^{x}q^{n-x},\\
= & (p\mathrm{e}^{t}+q)^{n},\quad \mbox{for any $t$.}
\end{alignat*}

\subsubsection{Applications}

We will discuss three applications of moment generating functions in
this book. The first is the fact that an MGF may be used to accurately
identify the probability distribution that generated it, which rests
on the following:




\begin{thm}[]
\label{thm:mgf-unique}
The moment generating function, if it exists in a
neighborhood of zero, determines a probability distribution
\textit{uniquely}.
\end{thm}

\begin{proof}
Unfortunately, the proof of such a theorem is beyond the scope of a
text like this one. Interested readers could consult Billingsley
\cite{Billingsley1995}.
\end{proof}

We will see an example of Theorem~\ref{thm:mgf-unique} in action.



\begin{example}[]
Suppose we encounter a random variable which has MGF
\[
M_{X}(t)=(0.3+0.7\mathrm{e}^{t})^{13}.
\]
Then \(X\sim\mathsf{binom}(\mathtt{size}=13,\,\mathtt{prob}=0.7)\).
\end{example}

An MGF is also known as a ``Laplace Transform'' and is manipulated in
that context in many branches of science and engineering.

\subsubsection{Why is it called a Moment Generating Function?}

This brings us to the second powerful application of MGFs. Many of the
models we study have a simple MGF, indeed, which permits us to
determine the mean, variance, and even higher moments very
quickly. Let us see why. We already know that
\begin{alignat*}{1}
M(t)= & \sum_{x\in S}\mathrm{e}^{tx}f(x).
\end{alignat*}
Take the derivative with respect to \(t\) to get
\begin{equation}
M'(t)=\frac{\mathrm{d}}{\mathrm{d} t}\left(\sum_{x\in S}\mathrm{e}^{tx}f(x)\right)=\sum_{x\in S}\ \frac{\mathrm{d}}{\mathrm{d} t}\left(\mathrm{e}^{tx}f(x)\right)=\sum_{x\in S}x\mathrm{e}^{tx}f(x),
\end{equation}
and so if we plug in zero for \(t\) we see
\begin{equation}
M'(0)=\sum_{x\in S}x\mathrm{e}^{0}f(x)=\sum_{x\in S}xf(x)=\mu=\mathbb{E} X.
\end{equation}
Similarly, \(M''(t) = \sum x^{2} \mathrm{e}^{tx} f(x)\) so that
\(M''(0) = \mathbb{E} X^{2}\). And in general, we can
see\footnote{We are glossing over some significant mathematical details in our derivation. Suffice it to say that when the MGF exists in a neighborhood of \(t=0\), the exchange of differentiation and summation is valid in that neighborhood, and our remarks hold true.} that
\begin{equation}
M_{X}^{(r)}(0)=\mathbb{E} X^{r}=\mbox{\(r^{\mathrm{th}}\) moment of \(X\) about the origin.}
\end{equation}

These are also known as \textit{raw moments} and are sometimes denoted
\(\mu_{r}'\). In addition to these are the so called \textit{central moments}
\(\mu_{r}\) defined by
\begin{equation}
\mu_{r}=\mathbb{E}(X-\mu)^{r},\quad r=1,2,\ldots
\end{equation}





\begin{example}[]
Let \(X \sim \mathsf{binom}(\mathtt{size} = n,\,\mathtt{prob} = p)\)
with \(M(t) = (q + p \mathrm{e}^{t})^{n}\).
\end{example}

We calculated the mean and variance of a binomial random variable in
Section~\ref{sec:binom-dist} by means of the binomial series. But
look how quickly we find the mean and variance with the moment
generating function.
\begin{alignat*}{1}
M'(t)= & n(q+p\mathrm{e}^{t})^{n-1}p\mathrm{e}^{t}\left|_{t=0}\right.,\\
= & n\cdot1^{n-1}p,\\
= & np.
\end{alignat*}
And
\begin{alignat*}{1}
M''(0)= & n(n-1)[q+p\mathrm{e}^{t}]^{n-2}(p\mathrm{e}^{t})^{2}+n[q+p\mathrm{e}^{t}]^{n-1}p\mathrm{e}^{t}\left|_{t=0}\right.,\\
\mathbb{E} X^{2}= & n(n-1)p^{2}+np.
\end{alignat*}
Therefore
\begin{alignat*}{1}
\sigma^{2}= & \mathbb{E} X^{2}-(\mathbb{E} X)^{2},\\
= & n(n-1)p^{2}+np-n^{2}p^{2},\\
= & np-np^{2}=npq.
\end{alignat*}
See how much easier that was?




\begin{rem}[]
We learned in this section that \(M^{(r)}(0) = \mathbb{E} X^{r}\). We
remember from Calculus II that certain functions \(f\) can be
represented by a Taylor series expansion about a point \(a\), which
takes the form
\begin{equation}
f(x)=\sum_{r=0}^{\infty}\frac{f^{(r)}(a)}{r!}(x-a)^{r},\quad \mbox{for all \(|x-a| < R\),}
\end{equation}
where \(R\) is called the \textit{radius of convergence} of the series (see
Appendix~\ref{sec:sequences-and-series}). We combine the two to say that if an
MGF exists for all \(t\) in the interval \((-\epsilon,\epsilon)\),
then we can write
\begin{equation}
M_{X}(t)=\sum_{r=0}^{\infty}\frac{\mathbb{E} X^{r}}{r!}t^{r},\quad \mbox{for all $|t|<\epsilon$.}
\end{equation}
\end{rem}

\subsubsection{How to do it with R}

The \texttt{distrEx} package \cite{distrEx} provides an expectation operator
\texttt{E} which can be used on random variables that have been defined in
the ordinary \texttt{distr} sense:

<<echo=TRUE>>=
X <- Binom(size = 3, prob = 0.45)
E(X)
E(3*X + 4)
@

For discrete random variables with finite support, the expectation is
simply computed with direct summation. In the case that the random
variable has infinite support and the function is crazy, then the
expectation is not computed directly, rather, it is estimated by first
generating a random sample from the underlying model and next
computing a sample mean of the function of interest.

There are methods for other population parameters:

<<echo=TRUE>>=
var(X)
sd(X)
@


There are even methods for \texttt{IQR}, \texttt{mad}, \texttt{skewness}, and \texttt{kurtosis}.

\section{The Empirical Distribution} \label{sec:empirical-distribution}

Do an experiment \(n\) times and observe \(n\) values \(x_{1}\),
\(x_{2}\), \ldots, \(x_{n}\) of a random variable \(X\). For simplicity
in most of the discussion that follows it will be convenient to
imagine that the observed values are distinct, but the remarks are
valid even when the observed values are repeated.



\begin{defn}[Empirical CDF.]
  The \textit{empirical cumulative distribution function} \(F_{n}\)
  (written ECDF) \index{Empirical distribution} is the probability
  distribution that places probability mass \(1/n\) on each of the
  values \(x_{1}\), \(x_{2}\), \ldots, \(x_{n}\). The empirical PMF takes
  the form
\begin{equation}
f_{X}(x)=\frac{1}{n},\quad x\in \{ x_{1},x_{2},\ldots,x_{n} \}.
\end{equation}
If the value \(x_{i}\) is repeated \(k\) times, the mass at \(x_{i}\)
is accumulated to \(k/n\).
\end{defn}

The mean of the empirical distribution is
\begin{equation}
\mu=\sum_{x\in S}xf_{X}(x)=\sum_{i=1}^{n}x_{i}\cdot\frac{1}{n}
\end{equation}
and we recognize this last quantity to be the sample mean, \(\overline{x}\). The variance of the empirical distribution is
\begin{equation}
\sigma^{2}=\sum_{x\in S}(x-\mu)^{2}f_{X}(x)=\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}\cdot\frac{1}{n}
\end{equation}
and this last quantity looks very close to what we already know to be the sample variance.
\begin{equation}
s^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}.
\end{equation}
The \textit{empirical quantile function} is the inverse of the
ECDF. See Section~\ref{sub:normal-quantiles-qf}.

\subsection{How to do it with \textsf{R}}

The empirical distribution is not directly available as a distribution
in the same way that the other base probability distributions are, but
there are plenty of resources available for the determined
investigator.  Given a data vector of observed values \texttt{x}, we
can see the empirical CDF with the \texttt{ecdf}
\index{ecdf@\texttt{ecdf}} function:

<<echo=TRUE>>=
x <- c(4, 7, 9, 11, 12)
ecdf(x)
@


The above shows that the returned value of \texttt{ecdf(x)} is not a
\textit{number} but rather a \textit{function}. The ECDF is not
usually used by itself in this form. More commonly it is used as an
intermediate step in a more complicated calculation, for instance, in
hypothesis testing (see Chapter~\ref{cha:hypothesis-testing}) or
resampling (see Chapter~\ref{cha:resampling-methods}). It is
nevertheless instructive to see what the \texttt{ecdf} looks like, and
there is a special plot method for \texttt{ecdf} objects.

<<empirical-cdf, echo=FALSE, fig=TRUE, include=FALSE, width=5, height=3.25>>=
plot(ecdf(x))
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-empirical-cdf}
\end{center}
\caption{{\small A graph of an empirical CDF.}}
\label{fig:empirical-cdf}
\end{figure}


See Figure~\ref{fig:empirical-cdf}. The graph is of a
right-continuous function with jumps exactly at the locations stored
in \texttt{x}. There are no repeated values in \texttt{x} so all of the jumps are
equal to \(1/5=0.2\).

The empirical PDF is not usually of particular interest in itself, but
if we really wanted we could define a function to serve as the
empirical PDF:

<<echo=TRUE>>=
epdf <- function(x) function(t){sum(x %in% t)/length(x)}
x <- c(0,0,1)
epdf(x)(0)       # should be 2/3
@

To simulate from the empirical distribution supported on the vector
\texttt{x}, we use the \texttt{sample} \index{sample@\texttt{sample}} function.

<<echo=TRUE>>=
x <- c(0,0,1)
sample(x, size = 7, replace = TRUE)
@


We can get the empirical quantile function in \textsf{R} with
\texttt{quantile(x, probs = p, type = 1)}; see Section~\ref{sub:normal-quantiles-qf}.

As we hinted above, the empirical distribution is significant more
because of how and where it appears in more sophisticated
applications. We will explore some of these in later chapters -- see,
for instance, Chapter~\ref{cha:resampling-methods}.

\section{Other Discrete Distributions} \label{sec:other-discrete-distributions}

The binomial and discrete uniform distributions are popular, and
rightly so; they are simple and form the foundation for many other
more complicated distributions. But the particular uniform and
binomial models only apply to a limited range of problems. In this
section we introduce situations for which we need more than what the
uniform and binomial offer.

\subsection{Dependent Bernoulli Trials} \label{sec:non-bernoulli-trials}

\subsubsection{The Hypergeometric Distribution} \label{sub:hypergeometric-dist}

Consider an urn with 7 white balls and 5 black balls. Let our random
experiment be to randomly select 4 balls, without replacement, from
the urn. Then the probability of observing 3 white balls (and thus 1
black ball) would be
\begin{equation}
\mathbb{P}(3W,1B)=\frac{{7 \choose 3}{5 \choose 1}}{{12 \choose 4}}.
\end{equation}
More generally, we sample without replacement \(K\) times from an urn
with \(M\) white balls and \(N\) black balls. Let \(X\) be the number
of white balls in the sample. The PMF of \(X\) is
\begin{equation}
f_{X}(x)=\frac{{M \choose x}{N \choose K-x}}{{M+N \choose K}}.
\end{equation}
We say that \(X\) has a \textit{hypergeometric distribution} and write
\(X\sim\mathsf{hyper}(\mathtt{m}=M,\,\mathtt{n}=N,\,\mathtt{k}=K)\).

The support set for the hypergeometric distribution is a little bit
tricky. It is tempting to say that \(x\) should go from 0 (no white
balls in the sample) to \(K\) (no black balls in the sample), but that
does not work if \(K>M\), because it is impossible to have more white
balls in the sample than there were white balls originally in the
urn. We have the same trouble if \(K>N\). The good news is that the
majority of examples we study have \(K\leq M\) and \(K\leq N\) and we
will happily take the support to be \(x=0,\ 1,\ \ldots,\ K\).

It is shown in Exercise~\ref{exr:hyper-mean-variance} that
\begin{equation}
\mu=K\frac{M}{M+N},\quad \sigma^{2}=K\frac{MN}{(M+N)^{2}}\frac{M+N-K}{M+N-1}.
\end{equation}

The associated \textsf{R} functions for the PMF and CDF are
\texttt{dhyper(x, m, n, k)} and \texttt{phyper}, respectively. There are two more
functions: \texttt{qhyper}, which we will discuss in Section~\ref{sub:normal-quantiles-qf}, and \texttt{rhyper}, discussed below.



\begin{example}[]
Suppose in a certain shipment of 250 Pentium processors there are 17
defective processors. A quality control consultant randomly collects 5
processors for inspection to determine whether or not they are
defective. Let \(X\) denote the number of defectives in the sample.
\end{example}

\textbf{Find the probability of exactly 3 defectives} in the sample. That is,
find \(\mathbb{P}(X=3)\).  \textbf{Solution:} We know that
\(X\sim\mathsf{hyper}(\mathtt{m}=17,\,\mathtt{n}=233,\,\mathtt{k}=5)\). So
the required probability is just \[ f_{X}(3)=\frac{{17 \choose 3}{233
\choose 2}}{{250 \choose 5}}.  \] To calculate it in \textsf{R} we
just type

<<echo=TRUE>>=
dhyper(3, m = 17, n = 233, k = 5)
@

\textbf{Find the probability that there are at most 2 defectives in the
sample.} That is, compute \(\mathbb{P}(X\leq2)\).  \textbf{Solution:} Since
\(\mathbb{P}(X\leq2)=\mathbb{P}(X=0,1,2)\), one way to do this would
be to add the 0, 1, and 2 values returned from \texttt{dhyper}.

<<echo=TRUE>>=
sum(dhyper(0:2, m = 17, n = 233, k = 5))
@

But another way would be just to use \texttt{phyper} and be done with it.

<<echo=TRUE>>=
phyper(2, m = 17, n = 233, k = 5)
@


\textbf{Find \(\mathbb{P}(X>1)\).}  Since
\(\mathbb{P}(X>1)=1-\mathbb{P}(X\leq1)=1-F_{X}(1)\), we can find the
probability with the \texttt{lower.tail} argument to \texttt{phyper}.

<<echo=TRUE>>=
phyper(1, m = 17, n = 233, k = 5, lower.tail = FALSE)
@

In general, the \texttt{phyper(x, \ldots, lower.tail = FALSE)} command
computes \(\mathbb{P}(X > x)\) for given \(x\).

\textbf{Generate \(100,000\) observations of the random variable \(X\).}  Here \texttt{rhyper} comes to he rescue.

<<echo=TRUE>>=
x <- rhyper(100000, m = 17, n = 233, k = 5)
@

Our simulations are stored in the vector \texttt{x}.  We could take a look at a histogram of \texttt{x} if we like (omitted). We know from our formulas that \(\mu=K\cdot
M/(M+N)=5\cdot17/250=0.34\). We can check our formulas using the fact that
with repeated observations of \(X\) we would expect about 0.34
defectives on the average. To see how our sample reflects the true
mean, we can compute the sample mean:

<<echo=TRUE>>=
mean(x)
@

We see that when given many independent observations of \(X\), the
sample mean is very close to the true mean \(\mu\). We can repeat the
same idea and use the sample standard deviation to estimate the true
standard deviation of \(X\).

<<echo=TRUE>>=
sd(x)
@

From the output above our estimate is \Sexpr{sd(x)}, and from our formulas we get
\[
\sigma^{2}=K\frac{MN}{(M+N)^{2}}\frac{M+N-K}{M+N-1}\approx0.3117896,
\]
with \(\sigma=\sqrt{\sigma^{2}}\approx0.5583811944\). Our estimate was
pretty close.

\subsubsection{Sampling With and Without Replacement} \label{sub:sampling-with-and}


Suppose that we have a large urn with, say, \(M\) white balls and
\(N\) black balls. We take a sample of size \(n\) from the urn, and
let \(X\) count the number of white balls in the sample. If we sample

\begin{description}
\item [{without replacement,}] then
     \(X\sim\mathsf{hyper}(\mathtt{m=}M,\,\mathtt{n}=N,\,\mathtt{k}=n)\)
     and has mean and variance
     \begin{alignat*}{1}
     \mu= & n\frac{M}{M+N},\\
     \sigma^{2}= & n\frac{MN}{(M+N)^{2}}\frac{M+N-n}{M+N-1},\\
     = & n\frac{M}{M+N}\left(1-\frac{M}{M+N}\right)\frac{M+N-n}{M+N-1}.
     \end{alignat*}
     On the other hand, if we sample
\item [{with replacement,}]  then
     \(X\sim\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=M/(M+N))\)
     with mean and variance
     \begin{alignat*}{1}
     \mu= & n\frac{M}{M+N},\\
     \sigma^{2}= & n\frac{M}{M+N}\left(1-\frac{M}{M+N}\right).
     \end{alignat*}
\end{description}

We see that both sampling procedures have the same mean, and the
method with the larger variance is the ``with replacement''
scheme. The factor by which the variances differ,
\begin{equation}
\frac{M+N-n}{M+N-1},
\end{equation}
is called a \textit{finite population correction}. For a fixed sample size
\(n\), as \(M,N\to\infty\) it is clear that the correction goes to 1,
that is, for infinite populations the sampling schemes are essentially
the same with respect to mean and variance.

\subsection{Waiting Time Distributions} \label{sec:waiting-time-distributions}


Another important class of problems is associated with the amount of
time it takes for a specified event of interest to occur. For example,
we could flip a coin repeatedly until we observe Heads. We could toss
a piece of paper repeatedly until we make it in the trash can.

\subsubsection{The Geometric Distribution} \label{sub:the-geometric-distribution}


Suppose that we conduct Bernoulli trials repeatedly, noting the
successes and failures. Let \(X\) be the number of failures before a
success. If \(\mathbb{P}(S)=p\) then \(X\) has PMF
\begin{equation}
f_{X}(x)=p(1-p)^{x},\quad x=0,1,2,\ldots
\end{equation}
(Why?) We say that \(X\) has a \textit{Geometric distribution} and we
write \(X\sim\mathsf{geom}(\mathtt{prob}=p)\). The associated \textsf{R}
functions are \texttt{dgeom(x, prob)}, \texttt{pgeom}, \texttt{qgeom},
and \texttt{rhyper}, which give the PMF, CDF, quantile function, and
simulate random variates, respectively.

Again it is clear that \(f(x)\geq0\) and we check that \(\sum f(x)=1\)
(see Equation \eqref{eq:geom-series} in Appendix~\ref{sec:sequences-and-series}):
\begin{alignat*}{1}
\sum_{x=0}^{\infty}p(1-p)^{x}= & p\sum_{x=0}^{\infty}q^{x}=p\,\frac{1}{1-q}=1.
\end{alignat*}
We will find in the next section that the mean and variance are
\begin{equation}
\mu=\frac{1-p}{p}=\frac{q}{p}\mbox{ and }\sigma^{2}=\frac{q}{p^{2}}.
\end{equation}



\begin{example}[]
The Pittsburgh Steelers place kicker, Jeff Reed, made 81.2\% of his
attempted field goals in his career up to 2006. Assuming that his
successive field goal attempts are approximately Bernoulli trials,
find the probability that Jeff misses at least 5 field goals before
his first successful goal.
\end{example}

\textbf{Solution:} If \(X=\) the number of missed goals until Jeff's first
success, then \(X\sim\mathsf{geom}(\mathtt{prob}=0.812)\) and we want
\(\mathbb{P}(X\geq5)=\mathbb{P}(X>4)\). We can find this in
R with

<<echo=TRUE>>=
pgeom(4, prob = 0.812, lower.tail = FALSE)
@




\begin{note}[]
Some books use a slightly different definition of the geometric
distribution. They consider Bernoulli trials and let \(Y\) count
instead the number of trials until a success, so that \(Y\) has PMF
\begin{equation}
f_{Y}(y)=p(1-p)^{y-1},\quad y=1,2,3,\ldots
\end{equation}
When they say ``geometric distribution'', this is what they mean. It
is not hard to see that the two definitions are related. In fact, if
\(X\) denotes our geometric and \(Y\) theirs, then
\(Y=X+1\). Consequently, they have \(\mu_{Y}=\mu_{X}+1\) and
\(\sigma_{Y}^{2}=\sigma_{X}^{2}\).
\end{note}

\subsubsection{The Negative Binomial Distribution} \label{sub:the-negative-binomial}


We may generalize the problem and consider the case where we wait for
\textit{more} than one success. Suppose that we conduct Bernoulli
trials repeatedly, noting the respective successes and failures. Let
\(X\) count the number of failures before \(r\) successes. If
\(\mathbb{P}(S)=p\) then \(X\) has PMF
\begin{equation}
f_{X}(x)={r+x-1 \choose r-1}\, p^{r}(1-p)^{x},\quad x=0,1,2,\ldots
\end{equation}

We say that \(X\) has a \textit{Negative Binomial distribution} and
write \(X\sim\mathsf{nbinom}(\mathtt{size}=r,\,\mathtt{prob}=p)\). The
associated \textsf{R} functions are \texttt{dnbinom(x, size, prob)},
\texttt{pnbinom}, \texttt{qnbinom}, and \texttt{rnbinom}, which give
the PMF, CDF, quantile function, and simulate random variates,
respectively.

As usual it should be clear that \(f_{X}(x)\geq 0\) and the fact that
\(\sum f_{X}(x)=1\) follows from a generalization of the geometric
series by means of a Maclaurin's series expansion:
\begin{alignat}{1}
\frac{1}{1-t}= & \sum_{k=0}^{\infty}t^{k},\quad \mbox{for \(-1 < t < 1\)},\mbox{ and}\\
\frac{1}{(1-t)^{r}}= & \sum_{k=0}^{\infty}{r+k-1 \choose r-1}\, t^{k},\quad \mbox{for \(-1 < t < 1\)}.
\end{alignat}
Therefore
\begin{equation}
\sum_{x=0}^{\infty}f_{X}(x)=p^{r}\sum_{x=0}^{\infty}{r+x-1 \choose r-1}\, q^{x}=p^{r}(1-q)^{-r}=1,
\end{equation}
since \(|q|=|1-p|<1\).



\begin{example}[]
  We flip a coin repeatedly and let \(X\) count the number of Tails
  until we get seven Heads. What is \(\mathbb{P}(X=5)?\)
  \textbf{Solution:} We know that
  \(X\sim\mathsf{nbinom}(\mathtt{size}=7,\,\mathtt{prob}=1/2)\).
\[
\mathbb{P}(X=5)=f_{X}(5)={7+5-1 \choose 7-1}(1/2)^{7}(1/2)^{5}={11
\choose 6}2^{-12}
\]
and we can get this in \textsf{R} with
\end{example}

<<echo=TRUE>>=
dnbinom(5, size = 7, prob = 0.5)
@

Let us next compute the MGF of
\(X\sim\mathsf{nbinom}(\mathtt{size}=r,\,\mathtt{prob}=p)\).
\begin{alignat*}{1}
M_{X}(t)= & \sum_{x=0}^{\infty}\mathrm{e}^{tx}\ {r+x-1 \choose r-1}p^{r}q^{x}\\
= & p^{r}\sum_{x=0}^{\infty}{r+x-1 \choose r-1}[q\mathrm{e}^{t}]^{x}\\
= & p^{r}(1-qe^{t})^{-r},\quad \mbox{provided $|q\mathrm{e}^{t}|<1$,}
\end{alignat*}
and so
\begin{equation}
M_{X}(t)=\left(\frac{p}{1-q\mathrm{e}^{t}}\right)^{r},\quad \mbox{for $q\mathrm{e}^{t}<1$}.
\end{equation}
We see that \(q\mathrm{e}^{t}<1\) when \(t<-\ln(1-p)\).

Let \(X\sim\mathsf{nbinom}(\mathtt{size}=r,\mathtt{prob}=p)\) with
\(M(t)=p^{r}(1-q\mathrm{e}^{t})^{-r}\). We proclaimed above the
values of the mean and variance. Now we are equipped with the tools to
find these directly.
\begin{alignat*}{1}
M'(t)= & p^{r}(-r)(1-q\mathrm{e}^{t})^{-r-1}(-q\mathrm{e}^{t}),\\
= & rq\mathrm{e}^{t}p^{r}(1-q\mathrm{e}^{t})^{-r-1},\\
= & \frac{rq\mathrm{e}^{t}}{1-q\mathrm{e}^{t}}M(t),\mbox{ and so }\\
M'(0)= & \frac{rq}{1-q}\cdot1=\frac{rq}{p}.
\end{alignat*}
Thus \(\mu=rq/p\). We next find \(\mathbb{E} X^{2}\).
\begin{alignat*}{1}
M''(0)= & \left.\frac{rq\mathrm{e}^{t}(1-q\mathrm{e}^{t})-rq\mathrm{e}^{t}(-q\mathrm{e}^{t})}{(1-q\mathrm{e}^{t})^{2}}M(t)+\frac{rq\mathrm{e}^{t}}{1-q\mathrm{e}^{t}}M'(t)\right|_{t=0},\\
= & \frac{rqp+rq^{2}}{p^{2}}\cdot1+\frac{rq}{p}\left(\frac{rq}{p}\right),\\
= & \frac{rq}{p^{2}}+\left(\frac{rq}{p}\right)^{2}.
\end{alignat*}
Finally we may say \(\sigma^{2} = M''(0) - [M'(0)]^{2} = rq/p^{2}.\)



\begin{example}[]
A random variable has MGF
\[
M_{X}(t)=\left(\frac{0.19}{1-0.81\mathrm{e}^{t}}\right)^{31}.
\]
Then \(X\sim\mathsf{nbinom}(\mathtt{size}=31,\,\mathtt{prob}=0.19)\).
\end{example}



\begin{note}[]
As with the Geometric distribution, some books use a slightly
different definition of the Negative Binomial distribution. They
consider Bernoulli trials and let \(Y\) be the number of trials until
\(r\) successes, so that \(Y\) has PMF
\begin{equation}
f_{Y}(y)={y-1 \choose r-1}p^{r}(1-p)^{y-r},\quad y=r,r+1,r+2,\ldots
\end{equation}
It is again not hard to see that if \(X\) denotes our Negative
Binomial and \(Y\) theirs, then \(Y=X+r\). Consequently, they have
\(\mu_{Y}=\mu_{X}+r\) and \(\sigma_{Y}^{2}=\sigma_{X}^{2}\).
\end{note}

\subsection{Arrival Processes} \label{sec:arrival-processes}

\subsubsection{The Poisson Distribution} \label{sub:the-poisson-distribution}


This is a distribution associated with ``rare events'', for reasons
which will become clear in a moment. The events might be:

\begin{itemize}
\item traffic accidents,
\item typing errors, or
\item customers arriving in a bank.
\end{itemize}

Let \(\lambda\) be the average number of events in the time interval
\([0,1]\). Let the random variable \(X\) count the number of events
occurring in the interval. Then under certain reasonable conditions it
can be shown that
\begin{equation}
f_{X}(x)=\mathbb{P}(X=x)=\mathrm{e}^{-\lambda}\frac{\lambda^{x}}{x!},\quad x=0,1,2,\ldots
\end{equation}
We use the notation
\(X\sim\mathsf{pois}(\mathtt{lambda}=\lambda)\). The associated
R functions are \texttt{dpois(x, lambda)}, \texttt{ppois}, \texttt{qpois}, and
\texttt{rpois}, which give the PMF, CDF, quantile function, and simulate
random variates, respectively.

\subsubsection{What are the reasonable conditions?}

Divide \([0,1]\) into subintervals of length \(1/n\). A \textit{Poisson
process} \index{Poisson process} satisfies the following conditions:

\begin{itemize}
\item the probability of an event occurring in a particular subinterval is
  \(\approx\lambda/n\).
\item the probability of two or more events occurring in any subinterval
  is \(\approx 0\).
\item occurrences in disjoint subintervals are independent.
\end{itemize}



\begin{rem}[]
If \(X\) counts the number of events in the
interval \([0,t]\) and \(\lambda\) is the average number that occur in
unit time, then \(X\sim\mathsf{pois}(\mathtt{lambda}=\lambda t)\),
that is,
\begin{equation}
\mathbb{P}(X=x)=\mathrm{e}^{-\lambda t}\frac{(\lambda t)^{x}}{x!},\quad x=0,1,2,3\ldots
\end{equation}
\label{rem-poisson-process}
\end{rem}




\begin{example}[]
On the average, five cars arrive at a particular car wash every
hour. Let \(X\) count the number of cars that arrive from 10AM to
11AM. Then \(X\sim\mathsf{pois}(\mathtt{lambda}=5)\). Also,
\(\mu=\sigma^{2}=5\). What is the probability that no car arrives
during this period?  \textit{Solution}: The probability that no car arrives
is \[
\mathbb{P}(X=0)=\mathrm{e}^{-5}\frac{5^{0}}{0!}=\mathrm{e}^{-5}\approx0.0067.
\]
\end{example}

We get this in \textsf{R} with

<<echo=TRUE>>=
dpois(0, lambda = 5)
@



\begin{example}[]
Suppose the car wash above is in operation from 8AM to 6PM, and we let
\(Y\) be the number of customers that appear in this period. Since
this period covers a total of 10 hours, from Remark~\ref{rem-poisson-process} we
get that \(Y\sim\mathsf{pois}(\mathtt{lambda}=5\ast10=50)\). What is
the probability that there are between 48 and 50 customers, inclusive?
\textbf{Solution:} We want \(\mathbb{P}(48\leq
Y\leq50)=\mathbb{P}(X\leq50)-\mathbb{P}(X\leq47)\).
\end{example}

<<echo=TRUE>>=
diff(ppois(c(47, 50), lambda = 50))
@


\section{Functions of Discrete Random Variables} \label{sec:functions-discrete-rvs}

We have built a large catalogue of discrete distributions, but the
tools of this section will give us the ability to consider infinitely
many more. Given a random variable \(X\) and a given function \(h\),
we may consider \(Y=h(X)\). Since the values of \(X\) are determined
by chance, so are the values of \(Y\). The question is, what is the
PMF of the random variable \(Y\)? The answer, of course, depends on
\(h\). In the case that \(h\) is one-to-one (see Appendix~\ref{sec:differential-and-integral}), the solution can be found by simple
substitution.



\begin{example}[]
  Let \(X\sim\mathsf{nbinom}(\mathtt{size}=r,\,\mathtt{prob}=p)\). We
  saw in Section~\ref{sec:other-discrete-distributions} that \(X\)
  represents the number of failures until \(r\) successes in a
  sequence of Bernoulli trials. Suppose now that instead we were
  interested in counting the number of trials (successes and failures)
  until the \(r^{\mathrm{th}}\) success occurs, which we will denote
  by \(Y\). In a given performance of the experiment, the number of
  failures (\(X\)) and the number of successes (\(r\)) together will
  comprise the total number of trials (\(Y\)), or in other words,
  \(X+r=Y\). We may let \(h\) be defined by \(h(x)=x+r\) so that
  \(Y=h(X)\), and we notice that \(h\) is linear and hence
  one-to-one. Finally, \(X\) takes values \(0,\ 1,\ 2,\ldots\)
  implying that the support of \(Y\) would be
  \(\{ r,\ r+1,\ r+2,\ldots \}\). Solving for \(X\) we get
  \(X=Y-r\). Examining the PMF of \(X\)
\begin{equation}
f_{X}(x)={r+x-1 \choose r-1}\, p^{r}(1-p)^{x},
\end{equation}
we can substitute \(x = y - r\) to get
\begin{eqnarray*}
f_{Y}(y) & = & f_{X}(y-r),\\
 & = & {r+(y-r)-1 \choose r-1}\, p^{r}(1-p)^{y-r},\\
 & = & {y-1 \choose r-1}\, p^{r}(1-p)^{y-r},\quad y=r,\, r+1,\ldots
\end{eqnarray*}
\end{example}

Even when the function \(h\) is not one-to-one, we may still find the
PMF of \(Y\) simply by accumulating, for each \(y\), the probability
of all the \(x\)'s that are mapped to that \(y\).



\begin{prop}[]
Let \(X\) be a discrete random variable with PMF \(f_{X}\) supported
on the set \(S_{X}\). Let \(Y=h(X)\) for some function \(h\). Then
\(Y\) has PMF \(f_{Y}\) defined by
\begin{equation}
f_{Y}(y)=\sum_{\{x\in S_{X}|\, h(x)=y\}}f_{X}(x)
\end{equation}
\end{prop}



\begin{example}[]
Let \(X\sim\mathsf{binom}(\mathtt{size}=4,\,\mathtt{prob}=1/2)\), and let \(Y=(X-1)^{2}\). Consider the following table:
\end{example}

\begin{table}[H]
\begin{tabular}{c|ccccc}
$x$ & 0 & 1 & 2 & 3 & 4\tabularnewline
\hline
$f_{X}(x)$ & $1/16$ & $1/4$ & $6/16$ & $1/4$ & $1/16$\tabularnewline
\hline
$y=(x-1)^{2}$ & 1 & 0 & 1 & 4 & 9\tabularnewline
\end{tabular}
\caption{Transforming a discrete random variable.}
\label{tab:disc-transf}
\end{table}

From this we see that \(Y\) has support \(S_{Y}=\{0,1,4,9\}\). We also
see that \(h(x)=(x-1)^{2}\) is not one-to-one on the support of \(X\),
because both \(x=0\) and \(x=2\) are mapped by \(h\) to
\(y=1\). Nevertheless, we see that \(Y=0\) only when \(X=1\), which
has probability \(1/4\); therefore, \(f_{Y}(0)\) should equal
\(1/4\). A similar approach works for \(y=4\) and \(y=9\). And \(Y=1\)
exactly when \(X=0\) or \(X=2\), which has total probability
\(7/16\). In summary, the PMF of \(Y\) may be written:

\begin{table}[H]
\begin{tabular}{c|cccc}
$y$ & 0 & 1 & 4 & 9\tabularnewline
\hline
$f_{X}(x)$ & $1/4$ & $7/16$ & $1/4$ & $1/16$\tabularnewline
\end{tabular}
\caption{Transforming a discrete random variable, its PMF.}
\label{tab:disc-transf-pmf}
\end{table}

There is not a special name for the distribution of \(Y\), it is just
an example of what to do when the transformation of a random variable
is not one-to-one. The method is the same for more complicated
problems.



\begin{prop}[]
If \(X\) is a random variable with \(\mathbb{E} X=\mu\) and \(\mbox{Var}(X)=\sigma^{2}\), then the mean and variance of \(Y=mX+b\) is
\begin{equation}
\mu_{Y}=m\mu+b,\quad \sigma_{Y}^{2}=m^{2}\sigma^{2},\quad \sigma_{Y}=|m|\sigma.
\end{equation}
\end{prop}



\section{Chapter Exercises}


\begin{Exercise}[]
For the following situations, decide what the distribution of \(X\)
should be. In nearly every case, there are additional assumptions that
should be made for the distribution to apply; identify those
assumptions (which may or may not hold in practice.)

\begin{itemize}
\item We shoot basketballs at a basketball hoop, and count the number of
  shots until we make a goal. Let \(X\) denote the number of missed
  shots. On a normal day we would typically make about 37\% of the
  shots.
\item In a local lottery in which a three digit number is selected
  randomly, let \(X\) be the number selected.
\item We drop a Styrofoam cup to the floor twenty times, each time
  recording whether the cup comes to rest perfectly right side up, or
  not. Let \(X\) be the number of times the cup lands perfectly right
  side up.
\item We toss a piece of trash at the garbage can from across the room. If
  we miss the trash can, we retrieve the trash and try again,
  continuing to toss until we make the shot. Let \(X\) denote the
  number of missed shots.
\item Working for the border patrol, we inspect shipping cargo as when it
  enters the harbor looking for contraband. A certain ship comes to
  port with 557 cargo containers. Standard practice is to select 10
  containers randomly and inspect each one very carefully, classifying
  it as either having contraband or not. Let \(X\) count the number of
  containers that illegally contain contraband.
\item At the same time every year, some migratory birds land in a bush
  outside for a short rest. On a certain day, we look outside and let
  \(X\) denote the number of birds in the bush.
\item We count the number of rain drops that fall in a circular area on a
  sidewalk during a ten minute period of a thunder storm.
\item We count the number of moth eggs on our window screen.
\item We count the number of blades of grass in a one square foot patch of
  land.
\item We count the number of pats on a baby's back until (s)he burps.
\end{itemize}

\end{Exercise}





\begin{Exercise}[]
A recent national study showed that approximately 44\% of college
students have used Wikipedia as a source in at least one of their term
papers. Let \(X\) equal the number of college students in a random sample of
size \(n=31\) who have used Wikipedia as a source.

\begin{enumerate}
\item How is \(X\) distributed?
\item Sketch the probability mass function (roughly).
\item Sketch the cumulative distribution function (roughly).
\item Find the probability that \(X\) is equal to 17.
\item Find the probability that \(X\) is at most 13.
\item Find the probability that \(X\) is bigger than 11.
\item Find the probability that \(X\) is at least 15.
\item Find the probability that \(X\) is between 16 and 19, inclusive.
\item Give the mean of \(X\), denoted \(\mathbb{E} X\).
\item Give the variance of \(X\).
\item Give the standard deviation of \(X\).
\item Find \(\mathbb{E}(4X + 51.324)\).
\end{enumerate}

\end{Exercise}




\begin{Exercise}[]
Defects in a certain type of masking tape occur on the average of 3.83 per linear foot. Suppose we randomly select 1 foot of masking tape and let \(X\) be the number of flaws in our tape.

\begin{enumerate}
\item What is an appropriate model for the distribution of X? You should write the family name and value(s) of any parameter(s).

\item Find the probability that we find exactly 8 flaws.

\item Find the probability that we find 9 or more flaws.

\item Now put down the first foot and suppose that we randomly select another 5 feet of masking tape.  Repeat the previous problems.
\end{enumerate}

\end{Exercise}




\begin{Exercise}[]
Customers arrive randomly at an archery shop. Given that n = 18 customers arrived during a 17 minute period, let \(X\) equal the number of customers wearing camoflauge. Suppose the probability
that any given person arrives at this shop wearing camoflauge is \(p = 0.45\). Find

\begin{enumerate}
\item The distribution of \(X\). You should write down the name of the distribution plus the numerical value of any parameter(s).

\item Find \(\mathbb{P}(X = 8)\).

\item Find \(\mathbb{B}(7 \leq X < 9)\).
\end{enumerate}

\end{Exercise}






\begin{Exercise}[label=exr:variance-shortcut]
Show that \(\mathbb{E}(X - \mu)^{2} =
\mathbb{E} X^{2} - \mu^{2}\). \textit{Hint:} expand the quantity \((X -
\mu)^{2}\) and distribute the expectation over the resulting terms.
\end{Exercise}





\begin{Exercise}[label=exr:binom-factorial-expectation]
If
\(X\sim\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p)\) show that
\(\mathbb{E} X(X - 1) = n(n - 1)p^{2}\).
\end{Exercise}




\begin{Exercise}[label=exr:hyper-mean-variance]
Calculate the mean and variance of the
hypergeometric distribution. Show that
\begin{equation}
\mu=K\frac{M}{M + N},\quad \sigma^{2} = K\frac{MN}{(M + N)^{2}}\frac{M + N - K}{M + N - 1}.
\end{equation}
\end{Exercise}






\chapter{Continuous Distributions} \label{cha:continuous-distributions}



<<echo=FALSE, include=FALSE>>=
# This chapter's package dependencies
library(actuar)
library(distrEx)
distroptions("WarningSim" = FALSE)
         # switches off warnings as to (In)accuracy due to simulations
distroptions("WarningArith" = FALSE)
         # switches off warnings as to arithmetics
@

The focus of the last chapter was on random variables whose support
can be written down in a list of values (finite or countably
infinite), such as the number of successes in a sequence of Bernoulli
trials. Now we move to random variables whose support is a whole range
of values, say, an interval \((a,b)\). It is shown in later classes
that it is impossible to write all of the numbers down in a list;
there are simply too many of them.

This chapter begins with continuous random variables and the
associated PDFs and CDFs The continuous uniform distribution is
highlighted, along with the Gaussian, or normal, distribution. Some
mathematical details pave the way for a catalogue of models.

The interested reader who would like to learn more about any of the
assorted discrete distributions mentioned below should take a look at
\textit{Continuous Univariate Distributions, Volumes 1} and \textit{2} by Johnson
\textit{et al} \cite{Johnson1994}, \cite{Johnson1995}.

\paragraph{What do I want them to know?}

\begin{itemize}
\item how to choose a reasonable continuous model under a variety of
  physical circumstances
\item basic correspondence between continuous versus discrete random
  variables
\item the general tools of the trade for manipulation of continuous random
  variables, integration, \textit{etc}.
\item some details on a couple of continuous models, and exposure to a
  bunch of other ones
\item how to make new continuous random variables from old ones
\end{itemize}


\section{Continuous Random Variables} \label{sec:continuous-random-variables}


\subsection{Probability Density Functions} \label{sub:probability-density-functions}


Continuous random variables have supports that look like
\begin{equation}
S_{X}=[a,b]\mbox{ or }(a,b),
\end{equation}
or unions of intervals of the above form. Examples of random variables
that are often taken to be continuous are:

\begin{itemize}
\item the height or weight of an individual,
\item other physical measurements such as the length or size of an object,
  and
\item durations of time (usually).
\end{itemize}

Every continuous random variable \(X\) has a \textit{probability
  density function} (PDF) denoted \(f_{X}\) associated with
it\footnote{Not true. There are pathological random variables with no
  density function. (This is one of the crazy things that can happen
  in the world of measure theory). But in this book we will not get
  even close to these anomalous beasts, and regardless it can be
  proved that the CDF always exists.} that satisfies three basic
properties:

\begin{enumerate}
\item \(f_{X}(x)>0\) for \(x\in S_{X}\),
\item \(\int_{x\in S_{X}}f_{X}(x)\,\mathrm{d} x=1\), and
\item \label{enu-contrvcond3} \(\mathbb{P}(X\in A)=\int_{x\in
   A}f_{X}(x)\:\mathrm{d} x\), for an event \(A\subset S_{X}\).
\end{enumerate}



\begin{rem}[]
We can say the following about continuous random variables:

\begin{itemize}
\item Usually, the set \(A\) in~\ref{enu-contrvcond3} takes the form of an
  interval, for example, \(A=[c,d]\), in which case
  \begin{equation}
  \mathbb{P}(X\in A)=\int_{c}^{d}f_{X}(x)\:\mathrm{d} x.
  \end{equation}
\item It follows that the probability that \(X\) falls in a given interval
  is simply the \textit{area under the curve} of \(f_{X}\) over the interval.
\item Since the area of a line \(x=c\) in the plane is zero,
  \(\mathbb{P}(X=c)=0\) for any value \(c\). In other words, the
  chance that \(X\) equals a particular value \(c\) is zero, and this
  is true for any number \(c\). Moreover, when \(a<b\) all of the
  following probabilities are the same:
  \begin{equation}
  \mathbb{P}(a\leq X\leq b)=\mathbb{P}(a<X\leq b)=\mathbb{P}(a\leq X<b)=\mathbb{P}(a<X<b).
  \end{equation}
\item The PDF \(f_{X}\) can sometimes be greater than 1. This is in
  contrast to the discrete case; every nonzero value of a PMF is a
  probability which is restricted to lie in the interval \([0,1]\).
\end{itemize}
\end{rem}



We met the cumulative distribution function, \(F_{X}\), in Chapter~\ref{cha:discrete-distributions}. Recall that it is defined by
\(F_{X}(t)=\mathbb{P}(X\leq t)\), for \(-\infty<t<\infty\). While in
the discrete case the CDF is unwieldy, in the continuous case the CDF
has a relatively convenient form:
\begin{equation}
F_{X}(t)=\mathbb{P}(X\leq t)=\int_{-\infty}^{t}f_{X}(x)\:\mathrm{d} x,\quad -\infty < t < \infty.
\end{equation}



\begin{rem}[]
For any continuous CDF \(F_{X}\) the following are true.

\begin{itemize}
\item \(F_{X}\) is nondecreasing , that is, \(t_{1}\leq t_{2}\) implies
  \(F_{X}(t_{1})\leq F_{X}(t_{2})\).
\item \(F_{X}\) is continuous (see Appendix~\ref{sec:differential-and-integral}. Note the distinction from the
  discrete case: CDFs of discrete random variables are not continuous,
  they are only right continuous.
\item \(\lim_{t\to-\infty}F_{X}(t)=0\) and
  \(\lim_{t\to\infty}F_{X}(t)=1\).
\end{itemize}
\end{rem}

There is a handy relationship between the CDF and PDF in the
continuous case. Consider the derivative of \(F_{X}\):
\begin{equation}
F'_{X}(t)=\frac{\mathrm{d}}{\mathrm{d} t}F_{X}(t)=\frac{\mathrm{d}}{\mathrm{d} t}\,\int_{-\infty}^{t}f_{X}(x)\,\mathrm{d} x=f_{X}(t),
\end{equation}
the last equality being true by the Fundamental Theorem of Calculus,
part (2) (see Appendix~\ref{sec:differential-and-integral}). In short,
\((F_{X})'=f_{X}\) in the continuous case\footnote{In the discrete case, \(f_{X}(x)=F_{X}(x)-\lim_{t\to
x^{-}}F_{X}(t)\).}.


\subsection{Expectation of Continuous Random Variables} \label{sub:expectation-of-continuous}


For a continuous random variable \(X\) the expected value of \(g(X)\)
is
\begin{equation}
\mathbb{E} g(X)=\int_{x\in S}g(x)f_{X}(x)\:\mathrm{d} x,
\end{equation}
provided the (potentially improper) integral \(\int_{S}|g(x)|\,
f(x)\mathrm{d} x\) is convergent. One important example is the mean
\(\mu\), also known as \(\mathbb{E} X\):
\begin{equation}
\mu=\mathbb{E} X=\int_{x\in S}xf_{X}(x)\:\mathrm{d} x,
\end{equation}
provided \(\int_{S}|x|f(x)\mathrm{d} x\) is finite. Also there is the variance
\begin{equation}
\sigma^{2}=\mathbb{E}(X-\mu)^{2}=\int_{x\in S}(x-\mu)^{2}f_{X}(x)\,\mathrm{d} x,
\end{equation}
which can be computed with the alternate formula
\(\sigma^{2}=\mathbb{E} X^{2}-(\mathbb{E} X)^{2}\). In addition, there
is the standard deviation \(\sigma=\sqrt{\sigma^{2}}\). The moment
generating function is given by
\begin{equation}
M_{X}(t)=\mathbb{E}\:\mathrm{e}^{tX}=\int_{-\infty}^{\infty}\mathrm{e}^{tx}f_{X}(x)\:\mathrm{d} x,
\end{equation}
provided the integral exists (is finite) for all \(t\) in a
neighborhood of \(t=0\).



\begin{example}[]
\label{exm:cont-pdf3x2}
Let the continuous random variable \(X\) have PDF
\[ f_{X}(x)=3x^{2},\quad 0\leq x\leq 1. \] We will see later that
\(f_{X}\) belongs to the \textit{Beta} family of distributions. It is easy to
see that \(\int_{-\infty}^{\infty}f(x)\mathrm{d} x=1\).
\begin{align*}
\int_{-\infty}^{\infty}f_{X}(x)\mathrm{d} x & =\int_{0}^{1}3x^{2}\:\mathrm{d} x\\
 & =\left.x^{3}\right|_{x=0}^{1}\\
 & =1^{3}-0^{3}\\
 & =1.
\end{align*}
This being said, we may find \(\mathbb{P}(0.14\leq X<0.71)\).
\begin{align*}
\mathbb{P}(0.14\leq X<0.71) & =\int_{0.14}^{0.71}3x^{2}\mathrm{d} x,\\
 & =\left.x^{3}\right|_{x=0.14}^{0.71}\\
 & =0.71^{3}-0.14^{3}\\
 & \approx0.355167.
\end{align*}
We can find the mean and variance in an identical manner.
\begin{align*}
\mu=\int_{-\infty}^{\infty}xf_{X}(x)\mathrm{d} x & =\int_{0}^{1}x\cdot3x^{2}\:\mathrm{d} x,\\
 & =\frac{3}{4}x^{4}|_{x=0}^{1},\\
 & =\frac{3}{4}.
\end{align*}
It would perhaps be best to calculate the variance with the shortcut
formula \(\sigma^{2}=\mathbb{E} X^{2}-\mu^{2}\):
\begin{align*}
\mathbb{E} X^{2}=\int_{-\infty}^{\infty}x^{2}f_{X}(x)\mathrm{d} x & =\int_{0}^{1}x^{2}\cdot3x^{2}\:\mathrm{d} x\\
 & =\left.\frac{3}{5}x^{5}\right|_{x=0}^{1}\\
 & =3/5.
\end{align*}
which gives \(\sigma^{2}=3/5-(3/4)^{2}=3/80\).
\end{example}




\begin{example}[]
\label{exm:cont-pdf-3x4}
We will try one with unbounded support to brush
up on improper integration. Let the random variable \(X\) have PDF \[
f_{X}(x)=\frac{3}{x^{4}},\quad x>1.  \] We can show that
\(\int_{-\infty}^{\infty}f(x)\mathrm{d} x=1\):
\begin{align*}
\int_{-\infty}^{\infty}f_{X}(x)\mathrm{d} x & =\int_{1}^{\infty}\frac{3}{x^{4}}\:\mathrm{d} x,\\
 & =\lim_{t\to\infty}\int_{1}^{t}\frac{3}{x^{4}}\:\mathrm{d} x,\\
 & =\lim_{t\to\infty}\ \left.3\,\frac{1}{-3}x^{-3}\right|_{x=1}^{t},\\
 & =-\left(\lim_{t\to\infty}\frac{1}{t^{3}}-1\right),\\
 & =1.
\end{align*}
We calculate \(\mathbb{P}(3.4\leq X<7.1)\):
\begin{align*}
\mathbb{P}(3.4\leq X<7.1) & =\int_{3.4}^{7.1}3x^{-4}\mathrm{d} x,\\
 & =\left.3\,\frac{1}{-3}x^{-3}\right|_{x=3.4}^{7.1},\\
 & =-1(7.1^{-3}-3.4^{-3}),\\
 & \approx0.0226487123.
\end{align*}
We locate the mean and variance just like before.
\begin{align*}
\mu=\int_{-\infty}^{\infty}xf_{X}(x)\mathrm{d} x & =\int_{1}^{\infty}x\cdot\frac{3}{x^{4}}\:\mathrm{d} x,\\
 & =\left.3\,\frac{1}{-2}x^{-2}\right|_{x=1}^{\infty},\\
 & =-\frac{3}{2}\left(\lim_{t\to\infty}\frac{1}{t^{2}}-1\right),\\
 & =\frac{3}{2}.
\end{align*}
Again we use the shortcut \(\sigma^{2}=\mathbb{E} X^{2}-\mu^{2}\):
\begin{align*}
\mathbb{E} X^{2}=\int_{-\infty}^{\infty}x^{2}f_{X}(x)\mathrm{d} x & =\int_{1}^{\infty}x^{2}\cdot\frac{3}{x^{4}}\:\mathrm{d} x,\\
 & =\left.3\:\frac{1}{-1}x^{-1}\right|_{x=1}^{\infty},\\
 & =-3\left(\lim_{t\to\infty}\frac{1}{t^{2}}-1\right),\\
 & =3,
\end{align*}
which closes the example with \(\sigma^{2}=3-(3/2)^{2}=3/4\).
\end{example}


\subsubsection{How to do it with \textsf{R}}

There exist utilities to calculate probabilities and expectations for
general continuous random variables, but it is better to find a
built-in model, if possible. Sometimes it is not possible. We show how
to do it the long way, and the \texttt{distr} \index{R packages@\textsf{R}
packages!distr@\texttt{distr}} package \cite{distr} way.



\begin{example}[]
Let \(X\) have PDF \(f(x)=3x^{2}\), \(0<x<1\) and find
\(\mathbb{P}(0.14\leq X\leq0.71)\). (We will ignore that \(X\) is a
beta random variable for the sake of argument.)
\end{example}

<<echo=TRUE>>=
f <- function(x) 3*x^2
integrate(f, lower = 0.14, upper = 0.71)
@

Compare this to the answer we found in Example~\ref{exm:cont-pdf3x2}. We could
integrate the function \(x \cdot f(x)= 3x^3\) from zero to one to
get the mean, and use the shortcut \(\sigma^{2}=\mathbb{E}
X^{2}-\left(\mathbb{E} X\right)^{2}\) for the variance.



\begin{example}[]
Let \(X\) have PDF \(f(x)=3/x^{4}\), \(x>1\). We may integrate the
function \(g(x) = x \cdot f(x)= 3/x^3\) from zero to infinity to get
the mean of \(X\).
\end{example}

<<echo=TRUE>>=
g <- function(x) 3/x^3
integrate(g, lower = 1, upper = Inf)
@

Compare this to the answer we got in Example~\ref{exm:cont-pdf-3x4}. Use \texttt{-Inf}
for \(-\infty\).



\begin{example}[]
Let us redo Example~\ref{exm:cont-pdf3x2} with the \texttt{distr} package.
\end{example}

The method is similar to that encountered in Section
\ref{sub:disc-rv-how-r} in Chapter~\ref{cha:discrete-distributions}. We define an absolutely continuous
random variable:

<<echo=TRUE>>=
f <- function(x) 3*x^2
X <- AbscontDistribution(d = f, low1 = 0, up1 = 1)
p(X)(0.71) - p(X)(0.14)
@


Compare this to the answer we found earlier. Now let us try
expectation with the \texttt{distrEx} package \cite{distrEx}:
<<echo=TRUE>>=
E(X); var(X); 3/80
@

Compare these answers to the ones we found in Example~\ref{exm:cont-pdf3x2}. Why are they different? Because the \texttt{distrEx}
package resorts to numerical methods when it encounters a model it
does not recognize. This means that the answers we get for
calculations may not exactly match the theoretical values. Be careful.


\section{The Continuous Uniform Distribution} \label{sec:the-continuous-uniform}


A random variable \(X\) with the continuous uniform distribution on
the interval \((a,b)\) has PDF
\begin{equation}
f_{X}(x)=\frac{1}{b-a}, \quad a < x < b.
\end{equation}
The associated \textsf{R} function is
\(\mathsf{dunif}(\mathtt{min}=a,\,\mathtt{max}=b)\). We write
\(X\sim\mathsf{unif}(\mathtt{min}=a,\,\mathtt{max}=b)\). Due to the
particularly simple form of this PDF we can also write down explicitly
a formula for the CDF \(F_{X}\): \begin{equation} \label{eq:unif-cdf} F_{X}(t) = \begin{cases} 0, & t < 0,\\ \frac{t-a}{b-a}, & a\leq t < b,\\ 1, & t \geq b. \end{cases} \end{equation}

The continuous uniform distribution is the continuous analogue of the
discrete uniform distribution; it is used to model experiments whose
outcome is an interval of numbers that are ``equally likely'' in the
sense that any two intervals of equal length in the support have the
same probability associated with them.



\begin{example}[]
Choose a number in \([0,1]\) at random, and let \(X\) be the number
chosen. Then \(X\sim\mathsf{unif}(\mathtt{min}=0,\,\mathtt{max}=1)\).
The mean of \(X\sim\mathsf{unif}(\mathtt{min}=a,\,\mathtt{max}=b)\) is
relatively simple to calculate:
\begin{align*}
\mu=\mathbb{E} X & =\int_{-\infty}^{\infty}x\, f_{X}(x)\,\mathrm{d} x,\\
 & =\int_{a}^{b}x\ \frac{1}{b-a}\ \mathrm{d} x,\\
 & =\left.\frac{1}{b-a}\ \frac{x^{2}}{2}\ \right|_{x=a}^{b},\\
 & =\frac{1}{b-a}\ \frac{b^{2}-a^{2}}{2},\\
 & =\frac{b+a}{2},
\end{align*}
using the popular formula for the difference of squares. The variance
is left to Exercise~\ref{exr:variance-dunif}.
\end{example}

\section{The Normal Distribution} \label{sec:the-normal-distribution}


We say that \(X\) has a \textit{normal distribution} if it has PDF
\begin{equation}
f_{X}(x)=\frac{1}{\sigma\sqrt{2\pi}}\exp \left\{ \frac{-(x-\mu)^{2}}{2\sigma^{2}} \right\},\quad -\infty < x < \infty.
\end{equation}
We write
\(X\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\), and
the associated \textsf{R} function is \texttt{dnorm(x, mean = 0, sd = 1)}.  See Figure~\ref{fig:norm-pdf} for a graph of the PDF when \(\mu = 4\) and \(\sigma = 1\).

<<norm-pdf, echo=FALSE, fig=TRUE, include=FALSE, width=5, height=2.9>>=
curve(dnorm(x, mean=4, sd = 1), from = 0, to=8, xlab = substitute(mu))
x = 1:7
y = dnorm(x, mean=4, sd = 1)
lines(x, y, type = "h", lty =2)
text(4.5, 0.05, expression(sigma))
text(3.5, 0.05, expression(sigma))
text(2.5, 0.05, expression(sigma))
text(5.5, 0.05, expression(sigma))
text(6.5, 0.25, expression(paste(frac(1, sigma*sqrt(2*pi)), " ",
                            plain(e)^{frac(-(x-mu)^2, 2*sigma^2)})),cex = 1.2)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-norm-pdf}
\end{center}
\caption[The \(\mathsf{norm}(\mathtt{mean}=4,\,\mathtt{sd}=1)\) PDF]{{\small The \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) PDF when \(\mu = 4\) and \(\sigma = 1\).  The bell is centered at  \(\mu = 4\) and the tick marks are spaced \(\sigma = 1\) on either side, the third one out being where the PDF almost touches the \(x\)-axis (but never quite does).}}
\label{fig:norm-pdf}
\end{figure}

The familiar bell-shaped curve, the normal distribution is also known
as the \textit{Gaussian distribution} because the German mathematician
C. F. Gauss largely contributed to its mathematical development. This
distribution is by far the most important distribution, continuous or
discrete. The normal model appears in the theory of all sorts of
natural phenomena, from to the way particles of smoke dissipate in a
closed room, to the journey of a bottle floating in the ocean, to the
white noise of cosmic background radiation.

When \(\mu=0\) and \(\sigma=1\) we say that the random variable has a
\textit{standard normal} distribution and we typically write
\(Z\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\). The lowercase
Greek letter phi (\(\phi\)) is used to denote the standard normal PDF
and the capital Greek letter phi \(\Phi\) is used to denote the
standard normal CDF: for \(-\infty<z<\infty\),
\begin{equation}
\phi(z)=\frac{1}{\sqrt{2\pi}}\,\mathrm{e}^{-z^{2}/2}\mbox{ and }\Phi(t)=\int_{-\infty}^{t}\phi(z)\,\mathrm{d} z.
\end{equation}



\begin{prop}[]
If \(X\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) then
\begin{equation}
Z=\frac{X-\mu}{\sigma}\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1).
\end{equation}
\end{prop}

The MGF of \(Z\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) is
relatively easy to derive:
\begin{eqnarray*}
M_{Z}(t) & = & \int_{-\infty}^{\infty}\mathrm{e}^{tz}\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-z^{2}/2}\mathrm{d} z,\\
 & = & \int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}\exp \{ -\frac{1}{2}\left(z^{2}+2tz+t^{2}\right)+\frac{t^{2}}{2} \} \mathrm{d} z,\\
 & = & \mathrm{e}^{t^{2}/2}\left(\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-[z-(-t)]^{2}/2}\mathrm{d} z\right),
\end{eqnarray*}
and the quantity in the parentheses is the total area under a \(\mathsf{norm}(\mathtt{mean}=-t,\,\mathtt{sd}=1)\) density, which is one. Therefore,
\begin{equation}
M_{Z}(t)=\mathrm{e}^{t^{2}/2},\quad -\infty < t < \infty.
\end{equation}




\begin{example}[]
The MGF of
\(X\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) is then
not difficult either because \[ Z=\frac{X-\mu}{\sigma},\mbox{ or
rewriting, }X=\sigma Z+\mu.  \] Therefore \[
M_{X}(t)=\mathbb{E}\mathrm{e}^{tX}=\mathbb{E}\mathrm{e}^{t(\sigma
Z+\mu)}=\mathbb{E}\mathrm{e}^{\sigma
tZ}\mathrm{e}^{t\mu}=\mathrm{e}^{t\mu}M_{Z}(\sigma t), \] and we know
that \(M_{Z}(t)=\mathrm{e}^{t^{2}/2}\), thus substituting we get \[
M_{X}(t)=\mathrm{e}^{t\mu}\mathrm{e}^{(\sigma t)^{2}/2}=\exp\left\{
\mu t+\sigma^{2}t^{2}/2\right\} , \] for \(-\infty<t<\infty\).
\end{example}



\begin{fact}[]
The same argument above shows that if \(X\) has MGF \(M_{X}(t)\) then
the MGF of \(Y=a+bX\) is
\begin{equation}
M_{Y}(t)=\mathrm{e}^{ta}M_{X}(bt).
\end{equation}
\end{fact}




\begin{example}[The 68-95-99.7 Rule]
We saw in Section~\ref{sub:measures-of-spread}
that when an empirical distribution is approximately bell shaped there
are specific proportions of the observations which fall at varying
distances from the (sample) mean. We can see where these come from --
and obtain more precise proportions -- with the following:
\end{example}

<<echo=TRUE>>=
pnorm(1:3) - pnorm(-(1:3))
@



\begin{example}[]
\label{exm:iq-model}
Let the random experiment consist of a person taking
an IQ test, and let \(X\) be the score on the test. The scores on such
a test are typically standardized to have a mean of 100 and a standard
deviation of 15, and IQ tests have (approximately and notoriously) a
bell-shaped distribution. What is \(\mathbb{P}(85\leq X\leq115)\)?
\end{example}

\textbf{Solution:} this one is easy because the limits 85 and 115 fall
exactly one standard deviation (below and above, respectively) from
the mean of 100. The answer is therefore approximately 68\%.



\subsection{Normal Quantiles and the Quantile Function} \label{sub:normal-quantiles-qf}


Until now we have been given two values and our task has been to find
the area under the PDF between those values. In this section, we go in
reverse: we are given an area, and we would like to find the value(s)
that correspond to that area.




\begin{example}[]
\label{exm:iq-quantile-state-problem}
Assuming the IQ model of Example~\ref{exm:iq-model}, what is the lowest possible IQ score that a person can
have and still be in the top 1\% of all IQ scores?  \textbf{Solution:} If a
person is in the top 1\%, then that means that 99\% of the people have
lower IQ scores. So, in other words, we are looking for a value \(x\)
such that \(F(x)=\mathbb{P}(X\leq x)\) satisfies \(F(x)=0.99\), or yet
another way to say it is that we would like to solve the equation
\(F(x)-0.99=0\). For the sake of argument, let us see how to do this
the long way. We define the function \(g(x)=F(x)-0.99\), and then look
for the root of \(g\) with the \texttt{uniroot} function. It uses numerical
procedures to find the root so we need to give it an interval of \(x\)
values in which to search for the root. We can get an educated guess
from the Empirical Rule, Fact~\ref{fac:empirical-rule}; the root should be
somewhere between two and three standard deviations (15 each) above
the mean (which is 100).
\end{example}

<<echo=TRUE>>=
g <- function(x) pnorm(x, mean = 100, sd = 15) - 0.99
uniroot(g, interval = c(130, 145))
@

<<echo=FALSE, include=FALSE>>=
temp <- round(uniroot(g, interval = c(130, 145))$root, 4)
@

The answer is shown in \texttt{\$root} which is approximately \Sexpr{temp}, that
is, a person with this IQ score or higher falls in the top 1\% of all
IQ scores.


The discussion in Example~\ref{exm:iq-quantile-state-problem} was
centered on the search for a value \(x\) that solved an equation
\(F(x)=p\), for some given probability \(p\), or in mathematical
parlance, the search for \(F^{-1}\), the inverse of the CDF of \(X\),
evaluated at \(p\). This is so important that it merits a definition
all its own.



\begin{defn}[Quantile function.]
  The \textit{quantile function}\footnote{The precise definition of
    the quantile function is
    \(Q_{X}(p)=\inf \{ x:\ F_{X}(x)\geq p \}\), so at least it is well
    defined (though perhaps infinite) for the values \(p=0\) and
    \(p=1\).} of a random variable \(X\) is the inverse of its
  cumulative distribution function:
\begin{equation}
Q_{X}(p)=\min\left\{ x:\ F_{X}(x)\geq p\right\} ,\quad 0 < p <1.
\end{equation}
\end{defn}




\begin{rem}[]
Here are some properties of quantile functions:

\begin{enumerate}
\item The quantile function is defined and finite for all \(0<p<1\).
\item \(Q_{X}\) is left-continuous (see Appendix~\ref{sec:differential-and-integral}). For discrete random variables
   it is a step function, and for continuous random variables it is a
   continuous function.
\item In the continuous case the graph of \(Q_{X}\) may be obtained by
   reflecting the graph of \(F_{X}\) about the line \(y=x\). In the
   discrete case, before reflecting one should: 1) connect the dots to
   get rid of the jumps -- this will make the graph look like a set of
   stairs, 2) erase the horizontal lines so that only vertical lines
   remain, and finally 3) swap the open circles with the solid
   dots. Please see Figure~\ref{fig:binom-plot-distr} for a comparison.
\item The two limits \[ \lim_{p\to0^{+}}Q_{X}(p)\quad \mbox{and}\quad
   \lim_{p\to1^{-}}Q_{X}(p) \] always exist, but may be infinite (that
   is, sometimes \(\lim_{p\to0}Q(p)=-\infty\) and/or
   \(\lim_{p\to1}Q(p)=\infty\)).
\end{enumerate}
\end{rem}

As the reader might expect, the standard normal distribution is a very
special case and has its own special notation.



\begin{defn}[Standard normal critical values.]
For \(0<\alpha<1\), the symbol \(z_{\alpha}\) denotes the unique
solution of the equation \(\mathbb{P} ( Z > z_{\alpha}) = \alpha\),
where \(Z \sim \mathsf{norm}(\mathtt{mean} = 0,\,\mathtt{sd} =
1)\). It can be calculated in one of two equivalent ways:
\(\mathtt{qnorm(} 1 - \alpha \mathtt{)}\) and \(\mathtt{qnorm(}
\alpha \mathtt{, lower.tail = FALSE)}\).
\end{defn}

There are a few other very important special cases which we will
encounter in later chapters.

\subsubsection{How to do it with \textsf{R}}

Quantile functions are defined for all of the base distributions with
the \texttt{q} prefix to the distribution name, except for the ECDF whose
quantile function is exactly the \(Q_{x}(p) = \mathsf{quantile}(x,
\mathtt{probs} = p, \mathtt{type} = 1)\) function.



\begin{example}[]
Back to Example~\ref{exm:iq-quantile-state-problem}, we are looking
for \(Q_{X}(0.99)\), where
\(X\sim\mathsf{norm}(\mathtt{mean}=100,\,\mathtt{sd}=15)\). It could
not be easier to do with \textsf{R}.
\end{example}

<<echo=TRUE>>=
qnorm(0.99, mean = 100, sd = 15)
@

Compare this answer to the one obtained earlier with \texttt{uniroot}.



\begin{example}[]
Find the values \(z_{0.025}\), \(z_{0.01}\), and \(z_{0.005}\) (these
will play an important role from Chapter~\ref{cha:estimation} onward).
\end{example}

<<echo=TRUE>>=
qnorm(c(0.025, 0.01, 0.005), lower.tail = FALSE)
@

Note the \texttt{lower.tail} argument. We would get the same answer with
\texttt{qnorm(c(0.975, 0.99, 0.995))}.


\section{Functions of Continuous Random Variables} \label{sec:functions-of-continuous}


The goal of this section is to determine the distribution of
\(U=g(X)\) based on the distribution of \(X\). In the discrete case
all we needed to do was back substitute for \(x=g^{-1}(u)\) in the PMF
of \(X\) (sometimes accumulating probability mass along the way). In
the continuous case, however, we need more sophisticated tools. Now
would be a good time to review Appendix~\ref{sec:differential-and-integral}.

\subsection{The PDF Method}


\begin{prop}[]
\label{prp:func-cont-rvs-pdf-formula}
Let \(X\) have PDF \(f_{X}\) and let
\(g\) be a function which is one-to-one with a differentiable inverse
\(g^{-1}\). Then the PDF of \(U=g(X)\) is given by
\begin{equation}
\label{eq:univ-trans-pdf-long}
f_{U}(u)=f_{X}\left[g^{-1}(u)\right]\ \left|\frac{\mathrm{d}}{\mathrm{d} u}g^{-1}(u)\right|.
\end{equation}
\end{prop}



\begin{rem}[]
The formula in Equation \eqref{eq:univ-trans-pdf-long} is nice, but does not
really make any sense. It is better to write in the intuitive form
\begin{equation}
\label{eq:univ-trans-pdf-short}
f_{U}(u)=f_{X}(x)\left|\frac{\mathrm{d} x}{\mathrm{d} u}\right|.
\end{equation}
\end{rem}




\begin{example}[]
\label{exm:lnorm-transformation}
Let
\(X\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\), and
let \(Y=\mathrm{e}^{X}\). What is the PDF of \(Y\)?  \textbf{Solution:}
Notice first that \(\mathrm{e}^{x}>0\) for any \(x\), so the support
of \(Y\) is \((0,\infty)\). Since the transformation is monotone, we
can solve \(y=\mathrm{e}^{x}\) for \(x\) to get \(x=\ln\, y\), giving
\(\mathrm{d} x/\mathrm{d} y=1/y\). Therefore, for any \(y>0\), \[
f_{Y}(y)=f_{X}(\ln
y)\cdot\left|\frac{1}{y}\right|=\frac{1}{\sigma\sqrt{2\pi}}\exp\left\{
\frac{(\ln y-\mu)^{2}}{2\sigma^{2}}\right\} \cdot\frac{1}{y}, \] where
we have dropped the absolute value bars since \(y>0\). The random
variable \(Y\) is said to have a \textit{lognormal distribution}; see
Section~\ref{sec:other-continuous-distributions}.
\end{example}




\begin{example}[]
\label{exm:lin-trans-norm}
Suppose
\(X\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) and let
\(Y=4-3X\). What is the PDF of \(Y\)?
\end{example}

The support of \(X\) is \((-\infty,\infty)\), and as \(x\) goes from
\(-\infty\) to \(\infty\), the quantity \(y=4-3x\) also traverses
\((-\infty,\infty)\). Solving for \(x\) in the equation \(y=4-3x\)
yields \(x=-(y-4)/3\) giving \(\mathrm{d} x/\mathrm{d} y=-1/3\). And
since \[ f_{X}(x)=\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-x^{2}/2}, \quad
-\infty < x < \infty , \] we have
\begin{eqnarray*}
f_{Y}(y) & = & f_{X}\left(\frac{y-4}{3}\right)\cdot\left|-\frac{1}{3}\right|,\quad -\infty < y < \infty,\\
 & = & \frac{1}{3\sqrt{2\pi}}\mathrm{e}^{-(y-4)^{2}/2\cdot3^{2}},\quad -\infty < y < \infty.
\end{eqnarray*}
We recognize the PDF of \(Y\) to be that of a
\(\mathsf{norm}(\mathtt{mean}=4,\,\mathtt{sd}=3)\)
distribution. Indeed, we may use an identical argument as the above to
prove the following fact:




\begin{fact}[]
If
\(X\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) and if
\(Y=a+bX\) for constants \(a\) and \(b\), with \(b\neq0\), then
\(Y\sim\mathsf{norm}(\mathtt{mean}=a+b\mu,\,\mathtt{sd}=|b|\sigma)\).
\label{fac:lin-trans-norm-is-norm}
\end{fact}

Note that it is sometimes easier to \textit{postpone} solving for the
inverse transformation \(x=x(u)\). Instead, leave the transformation
in the form \(u=u(x)\) and calculate the derivative of the
\textit{original} transformation
\begin{equation}
\mathrm{d} u/\mathrm{d} x=g'(x).
\end{equation}
Once this is known, we can get the PDF of \(U\) with
\begin{equation}
f_{U}(u)=f_{X}(x)\left|\frac{1}{\mathrm{d} u/\mathrm{d} x}\right|.
\end{equation}
In many cases there are cancellations and the work is shorter. Of
course, it is not always true that
\begin{equation}
\label{eq:univ-jacob-recip}
\frac{\mathrm{d} x}{\mathrm{d} u}=\frac{1}{\mathrm{d} u/\mathrm{d} x},
\end{equation}
but for the well-behaved examples in this book the trick works just fine.



\begin{rem}[]
In the case that \(g\) is not monotone we cannot apply Proposition~\ref{prp:func-cont-rvs-pdf-formula} directly. However, hope is not
lost. Rather, we break the support of \(X\) into pieces such that
\(g\) is monotone on each one. We apply Proposition~\ref{prp:func-cont-rvs-pdf-formula} on each piece, and finish up by
adding the results together.
\end{rem}

\subsection{The CDF method}

We know from Section~\ref{sec:continuous-random-variables} that
\(f_{X}=F_{X}'\) in the continuous case. Starting from the equation
\(F_{Y}(y)=\mathbb{P}(Y\leq y)\), we may substitute \(g(X)\) for
\(Y\), then solve for \(X\) to obtain \(\mathbb{P}[X\leq g^{-1}(y)]\),
which is just another way to write
\(F_{X}[g^{-1}(y)]\). Differentiating this last quantity with respect
to \(y\) will yield the PDF of \(Y\).



\begin{example}[]
Suppose \(X\sim\mathsf{unif}(\mathtt{min}=0,\,\mathtt{max}=1)\) and
suppose that we let \(Y=-\ln\, X\). What is the PDF of \(Y\)?
\end{example}

The support set of \(X\) is \((0,1),\) and \(y\) traverses
\((0,\infty)\) as \(x\) ranges from \(0\) to \(1\), so the support set
of \(Y\) is \(S_{Y}=(0,\infty)\). For any \(y>0\), we consider \[
F_{Y}(y)=\mathbb{P}(Y\leq y)=\mathbb{P}(-\ln\, X\leq
y)=\mathbb{P}(X\geq\mathrm{e}^{-y})=1-\mathbb{P}(X<\mathrm{e}^{-y}),
\] where the next to last equality follows because the exponential
function is \textit{monotone} (this point will be revisited
later). Now since \(X\) is continuous the two probabilities
\(\mathbb{P}(X<\mathrm{e}^{-y})\) and
\(\mathbb{P}(X\leq\mathrm{e}^{-y})\) are equal; thus
\[ 1-\mathbb{P}(X <
  \mathrm{e}^{-y})=1-\mathbb{P}(X\leq\mathrm{e}^{-y})=1-F_{X}(\mathrm{e}^{-y}).
\] Now recalling that the CDF of a
\(\mathsf{unif}(\mathtt{min}=0,\,\mathtt{max}=1)\) random variable
satisfies \(F(u)=u\) (see Equation \eqref{eq:unif-cdf}), we can say \[
F_{Y}(y)=1-F_{X}(\mathrm{e}^{-y})=1-\mathrm{e}^{-y},\quad \mbox{for
}y>0.  \] We have consequently found the formula for the CDF of \(Y\);
to obtain the PDF \(f_{Y}\) we need only differentiate \(F_{Y}\): \[
f_{Y}(y)=\frac{\mathrm{d}}{\mathrm{d}
y}\left(1-\mathrm{e}^{-y}\right)=0-\mathrm{e}^{-y}(-1), \] or
\(f_{Y}(y)=\mathrm{e}^{-y}\) for \(y>0\). This turns out to be a
member of the exponential family of distributions, see Section~\ref{sec:other-continuous-distributions}.




\begin{prop}[The Probability Integral Transform]
Given a continuous random
variable \(X\) with strictly increasing CDF \(F_{X}\), let the random
variable \(Y\) be defined by \(Y=F_{X}(X)\). Then the distribution of
\(Y\) is \(\mathsf{unif}(\mathtt{min}=0,\,\mathtt{max}=1)\).
\end{prop}



\begin{proof}
We employ the CDF method. First note that the support of \(Y\) is
\((0,1)\). Then for any \(0<y<1\), \[ F_{Y}(y)=\mathbb{P}(Y\leq
y)=\mathbb{P}(F_{X}(X)\leq y).  \] Now since \(F_{X}\) is strictly
increasing, it has a well defined inverse function
\(F_{X}^{-1}\). Therefore, \[ \mathbb{P}(F_{X}(X)\leq
y)=\mathbb{P}(X\leq F_{X}^{-1}(y))=F_{X}[F_{X}^{-1}(y)]=y.  \]
Summarizing, we have seen that \(F_{Y}(y)=y\), \(0<y<1\). But this is
exactly the CDF of a
\(\mathsf{unif}(\mathtt{min}=0,\,\mathtt{max}=1)\) random variable.
\end{proof}



\begin{fact}[]
The Probability Integral Transform is true for all continuous random
variables with continuous CDFs, not just for those with strictly
increasing CDFs (but the proof is more complicated). The transform is
\textit{not} true for discrete random variables, or for continuous random
variables having a discrete component (that is, with jumps in their
CDF).
\end{fact}



\begin{example}[]
\label{exm:distn-of-z-squared}
Let
\(Z\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) and let
\(U=Z^{2}\). What is the PDF of \(U\)?  Notice first that
\(Z^{2}\geq0\), and thus the support of \(U\) is \([0,\infty)\). And
for any \(u\geq0\), \[ F_{U}(u)=\mathbb{P}(U\leq
u)=\mathbb{P}(Z^{2}\leq u).  \] But \(Z^{2}\leq u\) occurs if and only
if \(-\sqrt{u}\leq Z\leq\sqrt{u}\). The last probability above is
simply the area under the standard normal PDF from \(-\sqrt{u}\) to
\(\sqrt{u}\), and since \(\phi\) is symmetric about 0, we have \[
\mathbb{P}(Z^{2}\leq u)=2\mathbb{P}(0\leq
Z\leq\sqrt{u})=2\left[F_{Z}(\sqrt{u})-F_{Z}(0)\right]=2\Phi(\sqrt{u})-1,
\] because \(\Phi(0)=1/2\). To find the PDF of \(U\) we differentiate
the CDF recalling that \(\Phi'= \phi\).  \[
f_{U}(u)=\left(2\Phi(\sqrt{u})-1\right)'=2\phi(\sqrt{u})\cdot\frac{1}{2\sqrt{u}}=u^{-1/2}\phi(\sqrt{u}).
\] Substituting,
\[ f_{U}(u) =
  u^{-1/2}\frac{1}{\sqrt{2\pi}}\,\mathrm{e}^{-(\sqrt{u})^{2}/2}=(2\pi
  u)^{-1/2}\mathrm{e}^{-u/2},\quad u > 0.  \] This is what we will
later call a \textit{chi-square distribution with 1 degree of
  freedom}. See Section~\ref{sec:other-continuous-distributions}.
\end{example}

\subsubsection{How to do it with \textsf{R}}

The \texttt{distr} package \cite{distr} has functionality to investigate
transformations of univariate distributions. There are exact results
for ordinary transformations of the standard distributions, and
\texttt{distr} takes advantage of these in many cases. For instance, the
\texttt{distr} package can handle the transformation in Example~\ref{exm:lin-trans-norm} quite nicely:

<<echo=TRUE, warning=FALSE>>=
X <- Norm(mean = 0, sd = 1)
Y <- 4 - 3*X
Y
@

So \texttt{distr} ``knows'' that a linear transformation of a normal random
variable is again normal, and it even knows what the correct \texttt{mean}
and \texttt{sd} should be. But it is impossible for \texttt{distr} to know
everything, and it is not long before we venture outside of the
transformations that \texttt{distr} recognizes. Let us try Example~\ref{exm:lnorm-transformation}:

<<echo=TRUE, warning=FALSE>>=
Y <- exp(X)
Y
@


The result is an object of class \texttt{AbscontDistribution}, which
is one of the classes that \texttt{distr} uses to denote general
distributions that it does not recognize (it turns out that \(Z\) has
a lognormal distribution; see Section~\ref{sec:other-continuous-distributions}). A simplified description
of the process that \texttt{distr} undergoes when it encounters a
transformation \(Y=g(X)\) that it does not recognize is

\begin{enumerate}
\item Randomly generate many, many copies \(X_{1}\), \(X_{2}\), \ldots,
   \(X_{n}\) from the distribution of \(X\),
\item Compute \(Y_{1}=g(X_{1})\), \(Y_{2}=g(X_{2})\), \ldots,
   \(Y_{n}=g(X_{n})\) and store them for use.
\item Calculate the PDF, CDF, quantiles, and random variates using the
   simulated values of \(Y\).
\end{enumerate}

As long as the transformation is sufficiently nice, such as a linear
transformation, the exponential, absolute value, \textit{etc}., the
\texttt{d-p-q} functions are calculated analytically based on the
\texttt{d-p-q} functions associated with \(X\). But if we try a crazy
transformation then we are greeted by a warning:

<<echo=TRUE, warning=FALSE>>=
W <- sin(exp(X) + 27)
W
@

The warning (not shown here) confirms that the \texttt{d-p-q}
functions are not calculated analytically, but are instead based on
the randomly simulated values of \(Y\). \textit{We must be careful to
  remember this.} The nature of random simulation means that we can
get different answers to the same question: watch what happens when we
compute \(\mathbb{P}(W\leq0.5)\) using the \(W\) above, then define
\(W\) again, and compute the (supposedly) same
\(\mathbb{P}(W\leq0.5)\) a few moments later.

<<echo=TRUE>>=
p(W)(0.5)
W <- sin(exp(X) + 27)
p(W)(0.5)
@

The answers are not the same! Furthermore, if we were to repeat the
process we would get yet another answer for \(\mathbb{P}(W\leq0.5)\).

The answers were close, though. And the underlying randomly generated
\(X\)'s were not the same so it should hardly be a surprise that the
calculated \(W\)'s were not the same, either. This serves as a warning
(in concert with the one that \texttt{distr} provides) that we should be
careful to remember that complicated transformations computed by
R are only approximate and may fluctuate slightly due to
the nature of the way the estimates are calculated.

\section{Other Continuous Distributions} \label{sec:other-continuous-distributions}


\subsection{Waiting Time Distributions} \label{sub:waiting-time-distributions}

In some experiments, the random variable being measured is the time
until a certain event occurs. For example, a quality control
specialist may be testing a manufactured product to see how long it
takes until it fails. An efficiency expert may be recording the
customer traffic at a retail store to streamline scheduling of staff.

\subsubsection{The Exponential Distribution} \label{sub:the-exponential-distribution}


We say that \(X\) has an \textit{exponential distribution} and write
\(X\sim\mathsf{exp}(\mathtt{rate}=\lambda)\).
\begin{equation}
f_{X}(x)=\lambda\mathrm{e}^{-\lambda x},\quad x>0
\end{equation}
The associated \textsf{R} functions are \texttt{dexp(x, rate = 1)},
\texttt{pexp}, \texttt{qexp}, and \texttt{rexp}, which give the PDF,
CDF, quantile function, and simulate random variates, respectively.

The parameter \(\lambda\) measures the rate of arrivals (to be
described later) and must be positive. The CDF is given by the formula
\begin{equation}
F_{X}(t)=1-\mathrm{e}^{-\lambda t},\quad t>0.
\end{equation}
The mean is \(\mu=1/\lambda\) and the variance is
\(\sigma^{2}=1/\lambda^{2}\).

The exponential distribution is closely related to the Poisson
distribution. If customers arrive at a store according to a Poisson
process with rate \(\lambda\) and if \(Y\) counts the number of
customers that arrive in the time interval \([0,t)\), then we saw in
Section~\ref{sec:other-discrete-distributions} that \(Y \sim
\mathsf{pois}(\mathtt{lambda}=\lambda t).\) Now consider a different
question: let us start our clock at time 0 and stop the clock when the
first customer arrives. Let \(X\) be the length of this random time
interval. Then \(X\sim\mathsf{exp}(\mathtt{rate}=\lambda)\). Observe
the following string of equalities:
\begin{align*}
\mathbb{P}(X>t) & =\mathbb{P}(\mbox{first arrival after time \emph{t}}),\\
 & =\mathbb{P}(\mbox{no events in [0,\emph{t})}),\\
 & =\mathbb{P}(Y=0),\\
 & =\mathrm{e}^{-\lambda t},
\end{align*}
where the last line is the PMF of \(Y\) evaluated at \(y=0\). In other
words, \(\mathbb{P}(X\leq t)=1-\mathrm{e}^{-\lambda t}\), which is
exactly the CDF of an \(\mathsf{exp}(\mathtt{rate}=\lambda)\)
distribution.

The exponential distribution is said to be \textit{memoryless} because
exponential random variables ``forget'' how old they are at every
instant. That is, the probability that we must wait an additional five
hours for a customer to arrive, given that we have already waited
seven hours, is exactly the probability that we needed to wait five
hours for a customer in the first place. In mathematical symbols, for
any \(s,\, t>0\),
\begin{equation}
\mathbb{P}(X>s+t\,|\, X>t)=\mathbb{P}(X>s).
\end{equation}
See Exercise~\ref{exr:prove-the-memoryless}.

\subsubsection{The Gamma Distribution} \label{sub:the-gamma-distribution}


This is a generalization of the exponential distribution. We say that
\(X\) has a gamma distribution and write
\(X\sim\mathsf{gamma}(\mathtt{shape}=\alpha,\,\mathtt{rate}=\lambda)\). It
has PDF
\begin{equation}
f_{X}(x)=\frac{\lambda^{\alpha}}{\Gamma(\alpha)}\: x^{\alpha-1}\mathrm{e}^{-\lambda x},\quad x>0.
\end{equation}

The associated \textsf{R} functions are \texttt{dgamma(x, shape, rate = 1)}, \texttt{pgamma}, \texttt{qgamma}, and \texttt{rgamma}, which give the PDF, CDF,
quantile function, and simulate random variates, respectively. If
\(\alpha=1\) then \(X\sim\mathsf{exp}(\mathtt{rate}=\lambda)\). The
mean is \(\mu=\alpha/\lambda\) and the variance is
\(\sigma^{2}=\alpha/\lambda^{2}\).

To motivate the gamma distribution recall that if \(X\) measures the
length of time until the first event occurs in a Poisson process with
rate \(\lambda\) then \(X\sim\mathsf{exp}(\mathtt{rate}=\lambda)\). If
we let \(Y\) measure the length of time until the
\(\alpha^{\mathrm{th}}\) event occurs then
\(Y\sim\mathsf{gamma}(\mathtt{shape}=\alpha,\,\mathtt{rate}=\lambda)\). When
\(\alpha\) is an integer this distribution is also known as the
\textit{Erlang} distribution.



\begin{example}[]
  At a car wash, two customers arrive per hour on the average. We
  decide to measure how long it takes until the third customer
  arrives. If \(Y\) denotes this random time then
  \(Y\sim\mathsf{gamma}(\mathtt{shape}=3,\,\mathtt{rate}=2)\).
\end{example}

\subsection{The Chi square, Student's t, and Snedecor's \(F\) Distributions} \label{sub:the-chi-Square-t-f}


\subsubsection{The Chi square Distribution} \label{sub:the-chi-square}


A random variable \(X\) with PDF
\begin{equation}
f_{X}(x)=\frac{1}{\Gamma(p/2)2^{p/2}}x^{p/2-1}\mathrm{e}^{-x/2},\quad x>0,
\end{equation}
is said to have a \textit{chi-square distribution} with \(p\)
\textit{degrees of freedom}. We write
\(X\sim\mathsf{chisq}(\mathtt{df}=p)\). The associated \textsf{R} functions are
\texttt{dchisq(x, df)}, \texttt{pchisq}, \texttt{qchisq}, and
\texttt{rchisq}, which give the PDF, CDF, quantile function, and
simulate random variates, respectively. See Figure~\ref{fig:chisq-dist-vary-df}. In an obvious notation we may define
\(\chi_{\alpha}^{2}(p)\) as the number on the \(x\)-axis such that
there is exactly \(\alpha\) area under the
\(\mathsf{chisq}(\mathtt{df}=p)\) curve to its right.

The code to produce Figure~\ref{fig:chisq-dist-vary-df} is

<<chisq-dist-vary-df, echo=FALSE, fig=TRUE, include=FALSE, width=5, height=3.25>>=
curve(dchisq(x, df = 3), from = 0, to = 20, ylab = "y")
ind <- c(4, 5, 10, 15)
for (i in ind) curve(dchisq(x, df = i), 0, 20, add = TRUE)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-chisq-dist-vary-df}
\end{center}
\caption{{\small The chi square distribution for various degrees of freedom.}}
\label{fig:chisq-dist-vary-df}
\end{figure}



\begin{rem}[]
Here are some useful things to know about the chi-square distribution.

\begin{enumerate}
\item If \(Z\sim\mathtt{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\), then
   \(Z^{2}\sim\mathsf{chisq}(\mathtt{df}=1)\). We saw this in Example~\ref{exm:distn-of-z-squared}, and the fact is important when it
   comes time to find the distribution of the sample variance,
   \(S^{2}\). See Theorem~\ref{thm:xbar-and-s} in Section~\ref{sub:samp-var-dist}.
\item The chi-square distribution is supported on the positive
   \(x\)-axis, with a right-skewed distribution.
\item The \(\mathsf{chisq}(\mathtt{df}=p)\) distribution is the same as a
   \(\mathsf{gamma}(\mathtt{shape}=p/2,\,\mathtt{rate}=1/2)\)
   distribution.
\item The MGF of \(X\sim\mathsf{chisq}(\mathtt{df}=p)\) is
   \begin{equation}
   \label{eq:mgf-chisq}
   M_{X}(t)=\left(1-2t\right)^{-p},\quad t < 1/2.
   \end{equation}
\end{enumerate}
\end{rem}

\subsubsection{Student's t distribution} \label{sub:students-t-distribution}

A random variable \(X\) with PDF
\begin{equation}
f_{X}(x) = \frac{\Gamma\left[ (r+1)/2\right] }{\sqrt{r\pi}\,\Gamma(r/2)}\left( 1 + \frac{x^{2}}{r} \right)^{-(r+1)/2},\quad -\infty < x < \infty
\end{equation}
is said to have \textit{Student's} \(t\) distribution with \(r\)
\textit{degrees of freedom}, and we write
\(X\sim\mathsf{t}(\mathtt{df}=r)\). The associated \textsf{R} functions are
\texttt{dt},\texttt{pt}, \texttt{qt}, and \texttt{rt}, which give the
PDF, CDF, quantile function, and simulate random variates,
respectively. See Section~\ref{sec:sampling-from-normal-dist}.

\subsubsection{Snedecor's F distribution} \label{sub:snedecor-F-distribution}


A random variable \(X\) with PDF
\begin{equation}
f_{X}(x)=\frac{\Gamma[(m+n)/2]}{\Gamma(m/2)\Gamma(n/2)}\left(\frac{m}{n}\right)^{m/2}x^{m/2-1}\left(1+\frac{m}{n}x\right)^{-(m+n)/2},\quad x>0.
\end{equation}
is said to have an \(F\) distribution with \((m,n)\) degrees of
freedom. We write
\(X\sim\mathsf{f}(\mathtt{df1}=m,\,\mathtt{df2}=n)\). The associated
R functions are \texttt{df(x, df1, df2)}, \texttt{pf}, \texttt{qf}, and \texttt{rf},
which give the PDF, CDF, quantile function, and simulate random
variates, respectively. We define \(F_{\alpha}(m,n)\) as the number on
the \(x\)-axis such that there is exactly \(\alpha\) area under the
\(\mathsf{f}(\mathtt{df1}=m,\,\mathtt{df2}=n)\) curve to its right.



\begin{rem}[]
Here are some notes about the \(F\) distribution.

\begin{enumerate}
\item If \(X\sim\mathsf{f}(\mathtt{df1}=m,\,\mathtt{df2}=n)\) and
   \(Y=1/X\), then
   \(Y\sim\mathsf{f}(\mathtt{df1}=n,\,\mathtt{df2}=m)\). Historically,
   this fact was especially convenient. In the old days, statisticians
   used printed tables for their statistical calculations. Since the
   \(F\) tables were symmetric in \(m\) and \(n\), it meant that
   publishers could cut the size of their printed tables in half. It
   plays less of a role today now that personal computers are
   widespread.
\item If \(X\sim\mathsf{t}(\mathtt{df}=r)\), then
   \(X^{2}\sim\mathsf{f}(\mathtt{df1}=1,\,\mathtt{df2}=r)\). We will
   see this again in Section~\ref{sub:slr-overall-f-statistic}.
\end{enumerate}
\end{rem}

\subsection{Other Popular Distributions} \label{sub:other-popular-distributions}


\subsubsection{The Cauchy Distribution} \label{sub:the-cauchy-distribution}


This is a special case of the Student's \(t\) distribution. It has PDF
\begin{equation}
f_{X}(x) = \frac{1}{\beta\pi} \left[ 1+\left( \frac{x-m}{\beta} \right)^{2} \right]^{-1},\quad -\infty < x < \infty.
\end{equation}
We write \(X \sim \mathsf{cauchy}(\mathtt{location} =
m,\,\mathtt{scale} = \beta)\). The associated \textsf{R} function
is \texttt{dcauchy(x, location = 0, scale = 1)}.

It is easy to see that a \(\mathsf{cauchy}(\mathtt{location} =
0,\,\mathtt{scale} = 1)\) distribution is the same as a
\(\mathsf{t}(\mathtt{df}=1)\) distribution. The \(\mathsf{cauchy}\)
distribution looks like a \(\mathsf{norm}\) distribution but with very
heavy tails. The mean (and variance) do not exist, that is, they are
infinite. The median is represented by the \(\mathtt{location}\)
parameter, and the \(\mathtt{scale}\) parameter influences the spread
of the distribution about its median.

\subsubsection{The Beta Distribution} \label{sub:the-beta-distribution}


This is a generalization of the continuous uniform distribution.
\begin{equation}
f_{X}(x)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\, x^{\alpha-1}(1-x)^{\beta-1},\quad 0 < x < 1.
\end{equation}
We write
\(X\sim\mathsf{beta}(\mathtt{shape1}=\alpha,\,\mathtt{shape2}=\beta)\). The
associated \textsf{R} function is \texttt{dbeta(x, shape1, shape2)}. The
mean and variance are
\begin{equation}
\mu=\frac{\alpha}{\alpha+\beta}\mbox{ and }\sigma^{2}=\frac{\alpha\beta}{\left(\alpha+\beta\right)^{2}\left(\alpha+\beta+1\right)}.
\end{equation}
See Example~\ref{exm:cont-pdf3x2}. This distribution comes up a lot in Bayesian
statistics because it is a good model for one's prior beliefs about a
population proportion \(p\), \(0\leq p\leq1\).

\subsubsection{The Logistic Distribution} \label{sub:the-logistic-distribution}


\begin{equation}
f_{X}(x)=\frac{1}{\sigma}\exp\left(-\frac{x-\mu}{\sigma}\right)\left[1+\exp\left(-\frac{x-\mu}{\sigma}\right)\right]^{-2},\quad -\infty < x < \infty.
\end{equation}
We write
\(X\sim\mathsf{logis}(\mathtt{location}=\mu,\,\mathtt{scale}=\sigma)\). The
associated \textsf{R} function is \texttt{dlogis(x, location = 0, scale = 1)}. The logistic distribution comes up in differential equations as a
model for population growth under certain assumptions. The mean is
\(\mu\) and the variance is \(\pi^{2}\sigma^{2}/3\).

\subsubsection{The Lognormal Distribution} \label{sub:the-lognormal-distribution}


This is a distribution derived from the normal distribution (hence the
name). If
\(U\sim\mathtt{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\), then
\(X = \mathrm{e}^{U}\) has PDF
\begin{equation}
f_{X}(x)=\frac{1}{\sigma x\sqrt{2\pi}}\exp\left[\frac{-(\ln x-\mu)^{2}}{2\sigma^{2}}\right], \quad 0 < x < \infty.
\end{equation}
We write
\(X\sim\mathsf{lnorm}(\mathtt{meanlog}=\mu,\,\mathtt{sdlog}=\sigma)\). The
associated \textsf{R} function is \texttt{dlnorm(x, meanlog = 0, sdlog = 1)}. Notice that the support is concentrated on the positive \(x\)
axis; the distribution is right-skewed with a heavy tail. See Example~\ref{exm:lnorm-transformation}.

\subsubsection{The Weibull Distribution} \label{sub:the-weibull-distribution}


This has PDF
\begin{equation}
f_{X}(x)=\frac{\alpha}{\beta}\left(\frac{x}{\beta}\right)^{\alpha-1}\exp\left(\frac{x}{\beta}\right)^{\alpha},\quad x>0.
\end{equation}
We write
\(X\sim\mathsf{weibull}(\mathtt{shape}=\alpha,\,\mathtt{scale}=\beta)\). The
associated \textsf{R} function is \texttt{dweibull(x, shape, scale = 1)}.

\subsubsection{How to do it with \textsf{R}}

There is some support of moments and moment generating functions for
some continuous probability distributions included in the \texttt{actuar}
package \cite{actuar}. The convention is \texttt{m} in front of the
distribution name for raw moments, and \texttt{mgf} in front of the
distribution name for the moment generating function. At the time of
this writing, the following distributions are supported: gamma,
inverse Gaussian, (non-central) chi-squared, exponential, and uniform.



\begin{example}[]
Calculate the first four raw moments for
\(X\sim\mathsf{gamma}(\mathtt{shape}=13,\,\mathtt{rate}=1)\) and plot
the moment generating function.
\end{example}

We load the \texttt{actuar} package and use the functions \texttt{mgamma} and
\texttt{mgfgamma}:

<<echo=TRUE>>=
mgamma(1:4, shape = 13, rate = 1)
@

For the plot we can use the function in the following form:

<<gamma-mgf, echo=FALSE, fig=TRUE, include=FALSE, height=3,width=5>>=
plot(function(x){mgfgamma(x, shape = 13, rate = 1)},
     from=-0.1, to=0.1, ylab = "gamma mgf")
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-gamma-mgf}
\end{center}
\caption{{\small A plot of the \textsf{gamma}(\texttt{shape} = 13, \texttt{rate} = 1) MGF.}}
\label{fig:gamma-mgf}
\end{figure}



\section{Chapter Exercises}

\begin{Exercise}[]
Find the constant \(C\) so that the given function is a valid PDF of a random variable \(X\).

\begin{enumerate}
\item \(f(x) = Cx^{n},\quad 0 < x <1\).
\item \(f(x) = Cx\mathrm{e}^{-x},\quad 0 < x < \infty\).
\item \(f(x) = \mathrm{e}^{-(x - C)}, \quad 7 < x < \infty.\)
\item \(f(x) = Cx^{3}(1 - x)^{2},\quad 0 < x < 1.\)
\item \(f(x) = C(1 + x^{2}/4)^{-1}, \quad -\infty < x < \infty.\)
\end{enumerate}

\end{Exercise}




\begin{Exercise}[]
For the following random experiments, decide what the distribution of
\(X\) should be. In nearly every case, there are additional
assumptions that should be made for the distribution to apply;
identify those assumptions (which may or may not strictly hold in
practice).

\begin{enumerate}
\item We throw a dart at a dart board. Let \(X\) denote the squared
   linear distance from the bulls-eye to the where the dart landed.
\item We randomly choose a textbook from the shelf at the bookstore and
   let \(P\) denote the proportion of the total pages of the book
   devoted to exercises.
\item We measure the time it takes for the water to completely drain out
   of the kitchen sink.
\item We randomly sample strangers at the grocery store and ask them how
   long it will take them to drive home.
\end{enumerate}

\end{Exercise}




\begin{Exercise}[]
If \(Z\) is \(\mathsf{norm}(\mathtt{mean} = 0,\,\mathtt{sd} = 1)\), find

\begin{enumerate}
\item \(\mathbb{P}(Z > 2.64)\)
\item \(\mathbb{P}(0 \leq Z < 0.87)\)
\item \(\mathbb{P}(|Z| > 1.39)\) (Hint: draw a picture!)
\end{enumerate}

\end{Exercise}




\begin{Exercise}[]
A certain type of zombie crossbow shoots bolts at 312 feet per second (fps) on average, with a standard deviation of 15fps. Let \(X\) represent the shooting speed of a randomly selected bolt. Assuming that shooting speed is normally distributed, find

\begin{enumerate}
\item The distribution of \(X\). This should include the distribution name and numerical value(s) of any parameter(s).
\item Find \(\mathbb{P}(X \geq 310.41)\).
\item Find \(\mathbb{P}(303.53 < X leq 310.58)\).
\item Find the mean, \(\mathbb{E}X = \mu\).
\item How fast does a bolt in the top 15\% of bolts (for this type) shoot?
\end{enumerate}

\end{Exercise}



\begin{Exercise}[label=exr:variance-dunif]
  Calculate the variance of
  \(X\sim\mathsf{unif}(\mathtt{min}=a,\,\mathtt{max}=b)\). \textit{Hint:}
  First calculate \(\mathbb{E} X^{2}\).
\end{Exercise}





\begin{Exercise}[label=exr:prove-the-memoryless]
Prove the memoryless property for
exponential random variables. That is, for \(X \sim
\mathsf{exp}(\mathtt{rate} = \lambda)\) show that for any \(s,t > 0\),
\[ \mathbb{P}(X > s + t\,|\, X > t) = \mathbb{P}(X > s).  \]

\end{Exercise}




\chapter{Multivariate Distributions} \label{cha:multivariable-distributions}


<<echo=FALSE, include=FALSE>>=
# This chapter's package dependencies
library(ggplot2)
library(grid)
library(lattice)
library(mvtnorm)
library(prob)
@


We have built up quite a catalogue of distributions, discrete and
continuous. They were all univariate, however, meaning that we only
considered one random variable at a time. We can imagine nevertheless
many random variables associated with a single person: their height,
their weight, their wrist circumference (all continuous), or their
eye/hair color, shoe size, whether they are right handed, left handed,
or ambidextrous (all categorical), and we can even surmise reasonable
probability distributions to associate with each of these variables.

But there is a difference: for a single person, these variables are
related. For instance, a person's height betrays a lot of information
about that person's weight.

The concept we are hinting at is the notion of \textit{dependence}
between random variables. It is the focus of this chapter to study
this concept in some detail. Along the way, we will pick up additional
models to add to our catalogue. Moreover, we will study certain
classes of dependence, and clarify the special case when there is no
dependence, namely, independence.

The interested reader who would like to learn more about any of the
below mentioned multivariate distributions should take a look at
\textit{Discrete Multivariate Distributions} by Johnson \textit{et al}
\cite{Johnson1997} or \textit{Continuous Multivariate Distributions}
\cite{Kotz2000} by Kotz \textit{et al}.

\paragraph{What do I want them to know?}

\begin{itemize}
\item the basic notion of dependence and how it is manifested with
  multiple variables (two, in particular)
\item joint versus marginal distributions/expectation (discrete and
  continuous)
\item some numeric measures of dependence
\item conditional distributions, in the context of independence and
  exchangeability
\item some details of at least one multivariate model (discrete and
  continuous)
\item what it looks like when there are more than two random variables
  present
\end{itemize}

\section{Joint and Marginal Probability Distributions} \label{sec:joint-probability-distributions}

Consider two discrete random variables \(X\) and \(Y\) with PMFs
\(f_{X}\) and \(f_{Y}\) that are supported on the sample spaces
\(S_{X}\) and \(S_{Y}\), respectively. Let \(S_{X,Y}\) denote the set
of all possible observed \textit{pairs} \((x,y)\), called the
\textit{joint support set} of \(X\) and \(Y\). Then the \textit{joint
  probability mass function} of \(X\) and \(Y\) is the function
\(f_{X,Y}\) defined by
\begin{equation}
\label{eq:joint-pmf}
f_{X,Y}(x,y)=\mathbb{P}(X=x,\, Y=y),\quad \mbox{for }(x,y)\in S_{X,Y}.
\end{equation}
Every joint PMF satisfies
\begin{equation}
f_{X,Y}(x,y)>0\mbox{ for all }(x,y)\in S_{X,Y},
\end{equation}
and
\begin{equation}
\sum_{(x,y)\in S_{X,Y}}f_{X,Y}(x,y)=1.
\end{equation}
It is customary to extend the function \(f_{X,Y}\) to be defined on
all of \(\mathbb{R}^{2}\) by setting \(f_{X,Y}(x,y)=0\) for
\((x,y)\not\in S_{X,Y}\).

In the context of this chapter, the PMFs \(f_{X}\) and \(f_{Y}\) are
called the \textit{marginal PMFs} of \(X\) and \(Y\), respectively. If we are
given only the joint PMF then we may recover each of the marginal PMFs
by using the Theorem of Total Probability (see Equation
\eqref{eq:theorem-total-probability}): observe
\begin{eqnarray}
f_{X}(x) & = & \mathbb{P}(X=x),\\
 & = & \sum_{y\in S_{Y}}\mathbb{P}(X=x,\, Y=y),\\
 & = & \sum_{y\in S_{Y}}f_{X,Y}(x,y).
\end{eqnarray}
By interchanging the roles of \(X\) and \(Y\) it is clear that
\begin{equation}
\label{eq:marginal-pmf}
f_{Y}(y)=\sum_{x\in S_{Y}}f_{X,Y}(x,y).
\end{equation}
Given the joint PMF we may recover the marginal PMFs, but the converse
is not true. Even if we have \textit{both} marginal distributions they are
not sufficient to determine the joint PMF; more information is
needed\footnote{We are not at a total loss, however. There are Frechet
bounds which pose limits on how large (and small) the joint
distribution must be at each point.}.


Associated with the joint PMF is the \textit{joint cumulative
  distribution function} \(F_{X,Y}\) defined by
\[ F_{X,Y}(x,y)=\mathbb{P}(X\leq x,\, Y\leq y),\quad \mbox{for
  }(x,y)\in\mathbb{R}^{2}.  \] The bivariate joint CDF is not quite as
tractable as the univariate CDFs, but in principle we could calculate
it by adding up quantities of the form in Equation
\eqref{eq:joint-pmf}. The joint CDF is typically not used in practice
due to its inconvenient form; one can usually get by with the joint
PMF alone.

We now introduce some examples of bivariate discrete
distributions. The first we have seen before, and the second is based
on the first.




\begin{example}[]
\label{exm:toss-two-dice-joint-pmf}
Roll a fair die twice. Let \(X\) be the face shown on the first roll,
and let \(Y\) be the face shown on the second roll. We have already
seen this example in Chapter~\ref{cha:probability}, Example~\ref{exm:toss-a-six-sided-die-twice}. For this example, it suffices to define
\[ f_{X,Y}(x,y)=\frac{1}{36},\quad x=1,\ldots,6,\ y=1,\ldots,6.  \]
The marginal PMFs are given by \(f_{X}(x)=1/6\), \(x=1,2,\ldots,6\),
and \(f_{Y}(y)=1/6\), \(y=1,2,\ldots,6\), since
\[ f_{X}(x)=\sum_{y=1}^{6}\frac{1}{36}=\frac{1}{6},\quad
  x=1,\ldots,6, \] and the same computation with the letters switched
works for \(Y\).
\end{example}


In the previous example, and in many other ones, the joint support can be written as a product set of the support of \(X\) ``times'' the support of \(Y\), that is, it may be represented as a cartesian product set, or rectangle, \(S_{X,Y}=S_{X}\times S_{Y}\), where \(S_{X} \times S_{Y}= \{ (x,y):\ x\in S_{X},\, y\in S_{Y} \}\). As we
shall see presently in Section~\ref{sec:independent-random-variables}, this form is a necessary condition for \(X\) and \(Y\) to be \textit{independent} (or alternatively \textit{exchangeable} when \(S_{X}=S_{Y}\)). But please note that in general it is not required for \(S_{X,Y}\) to be of rectangle form. We next investigate just such an example.



\begin{example}[]
\label{exm:max-sum-two-dice}
Let the random experiment again be to roll a fair die twice, except
now let us define the random variables \(U\) and \(V\) by
\begin{eqnarray*}
U & = & \mbox{the maximum of the two rolls, and }\\
V & = & \mbox{the sum of the two rolls.}
\end{eqnarray*}
We see that the support of \(U\) is \(S_{U}= \{ 1,2,\ldots,6 \}\) and
the support of \(V\) is \(S_{V}= \{ 2,3,\ldots,12 \}\). We may
represent the sample space with a matrix, and for each entry in the
matrix we may calculate the value that \(U\) assumes. The result is in
the left half of Table~\ref{tab:max-and-sum-two-dice}.
\end{example}


\begin{table}[ht]
\centering
\subfloat[Subtable 1 list of tables text][\(U = \text{max}(X_{1},X_{2})\)]{
\begin{tabular}{c|cccccc}
First & \multicolumn{6}{c}{Second roll}\tabularnewline
roll & 1 & 2 & 3 & 4 & 5 & 6\tabularnewline
\hline
1 & 1 & 2 & 3 & 4 & 5 & 6\tabularnewline
2 & 2 & 2 & 3 & 4 & 5 & 6\tabularnewline
3 & 3 & 3 & 3 & 4 & 5 & 6\tabularnewline
4 & 4 & 4 & 4 & 4 & 5 & 6\tabularnewline
5 & 5 & 5 & 5 & 5 & 5 & 6\tabularnewline
6 & 6 & 6 & 6 & 6 & 6 & 6\tabularnewline
\end{tabular}}
\qquad
\subfloat[Subtable 2 list of tables text][\(V = X_{1} + X_{2}\)]{
\begin{tabular}{c|cccccc}
First & \multicolumn{6}{c}{Second roll}\tabularnewline
roll & 1 & 2 & 3 & 4 & 5 & 6\tabularnewline
\hline
1 & 2 & 3 & 4 & 5 & 6 & 7\tabularnewline
2 & 3 & 4 & 5 & 6 & 7 & 8\tabularnewline
3 & 4 & 5 & 6 & 7 & 8 & 9\tabularnewline
4 & 5 & 6 & 7 & 8 & 9 & 10\tabularnewline
5 & 6 & 7 & 8 & 9 & 10 & 11\tabularnewline
6 & 7 & 8 & 9 & 10 & 11 & 12\tabularnewline
\end{tabular}}
\caption[Rolling two dice.]{{\small Rolling two dice. The value of \(U\) is the maximum of the two rolls, while the value of \(V\) is the sum of the two rolls.}}
\label{tab:max-and-sum-two-dice}
\end{table}


We can use the table to calculate the marginal PMF of \(U\), because from Example~\ref{exm:toss-a-six-sided-die-twice} we know that each entry in the matrix has probability \(1/36\) associated with it. For instance, there is only one outcome in the matrix with \(U=1\), namely, the bottom left corner. This single entry has probability \(1/36\), therefore, it must be that \(f_{U}(1)=\mathbb{P}(U=1)=1/36\). Similarly we see that there are three entries in the matrix with \(U=2\), thus \(f_{U}(2)=3/36\). Continuing in this fashion we will find the marginal distribution of \(U\) may be written
\begin{equation}
f_{U}(u)=\frac{2u-1}{36},\quad u=1,\,2,\ldots,6.
\end{equation}
We may do a similar thing for \(V\); see the right half of Table~\ref{tab:max-and-sum-two-dice}. Collecting all of the probability we
will find that the marginal PMF of \(V\) is
\begin{equation}
f_{V}(v)=\frac{6-|v-7|}{36},\quad v=2,\,3,\ldots,12.
\end{equation}

We may collapse the two matrices from Figure~\ref{fig:max-and-sum-two-dice} into one, big matrix of pairs of values \((u,v)\). The result is shown in Table~\ref{tab:max-sum-two-dice-joint}.


\begin{table}
\begin{centering}
\begin{tabular}{c|cccccc}
First & \multicolumn{6}{c}{Second roll}\tabularnewline
Roll & 1 & 2 & 3 & 4 & 5 & 6\tabularnewline
\hline
1 & (1,2) & (2,3) & (3,4) & (4,5) & (5,6) & (6,7)\tabularnewline
2 & (2,3) & (2,4) & (3,5) & (4,6) & (5,7) & (6,8)\tabularnewline
3 & (3,4) & (3,5) & (3,6) & (4,7) & (5,8) & (6,9)\tabularnewline
4 & (4,5) & (4,6) & (4,7) & (4,8) & (5,9) & (6,10)\tabularnewline
5 & (5,6) & (5,7) & (5,8) & (5,9) & (5,10) & (6,11)\tabularnewline
6 & (6,7) & (6,8) & (6,9) & (6,10) & (6,11) & (6,12)\tabularnewline
\end{tabular}
\par\end{centering}\
\caption[Rolling two dice, again.]{{\small Rolling two dice. Joint values of \(U\) and \(V\) are shown as pairs, for each outcome in the sample space.}}
\label{tab:max-sum-two-dice-joint}
\end{table}


Again, each of these pairs has probability \(1/36\) associated with it and we are looking at the joint PDF of \((U,V)\) albeit in an unusual form. Many of the pairs are repeated, but some of them are not: \((1,2)\) appears only once, but \((2,3)\) appears twice. We can make more sense out of this by writing a new table with \(U\) on one side and \(V\) along the top. We will accumulate the probability just like we did in Example~\ref{exm:toss-two-dice-joint-pmf}. See Table~\ref{tab:max-sum-joint-pmf}.

\begin{table}
\begin{centering}
\begin{tabular}{|c|ccccccccccc|c|}
\hline
 & {\small 2} & {\small 3} & {\small 4} & {\small 5} & {\small 6} & {\small 7} & {\small 8} & {\small 9} & {\small 10} & {\small 11} & {\small 12} & {\small Total}\tabularnewline
\hline
{\small 1} & {\small $\frac{1}{36}$} &  &  &  &  &  &  &  &  &  &  & {\small $1/36$}\tabularnewline
{\small 2} &  & {\small $\frac{2}{36}$} & {\small $\frac{1}{36}$} &  &  &  &  &  &  &  &  & {\small $3/36$}\tabularnewline
{\small 3} &  &  & {\small $\frac{2}{36}$} & {\small $\frac{2}{36}$} & {\small $\frac{1}{36}$} &  &  &  &  &  &  & {\small $5/36$}\tabularnewline
{\small 4} &  &  &  & {\small $\frac{2}{36}$} & {\small $\frac{2}{36}$} & {\small $\frac{2}{36}$} & {\small $\frac{1}{36}$} &  &  &  &  & {\small $7/36$}\tabularnewline
{\small 5} &  &  &  &  & {\small $\frac{2}{36}$} & {\small $\frac{2}{36}$} & {\small $\frac{2}{36}$} & {\small $\frac{2}{36}$} & {\small $\frac{1}{36}$} &  &  & {\small $9/36$}\tabularnewline
{\small 6} &  &  &  &  &  & {\small $\frac{2}{36}$} & {\small $\frac{2}{36}$} & {\small $\frac{2}{36}$} & {\small $\frac{2}{36}$} & {\small $\frac{2}{36}$} & {\small $\frac{1}{36}$} & {\small $11/36$}\tabularnewline
\hline
{\small Total} & {\small $\frac{1}{36}$} & {\small $\frac{2}{36}$} & {\small $\frac{3}{36}$} & {\small $\frac{4}{36}$} & {\small $\frac{5}{36}$} & {\small $\frac{6}{36}$} & {\small $\frac{5}{36}$} & {\small $\frac{4}{36}$} & {\small $\frac{3}{36}$} & {\small $\frac{2}{36}$} & {\small $\frac{1}{36}$} & {\small 1}\tabularnewline
\hline
\end{tabular}
\par\end{centering}\
\caption[The joint PMF of $(U,V)$]
{\small The joint PMF of $(U,V)$. The outcomes of $U$ are along the left and the outcomes of $V$ are along the top. Empty entries in the table have zero probability. The row totals (on the right) and column totals (on the bottom) correspond to the marginal distribution of $U$ and $V$, respectively. }
\label{tab:max-sum-joint-pmf}
\end{table}

The joint support of \((U,V)\) is concentrated along the main diagonal; note that the nonzero entries do not form a rectangle. Also notice that if we calculate row and column totals we are doing exactly the same thing as Equation \eqref{eq:marginal-pmf}, so that the marginal distribution of \(U\) is the list of totals in the right ``margin'' of
the Table~\ref{tab:max-sum-joint-pmf}, and the marginal distribution of \(V\) is the list of totals in the bottom ``margin''.

Continuing the reasoning for the discrete case, given two continuous random variables \(X\) and \(Y\) there similarly exists\footnote{Strictly speaking, the joint density function does not necessarily exist. But the joint CDF always exists.} a function \(f_{X,Y}(x,y)\) associated with \(X\) and \(Y\) called the \textit{joint probability density function} of \(X\) and \(Y\). Every joint PDF satisfies
\begin{equation}
f_{X,Y}(x,y)\geq0\mbox{ for all }(x,y)\in S_{X,Y},
\end{equation}
and
\begin{equation}
\iintop_{S_{X,Y}}f_{X,Y}(x,y)\,\mathrm{d} x\,\mathrm{d} y=1.
\end{equation}
In the continuous case there is not such a simple interpretation for the joint PDF; however, we do have one for the joint CDF, namely, 
\[
F_{X,Y}(x,y)=\mathbb{P}(X\leq x,\, Y\leq
y)=\int_{-\infty}^{x}\int_{-\infty}^{y}f_{X,Y}(u,v)\,\mathrm{d}
v\,\mathrm{d} u, 
\] 
for \((x,y)\in\mathbb{R}^{2}\). If \(X\) and \(Y\) have the joint PDF \(f_{X,Y}\), then the marginal density of \(X\) may be recovered by
\begin{equation}
f_{X}(x)=\int_{S_{Y}}f_{X,Y}(x,y)\,\mathrm{d} y,\quad x \in S_{X}
\end{equation}
and the marginal PDF of \(Y\) may be found with
\begin{equation}
f_{Y}(y)=\int_{S_{X}}f_{X,Y}(x,y)\,\mathrm{d} x, \quad y \in S_{Y}.
\end{equation}



\begin{example}[]
\label{exm:joint-pdf}
Let the joint PDF of \((X,Y)\) be given by \[
f_{X,Y}(x,y)=\frac{6}{5}\left(x+y^{2}\right),\quad 0 < x < 1,\ 0 < y
< 1.  \] The marginal PDF of \(X\) is
\begin{eqnarray*}
f_{X}(x) & = & \int_{0}^{1}\frac{6}{5}\left(x+y^{2}\right)\,\mathrm{d} y,\\
 & = & \left.\frac{6}{5}\left(xy+\frac{y^{3}}{3}\right)\right|_{y=0}^{1},\\
 & = & \frac{6}{5}\left(x+\frac{1}{3}\right),
\end{eqnarray*}
for \(0 < x < 1\), and the marginal PDF of \(Y\) is
\begin{eqnarray*}
f_{Y}(y) & = & \int_{0}^{1}\frac{6}{5}\left(x+y^{2}\right)\,\mathrm{d} x,\\
 & = & \left.\frac{6}{5}\left(\frac{x^{2}}{2}+xy^{2}\right)\right|_{x=0}^{1},\\
 & = & \frac{6}{5}\left(\frac{1}{2}+y^{2}\right),
\end{eqnarray*}
for \(0 < y < 1\). In this example the joint support set was a
rectangle \([0,1]\times[0,1]\), but it turns out that \(X\) and \(Y\)
are not independent. See Section~\ref{sec:independent-random-variables}.
\end{example}

\subsection{How to do it with \textsf{R}}

We will show how to do Example~\ref{exm:max-sum-two-dice} using \textsf{R}; it is much simpler to do it with \textsf{R} than without. First we set up the sample space with the \texttt{rolldie} function. Next, we add random variables \(U\) and \(V\) with the \texttt{addrv} function. We take a look at the very top of the data frame (probability space) to make sure that everything is operating according to plan.

<<echo=TRUE>>=
S <- rolldie(2, makespace = TRUE)
S <- addrv(S, FUN = max, invars = c("X1","X2"), name = "U")
S <- addrv(S, FUN = sum, invars = c("X1","X2"), name = "V")
head(S)
@

Yes, the \(U\) and \(V\) columns have been added to the data frame and have been computed correctly. This result would be fine as it is, but the data frame has too many rows: there are repeated pairs \((u,v)\) which show up as repeated rows in the data frame. The goal is to aggregate the rows of \(S\) such that the result has exactly one row for each unique pair \((u,v)\) with positive probability. This sort of thing is exactly the task for which the \texttt{marginal} function was designed. We may take a look at the joint distribution of \(U\) and \(V\) (we only show the first few rows of the data frame, but the complete one has 11 rows).

<<echo=TRUE>>=
UV <- marginal(S, vars = c("U", "V"))
head(UV)
@

The data frame is difficult to understand. It would be better to have a tabular display like Table~\ref{tab:max-sum-joint-pmf}. We can do that with the \texttt{xtabs} function.

<<echo=TRUE>>=
xtabs(round(probs,3) ~ U + V, data = UV)
@

Compare these values to the ones shown in Table~\ref{tab:max-sum-joint-pmf}. We can repeat the process with \texttt{marginal} to get the univariate marginal distributions of \(U\) and \(V\) separately.

<<echo=TRUE>>=
marginal(UV, vars = "U")
head(marginal(UV, vars = "V"))
@

Another way to do the same thing is with the \texttt{rowSums} and \texttt{colSums} of the \texttt{xtabs} object. Compare

<<echo=TRUE>>=
temp <- xtabs(probs ~ U + V, data = UV)
rowSums(temp)
colSums(temp)
@

You should check that the answers that we have obtained exactly match the same (somewhat laborious) calculations that we completed in Example~\ref{exm:max-sum-two-dice}.

\section{Joint and Marginal Expectation} \label{sec:joint-and-marginal-expectation}

Given a function \(g\) with arguments \((x,y)\) we would like to know
the long-run average behavior of \(g(X,Y)\) and how to mathematically
calculate it. Expectation in this context is computed in the
pedestrian way. We simply integrate (sum) with respect to the joint
probability density (mass) function.
\begin{equation}
\mathbb{E}\, g(X,Y)=\iintop_{S_{X,Y}}g(x,y)\, f_{X,Y}(x,y)\,\mathrm{d} x\,\mathrm{d} y,
\end{equation}
or in the discrete case,
\begin{equation}
\mathbb{E}\, g(X,Y)=\mathop{\sum\sum}\limits _{(x,y)\in S_{X,Y}}g(x,y)\, f_{X,Y}(x,y).
\end{equation}

\subsection{Covariance and Correlation}

There are two very special cases of joint expectation: the
\textit{covariance} and the \textit{correlation}. These are measures
which help us quantify the dependence between \(X\) and \(Y\).



\begin{defn}
The \textit{covariance} of \(X\) and \(Y\) is
\begin{equation}
\mbox{Cov}(X,Y)=\mathbb{E}(X-\mathbb{E} X)(Y-\mathbb{E} Y).
\end{equation}
\end{defn}

By the way, there is a shortcut formula for covariance which is almost
as handy as the shortcut for the variance:
\begin{equation}
\mbox{Cov}(X,Y)=\mathbb{E}(XY)-(\mathbb{E} X)(\mathbb{E} Y).
\end{equation}
The proof is left to Exercise~\ref{exr:prove-cov-shortcut}.

The Pearson product moment correlation between \(X\) and \(Y\) is the
covariance between \(X\) and \(Y\) rescaled to fall in the interval
\([-1,1]\). It is formally defined by
\begin{equation}
\mbox{Corr}(X,Y)=\frac{\mbox{Cov}(X,Y)}{\sigma_{X}\sigma_{Y}}.
\end{equation}

The \textit{correlation} is usually denoted by \(\rho_{X,Y}\) or simply
\(\rho\) if the random variables are clear from context. There are
some important facts about the correlation coefficient:

\begin{enumerate}
\item The range of correlation is \(-1\leq\rho_{X,Y}\leq1\).
\item Equality holds above (\(\rho_{X,Y}=\pm1\)) if and only if \(Y\) is a linear function of \(X\) with probability one.
\end{enumerate}



\begin{example}[]
\label{exm:max-sum-dice-covariance}
We will compute the covariance for the
discrete distribution in Example~\ref{exm:max-sum-two-dice}. The expected
value of \(U\) is \[ \mathbb{E} U=\sum_{u=1}^{6}u\,
f_{U}(u)=\sum_{u=1}^{6}u\,\frac{2u-1}{36}=1\left(\frac{1}{36}\right)+2\left(\frac{3}{36}\right)+\cdots+6\left(\frac{11}{36}\right)=\frac{161}{36},
\] and the expected value of \(V\) is \[ \mathbb{E}
V=\sum_{v=2}^{12}v\,\frac{6-|7-v|}{36}=2\left(\frac{1}{36}\right)+3\left(\frac{2}{36}\right)+\cdots+12\left(\frac{1}{36}\right)=7,
\] and the expected value of \(UV\) is \[ \mathbb{E}
UV=\sum_{u=1}^{6}\sum_{v=2}^{12}uv\,
f_{U,V}(u,v)=1\cdot2\left(\frac{1}{36}\right)+2\cdot3\left(\frac{2}{36}\right)+\cdots+6\cdot12\left(\frac{1}{36}\right)=\frac{308}{9}.
\]
Therefore the covariance of \((U,V)\) is
\[
\mbox{Cov}(U,V)=\mathbb{E} UV-\left(\mathbb{E}
U\right)\left(\mathbb{E}
V\right)=\frac{308}{9}-\frac{161}{36}\cdot7=\frac{35}{12}.
\]
All we
need now are the standard deviations of \(U\) and \(V\) to calculate
the correlation coefficient (omitted).
\end{example}


We will do a continuous example so that you can see how it works.



\begin{example}[]
Let us find the covariance of the variables \((X,Y)\) from Example~\ref{exm:joint-pdf}. The expected value of \(X\) is
\[ \mathbb{E}
X=\int_{0}^{1}x\cdot\frac{6}{5}\left(x+\frac{1}{3}\right)\mathrm{d}
x=\left.\frac{2}{5}x^{3}+\frac{1}{5}x^{2}\right|_{x=0}^{1}=\frac{3}{5},
\]
and the expected value of \(Y\) is
\[ \mathbb{E}
Y=\int_{0}^{1}y\cdot\frac{6}{5}\left(\frac{1}{2}+y^{2}\right)\mathrm{d}
x=\left.\frac{3}{10}y^{2}+\frac{3}{20}y^{4}\right|_{y=0}^{1}=\frac{9}{20}.
\]
Finally, the expected value of \(XY\) is
\begin{eqnarray*}
\mathbb{E} XY & = & \int_{0}^{1}\int_{0}^{1}xy\,\frac{6}{5}\left(x+y^{2}\right)\mathrm{d} x\,\mathrm{d} y,\\
 & = & \int_{0}^{1}\left.\left(\frac{2}{5}x^{3}y+\frac{3}{5}x^{2}y^{3}\right)\right|_{x=0}^{1}\mathrm{d} y,\\
 & = & \int_{0}^{1}\left(\frac{2}{5}y+\frac{3}{5}y^{3}\right)\mathrm{d} y,\\
 & = & \frac{1}{5}+\frac{3}{20},
\end{eqnarray*}
which is 7/20. Therefore the covariance of \((X,Y)\) is
\[
\mbox{Cov}(X,Y)=\frac{7}{20}-\left(\frac{3}{5}\right)\left(\frac{9}{20}\right)=\frac{2}{25}.
\]
\end{example}



\subsubsection{How to do it with \textsf{R}}

There are not any specific functions in the \texttt{prob} package \cite{prob}
designed for multivariate expectation. This is not a problem, though,
because it is easy enough to do expectation the long way -- with
column operations. We just need to keep the definition in mind. For
instance, we may compute the covariance of \((U,V)\) from Example~\ref{exm:max-sum-dice-covariance}.

<<echo=TRUE>>=
Eu <- sum(S$U*S$probs)
Ev <- sum(S$V*S$probs)
Euv <- sum(S$U*S$V*S$probs)
Euv - Eu * Ev
@

Compare this answer to what we got in Example~\ref{exm:max-sum-dice-covariance}.

To do the continuous case we could use the computer algebra utilities
of \texttt{Yacas} and the associated \textsf{R} package \texttt{Ryacas}
\cite{Ryacas}. See Section~\ref{sub:bivariate-transf-r} for another example
where the \texttt{Ryacas} package appears.

\section{Conditional Distributions} \label{sec:conditional-distributions}

If \(x\in S_{X}\) is such that \(f_{X}(x)>0\), then we define the
\textit{conditional density} of \(Y|\, X=x\), denoted \(f_{Y|x}\), by
\begin{equation}
f_{Y|x}(y|x)=\frac{f_{X,Y}(x,y)}{f_{X}(x)},\quad y\in S_{Y}.
\end{equation}
We define \(f_{X|y}\) in a similar fashion.

\begin{example}[]
Let the joint PMF of \(X\) and \(Y\) be given by
\[
f_{X,Y}(x,y) = x + y,\ 0 \leq x \leq 1, \ 0 \leq y \leq 1.
\]
Then the marginal PDF of \(X\) is:
\[
f_{X}(x) = \int_{0}^{1} \left(x + y\right)\,\mathrm{d}y =\left. xy + y^{2} \right|_{x=0}^{1} = x + 1,
\]
for any \(0 < x < 1\).  Let's fix \(x = 0.5\).  Then the conditional PDF of $Y$ given that $X = 0.5$ will be
\[
f_{Y|0.5}(y \vert 0.5) = \frac{f_{X,Y}(0.5, y)}{f_{X}(0.5)} = \frac{0.5 + y}{0.5 + 1} = \frac{2}{3}\left(0.5 + y\right),
\]
for all $0< y < 1$.

\end{example}


\subsection{Bayesian Connection}

Conditional distributions play a fundamental role in Bayesian
probability and statistics. There is a parameter \(\theta\) which is
of primary interest, and about which we would like to learn. But
rather than observing \(\theta\) directly, we instead observe a random
variable \(X\) whose probability distribution depends on
\(\theta\). Using the information we provided by \(X,\) we would like
to update the information that we have about \(\theta\).

Our initial beliefs about \(\theta\) are represented by a probability
distribution, called the \textit{prior distribution}, denoted by
\(\pi\). The PDF \(f_{X|\theta}\) is called the \textit{likelihood
  function}, also called the \textit{likelihood of} \(X\)
\textit{conditional on} \(\theta\). Given an observation \(X=x\), we
would like to update our beliefs \(\pi\) to a new distribution, called
the \textit{posterior distribution of} \(\theta\) \textit{given the
  observation} \(X=x\), denoted \(\pi_{\theta|x}\). It may seem a
mystery how to obtain \(\pi_{\theta|x}\) based only on the information
provided by \(\pi\) and \(f_{X|\theta}\), but it should not be. We
have already studied this in Section~\ref{sec:bayes-rule} where it
was called Bayes' Rule:
\begin{equation}
\pi(\theta|x)=\frac{\pi(\theta)\, f(x|\theta)}{\int\pi(u)\, f(x|u)\mathrm{d} u}.
\end{equation}
Compare the above expression to Equation \eqref{eq:bayes-rule}.




\begin{example}[]
Suppose the parameter \(\theta\) is the \(\mathbb{P}(\mbox{Heads})\)
for a biased coin. It could be any value from 0 to 1. Perhaps we have
some prior information about this coin, for example, maybe we have
seen this coin before and we have reason to believe that it shows
Heads less than half of the time. Suppose that we represent our
beliefs about \(\theta\) with a
\(\mathsf{beta}(\mathtt{shape1}=1,\,\mathtt{shape2}=3)\) prior
distribution, that is, we assume \[
\theta\sim\pi(\theta)=3(1-\theta)^{2},\quad 0 < \theta < 1.  \] To
learn more about \(\theta\), we will do what is natural: flip the
coin. We will observe a random variable \(X\) which takes the value
\(1\) if the coin shows Heads, and 0 if the coin shows Tails. Under
these circumstances, \(X\) will have a Bernoulli distribution, and in
particular,
\(X|\theta\sim\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=\theta)\):
\[ f_{X|\theta}(x|\theta)=\theta^{x}(1-\theta)^{1-x},\quad x=0,1.  \]
Based on the observation \(X=x\), we will update the prior
distribution to the posterior distribution, and we will do so with
Bayes' Rule: it says
\begin{eqnarray*}
\pi(\theta|x) & \propto & f(x|\theta) \, \pi(\theta),\\
 & = & \theta^{x}(1-\theta)^{1-x}\cdot3(1-\theta)^{2},\\
 & = & 3\,\theta^{x}(1-\theta)^{3-x},\quad 0 < \theta < 1,
\end{eqnarray*}
where the constant of proportionality is given by \[ \int3\,
u^{x}(1-u)^{3-x}\mathrm{d} u=\int3\,
u^{(1+x)-1}(1-u)^{(4-x)-1}\mathrm{d}
u=3\,\frac{\Gamma(1+x)\Gamma(4-x)}{\Gamma[(1+x)+(4-x)]}, \] the
integral being calculated by inspection of the formula for a
\(\mathsf{beta}(\mathtt{shape1}=1+x,\,\mathtt{shape2}=4-x)\)
distribution. That is to say, our posterior distribution is precisely
\[
\theta|x\sim\mathsf{beta}(\mathtt{shape1}=1+x,\,\mathtt{shape2}=4-x).
\] The Bayesian statistician uses the posterior distribution for all
matters concerning inference about \(\theta\).
\end{example}



\begin{rem}[]
We usually do not restrict ourselves to the observation of only one
\(X\) conditional on \(\theta\). In fact, it is common to observe an
entire sample \(X_{1}\), \(X_{2}\),\ldots,\(X_{n}\) conditional on
\(\theta\) (which itself is often multidimensional). Do not be
frightened, however, because the intuition is the same. There is a
prior distribution \(\pi(\theta)\), a likelihood
\(f(x_{1},x_{2},\ldots,x_{n}|\theta)\), and a posterior distribution
\(\pi(\theta|x_{1},x_{2},\ldots,x_{n})\). Bayes' Rule states that the
relationship between the three is \[
\pi(\theta|x_{1},x_{2},\ldots,x_{n})\propto\pi(\theta)\,
f(x_{1},x_{2},\ldots,x_{n}|\theta), \] where the constant of
proportionality is \(\int\pi(u)\,
f(x_{1},x_{2},\ldots,x_{n}|u)\,\mathrm{d} u\).
\end{rem}

Any good textbook on
Bayesian Statistics will explain these notions in detail; to the
interested reader I recommend Gelman \cite{Gelman2004} or Lee
\cite{Lee1997}.


\section{Independent Random Variables} \label{sec:independent-random-variables}

\subsection{Independent Random Variables} \label{sub:independent-random-variables}


We recall from Chapter~\ref{cha:probability} that the events \(A\) and \(B\) are
said to be independent when
\begin{equation}
\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B).
\end{equation}
If it happens that
\begin{equation}
\mathbb{P}(X=x,Y=y)=\mathbb{P}(X=x)\mathbb{P}(Y=y),\quad \mbox{for every }x\in S_{X},\ y\in S_{Y},
\end{equation}
then we say that \(X\) and \(Y\) are \textit{independent random
  variables}. Otherwise, we say that \(X\) and \(Y\) are
\textit{dependent}. Using the PMF notation from above, we see that
independent discrete random variables satisfy
\begin{equation}
f_{X,Y}(x,y)=f_{X}(x)f_{Y}(y)\quad \mbox{for every }x\in S_{X},\ y\in S_{Y}.
\end{equation}
Continuing the reasoning, given two continuous random variables \(X\)
and \(Y\) with joint PDF \(f_{X,Y}\) and respective marginal PDFs
\(f_{X}\) and \(f_{Y}\) that are supported on the sets \(S_{X}\) and
\(S_{Y}\), if it happens that
\begin{equation}
f_{X,Y}(x,y)=f_{X}(x)f_{Y}(y)\quad \mbox{for every }x\in S_{X},\ y\in S_{Y},
\end{equation}
then we say that \(X\) and \(Y\) are independent.



\begin{example}[]
In Example~\ref{exm:toss-two-dice-joint-pmf} we considered the random experiment of
rolling a fair die twice. There we found the joint PMF to be
\[
f_{X,Y}(x,y)=\frac{1}{36},\quad x=1,\ldots,6,\ y=1,\ldots,6,
\]
and we
found the marginal PMFs \(f_{X}(x)=1/6\), \(x=1,2,\ldots,6\), and
\(f_{Y}(y)=1/6\), \(y=1,2,\ldots,6\). Therefore in this experiment
\(X\) and \(Y\) are independent since for every \(x\) and \(y\) in the
joint support the joint PMF satisfies
\[
f_{X,Y}(x,y)=\frac{1}{36}=\left(\frac{1}{6}\right)\left(\frac{1}{6}\right)=f_{X}(x)\,
f_{Y}(y).
\]
\end{example}



\begin{example}[]
In Example~\ref{exm:max-sum-two-dice} we considered the same experiment but
different random variables \(U\) and \(V\). We can prove that \(U\)
and \(V\) are not independent if we can find a single pair \((u,v)\)
where the independence equality does not hold. There are many such
pairs. One of them is \((6,12)\):
\[
f_{U,V}(6,12)=\frac{1}{36}\neq\left(\frac{11}{36}\right)\left(\frac{1}{36}\right)=f_{U}(6)\,
f_{V}(12).
\]
\end{example}

Independent random variables are very useful to the
mathematician. They have many, many, tractable properties. We mention
some of the more important ones.




\begin{prop}[]
\label{prp:indep-implies-prodexpect}
If \(X\) and \(Y\) are independent,
then for any functions \(u\) and \(v\),
\begin{equation}
\mathbb{E}\left(u(X)v(Y)\right)=\left(\mathbb{E} u(X)\right)\left(\mathbb{E} v(Y)\right).
\end{equation}
\end{prop}



\begin{proof}
This is straightforward from the definition.
\begin{eqnarray*}
\mathbb{E}\left(u(X)v(Y)\right) & = & \iint\, u(x)v(y)\, f_{X,Y}(x,y)\,\mathrm{d} x\mathrm{d} y\\
 & = & \iint\, u(x)v(y)\, f_{X}(x)\, f_{Y}(y)\,\mathrm{d} x\mathrm{d} y\\
 & = & \int u(x)\, f_{X}(x)\,\mathrm{d} x\ \int v(y)\, f_{Y}(y)\,\mathrm{d} y
\end{eqnarray*}
and this last quantity is exactly \(\left(\mathbb{E} u(X)\right)\left(\mathbb{E} v(Y)\right)\).
\end{proof}

Now that we have Proposition~\ref{prp:indep-implies-prodexpect} we mention a
corollary that will help us later to quickly identify those random
variables which are \textit{not} independent.




\begin{cor}[]
\label{cor:indep-implies-uncorr}
If \(X\) and \(Y\) are independent, then
\(\mbox{Cov}(X,Y)=0\), and consequently, \(\mbox{Corr}(X,Y)=0\).
\end{cor}



\begin{proof}
When \(X\) and \(Y\) are independent then \(\mathbb{E} XY=\mathbb{E}
X\,\mathbb{E} Y\). And when the covariance is zero the numerator of
the correlation is 0.
\end{proof}



\begin{rem}[]
  Unfortunately, the converse of
  Corollary~\ref{cor:indep-implies-uncorr} is not true. That is, there
  are many random variables which are dependent yet their covariance
  and correlation is zero.
\label{rem-cov0-not-imply-indep}
\end{rem}

For more details, see Casella and Berger
\cite{Casella2002}. Proposition~\ref{prp:indep-implies-prodexpect} is
useful to us and we will receive mileage out of it, but there is
another fact which will play an even more important
role. Unfortunately, the proof is beyond the techniques presented
here. The inquisitive reader should consult Casella and Berger
\cite{Casella2002}, Resnick \cite{Resnick1999}, \textit{etc}.



\begin{fact}[]
If \(X\) and \(Y\) are independent,
then \(u(X)\) and \(v(Y)\) are independent for any functions \(u\) and
\(v\).
\label{fac:indep-then-function-indep}
\end{fact}

\subsection{Combining Independent Random Variables} \label{sub:combining-independent-random}

Another important corollary of Proposition~\ref{prp:indep-implies-prodexpect} will allow us to find the
distribution of sums of random variables.



\begin{cor}[]
If \(X\) and \(Y\) are independent, then the moment generating
function of \(X+Y\) is
\begin{equation}
M_{X+Y}(t)=M_{X}(t)\cdot M_{Y}(t).
\end{equation}
\end{cor}



\begin{proof}
Choose \(u(x)=\mathrm{e}^{x}\) and \(v(y)=\mathrm{e}^{y}\) in
Proposition~\ref{prp:indep-implies-prodexpect}, and remember the identity
\(\mathrm{e}^{t(x+y)}=\mathrm{e}^{tx}\,\mathrm{e}^{ty}\).
\end{proof}

Let us take a look at some examples of the corollary in action.



\begin{example}[]
Let \(X\sim\mathsf{binom}(\mathtt{size}=n_{1},\,\mathtt{prob}=p)\) and
\(Y\sim\mathsf{binom}(\mathtt{size}=n_{2},\,\mathtt{prob}=p)\) be
independent. Then \(X+Y\) has MGF \[ M_{X+Y}(t)=M_{X}(t)\,
M_{Y}(t)=\left(q+p\mathrm{e}^{t}\right)^{n_{1}}\left(q+p\mathrm{e}^{t}\right)^{n_{2}}=\left(q+p\mathrm{e}^{t}\right)^{n_{1}+n_{2}},
\] which is the MGF of a
\(\mathsf{binom}(\mathtt{size}=n_{1}+n_{2},\,\mathtt{prob}=p)\)
distribution. Therefore,
\(X+Y\sim\mathsf{binom}(\mathtt{size}=n_{1}+n_{2},\,\mathtt{prob}=p)\).
\end{example}



\begin{example}[]
Let
\(X\sim\mathsf{norm}(\mathtt{mean}=\mu_{1},\,\mathtt{sd}=\sigma_{1})\)
and
\(Y\sim\mathsf{norm}(\mathtt{mean}=\mu_{2},\,\mathtt{sd}=\sigma_{2})\)
be independent. Then \(X+Y\) has MGF
\begin{eqnarray*}
M_{X}(t)\, M_{Y}(t) & = & \exp\left\{ \mu_{1}t+t^{2}\sigma_{1}^{2}/2\right\}
\exp\left\{ \mu_{2}t+t^{2}\sigma_{2}^{2}/2\right\} \\
          & = &\exp\left\{\left(\mu_{1}+\mu_{2}\right)t+t^{2}\left(\sigma_{1}^{2}+\sigma_{2}^{2}\right)/2\right\},
\end{eqnarray*}
which is the MGF of a
\(\mathsf{norm}\left(\mathtt{mean}=\mu_{1}+\mu_{2},\,\mathtt{sd}=\sqrt{\sigma_{1}^{2}+\sigma_{2}^{2}}\right)\)
distribution.
\end{example}


Even when we cannot use the MGF trick to identify the exact
distribution of a linear combination of random variables, we can still
say something about its mean and variance.




\begin{prop}[]
\label{prp:mean-sd-lin-comb-two}
Let \(X_{1}\) and \(X_{2}\) be
independent with respective population means \(\mu_{1}\) and
\(\mu_{2}\) and population standard deviations \(\sigma_{1}\) and
\(\sigma_{2}\). For given constants \(a_{1}\) and \(a_{2}\), define
\(Y=a_{1}X_{1}+a_{2}X_{2}\). Then the mean and standard deviation of
\(Y\) are given by the formulas
\begin{equation}
\mu_{Y}=a_{1}\mu_{1}+a_{2}\mu_{2},\quad \sigma_{Y}=\left(a_{1}^{2}\sigma_{1}^{2}+a_{2}^{2}\sigma_{2}^{2}\right)^{1/2}.
\end{equation}
\end{prop}



\begin{proof}
We use Proposition~\ref{prp:expectation-properties} to see
\[
\mathbb{E}
Y=\mathbb{E}\left(a_{1}X_{1}+a_{2}X_{2}\right)=a_{1}\mathbb{E}X_{1}+a_{2}\mathbb{E} X_{2}=a_{1}\mu_{1}+a_{2}\mu_{2}.
\]
For the
standard deviation, we will find the variance and take the square root
at the end. And to calculate the variance we will first compute
\(\mathbb{E} Y^{2}\) with an eye toward using the identity
\(\sigma_{Y}^{2}=\mathbb{E} Y^{2}-\left(\mathbb{E} Y\right)^{2}\) as a
final step.
\[ \mathbb{E}
Y^{2}=\mathbb{E}\left(a_{1}X_{1}+a_{2}X_{2}\right)^{2}=\mathbb{E}\left(a_{1}^{2}X_{1}^{2}+a_{2}^{2}X_{2}^{2}+2a_{1}a_{2}X_{1}X_{2}\right).
\]
Using linearity of expectation the \(\mathbb{E}\) distributes
through the sum. Now \(\mathbb{E}
X_{i}^{2}=\sigma_{i}^{2}+\mu_{i}^{2}\), for \(i=1\) and 2 and
\(\mathbb{E} X_{1}X_{2}=\mathbb{E} X_{1}\mathbb{E}
X_{2}=\mu_{1}\mu_{2}\) because of independence. Thus
\begin{eqnarray*}
\mathbb{E} Y^{2} & = & a_{1}^{2}(\sigma_{1}^{2}+\mu_{1}^{2})+a_{2}^{2}(\sigma_{2}^{2}+\mu_{2}^{2})+2a_{1}a_{2}\mu_{1}\mu_{2},\\
 & = & a_{1}^{2}\sigma_{1}^{2}+a_{2}^{2}\sigma_{2}^{2}+\left(a_{1}^{2}\mu_{1}^{2}+a_{2}^{2}\mu_{2}^{2}+2a_{1}a_{2}\mu_{1}\mu_{2}\right).
\end{eqnarray*}
But notice that the expression in the parentheses is exactly
\(\left(a_{1}\mu_{1}+a_{2}\mu_{2}\right)^{2}=\left(\mathbb{E}
Y\right)^{2}\), so the proof is complete.
\end{proof}

\section{Exchangeable Random Variables} \label{sec:exchangeable-random-variables}


Two random variables \(X\) and \(Y\) are said to be
\textit{exchangeable} if their joint CDF is a symmetric function of
its arguments:
\begin{equation}
F_{X,Y}(x,y)=F_{X,Y}(y,x),\quad \mbox{for all }(x,y)\in\mathbb{R}^{2}.
\end{equation}
When the joint density \(f\) exists, we may equivalently say that
\(X\) and \(Y\) are exchangeable if \(f(x,y)=f(y,x)\) for all
\((x,y)\).

Exchangeable random variables exhibit symmetry in the sense that a
person may exchange one variable for the other with no substantive
changes to their joint random behavior. While independence speaks to a
\textit{lack of influence} between the two variables, exchangeability
aims to capture the \textit{symmetry} between them.



\begin{example}[]
Let \(X\) and \(Y\) have joint PDF
\[
f_{X,Y}(x,y)=(1+\alpha)\lambda^{2}\mathrm{e}^{-\lambda(x+y)}+\alpha(2\lambda)^{2}\mathrm{e}^{-2\lambda(x+y)}-2\alpha\lambda^{2}\left(\mathrm{e}^{-\lambda(2x+y)}+\mathrm{e}^{-\lambda(x+2y)}\right).
\]
It is straightforward and tedious to check that \(\iint f=1\). We may
see immediately that \(f_{X,Y}(x,y)=f_{X,Y}(y,x)\) for all \((x,y)\),
which confirms that \(X\) and \(Y\) are exchangeable. Here, \(\alpha\)
is said to be an association parameter.
\end{example}

The example above is one from the Farlie-Gumbel-Morgenstern family of
distributions; see \cite{Kotz2000}.




\begin{example}[]
\label{exm:binom-exchangeable}
Suppose \(X\) and \(Y\) are IID
\(\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p)\). Then their
joint PMF is
\begin{eqnarray*}
f_{X,Y}(x,y) & = & f_{X}(x)f_{Y}(y)\\
 & = & {n \choose x}\, p^{x}(1-p)^{n-x}\,{n \choose y}\, p^{y}(1-p)^{n-y},\\
 & = & {n \choose x}{n \choose y}\, p^{x+y}(1-p)^{2n-(x+y)},
\end{eqnarray*}
and the value is the same if we exchange \(x\) and \(y\). Therefore
\((X,Y)\) are exchangeable.
\end{example}



Looking at Example~\ref{exm:binom-exchangeable} more closely we see that the
fact that \((X,Y)\) are exchangeable has nothing to do with the
\(\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p)\) distribution; it
only matters that they are independent (so that the joint PDF factors)
and they are identically distributed (in which case we may swap
letters to no effect). We could just have easily used any other
marginal distribution. We will take this as a proof of the following
proposition.



\begin{prop}[]
If \(X\) and \(Y\) are IID (with common marginal distribution \(F\))
then \(X\) and \(Y\) are exchangeable.
\end{prop}

Exchangeability thus contains IID as a special case.

\section{The Bivariate Normal Distribution} \label{sec:the-bivariate-normal}


The bivariate normal PDF is given by the unwieldy formula
\begin{multline}
f_{X,Y}(x,y)=\frac{1}{2\pi\,\sigma_{X}\sigma_{Y}\sqrt{1-\rho^{2}}}\exp\left\{ -\frac{1}{2(1-\rho^{2})}\left[\left(\frac{x-\mu_{X}}{\sigma_{X}}\right)^{2}+\cdots\right.\right.\\
\left.\left.\cdots+2\rho\left(\frac{x-\mu_{X}}{\sigma_{X}}\right)\left(\frac{y-\mu_{Y}}{\sigma_{Y}}\right)+\left(\frac{y-\mu_{Y}}{\sigma_{Y}}\right)^{2}\right]\right\} ,
\end{multline}
for \((x,y)\in\mathbb{R}^{2}\). We write \((X,Y)\sim\mathsf{mvnorm}(\mathtt{mean}=\upmu,\,\mathtt{sigma}=\Sigma)\), where
\begin{equation}
\upmu=(\mu_{X},\,\mu_{Y})^{T},\quad \sum=\left(
\begin{array}{cc}
\sigma_{X}^{2} & \rho\sigma_{X}\sigma_{Y}\\
\rho\sigma_{X}\sigma_{Y} & \sigma_{Y}^{2}
\end{array}
\right).
\end{equation}
See Appendix~\ref{cha:mathematical-machinery}. The vector notation allows for a
more compact rendering of the joint PDF:
\begin{equation}
f_{X,Y}(\mathbf{x})=\frac{1}{2\pi\left|\Sigma\right|^{1/2}}\exp\left\{ -\frac{1}{2}\left(\mathbf{x}-\upmu\right)^{\top}\Sigma^{-1}\left(\mathbf{x}-\upmu\right)\right\} ,
\end{equation}
where in an abuse of notation we have written \(\mathbf{x}\) for
\((x,y)\). Note that the formula only holds when \(\rho\neq\pm1\).



\begin{rem}[]
In Remark~\ref{rem-cov0-not-imply-indep} we noted that just because random
variables are uncorrelated it does not necessarily mean that they are
independent. However, there is an important exception to this rule:
the bivariate normal distribution. Indeed,
\((X,Y)\sim\mathsf{mvnorm}(\mathtt{mean}=\upmu,\,\mathtt{sigma}=\Sigma)\)
are independent if and only if \(\rho=0\).
\end{rem}



\begin{rem}[]
Inspection of the joint PDF shows that if \(\mu_{X}=\mu_{Y}\) and
\(\sigma_{X}=\sigma_{Y}\) then \(X\) and \(Y\) are exchangeable.
\end{rem}

The bivariate normal MGF is
\begin{equation}
M_{X,Y}(\mathbf{t})=\exp\left(\upmu^{\top}\mathbf{t}+\frac{1}{2}\mathbf{t}^{\top}\Sigma\mathbf{t}\right),
\end{equation}
where \(\mathbf{t}=(t_{1},t_{2})\).

The bivariate normal distribution may be intimidating at first but it
turns out to be very tractable compared to other multivariate
distributions. An example of this is the following fact about the
marginals.



\begin{fact}[]
If \((X,Y)\sim\mathsf{mvnorm}(\mathtt{mean}=\upmu,\,\mathtt{sigma}=\Sigma)\) then
\begin{equation}
X\sim\mathsf{norm}(\mathtt{mean}=\mu_{X},\,\mathtt{sd}=\sigma_{X})\mbox{ and }Y\sim\mathsf{norm}(\mathtt{mean}=\mu_{Y},\,\mathtt{sd}=\sigma_{Y}).
\end{equation}
\end{fact}

From this we immediately get that \(\mathbb{E} X=\mu_{X}\) and
\(\mbox{Var}(X)=\sigma_{X}^{2}\) (and the same is true for \(Y\) with
the letters switched). And it should be no surprise that the
correlation between \(X\) and \(Y\) is exactly
\(\mbox{Corr}(X,Y)=\rho\).




\begin{prop}[]
\label{prp:mvnorm-cond-dist}
The conditional distribution of \(Y|\, X=x\)
is \(\mathsf{norm}(\mathtt{mean} = \mu_{Y|x}, \, \mathtt{sd} =
\sigma_{Y|x})\), where
\begin{equation}
\mu_{Y|x}=\mu_{Y}+\rho\frac{\sigma_{Y}}{\sigma_{X}}\left(x-\mu_{X}\right),\mbox{ and }\sigma_{Y|x}=\sigma_{Y}\sqrt{1-\rho^{2}}.
\end{equation}
\end{prop}

There are a few things to note about Proposition~\ref{prp:mvnorm-cond-dist}
which will be important in Chapter~\ref{cha:simple-linear-regression}. First,
the conditional mean of \(Y|x\) is linear in \(x\), with slope
\begin{equation}
\label{eq:population-slope-slr}
\rho\,\frac{\sigma_{Y}}{\sigma_{X}}.
\end{equation}
Second, the conditional variance of \(Y|x\) is independent of \(x\).

\subsection{How to do it with \textsf{R}}

The multivariate normal distribution is implemented in both the
\texttt{mvtnorm} package \cite{mvtnorm} and the \texttt{mnormt}
package \cite{mnormt}. We use the \texttt{mvtnorm} package in this
book simply because it is a dependency of another package used in the
book.

The \texttt{mvtnorm} package has functions \texttt{dmvnorm} and
\texttt{rmvnorm} for the PDF and to generate random vectors,
respectively. Let us get started with a graph of the bivariate normal
PDF. We can make the plot with the following code, where the workhorse
is the \texttt{persp} function in base \textsf{R}.

Another way to do this is with the \texttt{curve3d} function in the
\texttt{emdbook} package \cite{emdbook}. It looks like this:

<<eval=FALSE>>=
library("emdbook"); library("mvtnorm") # note: the order matters
mu <- c(0,0); sigma <- diag(2)
f <- function(x,y) dmvnorm(c(x,y), mean = mu, sigma = sigma)
curve3d(f(x,y), from=c(-3,-3), to=c(3,3), theta=-30, phi=30)
@

The code above is slightly shorter than that using \texttt{persp} and is
easier to understand. One must be careful, however. If the \texttt{library}
calls are swapped then the code will not work because both packages
\texttt{emdbook} and \texttt{mvtnorm} have a function called \texttt{dmvnorm}; one must
load them to the search path in the correct order or \textsf{R}
will use the wrong one (the arguments are named differently and the
underlying algorithms are different).

<<echo=TRUE, eval=FALSE>>=
x <- y <- seq(from = -3, to = 3, length.out = 30)
f <- function(x,y) dmvnorm(cbind(x,y), mean = c(0,0),
                           sigma = diag(2))
z <- outer(x, y, FUN = f)
persp(x, y, z, theta = -30, phi = 30, ticktype = "detailed")
@

We chose the standard bivariate normal,
\(\mathsf{mvnorm}(\mathtt{mean}=\mathbf{0},\,\mathtt{sigma}=\mathbf{I})\),
to display.


<<mvnorm-pdf, echo=FALSE, fig=TRUE, include=FALSE, width=5>>=
x <- y <- seq(from = -3, to = 3, length.out = 30)
f <- function(x,y) dmvnorm(cbind(x,y), mean = c(0,0), sigma = diag(2))
z <- outer(x, y, FUN = f)
persp(x, y, z, theta = -30, phi = 30, ticktype = "detailed")
@


\begin{figure}
\begin{center}
\includegraphics{IPSUR-mvnorm-pdf}
\end{center}
\caption{{\small A graph of a bivariate normal PDF.}}
\label{fig:mvnorm-pdf}
\end{figure}



\section{Bivariate Transformations of Random Variables} \label{sec:transformations-multivariate}

We studied in Section~\ref{sec:functions-of-continuous} how to find the PDF
of \(Y=g(X)\) given the PDF of \(X\). But now we have two random
variables \(X\) and Y, with joint PDF \(f_{X,Y}\), and we would like
to consider the joint PDF of two new random variables
\begin{equation}
U=g(X,Y)\quad \mbox{and}\quad V=h(X,Y),
\end{equation}
where \(g\) and \(h\) are two given functions, typically ``nice'' in
the sense of Appendix~\ref{sec:multivariable-calculus}.

Suppose that the transformation \((x,y)\longmapsto(u,v)\) is
one-to-one. Then an inverse transformation \(x=x(u,v)\) and
\(y=y(u,v)\) exists, so let \(\partial(x,y)/\partial(u,v)\) denote the
Jacobian of the inverse transformation. Then the joint PDF of
\((U,V)\) is given by
\begin{equation}
f_{U,V}(u,v)=f_{X,Y}\left[x(u,v),\, y(u,v)\right]\left|\frac{\partial(x,y)}{\partial(u,v)}\right|,
\end{equation}
or we can rewrite more shortly as
\begin{equation}
\label{eq:biv-trans-pdf-short}
f_{U,V}(u,v)=f_{X,Y}(x,y)\left|\frac{\partial(x,y)}{\partial(u,v)}\right|.
\end{equation}
Take a moment and compare Equation \eqref{eq:biv-trans-pdf-short} to Equation
\eqref{eq:univ-trans-pdf-short}. Do you see the connection?




\begin{rem}[]
It is sometimes easier to \textit{postpone} solving for the inverse
transformation \(x=x(u,v)\) and \(y=y(u,v)\). Instead, leave the
transformation in the form \(u=u(x,y)\) and \(v=v(x,y)\) and calculate
the Jacobian of the \textit{original} transformation
\begin{equation}
\frac{\partial(u,v)}{\partial(x,y)}=\left|\begin{array}{cc}
\frac{\partial u}{\partial x} & \frac{\partial u}{\partial y}\\
\frac{\partial v}{\partial x} & \frac{\partial v}{\partial y}\end{array}\right|=\frac{\partial u}{\partial x}\frac{\partial v}{\partial y}-\frac{\partial u}{\partial y}\frac{\partial v}{\partial x}.
\end{equation}
Once this is known, we can get the PDF of \((U,V)\) by
\begin{equation}
f_{U,V}(u,v)=f_{X,Y}(x,y)\left|\frac{1}{\frac{\partial(u,v)}{\partial(x,y)}}\right|.
\end{equation}
In some cases there will be a cancellation and the work will be lot
shorter. Of course, it is not always true that
\begin{equation}
\label{eq:biv-jacob-recip}
\frac{\partial(x,y)}{\partial(u,v)}=\frac{1}{\frac{\partial(u,v)}{\partial(x,y)}},
\end{equation}
but for the well-behaved examples that we will see in this book it
works just fine\ldots do you see the connection between Equations
\eqref{eq:biv-jacob-recip} and \eqref{eq:univ-jacob-recip}?
\end{rem}



\begin{example}[]
Let
\((X,Y)\sim\mathsf{mvnorm}(\mathtt{mean}=\mathbf{0}_{2\times1},\,\mathtt{sigma}=\mathbf{I}_{2\times2})\)
and consider the transformation
\begin{align*}
U= & \ 3X+4Y,\\
V= & \ 5X+6Y.
\end{align*}
We can solve the system of equations to find the inverse
transformations; they are
\begin{align*}
X= & -3U+2V,\\
Y= & \ \frac{5}{2}U-\frac{3}{2}V,
\end{align*}
in which case the Jacobian of the inverse transformation is
\[ \begin{vmatrix} -3 & 2\\ \frac{5}{2} & -\frac{3}{2} \end{vmatrix} = -3\left(-\frac{3}{2}\right)-2\left(\frac{5}{2}\right) = -\frac{1}{2}.\]
As \((x,y)\) traverses \(\mathbb{R}^{2}\), so too does \((u,v)\). Since the joint PDF of \((X,Y)\) is
\[
f_{X,Y}(x,y)=\frac{1}{2\pi}\exp\left\{ -\frac{1}{2}\left(x^{2}+y^{2}\right)\right\} ,\quad (x,y)\in\mathbb{R}^{2},
\]
we get that the joint PDF of \((U,V)\) is
\begin{equation}
\label{eq:biv-norm-hidden}
f_{U,V}(u,v)=\frac{1}{2\pi}\exp\left\{ -\frac{1}{2}\left[\left(-3u+2v\right)^{2}+\left(\frac{5u-3v}{2}\right)^{2}\right]\right\} \cdot\frac{1}{2},\quad (u,v)\in\mathbb{R}^{2}.
\end{equation}
\end{example}



\begin{rem}[]
It may not be obvious, but Equation \eqref{eq:biv-norm-hidden} is the PDF of a
\(\mathsf{mvnorm}\) distribution. For a more general result see
Theorem~\ref{thm:mvnorm-dist-matrix-prod}.
\end{rem}

\subsubsection{How to do it with \textsf{R}} \label{sub:bivariate-transf-r}

It is possible to do the computations above in \textsf{R} with the \texttt{Ryacas} package. The package is an interface to the open-source computer algebra system, ``Yacas''. The user installs Yacas, then employs \texttt{Ryacas} to submit commands to Yacas, after which the output is displayed in the \textsf{R} console.

There are not yet any examples of Yacas in this book, but there are online materials to help the interested reader: see \url{http://r-cas.github.io/ryacas/} to get
started.

\section{Remarks for the Multivariate Case} \label{sec:remarks-for-the-multivariate}


There is nothing spooky about \(n\geq3\) random variables. We just
have a whole bunch of them: \(X_{1}\), \(X_{2}\),\ldots, \(X_{n}\), which
we can shorten to
\(\mathbf{X}=(X_{1},X_{2},\ldots,X_{n})^{\mathrm{T}}\) to make the
formulas prettier (now may be a good time to check out Appendix~\ref{sec:linear-algebra}). For \(\mathbf{X}\) supported on the set
\(S_{\mathbf{X}}\), the joint PDF \(f_{\mathbf{X}}\) (if it exists)
satisfies
\begin{equation}
f_{\mathbf{X}}(\mathbf{x})>0,\quad \mbox{for }\mathbf{x}\in S_{\mathbf{X}},
\end{equation}
and
\begin{equation}
\int\!\!\!\int\cdots\int f_{\mathbf{X}}(\mathbf{x})\,\mathrm{d} x_{1}\mathrm{d} x_{2}\cdots\mathrm{d} x_{n}=1,
\end{equation}
or even shorter: \(\int
f_{\mathbf{X}}(\mathbf{x})\,\mathrm{d}\mathbf{x}=1\). The joint CDF
\(F_{\mathbf{X}}\) is defined by
\begin{equation}
F_{\mathbf{X}}(\mathbf{x})=\mathbb{P}(X_{1}\leq x_{1},\, X_{2}\leq x_{2},\ldots,\, X_{n}\leq x_{n}),
\end{equation}
for \(\mathbf{x}\in\mathbb{R}^{n}\). The expectation of a function
\(g(\mathbf{X})\) is defined just as we would imagine:
\begin{equation}
\mathbb{E} g(\mathbf{X})=\int g(\mathbf{x})\, f_{\mathbf{X}}(\mathbf{x})\,\mathrm{d}\mathbf{x}.
\end{equation}
provided the integral exists and is finite. And the moment generating
function in the multivariate case is defined by
\begin{eqnarray}
M_{\mathbf{X}}(\mathbf{t}) & = & \mathbb{E}\exp\left\{ \mathbf{t}^{\mathrm{T}}\mathbf{X}\right\},
\end{eqnarray}
whenever the integral exists and is finite for all \(\mathbf{t}\) in a
neighborhood of \(\mathbf{0}_{\mathrm{n}\times1}\) (note that
\(\mathbf{t}^{\mathrm{T}}\mathbf{X}\) is shorthand for
\(t_{1}X_{1}+t_{2}X_{2}+\cdots+t_{n}X_{n}\)). The only difference in
any of the above for the discrete case is that integrals are replaced
by sums.

Marginal distributions are obtained by integrating out remaining
variables from the joint distribution. And even if we are given all of
the univariate marginals it is not enough to determine the joint
distribution uniquely.

We say that \(X_{1}\), \(X_{2}\), \ldots, \(X_{n}\) are \textit{mutually
  independent} if their joint PDF factors into the product of the
marginals
\begin{equation}
f_{\mathbf{X}}(\mathbf{x})=f_{X_{1}}(x_{1})\, f_{X_{2}}(x_{2})\,\cdots\, f_{X_{n}}(x_{n}),
\end{equation}
for every \(\mathbf{x}\) in their joint support \(S_{\mathbf{X}}\),
and we say that \(X_{1}\), \(X_{2}\), \ldots, \(X_{n}\) are
\textit{exchangeable} if their joint PDF (or CDF) is a symmetric function of
its \(n\) arguments, that is, if
\begin{equation}
f_{\mathbf{X}}(\mathbf{x^{\ast}})=f_{\mathbf{X}}(\mathbf{x}),
\end{equation}
for any reordering (permutation) \(\mathbf{x^{\ast}}\) of the elements of \(\mathbf{x}=(x_{1},x_{2},\ldots,x_{n})\) in the joint support.



\begin{prop}[]
\label{prp:mean-sd-lin-comb}
Let \(X_{1}\), \(X_{2}\), \ldots, \(X_{n}\) be
independent with respective population means \(\mu_{1}\), \(\mu_{2}\),
\ldots, \(\mu_{n}\) and standard deviations \(\sigma_{1}\),
\(\sigma_{2}\), \ldots, \(\sigma_{n}\). For given constants \(a_{1}\),
\(a_{2}\), \ldots,\(a_{n}\) define \(Y=\sum_{i=1}^{n}a_{i}X_{i}\). Then
the mean and standard deviation of \(Y\) are given by the formulas
\begin{equation}
\mu_{Y}=\sum_{i=1}^{n}a_{i}\mu_{i},\quad \sigma_{Y}=\left(\sum_{i=1}^{n}a_{i}^{2}\sigma_{i}^{2}\right)^{1/2}.
\end{equation}
\end{prop}



\begin{proof}
The mean is easy: \[ \mathbb{E}
Y=\mathbb{E}\left(\sum_{i=1}^{n}a_{i}X_{i}\right)=\sum_{i=1}^{n}a_{i}\mathbb{E}
X_{i}=\sum_{i=1}^{n}a_{i}\mu_{i}.  \] The variance is not too
difficult to compute either. As an intermediate step, we calculate
\(\mathbb{E} Y^{2}\).  \[ \mathbb{E}
Y^{2}=\mathbb{E}\left(\sum_{i=1}^{n}a_{i}X_{i}\right)^{2}=\mathbb{E}\left(\sum_{i=1}^{n}a_{i}^{2}X_{i}^{2}+2\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}a_{i}a_{j}X_{i}X_{j}\right).
\] Using linearity of expectation the \(\mathbb{E}\) distributes
through the sums. Now \(\mathbb{E}
X_{i}^{2}=\sigma_{i}^{2}+\mu_{i}^{2}\) and \(\mathbb{E}
X_{i}X_{j}=\mathbb{E} X_{i}\mathbb{E} X_{j}=\mu_{i}\mu_{j}\) when
\(i\neq j\) because of independence. Thus
\begin{eqnarray*}
\mathbb{E} Y^{2} & = & \sum_{i=1}^{n}a_{i}^{2}(\sigma_{i}^{2}+\mu_{i}^{2})+2\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}a_{i}a_{j}\mu_{i}\mu_{j}\\
 & = & \sum_{i=1}^{n}a_{i}^{2}\sigma_{i}^{2}+\left(\sum_{i=1}^{n}a_{i}^{2}\mu_{i}^{2}+2\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}a_{i}a_{j}\mu_{i}\mu_{j}\right)
\end{eqnarray*}
To complete the proof, note that the expression in the parentheses is
exactly \(\left(\mathbb{E} Y\right)^{2}\), and recall the identity
\(\sigma_{Y}^{2}=\mathbb{E} Y^{2}-\left(\mathbb{E} Y\right)^{2}\).
\end{proof}

There is a corresponding statement of Fact~\ref{fac:indep-then-function-indep} for the multivariate case. The proof is
also omitted here.

\begin{fact}[]
If \(\mathbf{X}\) and \(\mathbf{Y}\) are mutually independent random
vectors, then \(u(\mathbf{X})\) and \(v(\mathbf{Y})\) are independent
for any functions \(u\) and \(v\).
\end{fact}

Bruno de Finetti was a strong proponent of the subjective approach to
probability. He proved an important theorem in 1931 which illuminates
the link between exchangeable random variables and independent random
variables. Here it is in one of its simplest forms.



\begin{thm}[De Finetti's Theorem]
Let \(X_{1}\), \(X_{2}\), \ldots be a sequence of
\(\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p)\) random variables
such that \((X_{1},\ldots,X_{k})\) are exchangeable for every
\(k\). Then there exists a random variable \(\Theta\) with support
\([0,1]\) and PDF \(f_{\Theta}(\theta)\) such that
\begin{equation}
\label{eq:definetti-binary}
\mathbb{P}(X_{1}=x_{1},\ldots,\, X_{k}=x_{k})=\int_{0}^{1}\theta^{\sum x_{i}}(1-\theta)^{k-\sum x_{i}}\, f_{\Theta}(\theta)\,\mathrm{d}\theta,
\end{equation}
for all \(x_{i}=0,\,1\), \(i=1,\,2,\ldots,k\).
\end{thm}

To get a handle on the intuitive content de Finetti's theorem, imagine
that we have a \texttt{bunch} of coins in our pocket with each having its own
unique value of \(\theta=\mathbb{P}(\mbox{Heads})\). We reach into our
pocket and select a coin at random according to some probability --
say, \(f_{\Theta}(\theta)\). We take the randomly selected coin and
flip it \(k\) times.

Think carefully: the conditional probability of observing a sequence
\(X_{1}=x_{1},\ldots,\, X_{k}=x_{k}\), given a specific coin
\(\theta\) would just be
\(\theta^{\sum x_{i}}(1-\theta)^{k-\sum x_{i}}\), because the coin
flips are an independent sequence of Bernoulli trials. But the coin is
random, so the Theorem of Total Probability says we can get the
\textit{unconditional} probability
\(\mathbb{P}(X_{1}=x_{1},\ldots,\, X_{k}=x_{k})\) by adding up terms
that look like
\begin{equation}
\theta^{\sum x_{i}}(1-\theta)^{k-\sum x_{i}}\, f_{\Theta}(\theta),
\end{equation}
where we sum over all possible coins. The right-hand side of Equation
\eqref{eq:definetti-binary} is a sophisticated way to denote this process.

Of course, the integral's value does not change if we jumble the
\(x_{i}\)'s, so \((X_{1},\ldots,X_{k})\) are clearly exchangeable. The
power of de Finetti's Theorem is that \textit{every} infinite binary
exchangeable sequence can be written in the above form.

The connection to subjective probability: our prior information about
\(\theta\) corresponds to \(f_{\Theta}(\theta)\) and the likelihood of
the sequence \(X_{1}=x_{1},\ldots,\, X_{k}=x_{k}\) (conditional on
\(\theta\)) corresponds to \(\theta^{\sum x_{i}}(1-\theta)^{k-\sum
x_{i}}\). Compare Equation \eqref{eq:definetti-binary} to Section~\ref{sec:bayes-rule} and Section~\ref{sec:conditional-distributions}.

The multivariate normal distribution immediately generalizes from the
bivariate case. If the matrix \(\Sigma\) is nonsingular then the joint
PDF of
\(\mathbf{X}\sim\mathsf{mvnorm}(\mathtt{mean}=\upmu,\,\mathtt{sigma}=\Sigma)\)
is
\begin{equation}
f_{\mathbf{X}}(\mathbf{x})=\frac{1}{(2\pi)^{n/2}\left|\Sigma\right|^{1/2}}\exp\left\{ -\frac{1}{2}\left(\mathbf{x}-\upmu\right)^{\top}\Sigma^{-1}\left(\mathbf{x}-\upmu\right)\right\},
\end{equation}
and the MGF is
\begin{equation}
M_{\mathbf{X}}(\mathbf{t})=\exp\left\{ \upmu^{\top}\mathbf{t}+\frac{1}{2}\mathbf{t}^{\top}\Sigma\mathbf{t}\right\}.
\end{equation}
We will need the following in Chapter~\ref{cha:multiple-linear-regression}.



\begin{thm}[]
\label{thm:mvnorm-dist-matrix-prod}
If \(\mathbf{X} \sim
\mathsf{mvnorm}(\mathtt{mean} = \upmu, \, \mathtt{sigma} = \Sigma)\)
and \(\mathbf{A}\) is any matrix, then the random vector
\(\mathbf{Y}=\mathbf{AX}\) is distributed
\begin{equation}
\mathbf{Y}\sim\mathsf{mvnorm}(\mathtt{mean}=\mathbf{A}\upmu,\,\mathtt{sigma}=\mathbf{A}\Sigma\mathbf{A}^{\mathrm{T}}).
\end{equation}
\end{thm}



\begin{proof}
Look at the MGF of \(\mathbf{Y}\):
\begin{eqnarray*}
M_{\mathbf{Y}}(\mathbf{t}) & = & \mathbb{E}\,\exp\left\{ \mathbf{t}^{\mathrm{T}}(\mathbf{AX})\right\} ,\\
 & = & \mathbb{E}\,\exp\left\{ (\mathbf{A}^{\mathrm{T}}\mathbf{t})^{\mathrm{T}}\mathbf{X}\right\} ,\\
 & = & \exp\left\{ \upmu^{\mathrm{T}}(\mathbf{A}^{\top}\mathbf{t})+\frac{1}{2}(\mathbf{A}^{\mathrm{T}}\mathbf{t})^{\mathrm{T}}\Sigma(\mathbf{A}^{\mathrm{T}}\mathbf{t})\right\} ,\\
 & = & \exp\left\{ \left(\mathbf{A}\upmu\right)^{\mathrm{T}}\mathbf{t}+\frac{1}{2}\mathbf{t}^{\mathrm{T}}\left(\mathbf{A}\Sigma\mathbf{A}^{\mathrm{T}}\right)\mathbf{t}\right\},
\end{eqnarray*}
and the last expression is the MGF of an
\(\mathsf{mvnorm}(\mathtt{mean}=\mathbf{A}\upmu,\,\mathtt{sigma}=\mathbf{A}\Sigma\mathbf{A}^{\mathrm{T}})\)
distribution.
\end{proof}

\section{The Multinomial Distribution} \label{sec:multinomial}


We sample \(n\) times, with replacement, from an urn that contains
balls of \(k\) different types. Let \(X_{1}\) denote the number of
balls in our sample of type 1, let \(X_{2}\) denote the number of
balls of type 2, \ldots, and let \(X_{k}\) denote the number of balls of
type \(k\). Suppose the urn has proportion \(p_{1}\) of balls of type
1, proportion \(p_{2}\) of balls of type 2, \ldots, and proportion
\(p_{k}\) of balls of type \(k\). Then the joint PMF of
\((X_{1},\ldots,X_{k})\) is
\begin{eqnarray}
f_{X_{1},\ldots,X_{k}}(x_{1},\ldots,x_{k}) & = & {n \choose x_{1}\, x_{2}\,\cdots\, x_{k}}\, p_{1}^{x_{1}}p_{2}^{x_{2}}\cdots p_{k}^{x_{k}},
\end{eqnarray}
for \((x_{1},\ldots,x_{k})\) in the joint support \(S_{X_{1},\ldots X_{K}}\). We write
\begin{equation}
(X_{1},\ldots,X_{k})\sim\mathsf{multinom}(\mathtt{size}=n,\,\mathtt{prob}=\mathbf{p}_{\mathrm{k}\times1}).
\end{equation}
Several comments are in order. First, the joint support set
\(S_{X_{1},\ldots X_{K}}\) contains all nonnegative integer
\(k\)-tuples \((x_{1},\ldots,x_{k})\) such that
\(x_{1}+x_{2}+\cdots+x_{k}=n\). A support set like this is called a
\textit{simplex}. Second, the proportions \(p_{1}\), \(p_{2}\),\ldots,
\(p_{k}\) satisfy \(p_{i}\geq0\) for all \(i\) and
\(p_{1}+p_{2}+\cdots+p_{k}=1\). Finally, the symbol
\begin{equation}
{n \choose x_{1}\, x_{2}\,\cdots\, x_{k}}=\frac{n!}{x_{1}!\, x_{2}!\,\cdots x_{k}!}
\end{equation}
is called a \textit{multinomial coefficient} which generalizes the notion of
a binomial coefficient we saw in Equation
\eqref{eq:binomial-coefficient}.

The form and notation we have just described matches the \textsf{R} usage but
is not standard among other texts. Most other books use the above for
a \(k-1\) dimension multinomial distribution, because the linear
constraint \(x_{1}+x_{2}+\cdots+x_{k}=n\) means that once the values
of \(X_{1}\), \(X_{2}\), \ldots, \(X_{k-1}\) are known the final value
\(X_{k}\) is determined, not random. Another term used for this is a
\textit{singular} distribution.

For the most part we will ignore these difficulties, but the careful
reader should keep them in mind. There is not much of a difference in
practice, except that below we will use a two-dimensional support set
for a three-dimension multinomial distribution. See Figure~\ref{fig:multinom-pmf2}.

When \(k=2\), we have \(x_{1}=x\) and \(x_{2}=n-x\), we have
\(p_{1}=p\) and \(p_{2}=1-p\), and the multinomial coefficient is
literally a binomial coefficient. In the previous notation we have
thus shown that the
\(\mathsf{multinom}(\mathtt{size}=n,\,\mathtt{prob}=\mathbf{p}_{2\times1})\)
distribution is the same as a
\(\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p)\) distribution.



\begin{example}[Dinner with Barack Obama]
During the 2008 U.S. presidential primary,
Barack Obama offered to have dinner with three randomly selected
monetary contributors to his campaign. Imagine the thousands of people
in the contributor database. For the sake of argument, Suppose that
the database was approximately representative of the U.S. population
as a whole, Suppose Barack Obama wants to have \url{http://pewresearch.org/pubs/773/fewer-voters-identify-as-republicans} with 36
democrats, 27 republicans, and 37 independents.
\end{example}

BLANK



\begin{rem}[]
Here are some facts about the multinomial distribution.

\begin{enumerate}
\item The expected value of \((X_{1},\, X_{2},\,\ldots,\, X_{k})\) is
   \(n\mathbf{p}_{k\times1}\).
\item The variance-covariance matrix \(\Sigma\) is symmetric with
   diagonal entries \(\sigma_{i}^{2}=np_{i}(1-p_{i})\),
   \(i=1,\,2,\,\ldots,\, k\) and off-diagonal entries
   \(\mbox{Cov}(X_{i},\, X_{j})=-np_{i}p_{j}\), for \(i\neq j\). The
   correlation between \(X_{i}\) and \(X_{j}\) is therefore
   \(\mbox{Corr}(X_{i},\,
   X_{j})=-\sqrt{p_{i}p_{j}/(1-p_{i})(1-p_{j})}\).
\item The marginal distribution of \((X_{1},\, X_{2},\,\ldots,\,
   X_{k-1})\) is
   \(\mathsf{multinom}(\mathtt{size}=n,\,\mathtt{prob}=\mathbf{p}_{(k-1)\times1})\)
   with
   \begin{equation}
   \mathbf{p}_{(k-1)\times1}=\left(p_{1},\, p_{2},\,\ldots,\, p_{k-2},\, p_{k-1}+p_{k}\right),
   \end{equation}
   and in particular,
   \(X_{i}\sim\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p_{i})\).
\end{enumerate}
\end{rem}



\subsection{How to do it with \textsf{R}}

There is support for the multinomial distribution in base
R, namely in the \texttt{stats} package \cite{stats}. The
\texttt{dmultinom} function represents the PMF and the \texttt{rmultinom} function
generates random variates.

<<echo=TRUE>>=
tmp <- t(xsimplex(3, 6))
p <- apply(tmp, MARGIN = 1, FUN = dmultinom, prob = c(36,27,37))
S <- probspace(tmp, probs = p)
ProbTable <- xtabs(probs ~ X1 + X2, data = S)
round(ProbTable, 3)
@

BLANK

Do some examples of \texttt{rmultinom}.

Another way to do the plot is with the \texttt{scatterplot3d} function in the
\texttt{scatterplot3d} package \cite{scatterplot3d}. It looks like this:

<<eval=FALSE>>=
library("scatterplot3d")
X <- t(as.matrix(expand.grid(0:6, 0:6)))
X <- X[, colSums(X) <= 6 ]; X <- rbind(X, 6 - colSums(X))
Z <- round(apply(X, 2, function(x) dmultinom(x, prob = 1:3)), 3)
A <- data.frame(x = X[1, ], y = X[2, ], probability = Z)
scatterplot3d(A, type = "h", lwd = 3, box = FALSE)
@

The \texttt{scatterplot3d} graph looks better in this example, but the code
is more difficult to understand. And with \texttt{cloud} one can easily do
conditional plots of the form \texttt{cloud(z ~ x + y | f)}, where \texttt{f} is a
factor.

<<echo=TRUE, eval=FALSE>>=
cloud(probs ~ X1 + X2, data = S, type = c("p","h"), lwd = 2,
            pch = 16, cex = 1.5, screen = list(z = 15, x = -70))
@

<<multinom-pmf2, echo=FALSE, fig=TRUE, include=FALSE, width=5>>=
print(cloud(probs ~ X1 + X2, data = S, type = c("p","h"), lwd = 2,
            pch = 16, cex = 1.5), screen = list(z = 15, x = -70))
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-multinom-pmf2}
\end{center}
\caption{{\small A plot of a multinomial PMF.}}
\label{fig:multinom-pmf2}
\end{figure}




\section{Chapter Exercises}

\begin{Exercise}[label=exr:prove-cov-shortcut]
Prove that \(\mbox{Cov}(X,Y)=\mathbb{E}(XY)-(\mathbb{E} X)(\mathbb{E} Y).\)
\end{Exercise}



\begin{Exercise}[label=exr:sum-indep-chisq]
Suppose
\(X\sim\mathsf{chisq}(\mathtt{df}=p_{1})\) and
\(Y\sim\mathsf{chisq}(\mathtt{df}=p_{2})\) are independent. Find the
distribution of \(X+Y\) (you may want to refer to Equation
\eqref{eq:mgf-chisq}).
\end{Exercise}





\begin{Exercise}[label=exr:diff-indep-norm]
Show that when \(X\) and \(Y\) are independent
the MGF of \(X-Y\) is \(M_{X}(t)M_{Y}(-t)\). Use this to find the
distribution of \(X-Y\) when
\(X\sim\mathsf{norm}(\mathtt{mean}=\mu_{1},\,\mathtt{sd}=\sigma_{1})\)
and
\(Y\sim\mathsf{norm}(\mathtt{mean}=\mu_{2},\,\mathtt{sd}=\sigma_{2})\)
are independent.
\end{Exercise}



\chapter{Sampling Distributions} 
\label{cha:sampling-distributions}


This is an important chapter; it is the bridge from probability and
descriptive statistics that we studied in Chapters
\ref{cha:describing-data-distributions} through~\ref{cha:multivariable-distributions} to inferential statistics which
forms the latter part of this book.

Here is the link: we are presented with a \textit{population} about which we
would like to learn. And while it would be desirable to examine every
single member of the population, we find that it is either impossible
or infeasible to for us to do so, thus, we resort to collecting a
\textit{sample} instead. We do not lose heart. Our method will suffice,
provided the sample is \textit{representative} of the population. A good way
to achieve this is to sample \textit{randomly} from the population.

Supposing for the sake of argument that we have collected a random
sample, the next task is to make some \textit{sense} out of the data
because the complete list of sample information is usually cumbersome,
unwieldy. We summarize the data set with a descriptive
\textit{statistic}, a quantity calculated from the data (we saw many
examples of these in Chapter~\ref{cha:describing-data-distributions}). But our sample was
random\ldots therefore, it stands to reason that our statistic will be
random, too. How is the statistic distributed?

The probability distribution associated with the population (from
which we sample) is called the \textit{population distribution}, and the
probability distribution associated with our statistic is called its
\textit{sampling distribution}; clearly, the two are interrelated. To learn
about the population distribution, it is imperative to know everything
we can about the sampling distribution. Such is the goal of this
chapter.

We begin by introducing the notion of simple random samples and
cataloguing some of their more convenient mathematical
properties. Next we focus on what happens in the special case of
sampling from the normal distribution (which, again, has several
convenient mathematical properties), and in particular, we meet the
sampling distribution of \(\overline{X}\) and \(S^{2}\). Then we
explore what happens to \(\overline{X}\)'s sampling distribution when
the population is not normal and prove one of the most remarkable
theorems in statistics, the Central Limit Theorem (CLT).

With the CLT in hand, we then investigate the sampling distributions
of several other popular statistics, taking full advantage of those
with a tractable form. We finish the chapter with an exploration of
statistics whose sampling distributions are not quite so tractable,
and to accomplish this goal we will use simulation methods that are
grounded in all of our work in the previous four chapters.

\paragraph{What do I want them to know?}

\begin{itemize}
\item the notion of population versus simple random sample, parameter
  versus statistic, and population distribution versus sampling
  distribution
\item the classical sampling distributions of the standard one and two
  sample statistics
\item how to generate a simulated sampling distribution when the statistic
  is crazy
\item the Central Limit Theorem, period.
\item some basic concepts related to sampling distribution utility, such
  as bias and variance
\end{itemize}

\section{Simple Random Samples} \label{sec:simple-random-samples}

\begin{defn}[Simple random sample.]
If \(X_{1}\), \(X_{2}\), \ldots, \(X_{n}\) are independent with
\(X_{i}\sim f\) for \(i=1,2,\ldots,n\), then we say that \(X_{1}\),
\(X_{2}\), \ldots, \(X_{n}\) are \textit{independent and identically
distributed} (IID) from the population \(f\) or alternatively we say
that \(X_{1}\), \(X_{2}\), \ldots, \(X_{n}\) are a \textit{simple random sample
of size} \(n\), denoted \(SRS(n)\), from the population \(f\).
\end{defn}



\begin{prop}[]
\label{prp:mean-sd-xbar}
Let \(X_{1}\), \(X_{2}\), \ldots, \(X_{n}\) be a
\(SRS(n)\) from a population distribution with mean \(\mu\) and finite
standard deviation \(\sigma\). Then the mean and standard deviation of
\(\overline{X}\) are given by the formulas \(\mu_{\overline{X}}=\mu\)
and \(\sigma_{\overline{X}}=\sigma/\sqrt{n}\).
\end{prop}



\begin{proof}
Plug in \(a_{1}=a_{2}=\cdots=a_{n}=1/n\) in Proposition~\ref{prp:mean-sd-lin-comb}.
\end{proof}

The next fact will be useful to us when it comes time to prove the
Central Limit Theorem in Section~\ref{sec:central-limit-theorem}.



\begin{prop}[]
\label{prp:mgf-xbar}
Let \(X_{1}\), \(X_{2}\), \ldots, \(X_{n}\) be a
\(SRS(n)\) from a population distribution with MGF \(M(t)\). Then the
MGF of \(\overline{X}\) is given by
\begin{equation}
M_{\overline{X}}(t)=\left[M\left(\frac{t}{n}\right)\right]^{n}.
\end{equation}
\end{prop}



\begin{proof}
Go from the definition:
\begin{eqnarray*}
M_{\overline{X}}(t) & = & \mathbb{E}\,\mathrm{e}^{t\overline{X}},\\
 & = & \mathbb{E}\,\mathrm{e}^{t(X_{1}+\cdots+X_{n})/n},\\
 & = & \mathbb{E}\,\mathrm{e}^{tX_{1}/n}\mathrm{e}^{tX_{2}/n}\cdots\mathrm{e}^{tX_{n}/n}.
\end{eqnarray*}
And because \(X_{1}\), \(X_{2}\), \ldots, \(X_{n}\) are independent,
Proposition~\ref{prp:indep-implies-prodexpect} allows us to distribute the
expectation among each term in the product, which is \[
\mathbb{E}\mathrm{e}^{tX_{1}/n}\,\mathbb{E}\mathrm{e}^{tX_{2}/n}\cdots\mathbb{E}\mathrm{e}^{tX_{n}/n}.
\] The last step is to recognize that each term in last product above
is exactly \(M(t/n)\).
\end{proof}

\section{Sampling from a Normal Distribution} \label{sec:sampling-from-normal-dist}


\subsection{The Distribution of the Sample Mean} \label{sub:samp-mean-dist-of}


\begin{prop}[]
Let \(X_{1}\), \(X_{2}\), \ldots, \(X_{n}\) be a \(SRS(n)\) from a
\(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\)
distribution. Then the sample mean \(\overline{X}\) has a
\(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma/\sqrt{n})\)
sampling distribution.
\end{prop}


\begin{proof}
The mean and standard deviation of \(\overline{X}\) follow directly
from Proposition~\ref{prp:mean-sd-xbar}. To address the shape, first
remember from Section~\ref{sec:the-normal-distribution} that the
\(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) MGF is of
the form \[ M(t)=\exp\left[ \mu t+\sigma^{2}t^{2}/2\right] .  \] Now
use Proposition~\ref{prp:mgf-xbar} to find
\begin{eqnarray*}
M_{\overline{X}}(t) & = & \left[M\left(\frac{t}{n}\right)\right]^{n},\\
 & = & \left[\exp\left( \mu(t/n)+\sigma^{2}(t/n)^{2}/2\right) \right]^{n},\\
 & = & \exp\left( \, n\cdot\left[\mu(t/n)+\sigma^{2}(t/n)^{2}/2\right]\right) ,\\
 & = & \exp\left( \mu t+(\sigma/\sqrt{n})^{2}t^{2}/2\right),
\end{eqnarray*}
and we recognize this last quantity as the MGF of a
\(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma/\sqrt{n})\)
distribution.
\end{proof}


\subsection{The Distribution of the Sample Variance} \label{sub:samp-var-dist}


\begin{thm}[]
\label{thm:xbar-and-s}
Let \(X_{1}\), \(X_{2}\), \ldots, \(X_{n}\) be a
\(SRS(n)\) from a
\(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\)
distribution, and let
\begin{equation}
\overline{X}=\sum_{i=1}^{n}X_{i}\quad \mbox{and}\quad S^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}.
\end{equation}
Then

\begin{enumerate}
\item \(\overline{X}\) and \(S^{2}\) are independent, and
\item The rescaled sample variance
    \begin{equation}
    \frac{(n-1)}{\sigma^{2}}S^{2}=\frac{\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}}{\sigma^{2}}
    \end{equation}
    has a \(\mathsf{chisq}(\mathtt{df}=n-1)\) sampling distribution.
\end{enumerate}
\end{thm}



\begin{proof}
The proof is beyond the scope of the present book, but the theorem is
simply too important to be omitted. The interested reader could
consult Casella and Berger \cite{Casella2002}, or Hogg \textit{et al}
\cite{Hogg2005}.
\end{proof}


\subsection{The Distribution of Student's \(t\) Statistic} \label{sub:student-t-distribution}


\begin{prop}[]
Let \(X_{1}\), \(X_{2}\), \ldots, \(X_{n}\) be a \(SRS(n)\) from a
\(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\)
distribution. Then the quantity
\begin{equation}
T=\frac{\overline{X}-\mu}{S/\sqrt{n}}
\end{equation}
has a \(\mathsf{t}(\mathtt{df}=n-1)\) sampling distribution.
\end{prop}



\begin{proof}
Divide the numerator and denominator by \(\sigma\) and rewrite \[
T=\frac{\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}}{S/\sigma}=\frac{\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}}{\sqrt{\left.\frac{(n-1)S^{2}}{\sigma^{2}}\right/
(n-1)}}.  \] Now let \[
Z=\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\quad \mbox{and}\quad
V=\frac{(n-1)S^{2}}{\sigma^{2}}, \] so that
\begin{equation}
T=\frac{Z}{\sqrt{V/r}},
\end{equation}
where \(r=n-1\).

We know from Section~\ref{sub:samp-mean-dist-of} that
\(Z\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) and we know
from Section~\ref{sub:samp-var-dist} that
\(V\sim\mathsf{chisq}(\mathtt{df}=n-1)\). Further, since we are
sampling from a normal distribution, Theorem~\ref{thm:xbar-and-s} gives
that \(\overline{X}\) and \(S^{2}\) are independent and by Fact~\ref{fac:indep-then-function-indep} so are \(Z\) and \(V\). In summary,
the distribution of \(T\) is the same as the distribution of the
quantity \(Z/\sqrt{V/r}\), where
\(Z\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) and
\(V\sim\mathsf{chisq}(\mathtt{df}=r)\) are independent. This is in
fact the definition of Student's \(t\) distribution.
\end{proof}

This distribution was first published by W. S. Gosset
\cite{Student1908} under the pseudonym Student, and the distribution
has consequently come to be known as Student's \(t\) distribution. The
PDF of \(T\) can be derived explicitly using the techniques of
Section~\ref{sec:functions-of-continuous}; it takes the form
\begin{equation}
f_{X}(x)=\frac{\Gamma[(r+1)/2]}{\sqrt{r\pi}\ \Gamma(r/2)}\left(1+\frac{x^{2}}{r}\right)^{-(r+1)/2},\quad -\infty < x < \infty.
\end{equation}
Any random variable \(X\) with the preceding PDF is said to have
Student's \(t\) distribution with \(r\) \textit{degrees of freedom},
and we write \(X\sim\mathsf{t}(\mathtt{df}=r)\). The shape of the PDF
is similar to the normal, but the tails are considerably heavier. See
Figure~\ref{fig:students-t-dist-vary-df}. As with the normal
distribution, there are four functions in \textsf{R} associated with the \(t\)
distribution, namely \texttt{dt}, \texttt{pt},\texttt{qt}, and
\texttt{rt}, which compute the PDF, CDF, quantile function, and
generate random variates, respectively.

The code to produce Figure~\ref{fig:students-t-dist-vary-df} is

<<students-t-dist-vary-df, echo=FALSE, fig=TRUE, include=FALSE, height=3.5,width=5>>=
curve(dt(x, df = 30), from = -3, to = 3, lwd = 3, ylab = "y")
ind <- c(1, 2, 3, 5, 10)
for (i in ind) curve(dt(x, df = i), -3, 3, add = TRUE)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-students-t-dist-vary-df}
\end{center}
\caption{{\small A plot of Student's \(t\) distribution for various degrees of freedom.}}
\label{fig:students-t-dist-vary-df}
\end{figure}



Similar to that done for the normal we may define
\(\mathsf{t}_{\alpha}(\mathtt{df}=n-1)\) as the number on the
\(x\)-axis such that there is exactly \(\alpha\) area under the
\(\mathsf{t}(\mathtt{df}=n-1)\) curve to its right.



\begin{example}[]
Find \(\mathsf{t}{}_{0.01}(\mathtt{df}=23)\) with the quantile
function.
\end{example}

<<echo=TRUE>>=
qt(0.01, df = 23, lower.tail = FALSE)
@



\begin{rem}[]
There are a few things to note about the \(\mathtt{t}(\mathtt{df}=r)\)
distribution.

\begin{enumerate}
\item The \(\mathtt{t}(\mathtt{df}=1)\) distribution is the same as the
   \(\mathsf{cauchy}(\mathtt{location}=0,\,\mathtt{scale}=1)\)
   distribution. The Cauchy distribution is rather pathological and is
   a counterexample to many famous results.
\item The standard deviation of \(\mathsf{t}(\mathtt{df}=r)\) is
   undefined (that is, infinite) unless \(r>2\). When \(r\) is more
   than 2, the standard deviation is always bigger than one, but
   decreases to 1 as \(r\to\infty\).
\item As \(r\to\infty\), the \(\mathtt{t}(\mathtt{df}=r)\) distribution
   approaches the \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\)
   distribution.
\end{enumerate}
\end{rem}

\section{The Central Limit Theorem} \label{sec:central-limit-theorem}

In this section we study the distribution of the sample mean when the
underlying distribution is \textit{not} normal. We saw in Section~\ref{sec:sampling-from-normal-dist} that when \(X_{1}\),
\(X_{2}\),\ldots , \(X_{n}\) is a \(SRS(n)\) from a
\(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) distribution
then
\(\overline{X} \sim \mathsf{norm}(\mathtt{mean} = \mu,\,\mathtt{sd} =
\sigma/\sqrt{n})\). In other words, we may say (owing to Fact~\ref{fac:lin-trans-norm-is-norm}) when the underlying population is
normal that the sampling distribution of \(Z\) defined by
\begin{equation}
Z=\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}
\end{equation}
is \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\).

However, there are many populations that are \textit{not} normal \ldots and the
statistician often finds herself sampling from such populations. What
can be said in this case? The surprising answer is contained in the
following theorem.



\begin{thm}[Central Limit Theorem]
\label{thm:central-limit-theorem}
Let \(X_{1}\), \(X_{2}\), \ldots, \(X_{n}\) be
a \(SRS(n)\) from a population distribution with mean \(\mu\) and
finite standard deviation \(\sigma\). Then the sampling distribution
of
\begin{equation}
Z=\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}
\end{equation}
approaches a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) distribution as \(n\to\infty\).
\end{thm}



\begin{rem}[]
  We suppose that \(X_{1}\), \(X_{2}\), \ldots , \(X_{n}\) are IID, and
  we learned in Section~\ref{sec:simple-random-samples} that
  \(\overline{X}\) has mean \(\mu\) and standard deviation
  \(\sigma/\sqrt{n}\), so we already knew that \(Z\) has mean zero and
  standard deviation one. The beauty of the CLT is that it addresses
  the \textit{shape} of \(Z\)'s distribution when the sample size is
  large.
\end{rem}



\begin{rem}[]
  Notice that the shape of the underlying population's distribution is
  not mentioned in Theorem~\ref{thm:central-limit-theorem}; indeed,
  the result is true for any population that is well-behaved enough to
  have a finite standard deviation. In particular, if the population
  is normally distributed then we know from Section~\ref{sub:samp-mean-dist-of} that the distribution of
  \(\overline{X}\) (and \(Z\) by extension) is \textit{exactly}
  normal, for \textit{every} \(n\).
\end{rem}



\begin{rem}[]
How large is ``sufficiently large''? It is here that the shape of the
underlying population distribution plays a role. For populations with
distributions that are approximately symmetric and mound-shaped, the
samples may need to be only of size four or five, while for highly
skewed or heavy-tailed populations the samples may need to be much
larger for the distribution of the sample means to begin to show a
bell-shape. Regardless, for a given population distribution (with
finite standard deviation) the approximation tends to be better for
larger sample sizes.
\end{rem}


\subsection{How to do it with \textsf{R}}

The \texttt{TeachingDemos} package \cite{TeachingDemos} has
\texttt{clt.examp} and the \texttt{distrTeach} \cite{distrTeach}
package has \texttt{illustrateCLT}. Try the following at the command
line (output omitted):
<<echo=TRUE, eval=FALSE>>=
example(clt.examp)
@
and
<<echo=TRUE, eval=FALSE>>=
example(illustrateCLT)
@

The \texttt{IPSUR} package \cite{IPSUR} has the functions \texttt{clt1}, \texttt{clt2}, and
\texttt{clt3} (see Exercise~\ref{exr:clt123} at the end of this chapter). Its purpose
is to investigate what happens to the sampling distribution of
\(\overline{X}\) when the population distribution is mound shaped,
finite support, and skewed, namely \(\mathsf{t}(\mathtt{df}=3)\),
\(\mathsf{unif}(\mathtt{a}=0,\,\mathtt{b}=10)\), and
\(\mathsf{gamma}(\mathtt{shape}=1.21,\,\mathtt{scale}=1/2.37)\),
respectively.

For example, when the command \texttt{clt1()} is issued a plot window opens
to show a graph of the PDF of a \(\mathsf{t}(\mathtt{df}=3)\)
distribution. On the display are shown numerical values of the
population mean and variance. While the students examine the graph the
computer is simulating random samples of size \texttt{sample.size = 2} from
the population distribution \texttt{rt} a total of \texttt{N.iter = 100000} times,
and sample means are calculated of each sample. Next follows a
histogram of the simulated sample means, which closely approximates
the sampling distribution of \(\overline{X}\), see Section~\ref{sec:simulated-sampling-distributions}. Also shown are the sample
mean and sample variance of all of the simulated \(\overline{X}\)
values. As a final step, when the student clicks the second plot, a
normal curve with the same mean and variance as the simulated
\(\overline{X}\) values is superimposed over the histogram. Students
should compare the population theoretical mean and variance to the
simulated mean and variance of the sampling distribution. They should
also compare the shape of the simulated sampling distribution to the
shape of the normal distribution.

The three separate \texttt{clt1}, \texttt{clt2}, and \texttt{clt3} functions were written
so that students could compare what happens overall when the shape of
the population distribution changes.

\section{Sampling Distributions of Two-Sample Statistics} \label{sec:samp-dist-two-samp}


There are often two populations under consideration, and it sometimes
of interest to compare properties between groups. To do so we take
independent samples from each population and calculate respective
sample statistics for comparison. In some simple cases the sampling
distribution of the comparison is known and easy to derive; such cases
are the subject of the present section.

\subsection{Difference of Independent Sample Means}


\begin{prop}[]
Let \(X_{1}\), \(X_{2}\), \ldots , \(X_{n_{1}}\) be an \(SRS(n_{1})\)
from a
\(\mathsf{norm}(\mathtt{mean}=\mu_{X},\,\mathtt{sd}=\sigma_{X})\)
distribution and let \(Y_{1}\), \(Y_{2}\), \ldots , \(Y_{n_{2}}\) be an
\(SRS(n_{2})\) from a
\(\mathsf{norm}(\mathtt{mean}=\mu_{Y},\,\mathtt{sd}=\sigma_{Y})\)
distribution. Suppose that \(X_{1}\), \(X_{2}\), \ldots , \(X_{n_{1}}\)
and \(Y_{1}\), \(Y_{2}\), \ldots , \(Y_{n_{2}}\) are independent
samples. Then the quantity
\begin{equation}
\label{eq:diff-indep-sample-means}
\frac{\overline{X}-\overline{Y}-(\mu_{X}-\mu_{Y})}{\sqrt{\left.\sigma_{X}^{2}\right/ n_{1}+\left.\sigma_{Y}^{2}\right/ n_{2}}}
\end{equation}
has a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) sampling
distribution. Equivalently, \(\overline{X}-\overline{Y}\) has a
\(\mathsf{norm}(\mathtt{mean}=\mu_{X}-\mu_{Y},\,\mathtt{sd}=\sqrt{\left.\sigma_{X}^{2}\right/
n_{1}+\left.\sigma_{Y}^{2}\right/ n_{2}})\) sampling distribution.
\end{prop}



\begin{proof}
We know that \(\overline{X}\) is
\(\mathsf{norm}(\mathtt{mean}=\mu_{X},\,\mathtt{sd}=\sigma_{X}/\sqrt{n_{1}})\)
and we also know that \(\overline{Y}\) is
\(\mathsf{norm}(\mathtt{mean}=\mu_{Y},\,\mathtt{sd}=\sigma_{Y}/\sqrt{n_{2}})\). And
since the samples \(X_{1}\), \(X_{2}\), \ldots, \(X_{n_{1}}\) and
\(Y_{1}\), \(Y_{2}\), \ldots, \(Y_{n_{2}}\) are independent, so too are
\(\overline{X}\) and \(\overline{Y}\). The distribution of their
difference is thus normal as well, and the mean and standard deviation
are given by Proposition~\ref{prp:mean-sd-lin-comb-two}.
\end{proof}



\begin{rem}[]
Even if the distribution of one or both of the samples is not normal,
the quantity in Equation \eqref{eq:diff-indep-sample-means} will be
approximately normal provided both sample sizes are large.
\end{rem}



\begin{rem}[]
For the special case of \(\mu_{X}=\mu_{Y}\) we have shown
 that
\begin{equation}
\frac{\overline{X} - \overline{Y}}{\sqrt{ \sigma_{X}^{2}/ n_{1} + \sigma_{Y}^{2}/n_{2}}}
\end{equation}
has a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) sampling
distribution, or in other words, \(\overline{X} - \overline{Y}\) has a
\(\mathsf{norm}(\mathtt{mean} = 0,\,\mathtt{sd} = \sqrt{\sigma_{X}^{2}
/ n_{1} + \sigma_{Y}^{2} / n_{2}})\) sampling distribution. This will
be important when it comes time to do hypothesis tests; see Section~\ref{sec:conf-interv-for-diff-means}.
\end{rem}

\subsection{Difference of Independent Sample Proportions}


\begin{prop}[]
Let \(X_{1}\), \(X_{2}\), \ldots, \(X_{n_{1}}\) be an \(SRS(n_{1})\) from
a \(\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p_{1})\)
distribution and let \(Y_{1}\), \(Y_{2}\), \ldots, \(Y_{n_{2}}\) be an
\(SRS(n_{2})\) from a
\(\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p_{2})\)
distribution. Suppose that \(X_{1}\), \(X_{2}\), \ldots , \(X_{n_{1}}\)
and \(Y_{1}\), \(Y_{2}\), \ldots , \(Y_{n_{2}}\) are independent
samples. Define
\begin{equation}
\hat{p}_{1}=\frac{1}{n_{1}}\sum_{i=1}^{n_{1}}X_{i}\quad \mbox{and}\quad \hat{p}_{2}=\frac{1}{n_{2}}\sum_{j=1}^{n_{2}}Y_{j}.
\end{equation}
Then the sampling distribution of
\begin{equation}
\frac{\hat{p}_{1}-\hat{p}_{2}-(p_{1}-p_{2})}{\sqrt{\frac{p_{1}(1-p_{1})}{n_{1}}+\frac{p_{2}(1-p_{2})}{n_{2}}}}
\end{equation}
approaches a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) distribution as both \(n_{1},\, n_{2}\to\infty\). In other words, the sampling distribution of \(\hat{p}_{1}-\hat{p}_{2}\) is approximately
\begin{equation}
\mathsf{norm}\left(\mathtt{mean}=p_{1}-p_{2},\,\mathtt{sd}=\sqrt{\frac{p_{1}(1-p_{1})}{n_{1}}+\frac{p_{2}(1-p_{2})}{n_{2}}}\right),
\end{equation}
provided both \(n_{1}\) and \(n_{2}\) are sufficiently large.
\end{prop}



\begin{proof}
  We know that \(\hat{p}_{1}\) is approximately normal for \(n_{1}\)
  sufficiently large by the CLT, and we know that \(\hat{p}_{2}\) is
  approximately normal for \(n_{2}\) sufficiently large, also by the
  CLT. Further, \(\hat{p}_{1}\) and \(\hat{p}_{2}\) are independent
  since they are derived from independent samples. And a difference of
  independent (approximately) normal distributions is (approximately)
  normal\footnote{This does not explicitly follow because of our
    cavalier use of ``approximately'' in too many places. To be more
    thorough, however, would require more concepts than we can afford
    at the moment. The interested reader may consult a more advanced
    text, specifically the topic of weak convergence, that is,
    convergence in distribution.}, by Exercise~\ref{exr:diff-indep-norm}.

The expressions for the mean and standard deviation follow immediately
from Proposition~\ref{prp:mean-sd-lin-comb-two} combined with the formulas
for the \(\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p)\)
distribution from Chapter~\ref{cha:discrete-distributions}.
\end{proof}


\subsection{Ratio of Independent Sample Variances}
\label{sub:ratio-indep-variance}

\begin{prop}[]
Let \(X_{1}\), \(X_{2}\), \ldots, \(X_{n_{1}}\) be an \(SRS(n_{1})\) from
a \(\mathsf{norm}(\mathtt{mean}=\mu_{X},\,\mathtt{sd}=\sigma_{X})\)
distribution and let \(Y_{1}\), \(Y_{2}\), \ldots , \(Y_{n_{2}}\) be an
\(SRS(n_{2})\) from a
\(\mathsf{norm}(\mathtt{mean}=\mu_{Y},\,\mathtt{sd}=\sigma_{Y})\)
distribution. Suppose that \(X_{1}\), \(X_{2}\), \ldots , \(X_{n_{1}}\)
and \(Y_{1}\), \(Y_{2}\), \ldots , \(Y_{n_{2}}\) are independent
samples. Then the ratio
\begin{equation}
F=\frac{\sigma_{Y}^{2}S_{X}^{2}}{\sigma_{X}^{2}S_{Y}^{2}}
\end{equation}
has an \(\mathsf{f}(\mathtt{df1}=n_{1}-1,\,\mathtt{df2}=n_{2}-1)\)
sampling distribution.
\end{prop}

\begin{proof}
We know from Theorem~\ref{thm:xbar-and-s} that
\((n_{1}-1)S_{X}^{2}/\sigma_{X}^{2}\) is distributed
\(\mathsf{chisq}(\mathtt{df}=n_{1}-1)\) and
\((n_{2}-1)S_{Y}^{2}/\sigma_{Y}^{2}\) is distributed
\(\mathsf{chisq}(\mathtt{df}=n_{2}-1)\). Now write \[
F=\frac{\sigma_{Y}^{2}S_{X}^{2}}{\sigma_{X}^{2}S_{Y}^{2}}=\frac{\left.(n_{1}-1)S_{X}^{2}\right/
(n_{1}-1)}{\left.(n_{2}-1)S_{Y}^{2}\right/
(n_{2}-1)}\cdot\frac{\left.1\right/ \sigma_{X}^{2}}{\left.1\right/
\sigma_{Y}^{2}}, \] by multiplying and dividing the numerator with
\(n_{1}-1\) and doing likewise for the denominator with
\(n_{2}-1\). Now we may regroup the terms into \[
F=\frac{\left.\frac{(n_{1}-1)S_{X}^{2}}{\sigma_{X}^{2}}\right/
(n_{1}-1)}{\left.\frac{(n_{2}-1)S_{Y}^{2}}{\sigma_{Y}^{2}}\right/
(n_{2}-1)}, \] and we recognize \(F\) to be the ratio of independent
\(\mathsf{chisq}\) distributions, each divided by its respective
numerator \(\mathtt{df}=n_{1}-1\) and denominator
\(\mathtt{df}=n_{1}-1\) degrees of freedom. This is, indeed, the
definition of Snedecor's \(F\) distribution.
\end{proof}



\begin{rem}[]
For the special case of \(\sigma_{X}=\sigma_{Y}\) we have shown that
\begin{equation}
F=\frac{S_{X}^{2}}{S_{Y}^{2}}
\end{equation}
has an \(\mathsf{f}(\mathtt{df1}=n_{1}-1,\,\mathtt{df2}=n_{2}-1)\)
sampling distribution. This will be important in Chapters~\ref{cha:estimation} onward.
\end{rem}


\section{Simulated Sampling Distributions} \label{sec:simulated-sampling-distributions}

There are many, many useful statistics besides sample means, variances, and differences/ratios of the two.  We would like to find their sampling distributions, too, but unless we are lucky, the answer will be not quite so tidy to write down in a closed-form expression.  What do we do then?

As it turns out, we do not need to know the exact analytical form of the sampling distribution; sometimes it is enough to approximate it with a simulated distribution. In this section we will show you how. Note that \textsf{R} is particularly well suited to compute simulated sampling distributions, much more so than, say, SPSS or SAS.

\subsection{The Interquartile Range}

We use the \(IQR\) all of the time; it is one of our go-to measures of spread when our data are skewed or show outliers.  What is the sampling distribution of the \(IQR\)? It turns out that there is a lot we can say about the \(IQR\), and there are many texts that do, see \cite{BLANK} for one.  But as with many results involving sample quantiles and the order statistics, the formulas are messy and often we are limited to large-sample results (which are almost all asymptotically normal).

Never fear.  Suppose we would like to know the sampling distribution of the \(IQR\) when sampling from, say, a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\), distribution, for samples of size \(n = 15\).  No problem.  Let us use the computer to generate 1000 samples of size 15 from  a \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) distribution.  For each generated sample, we will compute an \(IQR\).  We will then have 10,000 \(IQR\)'s to play with, and all the methods of Chapter~\ref{BLANK} at our disposal to describe them.

Perhaps the quickest way to do what we want with \textsf{R} is via the \texttt{replicate} function.  Here we will generate all the samples and calculate all the \(IQR\)'s in one fell swoop:

<<echo=TRUE>>=
iqrs <- replicate(10000, IQR(rnorm(15)))
@

We can look at the mean of the simulated values
<<echo=TRUE>>=
mean(iqrs)    # close to 1.23
@

and we can see the standard deviation
<<echo=TRUE>>=
sd(iqrs) # close to 0.35
@

Now let's take a look at a plot of the simulated values.

<<simulated-iqr, echo=TRUE, fig=TRUE, include=FALSE, height=2.7,width=5>>=
hist(iqrs, breaks = 20)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-simulated-iqr}
\end{center}
\caption[A histogram of simulated \(IQR\)'s.]{{\small A histogram of simulated \(IQR\)'s on samples of size \(n = 15\) from \(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1)\).  The mean of the sampling distribution is approximately 1.23, and the standard deviation (AKA standard error of the statistic) is approximately 0.36.  The shape appears slightly skewed to the right.}}
\label{fig:simulated-iqr}
\end{figure}

\subsection{The Median Absolute Deviation}

The \(MAD\) is another statistic that we know well, and which also has an intractable sampling distribution.  Let us investigate its sampling distribution for samples of size \(n = 17\) from an \(\mathsf{exp}(\mathtt{rate}=1)\) population. We will again simulate 10,000 \(MAD\)'s.

<<echo=TRUE>>=
mads <- replicate(10000, mad(rexp(17)))
@

We can look at the mean and standard deviation of the simulated values:

<<echo=TRUE>>=
mean(mads); sd(mads)
@

Now let's take a look at a plot of the simulated values

<<simulated-mads, echo=FALSE, fig=TRUE, include=FALSE, height=2.7,width=5>>=
hist(mads, breaks = 20)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-simulated-mads}
\end{center}
\caption[A histogram of simulated \(MAD\)'s.]{{\small A histogram of simulated \(MAD\)'s on samples of size \(n = 17\) from \(\mathsf{exp}(\mathtt{rate}=1)\).  The mean of the sampling distribution is approximately 0.7, and the standard deviation (AKA standard error of the statistic) is approximately 0.25.  The shape appears skewed to the right.}}
\label{fig:simulated-mads}
\end{figure}



\section{Chapter Exercises}

<<echo=FALSE, include=FALSE>>=
k <- 1
n <- sample(10:30, size=10, replace = TRUE)
mu <- round(rnorm(10, mean = 20))
@

\begin{Exercise}[]
Suppose that we observe a random sample \(X_{1}\), \(X_{2}\), \ldots ,
\(X_{n}\) of size \(n = \) \Sexpr{n} from a
\(\mathsf{norm}(\mathtt{mean}= \Sexpr{mu})\) distribution.

\begin{enumerate}
\item What is the mean of \(\overline{X}\)?
\item What is the standard deviation of \(\overline{X}\)?
\item What is the distribution of \(\overline{X}\)? (approximately)
\item Find \(\mathbb{P}(a< \overline{X} \leq b)\)
\item Find \(\mathbb{P}(\overline{X} > c)\).
\end{enumerate}

\end{Exercise}





\begin{Exercise}[label=exr:clt123]
In this exercise we will investigate how the shape of
the population distribution affects the time until the distribution of
\(\overline{X}\) is acceptably normal.

\end{Exercise}




\begin{Exercise}[]
Let \(X_{1}\),\ldots, \(X_{25}\) be a random sample from a
\(\mathsf{norm}(\mathtt{mean}=37,\,\mathtt{sd}=45)\) distribution, and
let \(\overline{X}\) be the sample mean of these \(n=25\)
observations.

\begin{enumerate}
\item How is \(\overline{X}\) distributed?
\item Find \(\mathbb{P}(\overline{X} > 43.1)\).
\end{enumerate}

\end{Exercise}




\begin{Exercise}[]
Previously, statistics students have estimated that the amount of
pocket change that old people carry around is exponentially
distributed with a mean of \$1.17. So if \(X\) denotes the amount
of change in a randomly selected old person's pocket, we would have
\(X\sim\mathsf{exp}(\mathtt{rate}=1/1.17)\).
Suppose that we randomly select 81 old people,
which we will write as $X_{1}$, $X_{2}$, \ldots{}, $X_{81}$,
and then calculate $\overline{X}=\frac{1}{81}\sum X_{i}$.

\begin{enumerate}
\item In words, \(\overline{X}\) represents BLANK.

\item \(\overline{X}\) has approximately a BLANK distribution.

\item Draw a picture of the probability that the first individual $X_{1}$
has between \(\$0.36\) and \(\$3.48\) in his/her pocket.
Draw a sketch of the PDF and shade in the area to be determined.

\item Draw a picture of the (approximate) probability that the average (that
is, the sample mean) of the 81 old people falls between  \(\$0.36\) and \(\$3.48\). Draw a sketch of the PDF and shade in the area
to be determined.

\item Are the two probabilities different? If so, which one is bigger, and why?
\end{enumerate}

\end{Exercise}




\chapter{Estimation} \label{cha:estimation}



<<echo=FALSE, include=FALSE>>=
library(Hmisc)
#library(RcmdrPlugin.IPSUR)
library(reshape)
library(stats4)
library(TeachingDemos)
@


We will discuss two branches of estimation procedures: point
estimation and interval estimation. We briefly discuss point
estimation first and then spend the rest of the chapter on interval
estimation.

We find an estimator with the methods of Section~\ref{sec:point-estimation}. We make some assumptions about the
underlying population distribution and use what we know from Chapter~\ref{cha:sampling-distributions} about sampling
distributions both to study how the estimator will perform, and to
find intervals of confidence for underlying parameters associated with
the population distribution. Once we have confidence intervals we can
do inference in the form of hypothesis tests in the next chapter.

\paragraph{What do I want them to know?}

\begin{itemize}
\item how to look at a problem, identify a reasonable model, and estimate a parameter associated with the model
\item about maximum likelihood, and in particular, how to
    \begin{itemize}
    \item eyeball a likelihood to get a maximum
    \item use calculus to find an MLE for one-parameter families
    \end{itemize}
\item about properties of the estimators they find, such as bias, minimum
  variance, MSE
\item point versus interval estimation, and how to find and interpret
  confidence intervals for basic experimental designs
\item the concept of margin of error and its relationship to sample size
\end{itemize}


\section{Point Estimation} \label{sec:point-estimation}


The following example is how I was introduced to maximum likelihood.



\begin{example}[Fishing, part one]
\label{exm:how-many-fish}
Imagine there is a small pond in our backyard,
and in the pond there live some fish. We would like to know how many
fish live in the pond. How can we estimate this? One procedure
developed by researchers is the capture-recapture method. Here is how
it works.
\end{example}

We go fishing, and on each fish that we catch we attach
an unobtrusive tag to its tail, and release it back into the water.
Suppose on this day that we capture \(M=7\) fish in total.

Next, we wait a few days for the fish to remix and become accustomed
to their new tag. Then we go fishing again. On the second trip some of
the fish we catch may be tagged; some may not be. Let \(X\) denote the
number of caught fish which are tagged\footnote{It is theoretically possible that we could catch the
same tagged fish more than once, which would inflate our count of
tagged fish. To avoid this difficulty, suppose that on the second trip
we use a tank on the boat to hold the caught fish until data
collection is completed.}, and suppose
for the sake of argument that we catch \(K=4\) fish and we find that 3
of them are tagged.


Now let \(F\) denote the (unknown) total number of fish in the
pond. We know that \(F\geq7\), because we tagged that many on the
first trip. In fact, if we let \(N\) denote the number of untagged
fish in the pond, then \(F=M+N\). Using our earlier language, we have
sampled \(K=4\) times, without replacement, from an urn (pond) which
has \(M=7\) white balls (tagged fish) and \(N=F-M\) black balls
(untagged fish), and we have observed \(x=3\) of them to be white
(tagged). What is the probability of this?

Looking back to Section~\ref{sec:other-discrete-distributions}, we see that the random variable \(X\) has a \(\mathsf{hyper}(\mathtt{m}=M,\,\mathtt{n}=F-M,\,\mathtt{k}=K)\)
distribution. Therefore, for an observed value \(X=x\) the probability
would be \[ \mathbb{P}(X=x)=\frac{{M \choose x}{F-M \choose K-x}}{{F
\choose K}}.  \] First we notice that \(F\) must be at least 7. Could
\(F\) be equal to seven? If \(F=7\) then all of the fish would have
been tagged on the first run, and there would be no untagged fish in
the pond, thus, \(\mathbb{P}(\mbox{3 successes in 4 trials})=0\).
What about \(F=8\); what would be the probability of observing \(X=3\)
tagged fish?  \[ \mathbb{P}(\mbox{3 successes in 4 trials})=\frac{{7
\choose 3}{1 \choose 1}}{{8 \choose 4}}=\frac{35}{70}=0.5.  \]
Similarly, if \(F=9\) then the probability of observing \(X=3\) tagged
fish would be \[ \mathbb{P}(\mbox{3 successes in 4 trials})=\frac{{7
\choose 3}{2 \choose 1}}{{9 \choose 4}}=\frac{70}{126}\approx0.556.
\] We can see already that the observed data \(X=3\) is more likely
when \(F=9\) than it is when \(F=8\). And here lies the genius of Sir
Ronald Aylmer Fisher: he asks, ``What is the value of \(F\) which has
the highest likelihood?'' In other words, for all of the different
possible values of \(F\), which one makes the above probability the
biggest? We can answer this question with a plot of
\(\mathbb{P}(X=x)\) versus \(F\). See Figure~\ref{fig:capture-recapture}.

The graph shows that \(\hat{F}=9\) is the number of fish with the maximum likelihood, and that is what we will take for our estimate of the number of fish in the the pond.

<<capture-recapture, echo=FALSE, fig=TRUE, include=FALSE, height=4,width=5>>=
heights = rep(0, 16)
for (j in 7:15) heights[j] <- dhyper(3, m = 7, n = j - 7, k = 4)
plot(6:15, heights[6:15], pch = 16, cex = 1.5, xlab = "number of fish in pond", ylab = "Likelihood")
abline(h = 0)
lines(6:15, heights[6:15], type = "h", lwd = 2, lty = 3)
text(9, heights[9]/6, bquote(hat(F)==.(9)), cex = 2, pos = 4)
lines(9, heights[9], type = "h", lwd = 2)
points(9, 0, pch = 4, lwd = 3, cex = 2)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-capture-recapture}
\end{center}
\caption[Maximum likelihood for the capture-recapture experiment.]{{\small A plot of maximum likelihood for the capture-recapture experiment.  We see that \(\hat{F}=9\) is the number of fish that results in the maximum  likelihood for the observed data.}}
\label{fig:capture-recapture}
\end{figure}

\begin{example}[Fishing, part two]
\label{exm:bass-bluegill}
In the last example we were only concerned with how many fish were in
the pond, but now, we will ask a different question. Suppose it is
known that there are only two species of fish in the pond: smallmouth
bass (\textit{Micropterus dolomieu}) and bluegill (\textit{Lepomis
  macrochirus}); perhaps we built the pond some years ago and stocked
it with only these two species. We would like to estimate the
proportion of fish in the pond which are bass.
\end{example}

Let \(p=\mbox{the proportion of bass}\). Without any other
information, it is conceivable for \(p\) to be any value in the
interval \([0,1]\), but for the sake of argument we will suppose that
\(p\) falls strictly between zero and one. How can we learn about the
true value of \(p\)? Go fishing! As before, we will use
catch-and-release, but unlike before, we will not tag the fish. We
will simply note the species of any caught fish before returning it to
the pond.

Suppose we catch \(n\) fish. Let \[ X_{i} = \begin{cases} 1, &
\mbox{if the \(i^{\text{th}}\) fish is a bass,}\\ 0, & \mbox{if the
\(i^{\text{th}}\) fish is a bluegill.} \end{cases} \] Since we are
returning the fish to the pond once caught, we may think of this as a
sampling scheme with replacement where the proportion of bass \(p\)
does not change. Given that we allow the fish sufficient time to ``mix''
once returned, it is not completely unreasonable to model our fishing
experiment as a sequence of Bernoulli trials, so that the \(X_{i}\)'s
would be IID
\(\mathsf{binom(\mathtt{size}}=1,\,\mathtt{prob}=p)\). Under those
assumptions we would have
\begin{eqnarray*}
\mathbb{P}(X_{1}=x_{1},\, X_{2}=x_{2},\,\ldots,\, X_{n}=x_{n}) & = & \mathbb{P}(X_{1}=x_{1})\,\mathbb{P}(X_{2}=x_{2})\,\cdots\mathbb{P}(X_{n}=x_{n}),\\
 & = & p^{x_{1}}(1-p)^{x_{1}}\, p^{x_{2}}(1-p)^{x_{2}}\cdots\, p^{x_{n}}(1-p)^{x_{n}},\\
 & = & p^{\sum x_{i}}(1-p)^{n-\sum x_{i}}.
\end{eqnarray*}
That is, \[ \mathbb{P}(X_{1}=x_{1},\, X_{2}=x_{2},\,\ldots,\,
X_{n}=x_{n})=p^{\sum x_{i}}(1-p)^{n-\sum x_{i}}.  \] This last
quantity is a function of \(p\), called the \textit{likelihood function}
\(L(p)\): \[ L(p)=p^{\sum x_{i}}(1-p)^{n-\sum x_{i}}.  \] A graph of
\(L\) for values of \(\sum x_{i}=3,\ 4\), and 5 when \(n=7\) is shown
in Figure~\ref{fig:fishing-part-two}.

<<fishing-part-two, echo=FALSE, fig=TRUE, include=FALSE, height=2.75,width=5>>=
curve(x^5*(1-x)^2, 0, 1, xlab = "p", ylab = "L(p)")
curve(x^4*(1-x)^3, 0, 1, add = TRUE)
curve(x^3*(1-x)^4, 0, 1, add = TRUE)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-fishing-part-two}
\end{center}
\vspace{-0.35in}
\caption[Assorted likelihood functions for fishing, part two.]{{\small Assorted likelihood functions for fishing, part two.   Three graphs are shown of \(L\) when \(\sum x_{i}\) equals 3, 4, and 5, respectively, from left to right. We pick an \(L\) that matches the observed data and then maximize \(L\) as a function of \(p\). If \(\sum x_{i}=4\), then the maximum appears to occur somewhere around \(p \approx 0.6\).}}
\label{fig:fishing-part-two}
\end{figure}

We want the value of \(p\) which has the highest likelihood, that is,
we again wish to maximize the likelihood. We know from calculus (see
Appendix~\ref{sec:differential-and-integral}) to differentiate \(L\)
and set \(L'=0\) to find a maximum.  \[ L'(p)=\left(\sum
x_{i}\right)p^{\sum x_{i}-1}(1-p)^{n-\sum x_{i}}+p^{\sum
x_{i}}\left(n-\sum x_{i}\right)(1-p)^{n-\sum x_{i}-1}(-1).  \] The
derivative vanishes (\(L'=0\)) when
\begin{eqnarray*}
\left(\sum x_{i}\right)p^{\sum x_{i}-1}(1-p)^{n-\sum x_{i}} & = & p^{\sum x_{i}}\left(n-\sum x_{i}\right)(1-p)^{n-\sum x_{i}-1},\\
\sum x_{i}(1-p) & = & \left(n-\sum x_{i}\right)p,\\
\sum x_{i}-p\sum x_{i} & = & np-p\sum x_{i},\\
\frac{1}{n}\sum_{i=1}^{n}x_{i} & = & p.
\end{eqnarray*}
This ``best'' \(p\), the one which maximizes the likelihood, is called
the maximum likelihood estimator (MLE) of \(p\) and is denoted
\(\hat{p}\). That is,
\begin{equation}
\hat{p}=\frac{\sum_{i=1}^{n}x_{i}}{n}=\overline{x}.
\end{equation}



\begin{rem}[]
Strictly speaking we have only shown that the derivative equals zero
at \(\hat{p}\), so it is theoretically possible that the critical
value \(\hat{p}=\overline{x}\) is located at a minimum\footnote{We can tell from the graph that our value of \(\hat{p}\) is a maximum instead of a minimum so we do not really need to worry for this example. Other examples are not so easy, however, and we should be careful to be cognizant of this extra step.}
instead of a maximum! We should be thorough and check that \(L'>0\)
when \(p<\overline{x}\) and \(L'<0\) when \(p>\overline{x}\). Then by
the First Derivative Test (Theorem~\ref{thm:first-derivative-test}) we could
be certain that \(\hat{p}=\overline{x}\) is indeed a maximum
likelihood estimator, and not a minimum likelihood estimator.
\end{rem}

The result is shown in Figure~\ref{fig:species-mle}.

<<species-mle, echo=FALSE, fig=TRUE, include=FALSE, height=3,width=5>>=
dat <- rbinom(27, size = 1, prob = 0.3)
like <- function(x){
r <- 1
for (k in 1:27){ r <- r*dbinom(dat[k], size = 1, prob = x)}
return(r)
}
curve(like, from = 0, to = 1, xlab = "parameter space", ylab = "Likelihood", lwd = 3, col = "blue")
abline(h = 0, lwd = 1, lty = 3, col = "grey")
mle <- mean(dat)
mleobj <- like(mle)
lines(mle, mleobj, type = "h", lwd = 2, lty = 3, col = "red")
points(mle, 0, pch = 4, lwd = 2, cex = 2, col = "red")
text(mle, mleobj/4, substitute(hat(theta)==a, list(a=round(mle, 4))), cex = 1.3, pos = 4)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-species-mle}
\end{center}
\vspace{-0.35in}
\caption[Species maximum likelihood.]{{\small Species maximum
    likelihood.  Here we see that
    \(\hat{\theta} \approx \Sexpr{round(mean(dat),2)}\) is the
    proportion of bass with the maximum likelihood.}}
\label{fig:species-mle}
\end{figure}


In general, we have a family of PDFs \(f(x|\theta)\) indexed by a
parameter \(\theta\) in some parameter space \(\Theta\). We want to
learn about \(\theta\). We take a \(SRS(n)\):
\begin{equation}
X_{1},\, X_{2},\,\ldots,X_{n}\mbox{ which are IID \(f(x| \theta )\).}
\end{equation}



\begin{defn}[Likelihood function.]
Given the observed data \(x_{1}\), \(x_{2}\), \ldots \(x_{n}\), the
\textit{likelihood function} \(L\) is defined by \[
L(\theta)=\prod_{i=1}^{n}f(x_{i}|\theta),\quad \theta\in\Theta.  \]
\end{defn}

The next step is to maximize \(L\). The method we will use in this
book is to find the derivative \(L'\) and solve the equation
\(L'(\theta)=0\). Call a solution \(\hat{\theta}\). We will check that
\(L\) is maximized at \(\hat{\theta}\) using the First Derivative Test
or the Second Derivative Test \(\left(L''(\hat{\theta})<0\right)\).



\begin{defn}[Maximum Likelihood estimators.]
  A value \(\theta\) that maximizes \(L\) is called a \textit{maximum
    likelihood estimator} (MLE) and is denoted \(\hat{\theta}\). It is
  a function of the sample,
  \(\hat{\theta}=\hat{\theta}\left(X_{1},\,
    X_{2},\,\ldots,X_{n}\right)\), and is called a \textit{point
    estimator} of \(\theta\).
\end{defn}




\begin{rem}[]
Some comments about maximum likelihood estimators:

\begin{itemize}
\item Often it is easier to maximize the \textit{log-likelihood}
  \(l(\theta)=\ln L(\theta)\) instead of the likelihood \(L\). Since
  the logarithmic function \(y=\ln x\) is a monotone transformation,
  the solutions to both problems are the same.
\item MLEs do not always exist (for instance, sometimes the likelihood has
  a vertical asymptote), and even when they do exist, they are not
  always unique (imagine a function with a bunch of humps of equal
  height). For any given problem, there could be zero, one, or any
  number of values of \(\theta\) for which \(L(\theta)\) is a maximum.
\item The problems we encounter in this book are all very nice with
  likelihood functions that have closed form representations and which
  are optimized by some calculus acrobatics. In practice, however,
  likelihood functions are sometimes nasty in which case we are
  obliged to use numerical methods to find maxima (if there are any).
\end{itemize}
\end{rem}

MLEs are just one of \textit{many} possible estimators. One of the
more popular alternatives are the \textit{method of moments
  estimators}; see Casella and Berger \cite{Casella2002} for more.

Notice, in Example~\ref{exm:bass-bluegill} we had \(X_{i}\) IID
\(\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p)\), and we saw that
the MLE was \(\hat{p}=\overline{X}\). But further
\begin{eqnarray*}
\mathbb{E}\overline{X} & = & \mathbb{E}\left(\frac{X_{1}+X_{2}+\cdots+X_{n}}{n}\right),\\
 & = & \frac{1}{n}\left(\mathbb{E} X_{1}+\mathbb{E} X_{2}+\cdots+\mathbb{E} X_{n}\right),\\
 & = & \frac{1}{n}\left(np\right),\\
 & = & p,
\end{eqnarray*}
which is exactly the same as the parameter which we estimated. More
concisely, \(\mathbb{E}\hat{p}=p\), that is, on the average, the
estimator is exactly right.



\begin{defn}[Unbiased estimators.]
  Let \(s(X_{1},X_{2},\ldots,X_{n})\) be a statistic which estimates
  \(\theta\). If \[ \mathbb{E} s(X_{1},X_{2},\ldots,X_{n})=\theta, \]
  then the statistic \(s(X_{1},X_{2},\ldots,X_{n})\) is said to be an
  \textit{unbiased estimator} of \(\theta\). Otherwise, it is
  \textit{biased}.
\end{defn}




\begin{example}[]
\label{exm:normal-mle-both}
Let \(X_{1}\), \(X_{2}\), \ldots , \(X_{n}\) be
an \(SRS(n)\) from a
\(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\)
distribution. It can be shown (in Exercise~\ref{exr:norm-mu-sig-mle}) that
if \(\mbox{$\theta$}=(\mu,\sigma^{2})\) then the MLE of \(\theta\) is
\begin{equation}
\hat{\theta}=(\hat{\mu},\hat{\sigma}^{2}),
\end{equation}
where \(\hat{\mu}=\overline{X}\) and
\begin{equation}
\hat{\sigma^{2}}=\frac{1}{n}\sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}=\frac{n-1}{n}S^{2}.
\end{equation}
\end{example}

We of course know from \@ref{pro:mean-sd-xbar} that \(\hat{\mu}\) is
unbiased. What about \(\hat{\sigma^{2}}\)? Let us check:
\begin{eqnarray*}
\mathbb{E}\,\hat{\sigma^{2}} & = & \mathbb{E}\,\frac{n-1}{n}S^{2},\\
 & = & \mathbb{E}\left(\frac{\sigma^{2}}{n}\frac{(n-1)S^{2}}{\sigma^{2}}\right),\\
 & = & \frac{\sigma^{2}}{n}\mathbb{E}\ \mathsf{chisq}(\mathtt{df}=n-1),\\
 & = & \frac{\sigma^{2}}{n}(n-1),
\end{eqnarray*}
from which we may conclude two things:

\begin{itemize}
\item \(\hat{\sigma^{2}}\) is a biased estimator of \(\sigma^{2}\), and
\item \(S^{2}=n\hat{\sigma^{2}}/(n-1)\) is an unbiased estimator of
  \(\sigma^{2}\).
\end{itemize}


One of the most common questions in an introductory statistics class
is, ``Why do we divide by \(n-1\) when we compute the sample variance?
It's an average, right? So why do we not divide by \(n\)?'' We see now
that division by \(n\) amounts to the use of a \textit{biased}
estimator for \(\sigma^{2}\), that is, if we divided by \(n\) then on
the average we would \textit{underestimate} the true value of
\(\sigma^{2}\). We use \(n-1\) so that, on the average, our estimator
of \(\sigma^{2}\) will be exactly right.

\subsection{How to do it with \textsf{R}}

R can be used to find maximum likelihood estimators in a
lot of diverse settings. We will discuss only the most basic here and
will leave the rest to more sophisticated texts.

For one parameter estimation problems we may use the \texttt{optimize}
function to find MLEs. The arguments are the function to be maximized
(the likelihood function), the range over which the optimization is to
take place, and optionally any other arguments to be passed to the
likelihood if needed.

Let us see how to do Example~\ref{exm:bass-bluegill}. Recall that our
likelihood function was given by
\begin{equation}
L(p)=p^{\sum x_{i}}(1-p)^{n-\sum x_{i}}.
\end{equation}
Notice that the likelihood is just a product of
\(\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p)\) PMFs. We first
give some sample data (in the vector \texttt{datavals}), next we define the
likelihood function \texttt{L}, and finally we \texttt{optimize} \texttt{L} over the range
\texttt{c(0,1)}.

<<echo=TRUE>>=
x <- mtcars$am
L <- function(p,x) prod(dbinom(x, size = 1, prob = p))
optimize(L, interval = c(0,1), x = x, maximum = TRUE)
@


<<echo=FALSE, include=FALSE>>=
A <- optimize(L, interval = c(0,1), x = x, maximum = TRUE)
amax <- A$maximum; aobj <- A$objective
@

Note that the \texttt{optimize} function by default minimizes the
function \texttt{L}, so we have to set \texttt{maximum = TRUE} to get
an MLE. The returned value of \texttt{\$maximum} gives an approximate
value of the MLE to be \Sexpr{round(amax, 3)} and \texttt{objective}
gives \texttt{L} evaluated at the MLE which is approximately
\Sexpr{round(aobj, 3)}.

We previously remarked that it is usually more numerically convenient
to maximize the log-likelihood (or minimize the negative
log-likelihood), and we can just as easily do this with \textsf{R}. We just
need to calculate the log-likelihood beforehand which (for this
example) is
\[ -l(p)=-\sum x_{i}\ln\, p-\left(n-\sum x_{i}\right)\ln(1-p).  \]

It is done in \textsf{R} with

<<echo=TRUE>>=
minuslogL <- function(p,x){
                -sum(dbinom(x, size = 1, prob = p, log = TRUE))
             }
optimize(minuslogL, interval = c(0,1), x = x)
@

Note that we did not need \texttt{maximum = TRUE} because we minimized
the negative log-likelihood. The answer for the MLE is essentially the
same as before, but the \texttt{\$objective} value was different, of
course.

For multiparameter problems we may use a similar approach by way of
the \texttt{mle} function in the \texttt{stats4} package \cite{stats4}.





\begin{example}[Plant Growth]
  We will investigate the \texttt{weight} variable of the
  \texttt{PlantGrowth} data. We will suppose that the weights
  constitute a random observations \(X_{1}\), \(X_{2}\), \ldots ,
  \(X_{n}\) that are IID
  \(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\) which is
  not unreasonable based on a histogram and other exploratory
  measures. We will find the MLE of \(\theta=(\mu,\sigma^{2})\). We
  claimed in Example~\ref{exm:normal-mle-both} that
  \(\hat{\theta}=(\hat{\mu},\hat{\sigma}^{2})\) had the form given
  above. Let us check whether this is plausible numerically. The
  negative log-likelihood function is
\end{example}

<<echo=TRUE>>=
minuslogL <- function(mu, sigma2){
  -sum(dnorm(x, mean = mu, sd = sqrt(sigma2), log = TRUE))
}
@

Note that we omitted the data as an argument to the log-likelihood
function; the only arguments were the parameters over which the
maximization is to take place. Now we will simulate some data and find
the MLE. The optimization algorithm requires starting values
(intelligent guesses) for the parameters. We choose values close to
the sample mean and variance (which turn out to be approximately 5 and
0.5, respectively) to illustrate the procedure.

<<echo=TRUE>>=
library(stats4)
x <- PlantGrowth$weight
MaxLikeEst <- mle(minuslogL, start = list(mu = 5, sigma2 = 0.5))
summary(MaxLikeEst)
@

The outputted MLEs are shown above, and \texttt{mle} even gives us
estimates for the standard errors of \(\hat{\mu}\) and
\(\hat{\sigma}^{2}\) (which were obtained by inverting the numerical
Hessian matrix at the optima; see Appendix~\ref{sec:multivariable-calculus}). Let us check how close the
numerical MLEs came to the theoretical MLEs:

<<echo=TRUE>>=
mean(x); var(x)*29/30; sd(x)/sqrt(30)
@


The numerical MLEs were very close to the theoretical MLEs. We already
knew that the standard error of \(\hat{\mu}=\overline{X}\) is
\(\sigma/\sqrt{n}\), and the numerical estimate of this was very close
too.

There is functionality in the \texttt{distrTEst} package to calculate
theoretical MLEs; we will skip examples of these for the time being.

\section{Confidence Intervals for Means} 
\label{sec:confidence-intervals-for-means}

We are given \(X_{1}\), \(X_{2}\), \ldots, \(X_{n}\) that are an
\(SRS(n)\) from a
\(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\)
distribution, where \(\mu\) is unknown. We know that we may estimate
\(\mu\) with \(\overline{X}\), and we have seen that this estimator is
the MLE. But how good is our estimate? We know that
\begin{equation}
\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=1).
\end{equation}
For a big probability \(1-\alpha\), for instance, 95\%, we can
calculate the quantile \(z_{\alpha/2}\). Then
\begin{equation}
\mathbb{P}\left(-z_{\alpha/2}\leq\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\leq z_{\alpha/2}\right)=1-\alpha.
\end{equation}
But now consider the following string of equivalent inequalities:
\[
-z_{\alpha/2}\leq\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\leq z_{\alpha/2},
\]
\[
-z_{\alpha/2}\left(\frac{\sigma}{\sqrt{n}}\right)\leq\overline{X}-\mu\leq z_{\alpha/2}\left(\frac{\sigma}{\sqrt{n}}\right),
\]
\[
-\overline{X} - z_{\alpha/2}\left(\frac{\sigma}{\sqrt{n}}\right)\leq - \mu \leq - \overline{X} + z_{\alpha/2} \left( \frac{\sigma}{\sqrt{n}} \right),
\]
\[
\overline{X} - z_{\alpha/2} \left( \frac{\sigma}{\sqrt{n}} \right) \leq \mu \leq \overline{X} + z_{\alpha/2} \left( \frac{\sigma}{\sqrt{n}} \right).
\]
That is,
\begin{equation}
\mathbb{P}\left(\overline{X}-z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\leq\mu\leq\overline{X}+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right)=1-\alpha.
\end{equation}



\begin{defn}[Confidence interval, confidence coefficient.]
The interval
\begin{equation}
\left[\overline{X}-z_{\alpha/2}\frac{\sigma}{\sqrt{n}},\ \overline{X}+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right]
\end{equation}
is a \(100(1-\alpha)\%\) \textit{confidence interval for} \(\mu\). The quantity \(1-\alpha\) is called the \textit{confidence coefficient}.
\end{defn}



\begin{rem}[]
The interval is also sometimes written more compactly as
\begin{equation}
\label{eq:z-interval}
\overline{X}\pm z_{\alpha/2}\frac{\sigma}{\sqrt{n}}.
\end{equation}
\end{rem}


The interpretation of confidence intervals is tricky and often
mistaken by novices. When I am teaching the concept ``live'' during
class, I usually ask the students to imagine that my piece of chalk
represents the ``unknown'' parameter, and I lay it down on the desk in
front of me. Once the chalk has been lain, it is \textit{fixed}; it
does not move. Our goal is to estimate the parameter. For the
estimator I pick up a sheet of loose paper lying nearby. The
estimation procedure is to randomly drop the piece of paper from
above, and observe where it lands. If the piece of paper covers the
piece of chalk, then we are successful -- our estimator covers the
parameter. If it falls off to one side or the other, then we are
unsuccessful; our interval fails to cover the parameter.

Then I ask them: suppose we were to repeat this procedure hundreds,
thousands, millions of times. Suppose we kept track of how many times
we covered and how many times we did not. What percentage of the time
would we be successful?

In the demonstration, the parameter corresponds to the chalk, the
sheet of paper corresponds to the confidence interval, and the random
experiment corresponds to dropping the sheet of paper. The percentage
of the time that we are successful \textit{exactly} corresponds to the
\textit{confidence coefficient}. That is, if we use a 95\% confidence
interval, then we can say that, in the long run, approximately 95\% of
our intervals will cover the true parameter (which is fixed, but
unknown).

See Figure~\ref{fig:ci-examp}, which is a graphical display of these ideas.

<<ci-examp, echo=FALSE, fig=TRUE, include=FALSE, width=5>>=
ci.examp()
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-ci-examp}
\end{center}
\caption[Confidence interval example.]{{\small The graph was generated by the \texttt{ci.examp}
    function from the \texttt{TeachingDemos} package. Fifty (50)
    samples of size twenty five (25) were generated from a
    \(\mathsf{norm}(\mathtt{mean}=100,\,\mathtt{sd}=10)\)
    distribution, and each sample was used to find a 95\% confidence
    interval for the population mean using Equation
    \eqref{eq:z-interval}. The 50 confidence intervals are represented
    above by horizontal lines, and the respective sample means are
    denoted by vertical slashes. Confidence intervals that ``cover''
    the true mean value of 100 are plotted in black; those that fail
    to cover are plotted in a lighter color. In the plot we see that
    only one (1) of the simulated intervals out of the 50 failed to
    cover \(\mu=100\), which is a success rate of 98\%. If the number
    of generated samples were to increase from 50 to 500 to 50000,
    \ldots, then we would expect our success rate to approach the
    exact value of 95\%.}}
\label{fig:ci-examp}
\end{figure}


Under the above framework, we can reason that an ``interval'' with a
\textit{larger} confidence coefficient corresponds to a \textit{wider}
sheet of paper. Furthermore, the width of the confidence interval
(sheet of paper) should be \textit{somehow} related to the amount of
information contained in the random sample, \(X_{1}\), \(X_{2}\),
\ldots, \(X_{n}\). The following remarks makes these notions precise.



\begin{rem}[]
For a fixed confidence coefficient \(1-\alpha\), if \(n\) increases,
then the confidence interval gets \textit{SHORTER}.

For a fixed sample size \(n\), if \(1-\alpha\) increases, then the
confidence interval gets \textit{WIDER}.
\end{rem}



\begin{example}[Results from an Experiment on Plant Growth]
\label{exm:plant-one-samp-z-int}
The \texttt{PlantGrowth} data frame gives the results of an experiment to measure plant yield (as measured by the weight of the plant). We would like to construct a 95\% confidence interval for the mean yield of the
plants. Suppose that we know from prior research that the true
population standard deviation of the plant yields is \(0.7\)\,g.

The parameter of interest is \(\mu\), which represents the true mean
yield of the population of all plants of the particular species in
the study. We will first take a look at a stem-and-leaf display of the
data:
\end{example}

<<echo=TRUE>>=
with(PlantGrowth, stem.leaf(weight))
@

The data appear to be approximately normal with no extreme values. The
data come from a designed experiment, so it is reasonable to suppose
that the observations constitute a simple random sample of
yields\footnote{Actually we will see later that there is reason to believe that the observations are simple random samples from three distinct populations. See Section~\ref{sec:analysis-of-variance}.}. We know the population standard deviation \(\sigma=0.70\) from prior research. We are going to use the one-sample \(z\)-interval.


<<echo=TRUE>>=
dim(PlantGrowth)   # sample size is first entry
@


<<echo=TRUE>>=
with(PlantGrowth, mean(weight))
@


<<echo=TRUE>>=
qnorm(0.975)
@


We find the sample mean of the data to be \(\overline{x}=5.073\) and
\(z_{\alpha/2}=z_{0.025}\approx1.96\). Our interval is therefore \[
\overline{x}\pm
z_{\alpha/2}\frac{\sigma}{\sqrt{n}}=5.073\pm1.96\cdot\frac{0.70}{\sqrt{30}},
\] which comes out to approximately \([4.82,\,5.33]\). In
conclusion, we are 95\% confident that the true population mean yield \(\mu\) lies somewhere between 4.82\,g and 5.33\,g,
that is, we are 95\% confident that the interval \([4.82,\,5.33]\)
covers \(\mu\).




\begin{rem}[]
What if \(\sigma\) is unknown? We instead use the interval
\begin{equation}
\overline{X}\pm z_{\alpha/2}\frac{S}{\sqrt{n}},
\end{equation}
where \(S\) is the sample standard deviation.

\begin{itemize}
\item If \(n\) is large, then \(\overline{X}\) will have an approximately
  normal distribution regardless of the underlying population (by the
  CLT) and \(S\) will be very close to the parameter \(\sigma\) (by
  the SLLN); thus the above interval will have approximately
  \(100(1-\alpha)\%\) confidence of covering \(\mu\).
\item If \(n\) is small, then
    \begin{itemize}
    \item If the underlying population is normal then we may replace
      \(z_{\alpha/2}\) with \(t_{\alpha/2}(\mathtt{df}=n-1)\). The
      resulting \(100(1-\alpha)\%\) confidence interval is
      \begin{equation}
      \label{eq:one-samp-t-int}
      \overline{X}\pm t_{\alpha/2}(\mathtt{df}=n-1)\frac{S}{\sqrt{n}}.
      \end{equation}
    \item If the underlying population is not normal, but approximately
      normal, then we may use the \(t\) interval, Equation
      \eqref{eq:one-samp-t-int}. The interval will have approximately
      \(100(1-\alpha)\%\) confidence of covering \(\mu\). However, if
      the population is highly skewed or the data have outliers, then
      we should ask a professional statistician for advice.
     \end{itemize}
\end{itemize}
\end{rem}

The author learned of a handy acronym from AP Statistics Exam graders
that summarizes the important parts of confidence interval estimation,
which is PANIC: \textit{P}-arameter, \textit{A}-ssumptions,
\textit{N}-ame, \textit{I}-nterval, and \textit{C}-onclusion.

\begin{description}
\item [Parameter:] identify the parameter of interest with the proper
                symbols. Write down what the parameter means in the
                context of the problem.
\item [Assumptions:]  list any assumptions made in the experiment. If
                  there are any other assumptions needed or that were
                  not checked, state what they are and why they are
                  important.
\item [Name:]  choose a statistical procedure from your bag of tricks
           based on the answers to the previous two parts. The
           assumptions of the procedure you choose should match those
           of the problem; if they do not match then either pick a
           different procedure or openly admit that the results may
           not be reliable. Write down any underlying formulas used.
\item [Interval:]  calculate the interval from the sample data. This can
               be done by hand but will more often be done with the
               aid of a computer. Regardless of the method, all
               calculations or code should be shown so that the entire
               process is repeatable by a subsequent reader.
\item [Conclusion:]  state the final results, using language in the
                 context of the problem. Include the appropriate
                 interpretation of the interval, making reference to
                 the confidence coefficient.
\end{description}




\begin{rem}[]
All of the above intervals for \(\mu\) were two-sided, but there are
also one-sided intervals for \(\mu\). They look like
\begin{equation}
\left[\overline{X}-z_{\alpha}\frac{\sigma}{\sqrt{n}},\ \infty\right)\quad \mbox{or}\quad \left(-\infty,\ \overline{X}+z_{\alpha}\frac{\sigma}{\sqrt{n}}\right]
\end{equation}
and satisfy
\begin{equation}
\mathbb{P}\left(\overline{X}-z_{\alpha}\frac{\sigma}{\sqrt{n}}\leq\mu\right)=1-\alpha\quad \mbox{and}\quad \mathbb{P}\left(\overline{X}+z_{\alpha}\frac{\sigma}{\sqrt{n}}\geq\mu\right)=1-\alpha.
\end{equation}
\end{rem}



\begin{example}[Michelson Speed of Light Data]
\label{exm:morley-one-samp-t-int}
The \texttt{morley} data frame gives the results of five experiments done in the year 1879 to measure the speed of light.  There were five experiments, and each experiment consisted of 20 consecutive runs. We would like to construct a 99\% two-sided confidence interval for the mean speed of light using the data from the first experiment.
\end{example}

The \textit{parameter} of interest here is \(\mu\), the unknown true mean speed of light.  For \textit{assumptions}, we must first suppose that the 20 observations constitute a simple random sample, which is not entirely unreasonable given that Michelson was doing a designed experiment to measure the speed of light.  We notice that the sample size here is \(n = 20\), which is not a large sample.  Further, the population standard deviation of measurements is likewise unknown; this example looks a clear candidate for Student's $t$ interval, but we have yet to verify whether the observations are normally distributed.

Let us check for normality via a quantile-quantile comparison plot.  First, we will need the data:

<<>>=
x <- with(morley, Speed[Expt == 1])
x
@

The above command looks inside the \texttt{morley} data frame and extracts the \texttt{Speed} column that contains the speed measurements, and at the moment, we are only interested in the subset of those speeds which occurred during the first experiment, that is, when the column \texttt{Expt == 1}.  Note that the measurements are measured in km/sec, and also note that the value 299,000 was subtracted from each observation in the dataset for clarity. Now let us inspect a \texttt{qqPlot}:

<<morley-qqplot, echo=FALSE, fig=TRUE, include=FALSE, width=5>>=
library(RcmdrMisc)
qqPlot(x)
@


\begin{figure}
\begin{center}
\includegraphics{IPSUR-morley-qqplot}
\end{center}
\caption[Michelson speed of light data.]{{\small This graph is a normal quantile-quantile comparison plot of the first experiment run by Michelson in 1879 to measure the speed of light.  These data appear normal, judging by the preponderance of dots within the dotted bands.  It was generated by the \texttt{qqPlot}
    function from the \texttt{RcmdrMisc} package.}} 
\label{fig:morley-qqplot}
\end{figure}

Judging by the graph in Figure~\ref{fig:morley-qqplot}, it is reasonable to suppose that the data are normally distributed.  Putting all of this together, we will use the Student's $t$ confidence interval, of the following form: 
\begin{equation}
\overline{x} \pm t_{\alpha/2}(\text{\texttt{df}} = n - 1) \cdot \frac{s}{\sqrt{n}},
\end{equation}
where in this case $n = 20$, the confidence level is 0.99, therefore \(\alpha/2 = 0.005\), and \(\overline{x}, s\) are computed by

<<>>=
mean(x); sd(x)
@

which are \Sexpr{mean(x)} and \Sexpr{round(sd(x), 3)}, respectively.  Our critical value will be

<<>>=
qt(0.995, df = 19)
@

\noindent
and, consequently our \textit{interval} will be 
\[
909 \pm 2.860935 \cdot \frac{104.926}{\sqrt{20}}, 
\]
that is, the interval $[841.8763,\ 976.1237]$, which we will round outward to the same precision of the measurements, which is 841 km/sec to 977 km/sec.  In \textit{conclusion}, we would say that we are 99\% confident that the true mean speed of light is covered by the interval 299,841 km/sec to 299,977 km/sec.


\subsection{How to do it with \textsf{R}}

We can do Example~\ref{exm:plant-one-samp-z-int} with the following code.

<<echo=TRUE>>=
with(PlantGrowth, z.test(weight, sd = 0.7))
@

The confidence interval bounds are shown in the sixth line down of the
output (please disregard all of the additional output information for
now -- we will use it in Chapter~\ref{cha:hypothesis-testing}).


We can do Example~\ref{exm:morley-one-samp-t-int} with the following code.

<<echo=TRUE>>=
t.test(x, conf.level = 0.99)
@

We see the confidence interval bounds shown in the fifth line of the
output.  If we had wanted to compute a 99\% one-sided interval instead, we can could have achieved it with the \texttt{alternative} argument to \texttt{t.test} in the following way:

<<echo=TRUE>>=
t.test(x, conf.level = 0.99, alternative = "greater")
@

\noindent
The command with \texttt{alternative = "greater"} yields an interval with a lower bound for \(\mu\), in this case \([841, \infty)\), while  \texttt{alternative = "less"} gives the interval with an upper bound for \(\mu\) (easy to remember, because it is opposite from what a person would expect it to be).


\section{Confidence Intervals for Differences of Means} 
\label{sec:conf-interv-for-diff-means}


Let \(X_{1}\), \(X_{2}\), \ldots, \(X_{n}\) be a \(SRS(n)\) from a
\(\mathsf{norm}(\mathtt{mean}=\mu_{X},\,\mathtt{sd}=\sigma_{X})\)
distribution and let \(Y_{1}\), \(Y_{2}\), \ldots, \(Y_{m}\) be a
\(SRS(m)\) from a
\(\mathsf{norm}(\mathtt{mean}=\mu_{Y},\,\mathtt{sd}=\sigma_{Y})\)
distribution. Further, assume that the \(X_{1}\), \(X_{2}\), \ldots,
\(X_{n}\) sample is independent of the \(Y_{1}\), \(Y_{2}\), \ldots,
\(Y_{m}\) sample.

Suppose that \(\sigma_{X}\) and \(\sigma_{Y}\) are known. We would
like a confidence interval for \(\mu_{X}-\mu_{Y}\). We know that
\begin{equation}
\overline{X}-\overline{Y}\sim\mathsf{norm}\left(\mathtt{mean}=\mu_{X}-\mu_{Y},\,\mathtt{sd}=\sqrt{\frac{\sigma_{X}^{2}}{n}+\frac{\sigma_{Y}^{2}}{m}}\right).
\end{equation}
Therefore, a \(100(1-\alpha)\%\) confidence interval for
\(\mu_{X}-\mu_{Y}\) is given by
\begin{equation}
\label{eq:two-samp-mean-CI}
\left(\overline{X}-\overline{Y}\right)\pm z_{\alpha/2}\sqrt{\frac{\sigma_{X}^{2}}{n}+\frac{\sigma_{Y}^{2}}{m}}.
\end{equation}
Unfortunately, most of the time the values of \(\sigma_{X}\) and
\(\sigma_{Y}\) are unknown. This leads us to the following:

\begin{itemize}
\item If both sample sizes are large, then we may appeal to the CLT/SLLN
  (see Section~\ref{sec:central-limit-theorem}) and substitute \(S_{X}^{2}\) and
  \(S_{Y}^{2}\) for \(\sigma_{X}^{2}\) and \(\sigma_{Y}^{2}\) in the
  interval \eqref{eq:two-samp-mean-CI}. The resulting confidence interval will
  have approximately \(100(1-\alpha)\%\) confidence.
\item If one or more of the sample sizes is small then we are in trouble,
  unless the underlying populations are both normal and
  \(\sigma_{X}=\sigma_{Y}\). In this case (setting
  \(\sigma=\sigma_{X}=\sigma_{Y}\)),
  \begin{equation}
  \overline{X}-\overline{Y}\sim\mathsf{norm}\left(\mathtt{mean}=\mu_{X}-\mu_{Y},\,\mathtt{sd}=\sigma\sqrt{\frac{1}{n}+\frac{1}{m}}\right).
  \end{equation}
  Now let
  \begin{equation}
  U=\frac{n-1}{\sigma^{2}}S_{X}^{2}+\frac{m-1}{\sigma^{2}}S_{Y}^{2}.
  \end{equation}
  Then by Exercise~\ref{exr:sum-indep-chisq} we know that
  \(U\sim\mathsf{chisq}(\mathtt{df}=n+m-2)\) and it is not a large
  leap to believe that \(U\) is independent of
  \(\overline{X}-\overline{Y}\); thus
  \begin{equation}
  T = \frac{Z}{\sqrt{\left.U\right/ (n+m-2)}}\sim\mathsf{t}(\mathtt{df}=n+m-2).
  \end{equation}
  But
  \begin{align*}
  T & =\frac{\frac{\overline{X}-\overline{Y}-(\mu_{X}-\mu_{Y})}{\sigma\sqrt{\frac{1}{n}+\frac{1}{m}}}}{\sqrt{\left.\frac{n-1}{\sigma^{2}}S_{X}^{2}+\frac{m-1}{\sigma^{2}}S_{Y}^{2}\right/ (n+m-2)}},\\
  & =\frac{\overline{X}-\overline{Y}-(\mu_{X}-\mu_{Y})}{\sqrt{\left(\frac{1}{n}+\frac{1}{m}\right)\left(\frac{(n-1)S_{X}^{2}+(m-1)S_{Y}^{2}}{n+m-2}\right)}},\\
  & \sim\mathsf{t}(\mathtt{df}=n+m-2).
  \end{align*}
  Therefore a \(100(1-\alpha)\%\) confidence interval for
  \(\mu_{X}-\mu_{Y}\) is given by
  \begin{equation}
  \left(\overline{X}-\overline{Y}\right)\pm t_{\alpha/2}(\mathtt{df}=n+m-2)\, S_{p}\sqrt{\frac{1}{n}+\frac{1}{m}},
  \end{equation}
  where
  \begin{equation}
  S_{p}=\sqrt{\frac{(n-1)S_{X}^{2}+(m-1)S_{Y}^{2}}{n+m-2}}
  \end{equation}
  is called the ``pooled'' estimator of \(\sigma\).
    \item If one of the samples is small, and both underlying populations
      are normal, but \(\sigma_{X}\neq\sigma_{Y}\), then we may use a
      Welch (or Satterthwaite) approximation to the degrees of
      freedom. See Welch \cite{Welch1947}, Satterthwaite
      \cite{Satterthwaite1946}, or Neter \textit{et al} \cite{Neter1996}. The
      idea is to use an interval of the form
      \begin{equation}
      \left(\overline{X}-\overline{Y}\right)\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=r)\,\sqrt{\frac{S_{X}^{2}}{n}+\frac{S_{Y}^{2}}{m}},
      \end{equation}
      where the degrees of freedom \(r\) is chosen so that the
      interval has nice statistical properties. It turns out that a
      good choice for \(r\) is given by
      \begin{equation} \label{eq:welch-df}
      r=\frac{\left(S_{X}^{2}/n+S_{Y}^{2}/m\right)^{2}}{\frac{1}{n-1}\left(S_{X}^{2}/n\right)^{2}+\frac{1}{m-1}\left(S_{Y}^{2}/m\right)^{2}},
      \end{equation}
      The resulting interval has approximately \(100(1-\alpha)\%\) confidence.
\end{itemize}


\begin{example}[Tooth Growth in Guinea Pigs]
\label{exm:tooth-CI}
In the \texttt{ToothGrowth} data, the response variable \texttt{len} is the length of odontoblasts (cells responsible for tooth growth) measured on 60 guinea pigs. Each animal received vitamin C by one of two delivery methods recorded in the variable \texttt{supp}, taking values \texttt{"OJ"} for orange juice or \texttt{"VC"} for ascorbic acid (a form of vitamin C).  We would like a 99\% two-sided confidence interval for the difference in means,  \(\mu_{VC}-\mu_{OJ}\), where \(\mu_{VC}\) is the true mean odontoblast length for the VC group and \(\mu_{OJ}\) is the true mean odontoblast length for the OJ group.
\end{example}


\subsection{How to do it with \textsf{R}}

The basic function is \texttt{t.test} which has a \texttt{var.equal} argument that
may be set to \texttt{TRUE} or \texttt{FALSE}, where the default is \texttt{FALSE}. The confidence interval is shown as
part of the output, although there is a lot of additional information
that is not needed until Chapter~\ref{cha:hypothesis-testing}.

There is not any specific functionality to handle the \(z\)-interval
for small samples, but if the samples are large then \texttt{t.test} with
\texttt{var.equal = FALSE} will be essentially the same thing. The standard
deviations are never (?) known in advance anyway so it does not really
matter in practice.

To make it more concrete, suppose we would like to tackle Example~\ref{exm:tooth-CI}.  First we will need the data.  Let us separate the \texttt{len} data into a VC vector \texttt{x} and an OJ vector \texttt{y}:

<<>>=
x = with(ToothGrowth, len[supp == "VC"])
y = with(ToothGrowth, len[supp == "OJ"])
@

Now it turns out that both \texttt{x} and \texttt{y} are exactly length 30, so it would be fair play to declare ``large sample!'' and go on about our business.  But we are not going to do that.  Let us check normality of the data.


<<tooth-qqplot, echo=FALSE, fig=TRUE, include=FALSE, width=5, results=hide>>=
library(RcmdrMisc)
par(mfrow = c(2,1))
qqPlot(x)
qqPlot(y)
par(mfrow = c(1,1))
@


\begin{figure}
\begin{center}
\includegraphics{IPSUR-tooth-qqplot}
\end{center}
\caption[Tooth growth data.]{{\small These are graphs of normal quantile-quantile comparison plots of the VC supplement group and the OJ supplement group in the \texttt{ToothGrowth} data. These data appear normal.  The graphs were generated by the \texttt{qqPlot} function from the \texttt{RcmdrMisc} package.}}
\label{fig:tooth-qqplot}
\end{figure}

Judging from Figure~\ref{fig:tooth-qqplot} both groups are normally distributed.  In the absence of additional information we may move forward with a Welch-Aspin interval, thusly:

<<>>=
t.test(x, y, conf.level = 0.99)
@

If we had reason to believe that the variances in the two groups were equal we could instead use the Pooled \(t\) interval like this:

<<>>=
t.test(x, y, conf.level = 0.99, var.equal = TRUE)
@

We can skip all of the subsetting into \texttt{x} and \texttt{y} vectors by using the formula interface to \texttt{t.test}.  The general form is \texttt{t.test(response ~ groups, data = A)}, where \texttt{A} is a data frame containing the \texttt{response} variable and a \texttt{groups} variable.  Here is how the Welch interval looks for \texttt{ToothGrowth}:

<<>>=
t.test(len ~ supp, data = ToothGrowth, conf.level = 0.99)
@

Watch out!  Notice with the formula interface that \textsf{R} is now computing a 99\% confidence interval for \(\mu_{OJ}-\mu_{VC}\), and \textbf{not} \(\mu_{VC}-\mu_{OJ}\), as before.  Why?  The computer takes the levels of the factor \texttt{supp} in \textit{alphabetical order} by default, and \texttt{"OJ"} comes before \texttt{"VC"}.


In closing, there are three things to notice about this example:
\begin{enumerate}
\item Regardless of whether we choose Welch or Pooled, the 99\% confidence interval covers the value zero.  In other words, with 99\% confidence, it is plausible that the means are the same (\(\mu_{VC}-\mu_{OJ} = 0\)).

\item Notice that the Pooled interval was ever-so-slightly more narrow than the Welch interval. This is not entirely a surprise; we would expect additional information such as the population variances are equal to generally increase the precision of our interval estimate, and in fact this is what we observed in this example.

\item We can save ourselves some typing by way of the formula interface to \texttt{t.test}, among other functions.  But PAY CLOSE ATTENTION to what parameter is being estimated and what the computer is doing under the hood.
\end{enumerate}


\section{Confidence Intervals for Proportions} 
\label{sec:confidence-intervals-proportions}


In this section we are trying to estimate \(p\) which is the ``proportion of
successes''. For instance, \(p\) could be:

\begin{itemize}
\item the proportion of U.S. citizens that support Obama,
\item the proportion of smokers among adults age 18 or over,
\item the proportion of people worldwide afflicted by COVID-19.
\end{itemize}

We are given an \(SRS(n)\) \(X_{1}\), \(X_{2}\), \ldots, \(X_{n}\)
distributed
\(\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p)\). Recall from
Section~\ref{sec:binom-dist} that the common mean of these variables
is \(\mathbb{E} X=p\) and the variance is
\(\mathbb{E}(X-p)^{2}=p(1-p)\). If we let \(Y=\sum X_{i}\), then from
Section~\ref{sec:binom-dist} we know that
\(Y\sim\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p)\) and that \[
\overline{X}=\frac{Y}{n}\mbox{ has }\mathbb{E}\overline{X}=p\mbox{ and
}\mathrm{Var}(\overline{X})=\frac{p(1-p)}{n}.  \] Thus if \(n\) is
large (here is the CLT) then an approximate \(100(1-\alpha)\%\)
confidence interval for \(p\) would be given by
\begin{equation}
\label{eq:ci-p-no-good}
\overline{X}\pm z_{\alpha/2}\sqrt{\frac{p(1-p)}{n}}.
\end{equation}
OOPS\ldots! Equation \eqref{eq:ci-p-no-good} is of no use to us because the
\underbar{unknown} parameter \(p\) is in the formula! (If we knew what
\(p\) was to plug in the formula then we would not need a confidence
interval in the first place.) There are two solutions to this problem.

\begin{enumerate}
\item Replace \(p\) with \(\hat{p}=\overline{X}\). Then an approximate
   \(100(1-\alpha)\%\) confidence interval for \(p\) is given by
   \begin{equation}
   \hat{p}\pm z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}.
   \end{equation}
   This approach is called the \textit{Wald interval} and is also known as
   the \textit{asymptotic interval} because it appeals to the CLT for large
   sample sizes.
\item Go back to first principles. Note that \[
   -z_{\alpha/2}\leq\frac{Y/n-p}{\sqrt{p(1-p)/n}}\leq z_{\alpha/2} \]
   exactly when the function \(f\) defined by \[
   f(p)=\left(Y/n-p\right)^{2}-z_{\alpha/2}^{2}\frac{p(1-p)}{n} \]
   satisfies \(f(p)\leq0\). But \(f\) is quadratic in \(p\) so its
   graph is a parabola; it has two roots, and these roots form the
   limits of the confidence interval. We can find them with the
   quadratic formula (see Exercise~\ref{exr:ci-quad-form}):
   \begin{equation}
   \left.\left[\left(\hat{p}+\frac{z_{\alpha/2}^{2}}{2n}\right)\pm z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}+\frac{z_{\alpha/2}^{2}}{(2n)^{2}}}\right]\right/ \left(1+\frac{z_{\alpha/2}^{2}}{n}\right)
   \end{equation}
   This approach is called the \textit{score interval} because it is based on
   the inversion of the ``Score test''. See Chapter~\ref{cha:categorical-data-analysis}. It is also known as the \textit{Wilson
   interval}; see Agresti \cite{Agresti2002}.
\end{enumerate}

For two proportions \(p_{1}\) and \(p_{2}\), we may collect
independent \(\mathsf{binom}(\mathtt{size}=1,\,\mathtt{prob}=p)\)
samples of size \(n_{1}\) and \(n_{2}\), respectively. Let \(Y_{1}\)
and \(Y_{2}\) denote the number of successes in the respective
samples.  We know that \[
\frac{Y_{1}}{n_{1}}\approx\mathsf{norm}\left(\mathtt{mean}=p_{1},\,\mathtt{sd}=\sqrt{\frac{p_{1}(1-p_{1})}{n_{1}}}\right)
\] and \[
\frac{Y_{2}}{n_{2}}\approx\mathsf{norm}\left(\mathtt{mean}=p_{2},\,\mathtt{sd}=\sqrt{\frac{p_{2}(1-p_{2})}{n_{2}}}\right)
\] so it stands to reason that an approximate \(100(1-\alpha)\%\)
confidence interval for \(p_{1}-p_{2}\) is given by
\begin{equation}
\left(\hat{p}_{1}-\hat{p}_{2}\right)\pm z_{\alpha/2}\sqrt{\frac{\hat{p}_{1}(1-\hat{p}_{1})}{n_{1}}+\frac{\hat{p}_{2}(1-\hat{p}_{2})}{n_{2}}},
\end{equation}
where \(\hat{p}_{1}=Y_{1}/n_{1}\) and \(\hat{p}_{2}=Y_{2}/n_{2}\).




\begin{rem}[]
When estimating a single proportion, one-sided intervals are sometimes
needed. They take the form
\begin{equation}
\left[0,\ \hat{p}+z_{\alpha}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}\right]
\end{equation}
or
\begin{equation}
\left[\hat{p}-z_{\alpha}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}},\ 1\right]
\end{equation}
or in other words, we know in advance that the true proportion is
restricted to the interval \([0,1]\), so we can truncate our
confidence interval to those values on either side.
\end{rem}


\subsection{How to do it with \textsf{R}}

Confidence interval estimation (two-sided only) of a single population proportion is perhaps slickest done through the \texttt{binconf} function in the \texttt{Hmisc} package.  For the \textit{Wald} interval set \texttt{method = "asymptotic"}.  Below \texttt{x} represents the number of successes in the sample, \texttt{n} represents the sample size, and \texttt{alpha} represents 1 minus the confidence level.  So, for a 95\% confidence interval for \(p\) after we observed 7 successes in 25 trials, we would do

<<echo=TRUE>>=
library(Hmisc)
binconf(x = 7, n = 25, alpha = 0.05, method = "asymptotic")
@

For the \textit{score} interval set \texttt{method = "wilson"}:

<<echo=TRUE>>=
binconf(x = 7, n = 25, alpha = 0.05, method = "wilson")
@

The default value of the \texttt{alpha} argument is \texttt{0.05} and the default \texttt{method} argument is \texttt{wilson}:

<<echo=TRUE>>=
binconf(x = 7, n = 25)
@

Another way to get the same Wilson interval is to use the \texttt{prop.test} function, but there we must specify the confidence level directly, and be sure to turn off the \textit{Yates' continuity correction}:

<<echo=TRUE>>=
prop.test(x = 7, n = 25, conf.level = 0.95, correct = FALSE)
@

The \texttt{binconf} function does not support one-sided confidence intervals, but \texttt{prop.test} does;  request them via the \texttt{alternative} argument.  Continuing this example, to get a 95\% one-sided confidence interval for \(p\) that provides a lower bound do

<<echo=TRUE>>=
prop.test(x = 7, n = 25, alternative = "greater", correct = FALSE)
@

Note that the default value of \texttt{conf.level} in \texttt{prop.test} is \texttt{0.95}, so we skipped that argument in this example.  A one-sided 95\% interval that gives an upper bound is found with \texttt{alternative = "less"}, that is, the opposite of what a normal human would expect it to be.

In the two-sample problem we are constructing a confidence interval for the difference, \(p_{1}-p_{2}\).  In this case we are limited to the \texttt{prop.test} function.  The syntax is the same, except our numbers of successes are now a vector \texttt{x = c(y1, y2)} and the sample sizes are a vector \texttt{n = c(n1, n2)}.  For instance, if we observed 7 successes out of 25 trials in the first sample and 11 successes out of 37 trials in the second (independent) sample, a 99\% two-sided confidence interval for \(p_{1}-p_{2}\) would be given by

<<echo=TRUE>>=
prop.test(x = c(7,11), n = c(25,37), 
          conf.level = 0.99, correct = FALSE)
@



\section{Confidence Intervals for Variances} 
\label{sec:confidence-intervals-for-variances}

We are given \(X_{1}\), \(X_{2}\), \ldots, \(X_{n}\) that are an
\(SRS(n)\) from a
\(\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\)
distribution, where this time \(\sigma\) is unknown and the target of our interest. We know that we may estimate
\(\sigma^2\) with \(S^2\), and we have seen that this estimator is unbiased. But how good is our estimate? We know that
\begin{equation}
\frac{(n - 1) S^2}{\sigma^2}\sim\mathsf{chisq}(\mathtt{df}=n - 1).
\end{equation}
For a big probability \(1-\alpha\) we can calculate the quantile \(\chi^2_{\alpha/2}(n - 1)\), but the Chi-square distribution is not symmetric about zero, so this time we will need a second quantile for the left-hand inequality, namely, \(\chi^2_{1 - \alpha/2}(n - 1)\). Then
\begin{equation}
\mathbb{P}\left(\chi^2_{1 - \alpha/2}(n - 1)\leq\frac{(n - 1) S^2}{\sigma^2}\leq \chi^2_{\alpha/2}(n - 1)\right)=1-\alpha.
\end{equation}
Next, solving this string of inequalities to put \(\sigma^2\) in the middle leads to:
\begin{equation}
\mathbb{P}\left(
\frac{(n - 1) S^2}{\chi^2_{\alpha/2}(n - 1)}
\leq
\sigma^2
\leq 
\frac{(n - 1) S^2}{\chi^2_{1 - \alpha/2}(n - 1)}
\right)=1-\alpha.
\end{equation}
That is, a \(100(1 - \alpha)\%\) confidence interval for \(\sigma^2\) is given by
\begin{equation}
\left[  
\frac{(n - 1) S^2}{\chi^2_{\alpha/2}(n - 1)}, \ 
\frac{(n - 1) S^2}{\chi^2_{1 - \alpha/2}(n - 1)}
\right].
\end{equation}


In the two-sample problem, we are given \(X_{1}\), \(X_{2}\), \ldots, \(X_{n}\) a \(SRS(n)\) from a
\(\mathsf{norm}(\mathtt{mean}=\mu_{X},\,\mathtt{sd}=\sigma_{X})\)
distribution and \(Y_{1}\), \(Y_{2}\), \ldots, \(Y_{m}\) a
\(SRS(m)\) from a
\(\mathsf{norm}(\mathtt{mean}=\mu_{Y},\,\mathtt{sd}=\sigma_{Y})\)
distribution that is independent of the first sample.  From Section~\ref{sub:ratio-indep-variance} we know that
\[F=\frac{\sigma_{Y}^{2}S_{X}^{2}}{\sigma_{X}^{2}S_{Y}^{2}}\sim \mathsf{f}(\mathtt{df1}=n-1,\,\mathtt{df2}=m-1).\]
Next find the quantiles \(\mathsf{f}_{1 - \alpha/2}(n-1,\,m-1)\) and \(\mathsf{f}_{\alpha/2}(n-1,\,m-1)\). Then 
\begin{equation}
\mathbb{P}\left(\mathsf{f}_{1 - \alpha/2}
\leq
\frac{\sigma_{Y}^{2}S_{X}^{2}}{\sigma_{X}^{2}S_{Y}^{2}}
\leq
\mathsf{f}_{\alpha/2}
\right)=1-\alpha,
\end{equation}
or, in other words,
\begin{equation}
\left[  
\frac{S_{X}^{2}/S_{Y}^{2}}{\mathsf{f}_{1 - \alpha/2}}, \ 
\frac{S_{X}^{2}/S_{Y}^{2}}{\mathsf{f}_{\alpha/2}}
\right]
\end{equation}
is a \(100(1 - \alpha)\%\) confidence interval for \(\sigma_{X}^2/\sigma_{Y}^2\).

\begin{rem}
In this section we have derived two confidence intervals, and how they work out is really quite beautiful as far as mathematics is concerned.  However, be warned that insofar as practice is concerned both the chi-square interval and the \(F\) interval are not said to perform very well and are sensitive to the normality assumption; use at your peril.
\end{rem}

\subsection{How to do it with \textsf{R}}
\label{sub:sigma-how-R}

The \texttt{sigma.test} function in the \texttt{TeachingDemos} package
\cite{TeachingDemos} is suited for one-sample problems.  Say, for instance, that Michelson (see Example~\ref{BLANK}) would like to estimate the variance for his measurements of the speed of light using the observations in the first experiment (see Example BLANK).  Recall that we already investigated the normality of the observations and were satisfied.  Suppose Michelson would like a \(99\%\) confidence interval for \(\sigma^2\):

<<>>=
x <- with(morley, Speed[Expt == 1])
library(TeachingDemos)
sigma.test(x, conf.level = 0.99)
@

For the two-sample problem we return to the \texttt{ToothGrowth} data of Example~\ref{BLANK} and employ \texttt{var.test}:

<<>>=
var.test(len ~ supp, data = ToothGrowth, conf.level = 0.99)
@

\noindent
We see that a 99\% confidence interval for \(\sigma^2_{OJ}/\sigma^2_{VC}\) is approximately \([0.238,\ 1.708]\) (round outward, recall that \textsf{R} orders factor levels alphabetically by default).


\section{Sample Size and Margin of Error} \label{sec:Sample-Size-and-moe}

Sections~\ref{sec:confidence-intervals-for-means} through
\ref{sec:confidence-intervals-for-variances} all began the same way:
we were given the sample size \(n\) and the confidence coefficient
\(1-\alpha\), and our task was to find a margin of error \(E\) so
that
\[ \hat{\theta}\pm E\mbox{ is a }100(1-\alpha)\%\mbox{ confidence
    interval for }\theta.  \]

Some examples we saw were:

\begin{itemize}
\item \(E=z_{\alpha/2}\sigma/\sqrt{n}\), in the one-sample \(z\)-interval,
\item \(E=t_{\alpha/2}(\mathtt{df}=n+m-2)S_{p}\sqrt{n^{-1}+m^{-1}}\), in
  the two-sample pooled \(t\)-interval.
\end{itemize}

We already know (we can see in the formulas above) that \(E\)
decreases as \(n\) increases. Now we would like to use this
information to our advantage: suppose that we have a fixed margin of
error \(E,\) say \(E=3\), and we want a \(100(1-\alpha)\%\) confidence
interval for \(\mu\). The question is: how big does \(n\) have to be?

For the case of a population mean the answer is easy: we set up an
equation and solve for \(n\).




\begin{example}[Michelson Speed of Light Data]
\label{exm:morley-sample-size}
Returning to the 1879 speed of light data collected by A.~A.~Michelson, suppose he would like to estimate the mean speed of light to within 1 km/sec with a 95\% two-sided confidence interval.  And for the sake of argument, suppose it is reasonable for Michelson to take the standard deviation of measurements from his apparatus to be \(\sigma = 105\).  How big would \(n\) need to be to ensure that \(\overline{X}\pm 1\) is a
95\% confidence interval for \(\mu\)?
\end{example}



\noindent
\textbf{Solution:}  We know from the confidence interval formula that 
\begin{equation}
E = z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{n}},
\end{equation}
or, solving this equation for \(n\),
\begin{equation}
n = \frac{z_{\alpha/2}^2\,\sigma^2}{E^2} = \left( \frac{z_{\alpha/2}\sigma}{E} \right)^2.
\end{equation}
Now plugging in \(\sigma = 105\), \(z_{0.025} = 1.959964\), and \(E = 1\), we get \(n = 42352.08\), that is, Michelson would need to collect 42,353 observations to achieve such precision!

\begin{rem}[]
Always round up any decimal values of \(n\), no matter how small the decimal is. Another name for \(E\) is the ``maximum error of the estimate''.
\end{rem}

For proportions, recall that the asymptotic formula to estimate \(p\)
was \[ \hat{p}\pm z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}.  \]
Reasoning as above we would want
\begin{align}
\label{eq:samp-size-prop-ME}
E & =z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}},\mbox{ or}\\
n & =z_{\alpha/2}^{2}\frac{\hat{p}(1-\hat{p})}{E^{2}}.
\end{align}
OOPS! Recall that \(\hat{p}=Y/n\), which would put the variable \(n\)
on both sides of Equation \eqref{eq:samp-size-prop-ME}. Again, there are two
solutions to the problem.

\begin{enumerate}
\item If we have a good idea of what \(p\) is, say \(p^{\ast}\) then we
   can plug it in to get
   \begin{equation}
   n=z_{\alpha/2}^{2}\frac{p^{\ast}(1-p^{\ast})}{E^{2}}.
   \end{equation}
\item Even if we have no idea what \(p\) is, we do know from calculus
   that \(p(1-p)\leq1/4\) because the function \(f(x)=x(1-x)\) is
   quadratic (so its graph is a parabola which opens downward) with
   maximum value attained at \(x=1/2\). Therefore, regardless of our
   choice for \(p^{\ast}\) the sample size must satisfy
   \begin{equation}
   n=z_{\alpha/2}^{2}\frac{p^{\ast}(1-p^{\ast})}{E^{2}}\leq\frac{z_{\alpha/2}^{2}}{4E^{2}}.
   \end{equation}
   The quantity \(z_{\alpha/2}^{2}/4E^{2}\) is large enough to
   guarantee \(100(1-\alpha)\%\) confidence.
\end{enumerate}


\begin{example}[Presidential Election 2020]
\label{exm:election-2020}
Suppose we are a pollster that would like to estimate the proportion of Ohio likely-voters in support of Democratic candidate Joe Biden.  Also suppose that we have the wherewithal to collect a simple random sample of likely-voters in the great state of Ohio.  If we use a 99\% confidence interval, how many voters should we sample to ensure that we are within 0.03 of th true proportion?  Suppose prior polls in the state measured his support level at \(p = 0.57\).
\end{example}



\noindent
\textbf{Solution:}  Incorporating the prior polling data we can set \(p^{\ast} = 0.57\) in the equation 
\begin{equation}
n=z_{\alpha/2}^{2}\frac{p^{\ast}(1-p^{\ast})}{E^{2}}
\end{equation}
and plugging in \(z_{0.005} = 2.575829\), \(E = 0.03\) yields \(1016.383\), that is, we would need to sample 1017 likely voters.  Maybe we would like to play it safe and set \(p^{\ast} = 0.5\); the new required sample size would be 1036.72, that is, 1037 voters.


\begin{rem}[]
For very small populations sometimes the value of \(n\) obtained from
the formula is too big. In this case we should use the hypergeometric
distribution for a sampling model rather than the binomial model. With
this modification the formulas change to the following: if \(N\)
denotes the population size then let
\begin{equation}
m=z_{\alpha/2}^{2}\frac{p^{\ast}(1-p^{\ast})}{E^{2}}
\end{equation}
and the sample size needed to ensure \(100(1-\alpha)\%\) confidence is
achieved is
\begin{equation}
n=\frac{m}{1+\frac{m-1}{N}}.
\end{equation}
If we do not have a good value for the estimate \(p^{\ast}\) then we may use \(p^{\ast}=1/2\).
\end{rem}


\subsection{How to do it with \textsf{R}}

We can nearly replicate the analysis for the Michelson speed of light data via the \texttt{power.t.test} function:

<<>>=
power.t.test(delta = 1, sd = 105, sig.level = 0.05, 
             type = "one.sample", power = 0.5)
@

\noindent
Note that we set \texttt{power = 0.5} and we get a value for \(n\) almost identical to the one we found earlier.


\section{Chapter Exercises}

\begin{Exercise}[label=exr:norm-mu-sig-mle]
Let \(X_{1}\), \(X_{2}\), \ldots, \(X_{n}\) be an
\(SRS(n)\) from a \(\mathsf{norm}(\mathtt{mean} = \mu, \, \mathtt{sd}
= \sigma)\) distribution. Find a two-dimensional MLE for
\(\theta=(\mu,\sigma)\).
\end{Exercise}





\begin{Exercise}[label=exr:ci-quad-form]
Find the upper and lower limits for the
confidence interval procedure by finding the roots of \(f\) defined by
\[ f(p)=\left(Y/n-p\right)^{2}-z_{\alpha/2}^{2}\frac{p(1-p)}{n}.  \]
You are going to need the quadratic formula.
\end{Exercise}



\chapter{Hypothesis Testing} \label{cha:hypothesis-testing}


<<echo=FALSE, include=FALSE>>=
# This chapter's package dependencies
library(TeachingDemos)
library(HH)
@

<<echo=FALSE, include=FALSE>>=
# need this for plotting hypothesis tests
# based on work with R. Heiberger in 2009-10

plot.htest <- function (x, hypoth.or.conf = 'Hypoth',...) {
  require(HH)
  if (x$method == "1-sample proportions test with continuity correction" || x$method == "1-sample proportions test without continuity correction"){
    mu <- x$null.value
    obs.mean <- x$estimate
    n <- NA
    std.dev <- abs(obs.mean - mu)/sqrt(x$statistic)
    deg.freedom <- NA
    if(x$alternative == "two.sided"){
      alpha.right <- (1 - attr(x$conf.int, "conf.level"))/2
      Use.alpha.left <- TRUE
      Use.alpha.right <- TRUE
    } else if (x$alternative == "less") {
      alpha.right <- 1 - attr(x$conf.int, "conf.level")
      Use.alpha.left <- TRUE
      Use.alpha.right <- FALSE
    } else {
      alpha.right <- 1 - attr(x$conf.int, "conf.level")
      Use.alpha.left <- FALSE
      Use.alpha.right <- TRUE
    }

  } else if (x$method == "One Sample z-test"){
    mu <- x$null.value
    obs.mean <- x$estimate
    n <- x$parameter[1]
    std.dev <- x$parameter[2]
    deg.freedom <- NA
    if(x$alternative == "two.sided"){
      alpha.right <- (1 - attr(x$conf.int, "conf.level"))/2
      Use.alpha.left <- TRUE
      Use.alpha.right <- TRUE
    } else if (x$alternative == "less") {
      alpha.right <- 1 - attr(x$conf.int, "conf.level")
      Use.alpha.left <- TRUE
      Use.alpha.right <- FALSE
    } else {
      alpha.right <- 1 - attr(x$conf.int, "conf.level")
      Use.alpha.left <- FALSE
      Use.alpha.right <- TRUE
    }
  } else if (x$method == "One Sample t-test" || x$method == "Paired t-test"){
    mu <- x$null.value
    obs.mean <- x$estimate
    n <- x$parameter + 1
    std.dev <- x$estimate/x$statistic*sqrt(n)
    deg.freedom <- x$parameter
    if(x$alternative == "two.sided"){
      alpha.right <- (1 - attr(x$conf.int, "conf.level"))/2
      Use.alpha.left <- TRUE
      Use.alpha.right <- TRUE
    } else if (x$alternative == "less") {
      alpha.right <- 1 - attr(x$conf.int, "conf.level")
      Use.alpha.left <- TRUE
      Use.alpha.right <- FALSE
    } else {
      alpha.right <- 1 - attr(x$conf.int, "conf.level")
      Use.alpha.left <- FALSE
      Use.alpha.right <- TRUE
    }
  } else if (x$method == "Welch Two Sample t-test"){
    mu <- x$null.value
    obs.mean <- -diff(x$estimate)
    n <- x$parameter + 2
    std.dev <- obs.mean/x$statistic*sqrt(n)
    deg.freedom <- x$parameter
    if(x$alternative == "two.sided"){
      alpha.right <- (1 - attr(x$conf.int, "conf.level"))/2
      Use.alpha.left <- TRUE
      Use.alpha.right <- TRUE
    } else if (x$alternative == "less") {
      alpha.right <- 1 - attr(x$conf.int, "conf.level")
      Use.alpha.left <- TRUE
      Use.alpha.right <- FALSE
    } else {
      alpha.right <- 1 - attr(x$conf.int, "conf.level")
      Use.alpha.left <- FALSE
      Use.alpha.right <- TRUE
    }
  } else if (x$method == " Two Sample t-test"){
    mu <- x$null.value
    obs.mean <- -diff(x$estimate)
    n <- x$parameter + 2
    std.dev <- obs.mean/x$statistic*sqrt(n)
    deg.freedom <- x$parameter
    if(x$alternative == "two.sided"){
      alpha.right <- (1 - attr(x$conf.int, "conf.level"))/2
      Use.alpha.left <- TRUE
      Use.alpha.right <- TRUE
    } else if (x$alternative == "less") {
      alpha.right <- 1 - attr(x$conf.int, "conf.level")
      Use.alpha.left <- TRUE
      Use.alpha.right <- FALSE
    } else {
      alpha.right <- 1 - attr(x$conf.int, "conf.level")
      Use.alpha.left <- FALSE
      Use.alpha.right <- TRUE
    }
  }
  return(normal.and.t.dist(
    mu.H0 = mu,
    obs.mean = obs.mean,
    std.dev = std.dev,
    n = n,
    deg.freedom = deg.freedom,
    alpha.right = alpha.right,
    Use.obs.mean = TRUE,
    Use.alpha.left = Use.alpha.left,
    Use.alpha.right = Use.alpha.right,
    hypoth.or.conf = hypoth.or.conf)
  )
}
@

\paragraph{What do I want them to know?}

\begin{itemize}
\item basic terminology and philosophy of the Neyman-Pearson paradigm
\item classical hypothesis tests for the standard one and two sample
  problems with means, variances, and proportions
\item the notion of between versus within group variation and how it plays
  out with one-way ANOVA
\item the concept of statistical power and its relation to sample size
\end{itemize}

\section{Introduction} \label{sec:introduction-hypothesis}

I spent a week during the summer of 2005 at the University of Nebraska
at Lincoln grading Advanced Placement Statistics exams, and while I
was there I attended a presentation by Dr.~Roxy Peck. At the end of
her talk she described an activity she had used with students to
introduce the basic concepts of hypothesis testing. I was impressed by
the activity and have used it in my own classes several times since.

\begin{quote}
The instructor (with a box of cookies in hand) enters a class of
fifteen or more students and produces a brand-new, sealed deck of
ordinary playing cards. The instructor asks for a student volunteer to
break the seal, and then the instructor prominently shuffles the
deck\footnote{The jokers are removed before shuffling.} several times in front of the class, after which
time the students are asked to line up in a row. They are going to
play a game. Each student will draw a card from the top of the deck,
in turn. If the card is black, then the lucky student will get a
cookie. If the card is red, then the unlucky student will sit down
empty-handed. Let the game begin.

The first student draws a card: red. There are jeers and outbursts,
and the student slinks off to his/her chair. (S)he is disappointed, of
course, but not really. After all, (s)he had a 50-50 chance of getting
black, and it did not happen. Oh well.

The second student draws a card: red, again. There are more jeers, and
the second student slips away. This student is also disappointed, but
again, not so much, because it is probably his/her unlucky day. On to
the next student.

The student draws: red again! There are a few wiseguys who yell (happy
to make noise, more than anything else), but there are a few other
students who are not yelling any more -- they are thinking. This is
the third red in a row, which is possible, of course, but what is
going on, here? They are not quite sure. They are now concentrating on
the next card\ldots it is bound to be black, right?

The fourth student draws: red. Hmmm\ldots now there are groans instead of
outbursts. A few of the students at the end of the line shrug their
shoulders and start to make their way back to their desk, complaining
that the teacher does not want to give away any cookies. There are
still some students in line though, salivating, waiting for the
inevitable black to appear.

The fifth student draws red. Now it isn't funny any more. As the
remaining students make their way back to their seats an uproar
ensues, from an entire classroom demanding cookies.
\end{quote}


Keep the preceding experiment in the back of your mind as you read the
following sections. When you have finished the entire chapter, come
back and read this introduction again. All of the mathematical jargon
that follows is connected to the above paragraphs. In the meantime, I
will get you started:

\begin{description}
\item [{Null hypothesis:}] it is an ordinary deck of playing cards, shuffled thoroughly.
\item [{Alternative hypothesis:}] something funny is going on. Either it is a trick deck of cards, or the instructor is doing some fancy shufflework.
\item {[Observed data:}] the sequence of draws from the deck, five reds in a row.
\end{description}

If it were truly an ordinary, well-shuffled deck of cards, the
probability of observing zero blacks out of a sample of size five
(without replacement) from a deck with 26 black cards and 26 red cards
would be

<<echo=TRUE>>=
dhyper(0, m = 26, n = 26, k = 5)
@

There are two very important final thoughts. First, everybody gets a
cookie in the end. Second, the students invariably (and aggressively)
attempt to get me to open up the deck and reveal the true nature of
the cards. I never do.

\section{Tests for Proportions} \label{sec:tests-for-proportions}

\begin{example}[]
\label{exm:widget-machine}
We have a machine that makes widgets.

\begin{itemize}
\item Under normal operation, about 0.10 of the widgets produced are
  defective.
\item Go out and purchase a torque converter.
\item Install the torque converter, and observe \(n=100\) widgets from the
  machine.
\item Let \(Y=\mbox{number of defective widgets observed}\).
\end{itemize}
\end{example}

\noindent
If

\begin{itemize}
\item \(Y=0\), then the torque converter is great!
\item \(Y=4\), then the torque converter seems to be helping.
\item \(Y=9\), then there is not much evidence that the torque converter helps.
\item \(Y=17\), then throw away the torque converter.
\end{itemize}


Let \(p\) denote the proportion of defectives produced by the
machine. Before the installation of the torque converter \(p\) was
\(0.10\). Then we installed the torque converter. Did \(p\) change?
Did it go up or down? We use statistics to decide. Our method is to
observe data and construct a 95\% confidence interval for \(p\),
\begin{equation}
\hat{p} \pm z_{\alpha/2}\sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}.
\end{equation}
If the confidence interval is

\begin{itemize}
\item \([0.01,\,0.05]\), then we are 95\% confident that \(0.01\leq
  p \leq 0.05\), so there is evidence that the torque converter is
  helping.
\item \([0.15,\,0.19]\), then we are 95\% confident that \(0.15\leq
  p \leq 0.19\), so there is evidence that the torque converter is
  hurting.
\item \([0.07,\,0.11]\), then there is not enough evidence to conclude
  that the torque converter is doing anything at all, positive or
  negative.
\end{itemize}

\subsection{Terminology}

The \textit{null hypothesis} \(H_{0}\) is a ``nothing'' hypothesis, whose
interpretation could be that nothing has changed, there is no
difference, there is nothing special taking place, \textit{etc}. In
Example~\ref{exm:widget-machine} the null hypothesis would be \(H_{0}:\, p = 0.10.\)
The \textit{alternative hypothesis} \(H_{1}\) is the hypothesis that
something has changed, in this case, \(H_{1}:\, p \neq 0.10\). Our
goal is to statistically \textit{test} the hypothesis \(H_{0}:\, p = 0.10\)
versus the alternative \(H_{1}:\, p \neq 0.10\). Our procedure will
be:

\begin{enumerate}
\item Go out and collect some data, in particular, a simple random sample
   of observations from the machine.
\item Suppose that \(H_{0}\) is true and construct a \(100(1-\alpha)\%\)
   confidence interval for \(p\).
\item If the confidence interval does not cover \(p = 0.10\), then we
   \textit{reject} \(H_{0}\). Otherwise, we \textit{fail to reject} \(H_{0}\).
\end{enumerate}



\begin{rem}[]
  Every time we make a decision it is possible to be wrong, and there
  are two possible mistakes we can make. We have committed a

\begin{description}
\item [{Type I Error}] if we reject \(H_{0}\) when in fact \(H_{0}\)
  is true. This would be akin to convicting an innocent person for a
  crime (s)he did not commit.
\item [{Type II Error}] if we fail to reject \(H_{0}\) when in fact
  \(H_{1}\) is true. This is analogous to a guilty person escaping
  conviction.
\end{description}
\end{rem}

\noindent
Type I Errors are usually considered worse\footnote{There is no
  mathematical difference between the errors, however. The bottom line
  is that we choose one type of error to control with an iron fist,
  and we try to minimize the probability of the other type. That being
  said, null hypotheses are often by design to correspond to the
  ``simpler'' model, so it is often easier to analyze (and thereby
  control) the probabilities associated with Type I Errors.}, and we
design our statistical procedures to control the probability of making
such a mistake. We define the
\begin{equation}
\mbox{significance level of the test} = \mathbb{P}(\mbox{Type I Error}) = \alpha.
\end{equation}
We want \(\alpha\) to be small which conventionally means, say,
\(\alpha=0.05\), \(\alpha=0.01\), or \(\alpha=0.005\) (but could mean
anything, in principle).

\begin{itemize}
\item The \textit{rejection region} (also known as the \textit{critical region}) for
  the test is the set of sample values which would result in the
  rejection of \(H_{0}\). For Example~\ref{exm:widget-machine}, the
  rejection region would be all possible samples that result in a 95\%
  confidence interval that does not cover \(p = 0.10\).
\item The above example with \(H_{1}:\,p \neq 0.10\) is called a
  \textit{two-sided} test. Many times we are interested in a
  \textit{one-sided} test, which would look like \(H_{1}:\,p < 0.10\)
  or \(H_{1}:\,p > 0.10\).
\end{itemize}

We are ready for tests of hypotheses for one proportion.  We know from
Section~\ref{sec:confidence-intervals-proportions} that when \(H_{0}:\,p = p_{0}\) is true and \(n\) is
large,
\begin{equation}
\hat{p} \sim \mathsf{norm}(\mathtt{mean} = p_{0}, \mathtt{sd} = \sqrt{p_{0}(1 - p_{0})/n}),
\end{equation}
approximately, and the approximation gets better as the sample size gets bigger. Another way to write this is
\begin{equation}
Z = \frac{\hat{p} - p_{0}}{\sqrt{p_{0}(1 - p_{0})/n}}  \sim \mathsf{norm}(\mathtt{mean} = 0, \mathtt{sd} = 1).
\end{equation}

\begin{table}
\begin{center}
\begin{tabular}{lll}
\(H_{0}\) & \(H_{a}\) & Rejection Region\\
\hline
\(p = p_{0}\) & \(p > p_{0}\) & \(z > z_{\alpha}\)\\
\(p = p_{0}\) & \(p < p_{0}\) & \(z < -z_{\alpha}\)\\
\(p = p_{0}\) & \(p \neq p_{0}\) & \(\vert z \vert > z_{\alpha/2}\)\\
\end{tabular}
\end{center}
\caption{Hypothesis tests, population proportion, large sample.}
\label{tab:ztest-one-sample-prop}
\end{table}


\textbf{Assumptions for a valid test:}

\begin{itemize}
\item A simple random sample from a Bernoulli population
\item The sample size \(n\) is large
\item need at least 15 successes and at least 15 failures
\end{itemize}



\begin{example}[]
\label{exm:prop-test-pvalue-a}
Suppose \(p = \text{the proportion of students}\) who are admitted to the graduate school of the University
of California at Berkeley, and suppose that a public relations officer
boasts that UCB has historically had a 40\% acceptance rate for its
graduate school. Consider the data stored in the table \texttt{UCBAdmissions}
from 1973. Assuming these observations constituted a simple random
sample, are they consistent with the officer's claim, or do they
provide evidence that the acceptance rate was significantly less than
40\%? Use an \(\alpha = 0.01\) significance level.
\end{example}

Our null hypothesis in this problem is \(H_{0}:\,p = 0.4\) and the
alternative hypothesis is \(H_{1}:\,p < 0.4\). We reject the null
hypothesis if \(\hat{p}\) is too small, that is, if
\begin{equation}
\frac{\hat{p} - 0.4}{\sqrt{0.4(1 - 0.4)/n}} < -z_{\alpha},
\end{equation}
where \(\alpha = 0.01\) and \(-z_{0.01}\) is

<<echo=TRUE>>=
-qnorm(0.99)
@

That is, \(-z_{0.01} \approx 2.326\).  Our only remaining task is to find the value of the test statistic and see where it falls relative to the critical value. We can find the
number of applicants admitted and not admitted to the UCB graduate school
with the following.

<<echo=TRUE>>=
xtabs(Freq ~ Admit, data = UCBAdmissions)
@

\noindent
So a point estimate for \(p\) would be \(\hat{p} = 1755/(1755 + 2771) \approx 0.388\).  Next we calculate the value of the \(z\) test statistic:

<<echo=TRUE>>=
phat <- 1755/(1755 + 2771)
(phat - 0.4)/sqrt(0.4 * 0.6/(1755 + 2771))
@

\noindent
With \(z \approx -1.681\) our test statistic is not less than \(-2.326\), so it does not fall
into the critical region. Therefore, we \textit{fail} to reject the null
hypothesis that the true proportion of students admitted to graduate
school is less than 40\% and say that the observed data are consistent
with the officer's claim at the \(\alpha = 0.01\) significance level.




\begin{example}[]
\label{exm:prop-test-pvalue-b}
We are going to do Example~\ref{exm:prop-test-pvalue-a} all over
again. Everything will be exactly the same except for one
change. Suppose we choose significance level \(\alpha = 0.05\) instead
of \(\alpha = 0.01\). Are the 1973 data consistent with the officer's
claim?
\end{example}

Our null and alternative hypotheses are the same. Our observed test statistic is the same: it was approximately \(-1.681\). But notice that our critical value has changed: \(\alpha = 0.05\) and \(-z_{0.05}\) is

<<echo=TRUE>>=
-qnorm(0.95)
@

\noindent
Our test statistic is less than \(-1.645\) so it now falls into the
critical region! We now \textbf{\textit{reject}} the null hypothesis and
conclude that the 1973 data provide evidence that the true proportion
of students admitted to the graduate school of UCB in 1973 was
significantly less than 40\%. The data are \textit{not} consistent
with the officer's claim at the \(\alpha = 0.05\) significance level.

What is going on, here? If we choose \(\alpha = 0.05\) then we reject
the null hypothesis, but if we choose \(\alpha = 0.01\) then we fail
to reject the null hypothesis. Our final conclusion seems to depend on
our selection of the significance level. This is bad; for a particular
test, we never know whether our conclusion would have been different
if we had chosen a different significance level.

Or do we?

Clearly, for some significance levels we reject, and for some
significance levels we do not. Where is the boundary? That is, what is
the significance level for which we would \textit{reject} at any
significance level \textit{bigger}, and we would \textit{fail to
  reject} at any significance level \textit{smaller}? This boundary
value has a special name: it is called the \textit{p-value} of the
test.



\begin{defn}[Observed significance level.]
  The \textit{p-value}, or \textit{observed significance level}, of a
  hypothesis test is the probability when the null hypothesis is true
  of obtaining the observed value of the test statistic (such as
  \(\hat{p}\)) or values more extreme -- meaning, in the direction of
  the alternative hypothesis\footnote{Bickel and Doksum \cite{Bickel2001}
    state the definition particularly well: the \(p\)-value is ``the
    smallest level of significance \(\alpha\) at which an experimenter
    using the test statistic \(T\) would reject \(H_{0}\) on the basis
    of the observed sample outcome \(x\)''.}.
\end{defn}



\begin{example}[]
Calculate the \(p\)-value for the tests in Examples~\ref{exm:prop-test-pvalue-a} and~\ref{exm:prop-test-pvalue-b}.
\end{example}


The \(p\)-value for this test is the probability of obtaining a
\(z\)-score equal to our observed test statistic (which had
\(z \approx-1.681\)) or more extreme, which in this
example is less than the observed test statistic. In other words, we
want to know the area under a standard normal curve on the interval
\((-\infty,\,-1.680919]\). We can get this easily with


<<echo=TRUE>>=
pnorm(-1.680919)
@

We see that the \(p\)-value is strictly between the significance
levels \(\alpha = 0.01\) and \(\alpha = 0.05\). This makes sense: it
has to be bigger than \(\alpha = 0.01\) (otherwise we would have
rejected \(H_{0}\) in Example~\ref{exm:prop-test-pvalue-a}) and it
must also be smaller than \(\alpha = 0.05\) (otherwise we would not
have rejected \(H_{0}\) in Example~\ref{exm:prop-test-pvalue-b}). Indeed, \(p\)-values are a
characteristic indicator of whether or not we would have rejected at
assorted significance levels, and for this reason a statistician will
often skip the calculation of critical regions and critical values
entirely. If (s)he knows the \(p\)-value, then (s)he knows immediately
whether or not (s)he would have rejected at \textit{any} given
significance level.

Thus, another way to phrase our significance test procedure is: we
will reject \(H_{0}\) at the \(\alpha\)-level of significance if the
\(p\)-value is less than \(\alpha\).


<<proptest-plot, echo=FALSE, fig=TRUE, include=FALSE, width=5>>=
plot(prop.test(1755, n = 4526, p = 0.4, alternative = "less", correct = FALSE))
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-proptest-plot}
\end{center}
\caption[A plot of the results from a \texttt{prop.test}.]{{\small A
    plot of the results from one-sample test for a population proportion.  The critical region for this one-sided test is shown by the shaded blue area.  The test statistic is marked by the dashed green vertical line, and the \(p\)-value is denoted by the area enclosed by the green box.  We see in the bottom right corner that the  \(p\)-value is just shy of the significance level \(\alpha = 0.05\).  This graph was generated by code based on joint work with Prof Richard Heiberger and uses the \texttt{normal.and.t.dist} function in the
    \texttt{HH} package \cite{HH}. }}
\label{fig:ttest-plot}
\end{figure}

\begin{rem}[]
If we have two populations with proportions \(p_{1}\) and \(p_{2}\)
then we can test the null hypothesis \(H_{0}:\,p_{1} = p_{2}\). In
that which follows,

\begin{itemize}
\item we observe independent simple random samples of size \(n_{1}\) and
  \(n_{2}\) from Bernoulli populations with respective probabilities
  of success \(p_{1}\) and \(p_{2}\),
\item we write \(y_{1}\) and \(y_{2}\) for the respective numbers of
  successes from each of the two groups,
\item we estimate \(p_{1}\) and \(p_{2}\) with \(\hat{p}_{1} = y_{1}/n_{1}
 \) and \(\hat{p}_{2} = y_{2}/n_{2}\), while we estimate the pooled
  probability \(\hat{p}\) with \((y_{1} + y_{2})/(n_{1} + n_{2}\), and
\item finally, the large sample \(z\) test statistic is
\begin{equation}
z = \frac{\hat{p}_{1} - \hat{p}_{2}}{\sqrt{\hat{p}(1 - \hat{p})\left( \frac{1}{n_{1}} + \frac{1}{n_{2}} \right)}}.
\end{equation}
\end{itemize}
\end{rem}

\begin{table}
\begin{center}
\begin{tabular}{lll}
\(H_{0}\) & \(H_{a}\) & Rejection Region\\
\hline
\(p_{1} = p_{2}\) & \(p_{1} - p_{2} > 0\) & \(z > z_{\alpha}\)\\
\(p_{1} = p_{2}\) & \(p_{1} - p_{2} < 0\) & \(z < -z_{\alpha}\)\\
\(p_{1} = p_{2}\) & \(p_{1} - p_{2} \neq 0\) & \(\vert z \vert > z_{\alpha/2}\)\\
\end{tabular}
\end{center}
\caption{Hypothesis tests, difference in proportions, large sample.}
\label{tab:ztest-two-sample-prop}
\end{table}


\subsubsection{How to do it with \textsf{R}}

The following code conducts the one-sample test for the U.C.~Berkeley data:

<<echo=TRUE>>=
prop.test(1755, n = 4526, p = 0.4, alternative = "less",
           correct = FALSE)
@

\noindent
To make the plot in Figure~\ref{BLANK}, first store the hypothesis test and then \texttt{plot} the resulting \texttt{htest} object:

<<eval = FALSE>>=
temp <- prop.test(1755, n = 4526, p = 0.4, alternative = "less",
           correct = FALSE)
plot(temp)
@

\noindent
The default significance level for plotting \texttt{htest} objects is \(\alpha = 0.05\), but the default can be changed via the \texttt{conf.level} argument to the hypothesis test; set it equal to \(1 - \alpha\).



\begin{rem}[]
In all of the above we set \texttt{correct = FALSE} to tell the computer
that we did not want to use Yates' continuity correction (reference
BLANK).  It is customary to use Yates when the expected frequency of
successes is less than 10. (reference BLANK) You can use it all of the
time, but you will have a decrease in power. For large samples the
correction does not matter.
\end{rem}

For the two-sample problem, consider Titanic survival data broken down by gender (and for the moment ridiculously suspend all disbelief that somehow passengers on the 1912 maiden voyage of the Olympic-class ocean liner RMS Titanic could possibly be a representative sample of any population other than themselves):

<<>>=
addmargins(xtabs(Freq ~ Sex + Survived, data = Titanic), 
           margin=2)
@

If we set \(p_{1}\) as the proportion of \texttt{Male} who died (\texttt{Survived == "No"}) and \(p_{2}\) the proportion of \texttt{Female} who died then we can test whether the proportions are the same this way:

<<>>=
prop.test(x = c(1364, 126), n = c(1731, 470), correct = FALSE)
@

\noindent
We reject the null hypothesis.  See also Section~\ref{BLANK}.

\subsubsection{With the \textsf{R} Commander}

If you already know the number of successes and failures, then you can
use the menu \texttt{Statistics} \(\triangleright\) \texttt{Proportions}
\(\triangleright\) \texttt{IPSUR Enter table for single sample\ldots}

Otherwise, your data---the raw successes and failures---should be in
a column of the Active Data Set. Furthermore, the data must be stored
as a ``factor'' internally. If the data are not a factor but are numeric
then you can use the menu \texttt{Data} \(\triangleright\)
\texttt{Manage variables in active data set}  \(\triangleright\)
\texttt{Convert numeric variables to factors\ldots} to
convert the variable to a factor. Or, you can always
use the \texttt{factor} function.

Once your unsummarized data is a column, then you can use the menu
\texttt{Statistics} \(\triangleright\) \texttt{Proportions} \(\triangleright\)
\texttt{Single-sample proportion test\ldots}





\section{One Sample Tests for Means and Variances} \label{sec:one-sample-tests}

\subsection{For Means}

Here, \(X_{1}\), \(X_{2}\), \ldots, \(X_{n}\) are a \(SRS(n)\) from a
\(\mathsf{norm}(\mathtt{mean} = \mu,\,\mathtt{sd} = \sigma)\)
distribution. We would like to test \(H_{0}:\mu = \mu_{0}\).

\textbf{Case A:}  Suppose \(\sigma\) is known. Then under \(H_{0}\),
\begin{equation}
   Z = \frac{\overline{X} - \mu_{0}}{\sigma/\sqrt{n}} \sim \mathsf{norm}(\mathtt{mean} = 0,\,\mathtt{sd} = 1).
\end{equation}

\begin{table}
\begin{center}
\begin{tabular}{lll}
\(H_{0}\) & \(H_{a}\) & Rejection Region\\
\hline
\(\mu = \mu_{0}\) & \(\mu > \mu_{0}\) & \(z > z_{\alpha}\)\\
\(\mu = \mu_{0}\) & \(\mu < \mu_{0}\) & \(z < -z_{\alpha}\)\\
\(\mu = \mu_{0}\) & \(\mu \neq \mu_{0}\) & \(\vert z \vert > z_{\alpha/2}\)\\
\end{tabular}
\end{center}
\caption{Hypothesis tests, population mean, large sample.}
\label{tab:ztest-one-sample}
\end{table}

\textbf{Case B:} When \(\sigma\) is unknown, under \(H_{0}\),
\begin{equation}
T = \frac{\overline{X} - \mu_{0}}{S/\sqrt{n}} \sim \mathsf{t}(\mathtt{df} = n - 1).
\end{equation}

\begin{table}
\begin{center}
\begin{tabular}{lll}
\(H_{0}\) & \(H_{a}\) & Rejection Region\\
\hline
\(\mu = \mu_{0}\) & \(\mu > \mu_{0}\) & \(t > t_{\alpha}(n - 1)\)\\
\(\mu = \mu_{0}\) & \(\mu < \mu_{0}\) & \(t < -t_{\alpha}(n - 1)\)\\
\(\mu = \mu_{0}\) & \(\mu \neq \mu_{0}\) & \(\vert t \vert > t_{\alpha/2}(n - 1)\)\\
\end{tabular}
\end{center}
\caption{Hypothesis tests, population mean, small sample.}
\label{tab:ttest-one-sample}
\end{table}



\begin{rem}[]
If \(\sigma\) is unknown but \(n\) is large then we can use the
\(z\)-test.
\end{rem}



\begin{example}[Results from an Experiment on Plant Growth]
\label{exm:plant-one-samp-z-test}
We studied the \texttt{PlantGrowth} data in Example~\ref{exm:plant-one-samp-z-int} where we constructed a 95\% confidence interval for \(\mu\), the population mean yield. Recall that the population standard deviation was known to be \(\sigma = 0.7\)\,g, and the data appeared approximately normal.

Suppose now that we would like to know whether the data provide evidence that the true mean yield is different from 4.8\,g at the \(\alpha = 0.05\) significance level.  The null hypothesis would be \(H_{0}:\mu = 4.8\), and the alternative hypothesis would be \(H_{a}:\mu \neq 4.8\).  Given the shape of the distribution and the population standard deviation being known, this is clearly a job for the \(z\)-test.


From Table~\ref{tab:ztest-one-sample} our rejection region (recall that \(z_{\alpha/2} \approx 1.96\) would be \(\vert z \vert > 1.96\), or in other words, we reject \(H_{0}\) if \(z > 1.96\) or if \(z < -1.96\), where our test statistic \(z\) is
\[
z = \frac{\overline{x} - \mu_{0}}{\sigma/\sqrt{n}} = \frac{5.073 - 4.8}{0.7/\sqrt{30}} \approx 2.136. 
\]
We see, then, that our test statistic does indeed fall into the rejection region, so we will reject the null hypothesis at the \(\alpha = 0.05\) significance level and conclude that these data do indeed provide evidence that the true population mean yield is different from 4.8\,g.  

The \(p\)-value for this problem is the probability that a standard normal \(Z\) is more extreme (in either direction) than what we observed (\(z \approx 2.136\)), which would be double the upper tail probability \texttt{1 - pnorm(2.136)}, that is,
<<>>=
2*(1 - pnorm(2.136))
@
Since our \(p\)-value is less than our significance level \(\alpha = 0.05\), all signs are pointing to the same place: we should reject the null hypothesis that \(\mu = 4.8\). A visual display of these ideas is shown in Figure~\ref{fig:ztest-plot}.

<<ztest-plot, echo=FALSE, fig=TRUE, include=FALSE,width=5>>=
plot(z.test(PlantGrowth$weight, mu = 4.8, sd = 0.7))
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-ztest-plot}
\end{center}
\caption[A plot of the results from a \(z\)-test.]{{\small A
plot of the results from two-sided test for a population mean. When \(H_{0}:\,\mu=4.8\) is true, the distribution of our \(Z\) test statistic is \(\mathsf{norm}(\mathtt{mean}=4.8,\,\mathtt{sd}=0.7/\sqrt{30})\), which matches the bell curve shown above.  The critical region is marked by the shaded blue area, and the observed value of sample mean \(\overline{x} = 5.073\) is denoted by a dashed vertical green line.  That value of \(\overline{x}\) translates to a \(z\)-score of 2.136, shown in the lower horizontal axes, along with the critical value \(z_{\alpha/2} \approx 1.96\). The \(p\)-value \(p\approx0.0327\) is marked by the area enclosed by the green box, and we see it is less than the significance level \(\alpha = 0.05\). }}
\label{fig:ztest-plot}
\end{figure}

\begin{rem}
\label{rem:ci-hyp-brothers}
But wait; there is more.  In Example~\ref{exm:plant-one-samp-z-int} we calculated a 95\% two-sided confidence interval for \(\mu\), and we were 95\% confident that the true mean was covered by the interval \([4.82,\,5.33]\), which we notice does \textbf{not} cover the null hypothesized value \(\mu_{0} = 4.8\).  So here is yet another sign that the two-sided hypothesis test will reject \(H_{0}:\mu=4.8\) at significance level \(\alpha = 1 - 0.95 = 0.05\).  There is a deep connection, a duality, between confidence intervals and hypothesis tests.
\end{rem}

\end{example}

The quantity \(\sigma/\sqrt{n}\), when \(\sigma\) is known, is called
the \textit{standard error of the sample mean}. In general, if we have
an estimator \(\hat{\theta}\) then \(\sigma_{\hat{\theta}}\) is called
the \textit{standard error} of \(\hat{\theta}\). We usually need to
estimate \(\sigma_{\hat{\theta}}\) with
\(\widehat{\sigma_{\hat{\theta}}}\).



\begin{example}[Michelson Speed of Light Data]
\label{exm:morley-one-samp-t-test}
Recall Example~\ref{exm:morley-one-samp-t-int} where we computed a 99\% two-sided \(t\)-interval for \(\mu\), the unknown true mean speed of light based on Michelson's measurements in the \texttt{morley} data frame.  We focused on the 20 observations in the first experiment, which is a small sample, but we checked normality with a \texttt{qqPlot} and were satisfied.

Now, suppose that Michelson asserts that the true mean speed of light is greater than 299,845\,km/sec\footnote{Modern estimates of the speed of light in a vacuum are around 299,742\,kps, but these early Michelson data were collected over 140yrs ago.}.  Do the data from the first 20 runs support Michelson's claim at the \(\alpha = 0.01\) significance level?  Our null hypothesis will be \(H_{0}:\mu=845\), the alternative will be \(H_{0}:\mu > 845\), and we will be using Student's \(t\)-test.  Table~\ref{tab:ttest-one-sample} tells us that we should reject \(H_{0}\) if our test statistic \(t\) is greater than \(t_{0.01}(\mathtt{df} = n - 1)\), which is \texttt{qt(0.99, df = 19)} \(\approx 2.54\).  It remains only to calculate the observed value of our test statistic, which is
\[
t = \frac{\overline{x} - \mu_{0}}{s/\sqrt{n}} = \frac{909 - 845}{104.926/\sqrt{20}} \approx 2.727796. 
\]
This test statistic falls in the rejection region, so Michelson rejects the null hypothesis \(H_{0}:\mu = 845\).  He infers that the data from the first 20 runs provide statistically significant evidence at the \(\alpha = 0.01\) level to conclude that the true mean speed of light is greater than 299,845\,km/sec.

The \(p\)-value for this test would be the probability that a \(T \sim \mathsf{t}(\mathtt{df} = 19)\) variable is larger than 2.727796, which is
<<>>=
1 - pt(2.727796, df = 19)
@
Here, we see that the \(p\)-value is less than \(\alpha = 0.01\), further confirming that Michelson's conclusion should be to reject the null hypothesis.
\end{example}


\begin{rem}
Hold the phone. In Remark~\ref{rem:ci-hyp-brothers} we said that there was a duality between confidence intervals and hypothesis tests, and we pointed out that in Example~\ref{BLANK} the two-sided 95\% confidence interval we computed, \([4.82,\,5.33]\), for the \(\mu\) in that problem didn't cover the null hypothesized value \(\mu_{0} = 4.8\), so it stood to reason when the hypothesis test rejected \(H_{0}:\mu = 4.8\) at the \(\alpha = 0.05\) significance level.

Yet, in Example~\ref{BLANK} we have computed a two-sided 99\% confidence interval for Michelson's \(\mu\), and the interval turns out to be \([841,\,977]\).  This interval covers \(\mu_{0}=845\). How is it that Michelson rejects \(H_{0}:\mu = 845\) at the \(\alpha = 0.01\) significance level?

The seeming inconsistency is due to the mixing and matching of one- and two-sided confidence intervals with hypothesis tests.  In Examples~\ref{BLANK} and \ref{BLANK}, both interval and test were two-sided, and the inferences matched.  In Examples~\ref{BLANK} and \ref{BLANK}, the interval was two-sided, but the test was one-sided, and the inferences did not match.

In general, a two-sided test rejects when the two-sided interval doesn't cover \(\mu_{0}\), and the same holds true for one-sided tests/intervals.  If you mix and match then the correspondence is not guaranteed.  This is explored more in Exercise~\ref{exr:test-int-brother}.
\end{rem}


\subsubsection{How to do it with \textsf{R}}

We will use the \texttt{z.test} \index{z.test@\texttt{z.test}} function in the 
\texttt{TeachingDemos} package.

<<echo=TRUE>>=
library(TeachingDemos)
z.test(PlantGrowth$weight, mu = 4.8, sd = 0.7, 
       alternative = "two.sided")
@

\noindent
Compare the output here to what we computed by hand in Example~\ref{exm:plant-one-samp-z-test}.  Just as with \texttt{prop.test}, we can make the plot in Figure~\ref{fig:ztest-plot} by saving the results of the \texttt{z.test} and \texttt{plot}-ting it.

For Michelson's \texttt{t.test} \index{t.test@\texttt{t.test}} we can do
<<echo=TRUE>>=
x <- with(morley, Speed[Expt == 1])
t.test(x, mu = 845, conf.level = 0.99, alternative = "greater")
@
Notice that the one-sided 99\% confidence interval extends \([849,\,\infty)\); this interval does not cover \(\mu_{0} = 845\), so in Remark~\ref{BLANK} there is no need to hold the phone after all.

\subsubsection{With the \textsf{R} Commander}

Your data should be in a single numeric column (a variable) of the
Active Data Set. Use the menu \texttt{Statistics} \(\triangleright\) \texttt{Means}
\(\triangleright\) \texttt{Single-sample t-test\ldots}

\subsection{For Variance}

Here, \(X_{1}\), \(X_{2}\), \ldots, \(X_{n}\) are a \(SRS(n)\) from a
\(\mathsf{norm}(\mathtt{mean} = \mu,\,\mathtt{sd} = \sigma)\)
distribution. We would like to test \(H_{0}:\,\sigma^{2} =
\sigma^{2}_{0}\). We know that under \(H_{0}\), \[ X^{2} = \frac{(n -
1)S^{2}}{\sigma^{2}} \sim \mathsf{chisq}(\mathtt{df} = n - 1).  \]

Fair warning to the reader that this test is known to be sensitive to departures from normality.

\begin{table}
\begin{center}
\begin{tabular}{lll}
\(H_{0}\) & \(H_{a}\) & Rejection Region\\
\hline
\(\sigma^{2} = \sigma^{2}_{0}\) & \(\sigma^{2} > \sigma^{2}_{0}\) & \(X^{2} > \chi^{2}_{\alpha}(n - 1)\)\\
\(\sigma^{2} = \sigma^{2}_{0}\) & \(\sigma^{2} < \sigma^{2}_{0}\) & \(X^{2} < \chi^{2}_{1 - \alpha}(n - 1)\)\\
\(\sigma^{2} = \sigma^{2}_{0}\) & \(\sigma^{2} \neq \sigma^{2}_{0}\) & \(X^{2} > \chi^{2}_{\alpha/2}(n - 1)\) or \(X^{2} < \chi^{2}_{1 - \alpha/2}(n - 1)\)\\
\end{tabular}
\end{center}
\caption{Hypothesis tests, population variance, normal population.}
\label{tab:sigmatest-one-sample}
\end{table}


\subsubsection{How to do it with \textsf{R}}

The \texttt{sigma.test} \index{sigma.test@\texttt{sigma.test}} function in the
\texttt{TeachingDemos} package \cite{TeachingDemos} implements this test.  We used it  to construct a confidence interval for \(\sigma^{2}\) in Section~\ref{sub:sigma-how-R}, and the hypothesis test functionality extends in the obvious way.  The user may either specify the null hypothesized value \(\mathtt{sigma} = \sigma_{0}\)  or  \(\mathtt{sigmasq} = \sigma^{2}_{0}\).  For instance, in the \texttt{morley} data we could test the null hypothesis \(H_{0}:\sigma = 79\), equivalently \(H_{0}:\sigma^2 = 79^2\), against a two-sided alternative with:

<<eval=FALSE>>=
sigma.test(morley$Speed, sigma = 79)
@

\noindent
Note that confidence intervals returned by \texttt{sigma.test} are invariably for \(\sigma^{2}\), not \(\sigma\).

\section{Two-Sample Tests for Means and Variances} 
\label{sec:two-sample-tests-for-means}

\subsection{Independent Samples}
\label{sub:two-samp-test-indep}

The basic idea for this section is the following. We have
\(X\sim\mathsf{norm}(\mathtt{mean} = \mu_{X},\,\mathtt{sd} =
\sigma_{X})\) and \(Y\sim\mathsf{norm}(\mathtt{mean} =
\mu_{Y},\,\mathtt{sd} = \sigma_{Y})\) distributed independently. We
would like to know whether \(X\) and \(Y\) come from the same
population distribution, in other words, we would like to know:
\begin{equation}
\mbox{Does }X\overset{\mathrm{d}}{=}Y?
\end{equation}
where the symbol \(\overset{\mathrm{d}}{=}\) means equality of
probability distributions.  Since both \(X\) and \(Y\) are normal, we
may rephrase the question:
\begin{equation}
\mbox{Does }\mu_{X} = \mu_{Y}\mbox{ and }\sigma_{X} = \sigma_{Y}?
\end{equation}
Suppose first that we do not know the values of \(\sigma_{X}\) and
\(\sigma_{Y}\), but we know that they are equal,
\(\sigma_{X}=\sigma_{Y}\). Our test would then simplify to
\(H_{0}:\,\mu_{X} = \mu_{Y}\). We collect data \(X_{1}\), \(X_{2}\),
\ldots, \(X_{n}\) and \(Y_{1}\), \(Y_{2}\), \ldots, \(Y_{m}\), both simple
random samples of size \(n\) and \(m\) from their respective normal
distributions. Then under \(H_{0}\) (that is, assuming \(H_{0}\) is
true) we have \(\mu_{X} = \mu_{Y}\), or rewriting, \(\mu_{X} - \mu_{Y}
= 0\), so
\begin{equation}
T = \frac{\overline{X} - \overline{Y}}{S_{p}\sqrt{\frac{1}{n} + \frac{1}{m}}} = \frac{\overline{X} - \overline{Y} - (\mu_{X} - \mu_{Y})}{S_{p}\sqrt{\frac{1}{n} + \frac{1}{m}}}\sim\mathsf{t}(\mathtt{df} = n + m - 2).
\end{equation}
A test based on this statistic is the ``Pooled \(t\)-test''.

In the much more common scenario where the variances are not known to be equal, we follow Section~\ref{sec:conf-interv-for-diff-means} and use the Welch adjustment to the degrees of freedom for the distribution of the statistic
\begin{equation}
T = \frac{\overline{X}-\overline{Y}}{\sqrt{\frac{S_{X}^{2}}{n}+\frac{S_{Y}^{2}}{m}}} \sim \mathsf{t}(\mathtt{df}=r),
\end{equation}
where the degrees of freedom \(r\) is chosen so that the statistic has nice properties. See Equation \eqref{eq:welch-df} for the formula. A test based on this statistic is called the ``Welch Two Sample \(t\)-test''.  The Welch test is the default.


\begin{rem}[]
If the values of \(\sigma_{X}\) and \(\sigma_{Y}\) are known, then we
can plug them in to our statistic:
\begin{equation}
Z = \frac{\overline{X} - \overline{Y}}{\sqrt{\sigma_{X}^{2}/n + \sigma_{Y}^{2}/m}};
\end{equation}
the result will have a \(\mathsf{norm}(\mathtt{mean} = 0,\,\mathtt{sd}
= 1)\) distribution when \(H_{0}:\,\mu_{X} = \mu_{Y}\) is true.
\end{rem}



\begin{rem}[]
Even if the values of \(\sigma_{X}\) and \(\sigma_{Y}\) are not known, if both \(n\) and \(m\) are large then we can plug in the sample estimates and the result will have approximately a
\(\mathsf{norm}(\mathtt{mean} = 0,\,\mathtt{sd} = 1)\) distribution when \(H_{0}:\,\mu_{X} = \mu_{Y}\) is true.
\begin{equation}
Z = \frac{\overline{X} - \overline{Y}}{\sqrt{S_{X}^{2}/n + S_{Y}^{2}/m}}.
\end{equation}
\end{rem}



\begin{rem}[]
It is often helpful to construct side-by-side boxplots and other
visual displays in concert with the hypothesis test. This gives a
visual comparison of the samples and helps to identify departures from the test's assumptions---such as outliers.
\end{rem}



\begin{rem}[]
WATCH YOUR ASSUMPTIONS.

\begin{itemize}
\item The normality assumption can be relaxed as long as the population distributions are not highly skewed.
\item The equal variance assumption can be relaxed as long as both sample sizes \(n\) and \(m\) are large. However, if one (or both) samples is small, then the test does not perform well; we should instead use the methods of Chapter~\ref{cha:resampling-methods}.
\end{itemize}
\end{rem}

For a nonparametric alternative to the two-sample \(t\)-test see
Chapter~\ref{cha:nonparametric-statistics}.

\begin{table}
\begin{center}
\begin{tabular}{lll}
\(H_{0}\) & \(H_{a}\) & Rejection Region\\
\hline
\(\mu_{D} = D_{0}\) & \(\mu_{D} > D_{0}\) & \(t > t_{\alpha}(n + m - 2)\)\\
\(\mu_{D} = D_{0}\) & \(\mu_{D} < D_{0}\) & \(t < -t_{\alpha}(n + m -  2)\)\\
\(\mu_{D} = D_{0}\) & \(\mu_{D} \neq D_{0}\) & \(\vert t \vert > t_{\alpha/2}(n + m - 2)\)\\
\end{tabular}
\end{center}
\caption{Rejection regions, difference in means, Pooled \(t\)-test.}
\label{tab:two-t-test-pooled}
\end{table}


For the two-sample variance problem, we rely on the work we already did with confidence intervals in Section~\ref{sec:confidence-intervals-for-variances}.  There we saw that
\begin{equation}
F=\frac{\sigma_{Y}^{2}S_{X}^{2}}{\sigma_{X}^{2}S_{Y}^{2}}\sim \mathsf{f}(\mathtt{df1}=n-1,\,\mathtt{df2}=m-1).
\end{equation}

\noindent
Then when \(H_{0}:\sigma_{X}^{2}=\sigma_{Y}^2\) is true, the statistic
\begin{equation}
F=\frac{S_{X}^{2}}{S_{Y}^{2}}\sim \mathsf{f}(\mathtt{df1}=n-1,\,\mathtt{df2}=m-1).
\end{equation}
which leads to Table~\ref{tab:two-f-test}.

\begin{table}
\begin{center}
\begin{tabular}{lll}
\(H_{0}\) & \(H_{a}\) & Rejection Region\\
\hline
\(\sigma^{2}_{X} = \sigma^{2}_{Y}\) & \(\sigma^{2}_{X} > \sigma^{2}_{Y}\) & \(f > \mathsf{f}_{\alpha}(n-1,m-1)\)\\
\(\sigma^{2}_{X} = \sigma^{2}_{Y}\) & \(\sigma^{2}_{X} < \sigma^{2}_{Y}\) & \(f < \mathsf{f}_{1 - \alpha}(n-1,m-1)\)\\
\(\sigma^{2}_{X} = \sigma^{2}_{Y}\) & \(\sigma^{2}_{X} \neq \sigma^{2}_{Y}\) & \(f < \mathsf{f}_{1 - \alpha/2}(n-1,m-1)\ \text{or } f > \mathsf{f}_{\alpha/2}(n-1, m-1)\)\\
\end{tabular}
\end{center}
\caption{Rejection regions, ratio of variances, \(F\)-test.}
\label{tab:two-f-test}
\end{table}


\subsubsection{How to do it with \textsf{R}}

Refer to Section~\ref{sec:conf-interv-for-diff-means} where we used \texttt{t.test} to construct confidence intervals for the difference in means.  Each of those intervals extends to a one- or two-sided hypothesis test in the straightforward way.  Most of the time we are interested in whether the means are the same (\(\mu_{X}=\mu_{Y}\)), but on the rare occasion that we need to test whether the difference in means is some constant \(D_{0} \neq 0\), we can specify so via the \(\mathtt{mu} = D_{0}\) argument to \texttt{t.test}.  The Welch-Aspin test (no equal variance assumption) is performed by default, but we can call the Pooled \(t\)-test with \(\mathtt{var.equal} = \mathtt{TRUE}\) if we like. 

The two-sample variances test is done with \texttt{var.test}.  See Section~\ref{sec:conf-interv-for-diff-means} for an example with the \texttt{ToothGrowth} data.

\subsection{Paired Samples}

Often times we have repeated measurements on a single experimental unit.  For instance, maybe we would like to decide between two golf clubs which is better, so we recruit a bunch of buddies to go to the golf course with us, to collect data on golf ball driving distances for each club.  It would be silly to have each buddy hit a single golf ball and go home---we would need twice as many buddies!  No, we should have each person hit a ball apiece with both clubs.  Indeed, we should ask our friends to hit a whole bunch of golf balls with each club, giving us as much  data as we can get, but suppose for the sake of argument that our buddies are in a hurry and only have time to hit one ball with each club apiece.

We will observe \(X_{1}\), \(X_{2}\), \ldots, \(X_{n}\) and \(Y_{1}\), \(Y_{2}\), \ldots, \(Y_{n}\), driving distances, and suppose that we conduct our experiment in such a way that our golfer buddies are independent of one another.  But there is a problem, because even if the \(X\)'s are independent of each other and the \(Y\)'s are all independent, the pair \((X_{1},\,Y_{1}\) are \textbf{not} independent. If I tell you that Tiger Woods hit \(X_{1}\) then it will give you a lot of information about \(Y_{1}\), and similarly about \(Y_{2}\) if I tell you that \(X_{2}\) was hit by somebody who has never golfed before (me).

But all is not lost.  Let us compute the difference of each pair:  \(D_{1} = X_{1} - Y_{1}\), \(D_{2} = X_{2} - Y_{2}\), and so on, up to \(D_{n} = X_{n} - Y_{n}\). Then the differences \(D_{1}, \ldots,D_{n}\) are a simple random sample, with mean
\begin{equation}
\mu_{D} = \mathbb{E}\,(D) = \mathbb{E}(X - Y) = \mathbb{E}\,X - \mathbb{E}\,Y = \mu_{X} - \mu_{Y},  
\end{equation}
and we can estimate, construct confidence intervals, and perform hypothesis tests on the parameter \(\mu_D\) via single-sample procedures on the \(D\)'s, subject to the same assumptions and limitations. 

A popular procedure to use is the ``Paired \(t\)-test'' which is appropriate for large samples or small samples when the population of differences is normally distributed.  See Table~\ref{tab:two-t-test-paired} for how the hypotheses and rejection regions are defined.

\begin{table}
\begin{center}
\begin{tabular}{lll}
\(H_{0}\) & \(H_{a}\) & Rejection Region\\
\hline
\(\mu_{D} = D_{0}\) & \(\mu_{D} > D_{0}\) & \(t > t_{\alpha}(n_{D} - 1)\)\\
\(\mu_{D} = D_{0}\) & \(\mu_{D} < D_{0}\) & \(t < -t_{\alpha}(n_{D} - 2)\)\\
\(\mu_{D} = D_{0}\) & \(\mu_{D} \neq D_{0}\) & \(\vert t \vert > t_{\alpha/2}(n_{D} - 1)\)\\
\end{tabular}
\end{center}
\caption{Rejection regions, difference in means, Paired \(t\)-test.}
\label{tab:two-t-test-paired}
\end{table}

\begin{example}[Student's sleep data]
 The \texttt{sleep} data measure the effect of two soporific (sleep-inducing) drugs by the increase in hours of sleep compared to control on 10 patients. The extra sleep measured while taking the first drug we will denoted by \texttt{extra.1} and the extra sleep measured while on the second drug will be denoted \texttt{extra.2}. Supposing that the 10 patients constitute a simple random sample, we would like to know if there is any difference between the two soporific drugs with respect to mean extra sleep obtained. 
\end{example}


\subsubsection{How to do it with \textsf{R}}

How the Paired \(t\)-test is done depends on how the data are formatted, whether in ``long'' or ``wide'' format.  The \texttt{sleep} data frame from the included \texttt{datasets} package are already in long format, that is, like this:

<<>>=
head(sleep)
@

\noindent
The \texttt{extra} variable (length 20) records the extra sleep for each patient, the \texttt{group} variable denotes which soporific drug was used (1 or 2), and the \texttt{ID} variable identifies the patient being measured (from 1 to 10).  When data are in this format the Paired \(t\)-test looks like this:

<<echo=TRUE>>=
t.test(extra ~ group, data = sleep, paired = TRUE)
@

\noindent
Here we would reject the null hypothesis that the mean extra sleep for the two soporific drugs is the same.  However, often times paired data are \textit{not} in long format by default, rather they are in \texttt{wide} format with the first measurement in one column and the second measurement in another column alongside the first one.  The help file for \texttt{t.test} shows how to convert the default \texttt{sleep} data frame into wide format:

<<>>=
sleep2 <- reshape(sleep, direction = "wide", 
                  idvar = "ID", timevar = "group")
head(sleep2)
@

\noindent
The \texttt{sleep2} data frame has 10 rows, \texttt{ID} denoting the patient, \texttt{extra.1} the extra sleep for the first drug, \texttt{extra.2} for the second drug.  Now the syntax for Paired \(t\)-test is a bit different:

<<>>=
t.test(Pair(extra.1, extra.2) ~ 1, data = sleep2)
@

\noindent
Aside from the \texttt{data:} line the output is exactly the same.  But wait, we have gotten ahead of ourselves, because we failed to check our assumptions, namely, that this small sample (\(n = 10\)) of differences is normally distributed.  The wide format of the data is more convenient to check this with a \texttt{qqPlot} (not shown, see Exercise~\ref{BLANK}).


\section{Other Hypothesis Tests} \label{sec:other-hypothesis-tests}

We have yet discussed only a small sample\footnote{No pun intended.  OK, maybe moderately intended, \(p = 0.07\).} of hypothesis tests available to the statistician.  We will see several more before we are through, but in the meantime, here are a couple more.


\subsection{Shapiro-Wilk Normality Test} \label{sub:shapiro-wilk-test}

Most of the small-sample procedures we have studied thus far require that the target population be normally distributed.  In later chapters we will be assuming that errors from a proposed model are normally distributed, and one method we will use to assess the model's adequacy will be to investigate the plausibility of that assumption. So, you see, determining whether or not a random sample comes from a normal distribution is an important problem for us.  We have already learned graphical methods to address the question (histograms, \texttt{qqPlot}s), but here we will study a formal hypothesis test of the same.  We will be given a simple random sample \(X_{1},X_{2},\ldots,X_{n},\), considered observations of a random variable \(X\), and our null hypothesis will be
\begin{equation}
H_{0}:\,X \sim \mathsf{norm}(\mathtt{mean} = \mu,\ \mathtt{sd}=\sigma),
\end{equation}
where \(\mu\) and \(\sigma\) are not specified, and the alternative hypothesis is simply 
\(H_{a}:\,\text{not }H_{0}\).


We do not specify the parameters \(\mu\) and \(\sigma\) because at the moment we do not really care about the center or spread of the distribution, instead, we care about its \textit{shape}. A hypothesis test like this, that is not focused on the value of this or that parameter, is a member of large class of \textit{nonparametric} methods, the subject of Chapter~\ref{BLANK}.

The test statistic we will use is Wilk's \(W\), which looks like this:
\begin{equation}
W = \frac{\left(\sum_{i = 1}^{n} a_{i}X_{(i)} \right)^{2}}{\sum_{i = 1}^{n}(X_{i} - \overline{X})^{2}},
\end{equation}
where the \(x_{(i)}\)'s are the order statistics (see Section BLANK) and the constants \(a_{i}\) form the components of a vector
\(\mathbf{a}_{1\times\mathrm{n}}\) defined by
\begin{equation}
\mathbf{a}=\frac{\mathbf{m}^{\mathrm{T}}\mathbf{V}^{-1}}{\sqrt{\mathbf{m}^{\mathrm{T}}\mathbf{V}^{-1}\mathbf{V}^{-1}\mathbf{m}}},
\end{equation}
where \(\mathbf{m}_{\mathrm{n}\times1}\) and \(\mathbf{V}_{\mathrm{n}
\times \mathrm{n}}\) are the mean and covariance matrix,
respectively, of the order statistics from an \(\mathsf{mvnorm}
\left(\mathtt{mean} = \mathbf{0},\,\mathtt{sigma} =
\mathbf{I}\right)\) distribution.  This test statistic \(W\) was
published in 1965 by (you guessed it): Shapiro and
Wilk \cite{Wilk1965}.  In contrast to most other test statistics we
know, we reject \(H_{0}\) if \(W\) is too \textit{small}.

\subsubsection{How to do it with \textsf{R}}

The \texttt{morley} data set has a \texttt{Speed} variable containing Michelson's 100 measurements of the speed of light.  We check normality of the data by way of the \texttt{shapiro.test} function.

<<echo=TRUE>>=
with(morley, shapiro.test(Speed))
@

In this case we do not reject the null hypothesis that the \texttt{Speed} data are normally distributed.  However, it is important to note that we have not \textbf{proved} that the data are normal, nor could we ever possibly prove such a thing via the Shapiro-Wilk test.  Indeed, it is here that our carefully worded hypothesis testing language comes into play.  We make exactly one of two (2) decisions: we either \textbf{reject} the null hypothesis, or we \textbf{fail to reject}.  In this example, we failed to reject, and an appropriate interpretation would be, ``These data are consistent with a simple random sample from a normal distribution,'' or some other wishy-washy remark.  

On the other hand, had the Shapiro-Wilk test come back highly significant, say, \(p = 0.0001\), we would \textbf{reject} the null hypothesis and forcefully declare from the rooftop, ``These data provide highly significant evidence that the population distribution of \(X\) is \textit{not} normal,'' and go on about our business.


\subsection{Kolmogorov-Smirnov Goodness-of-Fit Test} \label{sub:kolmogorov-smirnov-goodness-of-fit-test}

Considering what led us to the Shapiro-Wilk test of normality, we may find ourselves wanting to know whether a given random sample of data follow any one of a cornucopia of possible population distributions, not necessarily normal.  The Kolmogorov-Smirnov (KS) test permits testing the fit of a specified candidate population distribution, which we assume to be continuous\footnote{Only as far as \texttt{base} \textsf{R} is concerned.  For discrete distributions or mixed discrete/continuous distributions see the \texttt{KSgeneral} package.}.  Alternatively, the KS test can be used in two-sample problems to test the null hypothesis that \(X\) sample data and \(Y\) sample data come from the same (continuous) distribution, regardless of what that population distribution may be.  Consequently, the KS test is another nonparametric test; see Chapter~\ref{BLANK}.

\subsubsection{How to do it with \textsf{R}}

<<echo=TRUE>>=
with(randu, ks.test(x, "punif"))
@


\section{Analysis of Variance} \label{sec:analysis-of-variance}

Given samples \(X_{1},\ldots,X_{n_{1}} \sim \mathsf{norm}(\mu_{1},\sigma)\) and \(Y_{1},\ldots,Y_{n_{2}}\sim\mathsf{norm}(\mu_{2},\sigma)\), if we wish to test the null hypothesis \(H_{0}:\mu_{1} = \mu_{2}\) we can use the (Pooled) two-sample \(t\)-test in Section~\ref{sub:two-samp-test-indep}.

But now we have \(k \geq 3\) independent normal samples with unknown means \(\mu_{1},\ \mu_{2},\ldots,\mu_{k}\) and common variance \(\sigma^2\).  We would like to test the null hypothesis
\begin{equation}
H_{0}:\,\mu_{1}=\mu_{2}=\cdots=\mu_{k}
\end{equation}
against all possible alternatives.  We will write the observed data like this:
\[
\left\{X_{11},\ldots,X_{n_{1}1}\right\},\  \left\{X_{12},\ldots,X_{n_{2}2}\right\},\ldots, \  \left\{X_{1k},\ldots,X_{n_{k}k}\right\},
\]
and we will write our model in this way:
\[
X_{ij} = \mu + \tau_{j} + \epsilon_{ij}, \quad j = 1,\ldots,k, \quad i = 1,\ldots,n_{j},
\]
where
\begin{itemize}
\item \(N = \sum_{j=1}^k n_{j}\) is the total sample size,
\item \(\mu\) is the grand mean,
\item \(\tau_{j}\) is the effect of treatment \(j\), for \(j=1,\ldots,k\), 
\item \(\sum_{j=1}^k \tau_{j} = 0\),
\item \(\epsilon_{ij}\)'s are a \(SRS(N)\) from a \(\mathsf{norm}(\mathtt{mean} = 0, \mathtt{sd} = \sigma)\) distribution.
\end{itemize}

\begin{example}[Chicken weights by feed type]
The \texttt{chickwts} dataset records the weights of 71 chicks according to the feed they were given, such as sunflower seeds, horsebean, meatmeal, \textit{etc}. (There were six different feeds used in the study.) We displayed side-by-side boxplots of these data in Figure~\ref{BLANK}.  Now, we are interested to know whether the feed types differ in terms of a chick's mean weight.  There are six feeds; do all feeds lead to the same average weight?  Maybe one or more feeds have an average weight above or below the others.

In symbols, we are looking to test the null hypothesis
\begin{equation}
H_{0}:\,\mu_{1}=\mu_{2}=\cdots=\mu_{6},
\end{equation} 
but making the connection to our model we write \(\mu_{1} = \mu + \tau_{1}\), \(\mu_{2} = \mu + \tau_{2}\), and so on, up to \(\mu_{6} = \mu + \tau_{6}\), so that the hypothesis BLANK corresponds to the equivalent hypothesis
\begin{equation}
H_{0}:\,\tau_{1}=\tau_{2}=\cdots=\tau_{6} = 0.
\end{equation} 

\end{example}

\textbf{Idea:}  If the groups have means that are far apart, the data will look like the top graph in Figure~\ref{BLANK}.  But if the groups have means that are close together, then the data will look like the bottom graph in Figure~\ref{BLANK}.

<<between-versus-within, echo=FALSE, fig=TRUE, include=FALSE,width=4, height=6>>=
par(mfrow = c(2,1))
y1 <- rnorm(300, mean = c(2,8,22))
plot(y1, xlim = c(-1,25), ylim = c(0,0.45) , type = "n", ylab="density", xlab="response")
f <- function(x){dnorm(x, mean = 2)}
curve(f, from = -1, to = 5, add = TRUE, lwd = 2, col = "green")
f <- function(x){dnorm(x, mean = 8)}
curve(f, from = 5, to = 11, add = TRUE, lwd = 2, col = "blue")
f <- function(x){dnorm(x, mean = 22)}
curve(f, from = 19, to = 25, add = TRUE, lwd = 2, col = "red")
rug(y1[3*(0:99) + 1], col = "green")
rug(y1[3*(0:99) + 2], col = "blue")
rug(y1[3*(0:99) + 3], col = "red")
y1 <- rnorm(300, mean = c(7,8,9))
plot(y1, xlim = c(0,16), ylim = c(0,0.45) , type = "n", ylab="density", xlab="response")
f <- function(x){dnorm(x, mean = 7)}
curve(f, from = 4, to = 10, add = TRUE, lwd = 2, col = "green")
f <- function(x){dnorm(x, mean = 8)}
curve(f, from = 5, to = 11, add = TRUE, lwd = 2, col = "blue")
f <- function(x){dnorm(x, mean = 9)}
curve(f, from = 6, to = 12, add = TRUE, lwd = 2, col = "red")
rug(y1[3*(0:99) + 1], col = "green")
rug(y1[3*(0:99) + 2], col = "blue")
rug(y1[3*(0:99) + 3], col = "red")
par(mfrow = c(1,1))
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-between-versus-within}
\end{center}
\caption[A plot of between group versus within group variation.]{{\small A plot of between group versus within group variation.  Both graphs represent a simulated \texttt{response} variable from three groups.  In the top graph, the group means are far apart, so the \textit{between} group variation is large relative to the within group variation.  In the bottom graph, the group means are relatively close to each other, so the \textit{within} group variation is large relative to the between group variation. It was the genius of Sir R.A.~Fisher which realized that to detect a difference in means, what we should be studying is the variation between/within groups, hence the term \textit{analysis of variance}.}}
\label{fig:between-versus-within}
\end{figure}


\subsubsection*{Important Quantities}

We need some notation.  Denote the sample mean of the \(j^{\text{th}}\) group by 
\[
\overline{X}_{\cdot j} = \frac{1}{n_{j}} \sum_{i = 1}^{n_{j}} X_{ij}
\]
for \(j=1,2,\ldots,k,\) and the sample variance of the \(j^{\text{th}}\) group by 
\[
S_{\cdot j}^{2} = \frac{1}{n_{j} - 1} \sum_{i = 1}^{n_{j}} (X_{ij} - \overline{X}_{\cdot j})^{2}
\]
for \(j=1,2,\ldots,k\), and the (sample) grand mean by 
\[
\overline{X}_{\cdot \cdot} = \frac{1}{N} \sum_{j = 1}^{k}\sum_{i = 1}^{n_{j}} X_{ij}.
\]

Now we will define \(SST\), the \textbf{Treatment Sum of Squares}, by
\begin{equation}
SST = \sum_{j = 1}^{k} n_{j}(\overline{X}_{\cdot j} - \overline{X}_{\cdot \cdot})^{2},
\end{equation}
which we will use to measure variation in treatment means, or \textit{between group} variability.  We will next define \(SSE\), the \textbf{Error Sum of Squares}, by
\begin{equation}
SSE = \sum_{j = 1}^{k} (n_{j} - 1)S_{j}^{2},
\end{equation}
which we will use to measure variation around treatment means, or \textit{within group} variability.  

Next, we will need the Mean Square for Treatment \(MST\) and the Mean Square for Error \(MSE\) defined respectively by
\[MST = \frac{SST}{k - 1}, \quad MSE = \frac{SSE}{N - k},\]
and finally the \(F\) ratio \[ F = \frac{MST}{MSE}.\]


If the population means were different, we would expect the sample means to be far apart, so the between group \(MST\) would be BIG, and the within group \(MSE\) would be {\footnotesize small}, so the \(F\) ratio would be 
\[F = \frac{MST}{MSE} \rightarrow \text{BIG}.\]
Now, it turns out that when the null hypothesis is true, that is, when \(\tau_{1}=\cdots=\tau_{k}=0\), we can prove that
\begin{equation}
F \sim \mathsf{f}(\mathtt{df1} = k - 1,\ \mathtt{df2} = N - k).
\end{equation}
Our ANOVA procedure, then, will be to collect data in accordance with the ANOVA assumptions, calculate the test statistic \(F\), and reject \(H_{0}\) when \(F\) is too BIG, that is, larger than \(\mathsf{f}_{\alpha}(k-1,\,N-k)\).

\subsubsection{Checking ANOVA assumptions}

We can distill the underlying assumptions of our ANOVA model into three conditions: (1) each group should be an independent, random sample, which (2) is normally distributed, with (3) the same (homogeneous) variance in all groups. 

We usually address whether or not (1) is satisfied in a given problem by inspecting the study design.  

We can check (2) visually with multiple qq-plots or histograms, one for each group, and we can formally test normality by groups with \texttt{shapiro.test}, see Exercise~\ref{BLANK}. But be warned that in a given problem there may be too many groups and too few observations per group to judge normality in each group separately with any power.  It is often better to wait until after fitting the model to examine the residuals all at the same time which, if the model assumptions hold, should all come from the same normal distribution---see below.  
To assess (3), Brown and Forsythe proposed a statistic to formally test homogeneity of variance;  see reference BLANK.  The idea is to calculate the absolute deviations of observations from their group medians, and then perform an ANOVA on the results; if the ANOVA rejects then it is evidence against homogeneous variance\footnote{Given that we are still in the middle of the ANOVA section the reader may find such reasoning a bit circular.  Agreed.}  We can assess the assumption visually with side-by-side boxplots of the absolute deviations which we will see in the next section. 

As a final comment to the reader about checking assumptions, note that the ANOVA process has been studied by statisticians for decades, under all manner of departures from the ANOVA assumptions, and the general sense is that ANOVA is quite robust to both nonnormality and heterogeneity of variance.  And in a nightmarish scenario where even ANOVA breaks down, we have a nonparametric alternative; see Section~\ref{BLANK}. 


\begin{example}[Chicken weights by feed type, checking assumptions]
We saw side-by-side boxplots of the \texttt{chickwts}  data in Figure~\ref{BLANK}.  Judging from the plot it looks like the mean of \texttt{horsebean} is less than the rest, but it is hard to tell what is going on with the other means.  In Section~\ref{BLANK}, we saw how to compute means, standard deviations, and cell counts for each group; see Example~\ref{BLANK}.  Formal hypothesis tests of normality can be done with \texttt{shapiro.test} and the \texttt{by} function; see Exercise~\ref{BLANK}.  The tests show that every cell passes its respective normality test at significance level \(\alpha=0.05\). 

Checking for homogeneity of variance is done via the \texttt{hovPlot} (graph) and \texttt{hov} (test) functions, both from the \texttt{HH} package.  See Exercise~\ref{BLANK}. We do not detect any problems\footnote{But even if we had found unequal variance, we could do a Welch correction to the denominator degrees of freedom; see \texttt{?oneway.test}.}. Putting it all together, the \texttt{chickwts} data seem to be well-suited for a standard analysis of variance.
\end{example}


\subsection{How to do it with \textsf{R}}

Aside from checking assumptions, we fit the ANOVA to the data via the \texttt{aov} function.  It is convenient to store the model fit as an object and then call \texttt{anova} on the object for the ANOVA table.  Note that model is specified by a model formula that looks like \[\mathtt{response} \sim \mathtt{treatment},\] and so typically the ANOVA is performed on a data frame that has \texttt{response} and \texttt{treatment} as quantitative/qualitative column variables, respectively.

<<>>=
weight.aov <- aov(weight ~ feed, data = chickwts)
anova(weight.aov)
@

\noindent 
At the top of the ANOVA table the name of the \texttt{Response} variable is identified, \texttt{weight} in this example. The first column of the table displays the \textit{treatment} or explanatory variable, \texttt{feed} in this case, followed by \texttt{Residuals} which stands for the error.  The \texttt{Df} column lists the numerator degrees of freedom \(k - 1\) (which is \(6 - 1 = 5\) in this example) followed by \(N - k = 71 - 6\).  The \texttt{Sum Sq} column displays \(SST\) followed by \(SSE\), then the \texttt{Mean Sq} column is the \(3^{\text{rd}}\) column divided by the \(2^{\text{nd}}\) (also known as \(MST\) followed by \(MSE\)), and finally the \(F\) ratio \(MST/MSE\).  The \(p\)-value is calculated under a null \(\mathsf{f}(\mathtt{df1}=k-1,\,\mathtt{df2}=N-k)\), distribution.

\noindent
This is a highly significant \(p\)-value and we consequently \textit{reject} the null hypothesis that the means are identical.  While we are here, note the following simpler way to calculate the means per group that we did before:

<<>>=
model.tables(weight.aov, type = "means")
@

We can check that our model is doing a good job by looking at the residuals.  Here is a normal qq-plot of the residuals, shown in Figure~\ref{fig:chickwts-qqplot-plot}, via the \texttt{qqPlot} function from the \texttt{RcmdrMisc} package. The syntax could not be simpler; just do \texttt{qqPlot(weight.aov)}.

<<chickwts-qqplot-plot, echo=FALSE, fig=TRUE, include=FALSE, width=4,height=4, results=hide>>=
library(RcmdrMisc)
qqPlot(weight.aov, ylab = "")
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-chickwts-qqplot-plot}
\end{center}
\caption[QQ-plot of residuals from \texttt{chickwts} ANOVA.]{{\small A
    qq-plot of the residuals from the ANOVA model on the \texttt{chickwts} data.  If the model fits well, we would expect the residuals to be normally distributed (looks pretty good in this case).  }}
\label{fig:chickwts-qqplot-plot}
\end{figure}


\section{Sample Size and Power} 
\label{sec:sample-size-and-power}

The power function of a test for a parameter \(\theta\) is
\[
\beta(\theta)=\mathbb{P}_{\theta}(\mbox{Reject }H_{0}),\quad -\infty < \theta < \infty.
\]
Here are some properties of power functions:

\begin{enumerate}
\item \(\beta(\theta)\leq\alpha\) for any \(\theta\in\Theta_{0}\), and
   \(\beta(\theta_{0})=\alpha\). We interpret this by saying that no
   matter what value \(\theta\) takes inside the null parameter space,
   there is never more than a chance of \(\alpha\) of rejecting the
   null hypothesis. We have controlled the Type I error rate to be no
   greater than \(\alpha\).
\item \(\lim_{n\to\infty}\beta(\theta^{\ast})=1\) for any fixed
   \(\theta^{\ast}\in\Theta_{1}\). In other words, as the sample size grows
   without bound we are able to detect a nonnull value of \(\theta\)
   with increasing accuracy, no matter how close it lies to the null
   parameter space. This may appear to be a good thing at first
   glance, but it often turns out to be a curse, because this means
   that our Type II error rate can grow out of control as the sample size increases.
\end{enumerate}

\subsection{How to do it with \textsf{R}}

I am thinking about \texttt{replicate} \index{replicate@\texttt{replicate}}
here, and also \texttt{power.examp} \index{power.examp@\texttt{power.examp}}
from the \texttt{TeachingDemos} package \cite{TeachingDemos}. See the
excellent plotting functionality in the \texttt{HH} package \cite{HH}.

<<power-examp, echo=TRUE, fig=TRUE, include=FALSE, width=5>>=
power.examp()
@

See Figure~\ref{fig:power-examp}. The plot corresponds to the
hypothesis test \(H_{0}:\,\mu=\mu_{0}\) versus \(H_{1}:\,\mu=\mu_{1}\)
(where \(\mu_{0}=0\) and \(\mu_{1}=1\), by default) based on a single
observation
\(X\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)\). The
top graph is of the \(H_{0}\) density while the bottom is of the
\(H_{1}\) density. The significance level is set at \(\alpha=0.05\),
the sample size is \(n=1\), and the standard deviation is
\(\sigma=1\). The pink area is the significance level, and the
critical value \(z_{0.05}\approx1.645\) is marked at the left boundary
-- this defines the rejection region. When \(H_{0}\) is true, the
probability of falling in the rejection region is exactly
\(\alpha=0.05\). The same rejection region is marked on the bottom
graph, and the probability of falling in it (when \(H_{1}\) is true)
is the blue area shown at the top of the display to be approximately
\(0.26\). This probability represents the \textit{power} to detect a
non-null mean value of \(\mu=1\). With the command the
\texttt{run.power.examp()} at the command line the same plot opens,
but in addition, there are sliders available that allow the user to
interactively change the sample size \(n\), the standard deviation
\(\sigma\), the true difference between the means \(\mu_{1}-\mu_{0}\),
and the significance level \(\alpha\). By playing around the student
can investigate the effect each of the aforementioned parameters has
on the statistical power. Note that you need the \texttt{tkrplot}
package \cite{tkrplot} for \texttt{run.power.examp}.

\begin{figure}
\begin{center}
\includegraphics{IPSUR-power-examp}
\end{center}
\caption[Plot of significance level and power.]{{\small A plot of
    significance level and power. The red shaded area corresponds to the significance level of the test \(H_{0}:\mu=0\) (here \(\alpha = 0.05\), and the blue shaded area represents the power of the test at the alternative \(\mu = 1\) (here \(\beta(1) = 0.26\)). This graph was generated by the \texttt{power.examp} function from the \texttt{TeachingDemos} package.}}
\label{fig:power-examp}
\end{figure}


\section{Chapter Exercises}
\label{sec-exr-hyp}


\begin{Exercise}[]
For the \texttt{morley} data, conduct a \texttt{z.test} that the true mean \texttt{Speed} of light is greater than 299,850 km/sec. Suppose the population standard deviation is known to be \(\sigma = 79\) km/sec. Check all assumptions. Use significance level \(\alpha = 0.05\). 
\end{Exercise}




\begin{Exercise}[]
In Exercise~\ref{BLANK} we conducted a \texttt{z.test} of the \texttt{morley} data where we were testing whether the true mean \texttt{Speed} was greater than 850.  Since we had such a large sample (100 observations), that was a reasonable thing to do.

However, we assumed that the population standard deviation was known to be \(\sigma = 79\) km/sec, and presumably Michelson did not have that information at the time.  Let us now drop that assumption and conduct Student's \(t\)-test (same hypotheses) instead.

How do the analyses differ, and how close are they to the same?  
\end{Exercise}



\begin{Exercise}[label=exr:test-int-brother]
For the \texttt{morley} data, focus on the \texttt{Speed} observations from the first 20 runs (\texttt{Expt == 1}).  Conduct a two-sided hypothesis test of \(H_{0}:\mu = 845\) at the \(\alpha = 0.01\) significance level, and compare your inference to the two-sided confidence interval for \(\mu\) that we constructed in Example~\ref{BLANK}.
\end{Exercise}



\begin{Exercise}[label=exr:tooth-t-test]
Conduct an appropriate hypothesis test on the \texttt{ToothGrowth} data for equality of mean odontoblast length, \(H_{0}:\mu_{VC} = \mu_{OJ}\), against a two-sided alternative.  Use significance level \(\alpha = 0.05\).  For the purposes of this problem, suppose it is known that \(\sigma_{OJ}=\sigma_{VC}\).
\end{Exercise}



\begin{Exercise}[label=exr:tooth-anova]
Refer to Exercise~\ref{exr:tooth-t-test} and the \texttt{ToothGrowth} data. This time conduct a standard ANOVA for response \texttt{len} by treatment group \texttt{supp}. Use significance level \(\alpha = 0.05\).  
\begin{itemize}
\item What do you notice about the \(p\)-value for this problem, compared to the \(p\)-value in Exercise~\ref{exr:tooth-t-test}? 
\item Take the observed \(t\) statistic you found in Exercise~\ref{exr:tooth-t-test}, and square it to get \(t^2\).  Does this number look familiar?  Can you find it listed anywhere in the ANOVA table you just calculated?  This is not a coincidence.
\end{itemize}
\end{Exercise}



\begin{Exercise}[label=exr:pair-t-ass]
Properly check the assumptions of the Paired \(t\)-test we conducted in Example~\ref{BLANK}.  In particular,
\begin{enumerate}
\item Figure out a way to do a \texttt{qqPlot} (or some other visual display) of the differences computed from the \texttt{sleep} data.  Start with the data in wide form (\texttt{sleep2}).

\item Check normality of the differences again, but this time with the data in the original long form (\texttt{sleep}).

\item Conduct a formal hypothesis test of normality of the differences, and compare with the visual display(s) you constructed in the earlier parts.  Use significance level \(\alpha = 0.05\).  

\end{enumerate}
\end{Exercise}




\begin{Exercise}
An investigator wanted to determine if chocolate milk was more effective
than Gatorade for postponing exhaustion in cyclists. A sample of \(n = \Sexpr{n = sample(12:15, size = 1)}\) cyclists biked a specified distance after exercise on two separate occasions. Let \(X\) and \(Y\) denote the time to exhaustion for the cyclist after chocolate milk and Gatorade, respectively. Given the following
data (we assume the \(n = \Sexpr{n}\) cyclists were randomly selected):

<<echo = FALSE>>=
mux <- 100 + sample(c(0, 2), size = 1)
x <- round(rnorm(n, mean = mux, sd = 13))/100
muy <- mux + sample(c(0, -2), size = 1)
y <- round(rnorm(n, mean = muy, sd = 13))/100
M = rbind(x,y)
rownames(M) = c("Choco", "Gator")
colnames(M) = as.character(1:n)
M
@

\begin{enumerate}
\item What is the parameter of interest in this problem?
\item Is there convincing evidence that chocolate milk is better than Gatorade
at postponing cyclist exhaustion? Set up the proper hypotheses for
this test. \textbf{\textit{Hint:}} be careful here. If you get exhausted
faster, what does that mean?
\item Check assumptions to decide which test to use. Support your answer
with an appropriate visual display, if applicable.
\item Calculate the value of the test statistic and the $p$-value of the
test.
\item At the \(\alpha = \Sexpr{alph <- sample(c(0.001, 0.005, 0.01, 0.025, 0.05), size = 1)}\)
significance level, state your conclusion in the context of the problem.
\end{enumerate}
\end{Exercise}



\begin{Exercise}[]
Consider the \texttt{chickwts} data.  Check normality of \texttt{weight} by each \texttt{feed} type with the following code.  
<<eval=FALSE>>=
attach(chickwts)
by(weight, list(feed = feed), shapiro.test)
detach(chickwts)
@
As far as normality is concerned, would a One-way ANOVA of these data be appropriate?
\end{Exercise}



\begin{Exercise}[]
Conduct a proper ANOVA of the \texttt{PlantGrowth} data.  The experiment measured yield (\texttt{weight}) of plants in three different \texttt{group}s: two treatment groups and a control group.  Check all ANOVA assumptions.  Determine whether there is a statistically significant difference between group yields at the \(\alpha = 0.01\) significance level.
\end{Exercise}




To check for normality in each group we can do \texttt{qqmath} plots from the \texttt{lattice} package.

<<chickwts-qqmath-plot, echo=FALSE, fig=TRUE, include=FALSE, width=5>>=
library(lattice)
qqmath(~weight | feed, data = chickwts)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-chickwts-qqmath-plot}
\end{center}
\caption[Side-by-side stripcharts of the \texttt{chickwts} data.]{{\small A
    plot BLANK}}
\label{fig:chickwts-qqmath-plot}
\end{figure}


The graphs are shown in Figure~\ref{fig:chickwts-qqmath-plot}.  These plots are functional, but with a little bit of extra code we can make them better by adding the straight line.

<<chickwts-qqmath2-plot, echo=FALSE, fig=TRUE, include=FALSE, width=5>>=
library(lattice)
qqmath(~weight | feed, data = chickwts, 
       prepanel = prepanel.qqmathline,
       panel = function(x, ...) {
          panel.qqmathline(x, ...)
          panel.qqmath(x, ...)
       })
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-chickwts-qqmath2-plot}
\end{center}
\caption[Multiple \texttt{qqmath} plots of the \texttt{chickwts} data.]{{\small A
    plot BLANK}}
\label{fig:chickwts-qqmath2-plot}
\end{figure}


See Figure~\ref{fig:chickwts-qqmath2-plot}.  The plots look good as far as normality goes.  
\textbf{Homoscedasticity}

For a visual check of homogeneity of variance, we make a plot of the quantities \[\left|\mathtt{weight - median(weight)}\right|\] by each group. We do this with the \texttt{hovPlot} function in the \texttt{HH} package. 

<<chickwts-homovar-plot, echo=FALSE, fig=TRUE, include=FALSE, width=5>>=
library(HH)
hovPlot(weight ~ feed, data = chickwts)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-chickwts-homovar-plot}
\end{center}
\caption[Homogeneity plot for the \texttt{chickwts} data.]{{\small A
    plot BLANK}}
\label{fig:chickwts-homovar-plot}
\end{figure}

The graph is shown in Figure~\ref{fig:chickwts-homovar-plot}. Judging from the
plot, there are no substantial departures from homogeneity. For a
formal test we can use the Brown-Forsythe statistic which may be called
by the \texttt{hov} function (also in the \texttt{HH} package).
The test works by performing a regular one-way ANOVA on
the values \(\left|\mbox{weight}-\mbox{median(weight)}\right|\):

<<>>=
hov(weight ~ feed, data = chickwts)
@

The test shows that there is not enough evidence to reject the null
hypothesis of homogeneity of variance. Putting all of the above together,
we may comfortably proceed with a one-way analysis of variance on
these data without further intervention.




\chapter{Simple Linear Regression} \label{cha:simple-linear-regression}


<<echo=FALSE, include=FALSE>>=
# This chapter's package dependencies
library(HH)
library(lmtest)
library(RcmdrMisc)
@

\paragraph{What do I want them to know?}

\begin{itemize}
\item basic philosophy of SLR and the regression assumptions
\item point and interval estimation of the model parameters, and how to
  use it to make predictions
\item point and interval estimation of future observations from the model
\item regression diagnostics, including \(R^{2}\) and basic residual
  analysis
\item the concept of influential versus outlying observations, and how to
  tell the difference
\end{itemize}

\section{Basic Philosophy} \label{sec:basic-philosophy}

Here we have two variables \(X\) and \(Y\). For our purposes, \(X\) is
not random (so we will write \(x\)), but \(Y\) is random. We believe
that \(Y\) depends in \textit{some} way on \(x\). Some typical examples of
\((x,Y)\) pairs are

\begin{itemize}
\item \(x =\) study time and \(Y =\) score on a test.
\item \(x =\) height and \(Y =\) weight.
\item \(x =\) smoking frequency and \(Y =\) age of first heart attack.
\end{itemize}

Given information about the relationship between \(x\) and \(Y\), we
would like to \textit{predict} future values of \(Y\) for particular values
of \(x\). This turns out to be a difficult problem\footnote{Yogi Berra
  once said, ``It is always difficult to make predictions, especially
  about the future.''}, so instead we first tackle an easier problem:
we estimate \(\mathbb{E}Y\). How can we accomplish this? Well, we know
that \(Y\) depends somehow on \(x\), so it stands to reason that
\begin{equation}
\mathbb{E} Y = \mu(x),\ \mbox{a function of }x.
\end{equation}


But we should be able to say more than that. To focus our efforts we
impose some structure on the functional form of \(\mu\). For instance,

\begin{itemize}
\item if \(\mu(x)=\beta_{0}+\beta_{1}x\), we try to estimate \(\beta_{0}\) and \(\beta_{1}\).
\item if \(\mu(x) = \beta_{0} + \beta_{1}x + \beta_{2}x^{2}\), we try to
  estimate \(\beta_{0}\), \(\beta_{1}\), and \(\beta_{2}\).
\item if \(\mu(x) = \beta_{0} \mathrm{e}^{\beta_{1}x}\), we try to
  estimate \(\beta_{0}\) and \(\beta_{1}\).
\end{itemize}

This helps us in the sense that we concentrate on the estimation of
just a few parameters, \(\beta_{0}\) and \(\beta_{1}\), say, rather
than some nebulous function. Our \textit{modus operandi} is simply to
perform the random experiment \(n\) times and observe the \(n\)
ordered pairs of data
\((x_{1},Y_{1}),\ (x_{2},Y_{2}),\ \ldots,(x_{n},Y_{n})\). We use these
\(n\) data points to estimate the parameters.

More to the point, there are \textit{three simple linear regression} (SLR)
assumptions \index{regression assumptions} that will form
the basis for the rest of this chapter:



\begin{assumption}
We assume that \(\mu\) is a linear function of \(x\), that is,
\begin{equation}
\mu(x)=\beta_{0}+\beta_{1}x,
\end{equation}
where \(\beta_{0}\) and \(\beta_{1}\) are unknown constants to be
estimated.
\end{assumption}



\begin{assumption}
We further assume that \(Y_{i}\) is \(\mu(x_{i})\), a ``signal'',
plus some ``error'' (represented by the symbol \(\epsilon_{i}\)):
\begin{equation}
Y_{i} = \beta_{0} + \beta_{1}x_{i} + \epsilon_{i}, \quad i = 1,2,\ldots,n.
\end{equation}
\end{assumption}



\begin{assumption}
We lastly assume that the errors are IID normal with mean 0 and variance \(\sigma^{2}\):
\begin{equation}
\epsilon_{1},\epsilon_{2},\ldots,\epsilon_{n}\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=\sigma).
\end{equation}
\end{assumption}



\begin{rem}[]
We assume both the normality of the errors \(\epsilon\) and the
linearity of the mean function \(\mu\). Recall from Proposition
\ref{prp:mvnorm-cond-dist} of Chapter~\ref{cha:multivariable-distributions} that if \((X,Y) \sim \mathsf{mvnorm}\) then the mean of \(Y|x\) is a linear function of
\(x\). This is not a coincidence. In more advanced classes we study
the case that both \(X\) and \(Y\) are random, and in particular, when
they are jointly normally distributed.
\end{rem}

\subsection{What does it all mean?}

See Figure~\ref{fig:philosophy}. Shown in the figure is a solid line,
the regression line \index{regression line} \(\mu\), which in this
display has slope 0.5 and \textit{y}-intercept 2.5, that is,
\(\mu(x)=2.5 + 0.5x\). The intuition is that for each given value of
\(x\), we observe a random value of \(Y\) which is normally
distributed with a mean equal to the height of the regression line at
that \(x\) value. Normal densities are superimposed on the plot to
drive this point home; in principle, the densities stand outside of
the page, perpendicular to the plane of the paper. The figure shows
three such values of \(x\), namely, \(x = 1\), \(x = 2.5\), and
\(x = 4\). Not only do we assume that the observations at the three
locations are independent, but we also assume that their distributions
have the same spread. In mathematical terms this means that the normal
densities all along the line have identical standard deviations --
there is no ``fanning out'' or ``scrunching in'' of the normal
densities as \(x\) increases\footnote{In practical terms, this
  constant variance assumption is often violated, in that we often
  observe scatterplots that fan out from the line as \(x\) gets large
  or small. We say under those circumstances that the data show
  \textit{heteroscedasticity}. There are methods to address it, but
  they fall outside the realm of SLR.}.


<<philosophy, echo=FALSE, fig=TRUE, include=FALSE, width=5>>=
plot(c(0,5), c(0,6.5), type = "n", xlab="x", ylab="y")
abline(h = 0, v = 0, col = "gray60")
abline(a = 2.5, b = 0.5, lwd = 2)
x <- 600:3000/600
y <- dnorm(x, mean = 3, sd = 0.5)
lines(y + 1.0, x)
lines(y + 2.5, x + 0.75)
lines(y + 4.0, x + 1.5)
abline(v = c(1, 2.5, 4), lty = 2, col = "grey")
segments(1, 3, 1 + dnorm(0,0,0.5),3, lty = 2, col = "gray")
segments(2.5, 3.75, 2.5 + dnorm(0,0,0.5), 3.75, lty = 2, col = "gray")
segments(4,4.5, 4 + dnorm(0,0,0.5),4.5, lty = 2, col = "gray")
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-philosophy}
\end{center}
\caption[Philosophical foundations of SLR.]{{\small Philosophical foundations of SLR. For each fixed value of \(x\), the distribution of \(Y\) is normal.  The mean of the normal distribution follows a straight, linear functional form, and it is the slope and \(y\)-intercept of that line which interests us the most for estimation and hypothesis testing.  But also important is that the spread, \(\sigma\) of those normal distributions are the same for all values of \(x\), so called \textit{homogeneity} of variance.}}
\label{fig:philosophy}
\end{figure}




\begin{example}[Speed and stopping distance of cars.]
\label{exm:speed-and-stopping}
We will use the data frame \texttt{cars} \index{Data
sets!cars@\texttt{cars}} from the \texttt{datasets} package
\cite{datasets}. It has two variables: \texttt{speed} and \texttt{dist}. We can take
a look at some of the values in the data frame:
\end{example}


<<echo=TRUE>>=
head(cars)
@

The \texttt{speed} represents how fast the car was going (\(x\)) in miles per hour and \texttt{dist} (\(Y\)) measures how far it took the car to stop, in
feet. We first make a simple scatterplot of the data.


<<carscatter, echo=FALSE, fig=TRUE, include=FALSE, height=3.0625,width=5>>=
plot(dist ~ speed, data = cars)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-carscatter}
\end{center}
\caption[A scatterplot of \texttt{dist} versus \texttt{speed} for the
\texttt{cars} data.]{{\small A scatterplot of \texttt{dist} versus
    \texttt{speed} for the \texttt{cars} data.  There is clearly an
    upward trend to the plot which is approximately linear.}}
\label{fig:carscatter}
\end{figure}



You can see the output in Figure~\ref{fig:carscatter}, which was
produced by the following code.

<<echo=TRUE, eval=FALSE>>=
plot(dist ~ speed, data = cars)
@

There is a pronounced upward trend to the data points, and the pattern
looks approximately linear. There does not appear to be substantial
fanning out of the points or extreme values.

\section{Estimation} \label{sec:slr-estimation}

\subsection{Point Estimates of the Parameters} \label{sub:point-estimate-mle-slr}

Where is \(\mu(x)\)? In essence, we would like to ``fit'' a line to
the points. But how do we determine a ``good'' line? Is there a \textit{best}
line? We will use maximum likelihood \index{maximum likelihood} to
find it. We know:
\begin{equation}
Y_{i} = \beta_{0} + \beta_{1}x_{i} + \epsilon_{i},\quad i=1,\ldots,n,
\end{equation}
where the \(\epsilon_{i}\) are IID
\(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=\sigma)\). Thus
\(Y_{i}\sim\mathsf{norm}(\mathtt{mean}=\beta_{0}+\beta_{1}x_{i},\,\mathtt{sd}=\sigma),\
i=1,\ldots,n\). Furthermore, \(Y_{1},\ldots,Y_{n}\) are independent
-- but not identically distributed. The likelihood
function \index{likelihood function} is:
\begin{alignat}{1}
L(\beta_{0},\beta_{1},\sigma)= & \prod_{i=1}^{n}f_{Y_{i}}(y_{i}),\\
= & \prod_{i=1}^{n}(2\pi\sigma^{2})^{-1/2}\exp\left\{ \frac{-(y_{i}-\beta_{0}-\beta_{1}x_{i})^{2}}{2\sigma^{2}}\right\} ,\\
= & (2\pi\sigma^{2})^{-n/2}\exp\left\{ \frac{-\sum_{i=1}^{n}(y_{i}-\beta_{0}-\beta_{1}x_{i})^{2}}{2\sigma^{2}}\right\} .
\end{alignat}
We take the natural logarithm to get
\begin{equation}
\label{eq:regML-lnL}
\ln L(\beta_{0},\beta_{1},\sigma)=-\frac{n}{2}\ln(2\pi\sigma^{2})-\frac{\sum_{i=1}^{n}(y_{i}-\beta_{0}-\beta_{1}x_{i})^{2}}{2\sigma^{2}}.
\end{equation}
We would like to maximize this function of \(\beta_{0}\) and \(\beta_{1}\). See Appendix~\ref{sec:multivariable-calculus} which tells us that
we should find critical points by means of the partial
derivatives. Let us start by differentiating with respect to
\(\beta_{0}\):
\begin{equation}
\frac{\partial}{\partial\beta_{0}}\ln L=0-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}2(y_{i}-\beta_{0}-\beta_{1}x_{i})(-1),
\end{equation}
and the partial derivative equals zero when \(\sum_{i=1}^{n}(y_{i}-\beta_{0}-\beta_{1}x_{i}) = 0\), that is, when
\begin{equation}
\label{eq:regML-a}
n \beta_{0} + \beta_{1} \sum_{i=1}^{n} x_{i} = \sum_{i = 1}^{n}y_{i}.
\end{equation}
Moving on, we next take the partial derivative of \(\ln L\)
(Equation \eqref{eq:regML-lnL}) with respect to \(\beta_{1}\) to get

\begin{alignat}{1}
\frac{\partial}{\partial \beta_{1}} \ln L = \ & 0 - \frac{1}{2\sigma^{2}} \sum_{i=1}^{n} 2 (y_{i} - \beta_{0} - \beta_{1} x_{i})(-x_{i}),\\ = & \frac{1}{\sigma^{2}}\sum_{i = 1}^{n}\left(x_{i} y_{i} - \beta_{0}x_{i} - \beta_{1}x_{i}^{2}\right),
\end{alignat}
and this equals zero when the last sum equals zero, that is, when
\begin{equation}
\label{eq:regML-b}
\beta_{0} \sum_{i = 1}^{n}x_{i} + \beta_{1} \sum_{i = 1}^{n}x_{i}^{2} = \sum_{i = 1}^{n}x_{i}y_{i}.
\end{equation}
Solving the system of equations \eqref{eq:regML-a} and \eqref{eq:regML-b}
\begin{eqnarray}
n\beta_{0} + \beta_{1}\sum_{i = 1}^{n}x_{i} & = & \sum_{i = 1}^{n}y_{i}\\
\beta_{0}\sum_{i = 1}^{n}x_{i}+\beta_{1}\sum_{i = 1}^{n}x_{i}^{2} & = & \sum_{i = 1}^{n}x_{i}y_{i}
\end{eqnarray}
for \(\beta_{0}\) and \(\beta_{1}\) (in Exercise~\ref{exr:find-mles-slr}) gives
\begin{equation}
\label{eq:regline-slope-formula}
\hat{\beta}_{1} = \frac{\sum_{i = 1}^{n}x_{i}y_{i} - \left.\left(\sum_{i = 1}^{n}x_{i}\right)\left(\sum_{i = 1}^{n}y_{i}\right)\right] n}{\sum_{i = 1}^{n}x_{i}^{2} - \left.\left(\sum_{i = 1}^{n}x_{i}\right)^{2}\right/ n}
\end{equation}
and
\begin{equation}
\hat{\beta}_{0} = \overline{y} - \hat{\beta}_{1}\overline{x}.
\end{equation}

The conclusion? To estimate the mean line
\begin{equation}
\mu(x) = \beta_{0} + \beta_{1}x,
\end{equation}
we use the ``line of best fit''
\begin{equation}
\hat{\mu}(x) = \hat{\beta}_{0} + \hat{\beta}_{1}x,
\end{equation}
where \(\hat{\beta}_{0}\) and \(\hat{\beta}_{1}\) are given as
above. For notation we will usually write \(b_{0} = \hat{\beta_{0}}\) and \(b_{1}=\hat{\beta_{1}}\) so that \(\hat{\mu}(x) = b_{0} + b_{1}x\).



\begin{rem}[]
The formula for \(b_{1}\) in Equation
\eqref{eq:regline-slope-formula} gets the job done but does not really
make any sense. There are many equivalent formulas for \(b_{1}\)
that are more intuitive, or at the least are easier to remember. One
of the author's favorites is
\begin{equation}
\label{eq:sample-correlation-formula}
b_{1} = r\frac{s_{y}}{s_{x}},
\end{equation}
where \(r\), \(s_{y}\), and \(s_{x}\) are the
sample correlation coefficient and the sample standard deviations of
the \(Y\) and \(x\) data, respectively. See Exercise~\ref{exr:show-alternate-slope-formula}. Also, notice the similarity
between Equation \eqref{eq:sample-correlation-formula} and Equation
\eqref{eq:population-slope-slr}.
\end{rem}


\subsubsection{How to do it with \textsf{R}}

<<echo=FALSE, include=FALSE>>=
tmpcoef <- round(as.numeric(coef(lm(dist ~ speed, cars))), 2)
@

Here we go. \textsf{R} will calculate the linear regression line
with the \texttt{lm} function. We will store the result in an object which we
will call \texttt{cars.lm}. Here is how it works:

<<echo=TRUE>>=
cars.lm <- lm(dist ~ speed, data = cars)
@

The first part of the input to the \texttt{lm} function, \texttt{dist
  ~ speed}, is a \textit{model formula}, read like this: \texttt{dist}
is described (or modeled) by \texttt{speed}. The \texttt{data = cars}
argument tells \textsf{R} where to look for the variables quoted in the model
formula. The output object \texttt{cars.lm} contains a multitude of
information. Let's first take a look at the coefficients of the fitted
regression line, which are extracted by the \texttt{coef} function
(alternatively, we could just type \texttt{cars.lm} to see the same
thing):

<<echo=TRUE>>=
coef(cars.lm)
@

The parameter estimates \(b_{0}\) and \(b_{1}\) for the intercept
and slope, respectively, are shown above.

It is good practice to visually inspect the data with the regression
line added to the plot. To do this we first scatterplot the original
data and then follow with a call to the \texttt{abline} function. The inputs
to \texttt{abline} are the coefficients of \texttt{cars.lm}; see
Figure~\ref{fig:carline}.


<<carline, echo=FALSE, fig=TRUE, include=FALSE, height=3.0625,width=5>>=
plot(dist ~ speed, data = cars)
abline(coef(cars.lm))
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-carline}
\end{center}
\caption{{\small A scatterplot with an added regression line for the \texttt{cars} data.}}
\label{fig:carline}
\end{figure}




To calculate points on the regression line we may simply plug the
desired \(x\) value(s) into \(\hat{\mu}\), either by hand, or with
the \texttt{predict} function. The inputs to \texttt{predict} are the fitted linear
model object, \texttt{cars.lm}, and the desired \(x\) value(s) represented by
a data frame. See the example below.




\begin{example}[]
\label{exm:regline-cars-interpret}
Using the regression line for the
\texttt{cars} data:
\end{example}

\begin{enumerate}
\item What is the meaning of \(\mu(8) = \beta_{0} + \beta_{1}(8)\)?
   This represents the average stopping distance (in feet) for a car
   going 8 mph.
\item Interpret the slope \(\beta_{1}\). The true slope \(\beta_{1}\)
   represents the increase in average stopping distance for each mile
   per hour faster that the car drives. In this case, we estimate the
   car to take approximately \Sexpr{tmpcoef[2]} additional feet
   to stop for each additional mph increase in speed.
\item Interpret the intercept \(\beta_{0}\). This would represent the
   mean stopping distance for a car traveling 0 mph (which our
   regression line estimates to be \(\Sexpr{tmpcoef[1]}\). Of
   course, this interpretation does not make any sense for this
   example, because a car travelling 0 mph takes 0 ft to stop (it was
   not moving in the first place)! What went wrong? Looking at the
   data, we notice that the smallest speed for which we have measured
   data is 4 mph. Therefore, if we predict what would happen for
   slower speeds then we would be \textit{extrapolating}, a dangerous
   practice which often gives nonsensical results.
\end{enumerate}

\subsection{Point Estimates of the Regression Line} \label{sub:slr-point-est-regline}

We said at the beginning of the chapter that our goal was to estimate
\(\mu = \mathbb{E} Y\), and the arguments in Section~\ref{sub:point-estimate-mle-slr} showed how to obtain an estimate \(\hat{\mu}\) of \(\mu\) when the regression assumptions hold. Now we
will reap the benefits of our work in more ways than we previously
disclosed. Given a particular value \(x_{0}\), there are two values we
would like to estimate:

\begin{enumerate}
\item the mean value of \(Y\) at \(x_{0}\), and
\item a future value of \(Y\) at \(x_{0}\). The first is a number,
   \(\mu(x_{0})\), and the second is a random variable, \(Y(x_{0})\),
   but our point estimate is the same for both: \(\hat{\mu}(x_{0})\).
\end{enumerate}




\begin{example}[]
\label{exm:regline-cars-pe-8mph}
We may use the regression line to obtain
a point estimate of the mean stopping distance for a car traveling 8
mph: \(\hat{\mu}(15) = b_{0} + (8) (b_{1})\) which is approximately
13.88. We would also use 13.88 as a point estimate for the stopping
distance of a future car traveling 8 mph.
\end{example}

Note that we actually have observed data for a car traveling 8 mph;
its stopping distance was 16 ft as listed in the fifth row of the
\texttt{cars} data which we saw in Example~\ref{exm:speed-and-stopping}.

<<echo=TRUE>>=
cars[5, ]
@

There is a special name for estimates \(\hat{\mu}(x_{0})\) when \(
x_{0}\) matches an observed value \(x_{i}\) from the data set. They
are called \textit{fitted values}, they are denoted by \(\hat{Y}_{1}\),
\(\hat{Y}_{2}\), \ldots, \(\hat{Y}_{n}\) (ignoring repetition), and they
play an important role in the sections that follow.

In an abuse of notation we will sometimes write \(\hat{Y}\) or
\(\hat{Y}(x_{0})\) to denote a point on the regression line even when
\(x_{0}\) does not belong to the original data if the context of the
statement obviates any danger of confusion.

We saw in Example~\ref{exm:regline-cars-interpret} that spooky things can
happen when we are cavalier about point estimation. While it is
usually acceptable to predict/estimate at values of \(x_{0}\) that
fall within the range of the original \(x\) data, it is reckless to
use \(\hat{\mu}\) for point estimates at locations outside that
range. Such estimates are usually worthless. \textit{Do not extrapolate}
unless there are compelling external reasons, and even then, temper it
with a good deal of caution.

\subsubsection{How to do it with \textsf{R}}

The fitted values are automatically computed as a byproduct of the
model fitting procedure and are already stored as a component of the
\texttt{cars.lm} object. We may access them with the \texttt{fitted} function (we
only show the first five entries):

<<echo=TRUE>>=
fitted(cars.lm)[1:5]
@

Predictions at \(x\) values that are not necessarily part of the
original data are done with the \texttt{predict} function. The first argument
is the original \texttt{cars.lm} object and the second argument \texttt{newdata}
accepts a dataframe (in the same form that was used to fit \texttt{cars.lm})
that contains the locations at which we are seeking predictions. Let
us predict the average stopping distances of cars traveling 6 mph, 8
mph, and 21 mph:

<<echo=TRUE>>=
predict(cars.lm, newdata = data.frame(speed = c(6, 8, 21)))
@

Note that there were no observed cars that traveled 6 mph or 21
mph. Also note that our estimate for a car traveling 8 mph matches the
value we computed by hand in Example~\ref{exm:regline-cars-pe-8mph}.

\subsection{Mean Square Error and Standard Error}

To find the MLE of \(\sigma^{2}\) we consider the partial derivative
\begin{equation}
\frac{\partial}{\partial\sigma^{2}}\ln L=\frac{n}{2\sigma^{2}}-\frac{1}{2(\sigma^{2})^{2}}\sum_{i=1}^{n}(y_{i}-\beta_{0}-\beta_{1}x_{i})^{2},
\end{equation}
and after plugging in \(\hat{\beta}_{0}\) and \(\hat{\beta}_{1}\) and
setting equal to zero we get
\begin{equation}
\hat{\sigma^{2}}=\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i})^{2}=\frac{1}{n}\sum_{i=1}^{n}[y_{i}-\hat{\mu}(x_{i})]^{2}.
\end{equation}
We write \(\hat{Yi}=\hat{\mu}(x_{i})\), and we let
\(E_{i}=Y_{i}-\hat{Y_{i}}\) be the \(i^{\mathrm{th}}\) \textit{residual}. We
see
\begin{equation}
n\hat{\sigma^{2}}=\sum_{i=1}^{n}E_{i}^{2}=SSE=\mbox{ the sum of squared errors.}
\end{equation}
For a point estimate of \(\sigma^{2}\) we use the \textit{mean square error}
\(S^{2}\) defined by
\begin{equation}
S^{2}=\frac{SSE}{n-2},
\end{equation}
and we estimate \(\sigma\) with the \textit{standard error}
\(S=\sqrt{S^{2}}\)\footnote{Be careful not to confuse the mean square error \(S^{2}\)
with the sample variance \(S^{2}\) in Chapter~\ref{cha:describing-data-distributions}. Other notation the reader may
encounter is the lowercase \(s^{2}\) or the bulky \(MSE\).}.


\subsubsection{How to do it with \textsf{R}}

The residuals for the model may be obtained with the \texttt{residuals}
function; we only show the first few entries in the interest of space:

<<echo=TRUE>>=
residuals(cars.lm)[1:5]
@


<<echo=FALSE, include=FALSE>>=
tmpred <- round(as.numeric(predict(cars.lm, newdata = data.frame(speed = 8))), 2)
tmps <- round(summary(cars.lm)$sigma, 2)
@

In the last section, we calculated the fitted value for \(x=8\) and
found it to be approximately \(\hat{\mu}(8) \approx\) \Sexpr{tmpred}. Now, it turns out that there was only one recorded observation
at \(x = 8\), and we have seen this value in the output of
\texttt{head(cars)} in Example~\ref{exm:speed-and-stopping}; it was
\(\mathtt{dist} = 16\) ft for a car with \(\mathtt{speed} = 8\) mph. Therefore, the
residual should be \(E = Y - \hat{Y}\) which is \(E \approx 16 -\)
\Sexpr{tmpred}. Now take a look at the last entry of
\texttt{residuals(cars.lm)}, above. It is not a coincidence.

The estimate \(S\) for \(\sigma\) is called the \texttt{Residual standard
error} and for the \texttt{cars} data is shown a few lines up on the
\texttt{summary(cars.lm)} output (see How to do it with \textsf{R} in
Section~\ref{sub:slr-interval-est-params}). We may read it from there
to be \(S\approx\) \Sexpr{tmps}, or we can access it directly from the
\texttt{summary} object.

<<echo=TRUE>>=
carsumry <- summary(cars.lm)
carsumry$sigma
@


\subsection{Interval Estimates of the Parameters} \label{sub:slr-interval-est-params}

We discussed general interval estimation in Chapter~\ref{cha:estimation}. There
we found that we could use what we know about the sampling
distribution of certain statistics to construct confidence intervals
for the parameter being estimated. We will continue in that vein, and
to get started we will determine the sampling distributions of the
parameter estimates, \(b_{1}\) and \(b_{0}\).

To that end, we can see from Equation \eqref{eq:regline-slope-formula} (and it
is made clear in Chapter~\ref{cha:multiple-linear-regression}) that \(b_{1}\) is
just a linear combination of normally distributed random variables, so
\(b_{1}\) is normally distributed too. Further, it can be shown that
\begin{equation}
b_{1}\sim\mathsf{norm}\left(\mathtt{mean}=\beta_{1},\,\mathtt{sd}=\sigma_{b_{1}}\right)
\end{equation}
where
\begin{equation}
\sigma_{b_{1}}=\frac{\sigma}{\sqrt{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}}
\end{equation}
is called \textit{the standard error of} \(b_{1}\) which unfortunately
depends on the unknown value of \(\sigma\). We do not lose heart,
though, because we can estimate \(\sigma\) with the standard error
\(S\) from the last section. This gives us an estimate \(S_{b_{1}}\)
for \(\sigma_{b_{1}}\) defined by
\begin{equation}
S_{b_{1}}=\frac{S}{\sqrt{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}}.
\end{equation}

Now, it turns out that \(b_{0}\), \(b_{1}\), and \(S\) are mutually
independent (see the footnote in Section~\ref{sub:mlr-interval-est-params}). Therefore, the quantity
\begin{equation}
T=\frac{b_{1}-\beta_{1}}{S_{b_{1}}}
\end{equation}
has a \(\mathsf{t}(\mathtt{df}=n-2)\) distribution and a
\(100(1-\alpha)\%\) confidence interval for \(\beta_{1}\) is given by
\begin{equation}
b_{1}\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)\, S_{b_{1}.}
\end{equation}

It is also sometimes of interest to construct a confidence interval
for \(\beta_{0}\) in which case we will need the sampling distribution
of \(b_{0}\). It is shown in Chapter~\ref{cha:multiple-linear-regression} that
\begin{equation}
b_{0}\sim\mathsf{norm}\left(\mathtt{mean}=\beta_{0},\,\mathtt{sd}=\sigma_{b_{0}}\right),
\end{equation}
where \(\sigma_{b_{0}}\) is given by
\begin{equation}
\sigma_{b_{0}}=\sigma\sqrt{\frac{1}{n}+\frac{\overline{x}^{2}}{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}},
\end{equation}
and which we estimate with the \(S_{b_{0}}\) defined by
\begin{equation}
S_{b_{0}}=S\sqrt{\frac{1}{n}+\frac{\overline{x}^{2}}{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}}.
\end{equation}
Thus the quantity
\begin{equation}
T=\frac{b_{0}-\beta_{0}}{S_{b_{0}}}
\end{equation}
has a \(\mathsf{t}(\mathtt{df}=n-2)\) distribution and a
\(100(1-\alpha)\%\) confidence interval for \(\beta_{0}\) is given by
\begin{equation}
b_{0}\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)\, S_{b_{0}}.
\end{equation}

\subsubsection{How to do it with \textsf{R}}

<<echo=FALSE, include=FALSE>>=
A <- matrix(as.numeric(round(carsumry$coef, 3)), nrow = 2)
B <- round(confint(cars.lm), 3)
@

Let us take a look at the output from \texttt{summary(cars.lm)}:

<<echo=TRUE>>=
summary(cars.lm)
@

In the \texttt{Coefficients} section we find the parameter estimates and
their respective standard errors in the second and third columns; the
other columns are discussed in Section~\ref{sec:model-utility-slr}. If we
wanted, say, a 95\% confidence interval for \(\beta_{1}\) we could use
\(b_{1} =\) \Sexpr{A[2,1]} and \(S_{b_{1}} =\) \Sexpr{A[2,2]} together with a \(\mathsf{t}_{0.025}(\mathtt{df}=23)\)
critical value to calculate \(b_{1} \pm
\mathsf{t}_{0.025}(\mathtt{df} = 23) \cdot S_{b_{1}}\).  Or, we could
use the \texttt{confint} function.

<<echo=TRUE>>=
confint(cars.lm)
@

With 95\% confidence, the random interval \Sexpr{B[2,1]} to
\Sexpr{B[2,2]} covers the parameter \(\beta_{1}\).

\subsection{Interval Estimates of the Regression Line} \label{sub:slr-interval-est-regline}

We have seen how to estimate the coefficients of regression line with
both point estimates and confidence intervals. We even saw how to
estimate a value \(\hat{\mu}(x)\) on the regression line for a given
value of \(x\), such as \(x=15\).

But how good is our estimate \(\hat{\mu}(15)\)? How much confidence do
we have in \textit{this} estimate? Furthermore, suppose we were going to
observe another value of \(Y\) at \(x=15\). What could we say?

Intuitively, it should be easier to get bounds on the mean (average)
value of \(Y\) at \(x_{0}\) -- called a \textit{confidence interval for the mean value of}
\(Y\) \textit{at} \(x_{0}\) -- than it is to get bounds on a
future observation of \(Y\) (called a \textit{prediction interval for} \(Y\)
\textit{at} \(x_{0}\)). As we shall see, the intuition serves us well and
confidence intervals are shorter for the mean value, longer for the
individual value.

Our point estimate of \(\mu(x_{0})\) is of course
\(\hat{Y}=\hat{Y}(x_{0})\), so for a confidence interval we will need
to know \(\hat{Y}\)'s sampling distribution. It turns out (see Section
) that \(\hat{Y}=\hat{\mu}(x_{0})\) is distributed
\begin{equation}
\hat{Y}\sim\mathsf{norm}\left(\mathtt{mean}=\mu(x_{0}),\:\mathtt{sd}=\sigma\sqrt{\frac{1}{n}+\frac{(x_{0}-\overline{x})^{2}}{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}}\right).
\end{equation}

Since \(\sigma\) is unknown we estimate it with \(S\) (we should
expect the appearance of a \(\mathsf{t}(\mathtt{df}=n-2)\)
distribution in the near future).

A \(100(1-\alpha)\%\) \textit{confidence interval (CI) for} \(\mu(x_{0})\)
is given by
\begin{equation}
\label{eq:SLR-conf-int-formula}
\hat{Y}\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-2)\, S\sqrt{\frac{1}{n}+\frac{(x_{0}-\overline{x}^{2})}{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}}.
\end{equation}
Prediction intervals are a little bit different. In order to find
confidence bounds for a new observation of \(Y\) (we will denote it
\(Y_{\mbox{new}}\)) we use the fact that
\begin{equation}
Y_{\mbox{new}}\sim\mathtt{norm}\left(\mathtt{mean}=\mu(x_{0}),\,\mathtt{sd}=\sigma\sqrt{1+\frac{1}{n}+\frac{(x_{0}-\overline{x})^{2}}{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}}\right).
\end{equation}
Of course, \(\sigma\) is unknown so we estimate it with \(S\) and a
\(100(1-\alpha)\%\) prediction interval (PI) for a future value of
\(Y\) at \(x_{0}\) is given by
\begin{equation}
\label{eq:SLR-pred-int-formula}
\hat{Y}(x_{0})\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)\: S\,\sqrt{1+\frac{1}{n}+\frac{(x_{0}-\overline{x})^{2}}{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}}.
\end{equation}
We notice that the prediction interval in Equation
\eqref{eq:SLR-pred-int-formula} is wider than the confidence interval
in Equation \eqref{eq:SLR-conf-int-formula}, as we expected at the
beginning of the section.

\subsubsection{How to do it with \textsf{R}}

Confidence and prediction intervals are calculated in \textsf{R}
with the \texttt{predict} \index{predict@\texttt{predict}} function, which we
encountered in Section~\ref{sub:slr-point-est-regline}. There we neglected to
take advantage of its additional \texttt{interval} argument. The general
syntax follows.



\begin{example}[]
  We will find confidence and prediction intervals for the stopping
  distance of a car travelling 5, 6, and 21 mph (note from the graph
  that there are no collected data for these speeds). We have computed
  \texttt{cars.lm} earlier, and we will use this for input to the
  \texttt{predict} function. Also, we need to tell \textsf{R} the values of
  \(x_{0}\) at which we want the predictions made, and store the
  \(x_{0}\) values in a data frame whose variable is labeled with the
  correct name. \textit{This is important.}
\end{example}

<<echo=TRUE>>=
new <- data.frame(speed = c(5, 6, 21))
@

Next we instruct \textsf{R} to calculate the intervals. Confidence
intervals are given by

<<echo=TRUE>>=
predict(cars.lm, newdata = new, interval = "confidence")
@


<<echo = FALSE>>=
carsCI <- round(predict(cars.lm, newdata = new, interval = "confidence"), 2)
@

Prediction intervals are given by

<<echo=TRUE>>=
predict(cars.lm, newdata = new, interval = "prediction")
@


<<echo=FALSE, include=FALSE>>=
carsPI <- round(predict(cars.lm, newdata = new, interval = "prediction"), 2)
@



The type of interval is dictated by the \texttt{interval} argument (which is
\texttt{none} by default), and the default confidence level is 95\% (which
can be changed with the \texttt{level} argument).



\begin{example}[]
Using the \texttt{cars} data,
\end{example}

\begin{enumerate}
\item Report a point estimate of and a 95\% confidence interval for the
   mean stopping distance for a car travelling 5 mph.  The fitted
   value for \(x = 5\) is \Sexpr{carsCI[1,1]}, so a point
   estimate would be \Sexpr{carsCI[1,1]} ft. The 95\% CI is
   given by \Sexpr{carsCI[1,2]} to \Sexpr{carsCI[1,3]}, so
   with 95\% confidence the mean stopping distance lies somewhere
   between \Sexpr{carsCI[1,2]} ft and \Sexpr{carsCI[1,3]} ft.
\item Report a point prediction for and a 95\% prediction interval for the
   stopping distance of a hypothetical car travelling 21 mph.  The
   fitted value for \(x = 21\) is \Sexpr{carsPI[3,1]}, so a point
   prediction for the stopping distance is \Sexpr{carsPI[3,1]} ft. The
   95\% PI is \Sexpr{carsPI[3,2]} to \Sexpr{carsPI[3,3]}, so with
   95\% confidence we may assert that the hypothetical stopping
   distance for a car travelling 21 mph would lie somewhere between
   \Sexpr{carsPI[3,2]} ft and \Sexpr{carsPI[3,3]} ft.
\end{enumerate}

\subsection{Graphing the Confidence and Prediction Bands}

We earlier guessed that a bound on the value of a single new
observation would be inherently less certain than a bound for an
average (mean) value; therefore, we expect the CIs for the mean to be
tighter than the PIs for a new observation. A close look at the
standard deviations in Equations \eqref{eq:SLR-conf-int-formula} and
\eqref{eq:SLR-pred-int-formula} confirms our guess, but we would like
to see a picture to drive the point home.

We may plot the confidence and prediction intervals with one fell
swoop using the \texttt{ci.plot} function from the \texttt{HH} package
\cite{HH}. The graph is displayed in Figure~\ref{fig:carscipi}.

<<echo=TRUE, eval=FALSE>>=
library(HH)
ci.plot(cars.lm)
@

Notice that the bands curve outward from the regression line as the
\(x\) values move away from the center. This is expected once we
notice the \((x_{0}-\overline{x})^{2}\) term in the standard deviation
formulas in Equations \eqref{eq:SLR-conf-int-formula} and
\eqref{eq:SLR-pred-int-formula}.

<<carscipi, echo=FALSE, fig=TRUE, include=FALSE, width=5>>=
print(ci.plot(cars.lm))
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-carscipi}
\end{center}
\caption{{\small A scatterplot with confidence/prediction bands for the \texttt{cars} data.}}
\label{fig:carscipi}
\end{figure}




\section{Model Utility and Inference} \label{sec:model-utility-slr}

\subsection{Hypothesis Tests for the Parameters} \label{sub:slr-hypoth-test-params}

Much of the attention of SLR is directed toward \(\beta_{1}\) because
when \(\beta_{1}\neq 0\) the mean value of \(Y\) increases (or
decreases) as \(x\) increases. It is really boring when
\(\beta_{1}=0\), because in that case the mean value of \(Y\) remains
the same, regardless of the value of \(x\) (when the regression
assumptions hold, of course). It is thus very important to decide
whether or not \(\beta_{1} = 0\). We address the question with a
statistical test of the null hypothesis \(H_{0}:\beta_{1}=0\) versus
the alternative hypothesis \(H_{1}:\beta_{1}\neq0\), and to do that we
need to know the sampling distribution of \(b_{1}\) when the null
hypothesis is true.

To this end we already know from Section~\ref{sub:slr-interval-est-params} that the quantity

\begin{equation}
T=\frac{b_{1}-\beta_{1}}{S_{b_{1}}}
\end{equation}
has a \(\mathsf{t}(\mathtt{df}=n-2)\) distribution; therefore, when
\(\beta_{1}=0\) the quantity \(b_{1}/S_{b_{1}}\) has a
\(\mathsf{t}(\mathtt{df}=n-2)\) distribution and we can compute a
\(p\)-value by comparing the observed value of \(b_{1}/S{}_{b_{1}}\)
with values under a \(\mathsf{t}(\mathtt{df}=n-2)\) curve.

Similarly, we may test the hypothesis \(H_{0}:\beta_{0}=0\) versus the
alternative \(H_{1}:\beta_{0}\neq0\) with the statistic
\(T=b_{0}/S_{b_{0}}\), where \(S_{b_{0}}\) is given in Section~\ref{sub:slr-interval-est-params}. The test is conducted the same way as for
\(\beta_{1}\).

\subsubsection{How to do it with \textsf{R}}

Let us take another look at the output from \texttt{summary(cars.lm)}:

<<echo=TRUE>>=
summary(cars.lm)
@

In the \texttt{Coefficients} section we find the \(t\) statistics and the
\(p\)-values associated with the tests that the respective parameters
are zero in the fourth and fifth columns. Since the \(p\)-values are
(much) less than 0.05, we conclude that there is strong evidence that
the parameters \(\beta_{1}\neq0\) and \(\beta_{0}\neq0\), and as such,
we say that there is a statistically significant linear relationship
between \texttt{dist} and \texttt{speed}.

\subsection{Simple Coefficient of Determination}

It would be nice to have a single number that indicates how well our
linear regression model is doing, and the \textit{simple coefficient
  of determination} is designed for that purpose. In what follows, we
observe the values \(Y_{1}\), \(Y_{2}\), \ldots,\(Y_{n}\), and the goal
is to estimate \(\mu(x_{0})\), the mean value of \(Y\) at the location
\(x_{0}\).

If we disregard the dependence of \(Y\) and \(x\) and base our
estimate only on the \(Y\) values then a reasonable choice for an
estimator is just the MLE of \(\mu\), which is \(\overline{Y}\). Then
the errors incurred by the estimate are just \(Y_{i}-\overline{Y}\)
and the variation about the estimate as measured by the sample
variance is proportional to
\begin{equation}
SSTO=\sum_{i=1}^{n}(Y_{i}-\overline{Y})^{2}.
\end{equation}
The acronym \(SSTO\) stands for \textit{total sum of squares}.  And we
have additional information, namely, we have values \(x_{i}\)
associated with each value of \(Y_{i}\). We have seen that this
information leads us to the estimate \(\hat{Y_{i}}\) and the errors
incurred are just the residuals, \(E_{i}=Y_{i}-\hat{Y_{i}}\). The
variation associated with these errors can be measured with
\begin{equation}
SSE=\sum_{i=1}^{n}(Y_{i}-\hat{Y_{i}})^{2}.
\end{equation}
We have seen the \(SSE\) before, which stands for the \textit{sum of
  squared errors} or \textit{error sum of squares}. Of course, we
would expect the error to be less in the latter case, since we have
used more information. The improvement in our estimation as a result
of the linear regression model can be measured with the difference
\[
  (Y_{i}-\overline{Y})-(Y_{i}-\hat{Y_{i}})=\hat{Y_{i}}-\overline{Y}, \]
and we measure the variation in these errors with
\begin{equation}
SSR=\sum_{i=1}^{n}(\hat{Y_{i}}-\overline{Y})^{2},
\end{equation}
also known as the \textit{regression sum of squares}. It is not obvious, but
some algebra proved a famous result known as the \textit{ANOVA Equality}:
\begin{equation}
\label{eq:anovaeq}
\sum_{i=1}^{n}(Y_{i}-\overline{Y})^{2}=\sum_{i=1}^{n}(\hat{Y_{i}}-\overline{Y})^{2}+\sum_{i=1}^{n}(Y_{i}-\hat{Y_{i}})^{2}
\end{equation}
or in other words,
\begin{equation}
SSTO=SSR+SSE.
\end{equation}
This equality has a nice interpretation. Consider \(SSTO\) to be the
\textit{total variation} of the errors. Think of a decomposition of
the total variation into pieces: one piece measuring the reduction of
error from using the linear regression model, or \textit{explained
  variation} (\(SSR\)), while the other represents what is left over,
that is, the errors that the linear regression model doesn't explain,
or \textit{unexplained variation} (\(SSE\)). In this way we see that
the ANOVA equality merely partitions the variation into
\[ \mbox{total variation}=\mbox{explained variation}+\mbox{unexplained
    variation}.
\] For a single number to summarize how well our model is doing we use
the \textit{simple coefficient of determination} \(r^{2}\), defined by
\begin{equation}
r^{2}=1-\frac{SSE}{SSTO}.
\end{equation}
We interpret \(r^{2}\) as the proportion of total variation that is
explained by the simple linear regression model. When \(r^{2}\) is
large, the model is doing a good job; when \(r^{2}\) is small, the
model is not doing a good job.

Related to the simple coefficient of determination is the sample
correlation coefficient, \(r\). As you can guess, the way we get \(r\)
is by the formula \(|r|=\sqrt{r^{2}}\). The sign of \(r\) is equal the
sign of the slope estimate \(b_{1}\). That is, if the regression line
\(\hat{\mu}(x)\) has positive slope, then
\(r=\sqrt{r^{2}}\). Likewise, if the slope of \(\hat{\mu}(x)\) is
negative, then \(r=-\sqrt{r^{2}}\).

\subsubsection{How to do it with \textsf{R}}

The primary method to display partitioned sums of squared errors is
with an \textit{ANOVA table}. The command in \textsf{R} to produce such a table
is \texttt{anova}. The input to \texttt{anova} is the result of an
\texttt{lm} call which for the \texttt{cars} data is \texttt{cars.lm}.

<<echo=TRUE>>=
anova(cars.lm)
@


The output gives
\[
r^{2}=1-\frac{SSE}{SSR+SSE}=1-\frac{11353.5}{21185.5+11353.5}\approx0.65.
\]

The interpretation should be: ``The linear regression line accounts for
approximately 65\% of the variation of \texttt{dist} as explained by \texttt{speed}''.

The value of \(r^{2}\) is stored in the \texttt{r.squared} component of
\texttt{summary(cars.lm)}, which we called \texttt{carsumry}.

<<echo=TRUE>>=
carsumry$r.squared
@

We already knew this. We saw it in the next to the last line of the
\texttt{summary(cars.lm)} output where it was called \texttt{Multiple R-squared}. Listed right beside it is the \texttt{Adjusted R-squared} which
we will discuss in Chapter~\ref{cha:multiple-linear-regression}.  For the \texttt{cars}
data, we find \(r\) to be

<<echo=TRUE>>=
sqrt(carsumry$r.squared)
@


We choose the principal square root because the slope of the
regression line is positive.

\subsection{Overall F statistic} \label{sub:slr-overall-f-statistic}

There is another way to test the significance of the linear regression
model. In SLR, the new way also tests the hypothesis
\(H_{0}:\beta_{1}=0\) versus \(H_{1}:\beta_{1}\neq0\), but it is done
with a new test statistic called the \textit{overall F statistic}. It
is defined by
\begin{equation}
\label{eq:slr-overall-F-statistic}
F=\frac{SSR}{SSE/(n-2)}.
\end{equation}

Under the regression assumptions and when \(H_{0}\) is true, the \(F\)
statistic has an \(\mathtt{f}(\mathtt{df1}=1,\,\mathtt{df2}=n-2)\)
distribution. We reject \(H_{0}\) when \(F\) is large -- that is, when
the explained variation is large relative to the unexplained
variation.

All this being said, we have not yet gained much from the overall
\(F\) statistic because we already knew from Section~\ref{sub:slr-hypoth-test-params} how to test
\(H_{0}:\beta_{1} = 0\)\ldots we use the Student's \(t\) statistic. What
is worse is that (in the simple linear regression model) it can be
proved that the \(F\) in Equation \eqref{eq:slr-overall-F-statistic}
is exactly the Student's \(t\) statistic for \(\beta_{1}\) squared,

\begin{equation}
F=\left(\frac{b_{1}}{S_{b_{1}}}\right)^{2}.
\end{equation}

So why bother to define the \(F\) statistic? Why not just square the
\(t\) statistic and be done with it? The answer is that the \(F\)
statistic has a more complicated interpretation and plays a more
important role in the multiple linear regression model which we will
study in Chapter~\ref{cha:multiple-linear-regression}. See Section~\ref{sub:mlr-overall-f-test} for details.

\subsubsection{How to do it with \textsf{R}}

The overall \(F\) statistic and \(p\)-value are displayed in the
bottom line of the \texttt{summary(cars.lm)} output. It is also shown in the
final columns of \texttt{anova(cars.lm)}:

<<echo=TRUE>>=
anova(cars.lm)
@


<<echo=FALSE, include=FALSE>>=
tmpf <- round(as.numeric(carsumry$fstatistic[1]), 2)
@

Here we see that the \(F\) statistic is \Sexpr{tmpf} with a
\(p\)-value very close to zero. The conclusion: there is very strong
evidence that \(H_{0}:\beta_{1} = 0\) is false, that is, there is
strong evidence that \(\beta_{1} \neq 0\). Moreover, we conclude
that the regression relationship between \texttt{dist} and \texttt{speed} is
significant.

Note that the value of the \(F\) statistic is the same as the
Student's \(t\) statistic for \texttt{speed} squared.

\section{Residual Analysis} \label{sec:residual-analysis-slr}

We know from our model that \(Y=\mu(x)+\epsilon\), or in other words,
\(\epsilon=Y-\mu(x)\). Further, we know that
\(\epsilon\sim\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=\sigma)\). We
may estimate \(\epsilon_{i}\) with the \textit{residual}
\(E_{i}=Y_{i}-\hat{Y_{i}}\), where
\(\hat{Y_{i}}=\hat{\mu}(x_{i})\). If the regression assumptions hold,
then the residuals should be normally distributed. We check this in
Section~\ref{sub:normality-assumption}. Further, the residuals should have mean
zero with constant variance \(\sigma^{2}\), and we check this in
Section~\ref{sub:constant-variance-assumption}. Last, the residuals should be
independent, and we check this in Section~\ref{sub:independence-assumption}.

In every case, we will begin by looking at residual plots -- that is,
scatterplots of the residuals \(E_{i}\) versus index or predicted
values \(\hat{Y_{i}}\) -- and follow up with hypothesis tests.

\subsection{Normality Assumption} \label{sub:normality-assumption}

We can assess the normality of the residuals with graphical methods
and hypothesis tests. To check graphically whether the residuals are
normally distributed we may look at histograms or \textit{q-q} plots. We
first examine a histogram in Figure~\ref{fig:normal-hist-cars}, which was produced by the following code.


<<normal-hist-cars, echo=FALSE, fig=TRUE, include=FALSE, height=2.75,width=5>>=
hist(residuals(cars.lm), breaks=15)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-normal-hist-cars}
\end{center}
\caption[Residuals from the linear regression model for the \texttt{cars} data.]{{\small A histogram of the residuals from the linear regression model for the \texttt{cars} data. Used for checking the normality assumption. Look out for any skewness or extreme values; hopefully the graph is mound shaped and symmetric about zero, with no outliers.}}
\label{fig:normal-hist-cars}
\end{figure}

There we see that the distribution of the residuals appears to be
mound shaped, for the most part. We can plot the order statistics of the studentized residuals versus quantiles from a
\(\mathsf{t}(\mathtt{mean}=0,\,\mathtt{sd}=1)\) distribution with the command \texttt{qqPlot(cars.lm)} from the \texttt{RcmdrMisc} package \cite{RcmdrMisc}, and the results are in Figure~\ref{fig:normal-q-q-plot-cars}. If the assumption of normality were true, then we would expect points randomly scattered about the dotted straight line displayed in the figure. In this case, we see a slight departure from normality in that the dots show a bit of clustering on one side or the other of the line. The points on the upper end of the plot also begin to stray from the line. We would say there is some evidence that the residuals are not perfectly normal.

<<normal-q-q-plot-cars, echo=FALSE, results=hide,fig=TRUE, include=FALSE, width=5>>=
library(RcmdrMisc)
qqPlot(cars.lm)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-normal-q-q-plot-cars}
\end{center}
\caption[Quantile-quantile plot of \texttt{cars} studentized residuals.]{{\small A quantile-quantile plot of the studentized residuals from the linear regression model for the \texttt{cars} data. Used for checking the normality assumption. Look out for any curvature or substantial departures from the straight line; hopefully the dots hug the line closely and stay within the bands.}}
\label{fig:normal-q-q-plot-cars}
\end{figure}

\subsubsection{Testing the Normality Assumption}

Even though we may be concerned about the plots, we can use tests to determine if the evidence present is statistically significant, or if it could have happened merely by chance. There are many statistical tests of normality. We will use the Shapiro-Wilk test, since it is known to be a good test and to be quite powerful. However, there are
many other fine tests of normality including the Anderson-Darling test
and the Lillefors test, just to mention two of them.

The Shapiro-Wilk test is based on the statistic
\begin{equation}
W=\frac{\left(\sum_{i=1}^{n}a_{i}E_{(i)}\right)^{2}}{\sum_{j=1}^{n}E_{j}^{2}},
\end{equation}
where the \(E_{(i)}\) are the ordered residuals and the \(a_{i}\) are
constants derived from the order statistics of a sample of size \(n\)
from a normal distribution.
We perform the Shapiro-Wilk test below, using the \texttt{shapiro.test}
function from the \texttt{stats} package \cite{stats}. The hypotheses are \[
H_{0}:\mbox{ the residuals are normally distributed } \] versus \[
H_{1}:\mbox{ the residuals are not normally distributed.}  \] The
results from \textsf{R} are

<<echo=TRUE>>=
shapiro.test(residuals(cars.lm))
@

For these data we would reject the assumption of normality of the
residuals at the \(\alpha=0.05\) significance level, but do not lose
heart, because the regression model is reasonably robust to departures
from the normality assumption. As long as the residual distribution is
not highly skewed, then the regression estimators will perform
reasonably well. Moreover, departures from constant variance and
independence will sometimes affect the quantile plots and histograms,
therefore it is wise to delay final decisions regarding normality
until all diagnostic measures have been investigated.

\subsection{Constant Variance Assumption} \label{sub:constant-variance-assumption}

We will again go to residual plots to try and determine if the spread
of the residuals is changing over time (or index). However, it is
unfortunately not that easy because the residuals do not have constant
variance! In fact, it can be shown that the variance of the residual
\(E_{i}\) is
\begin{equation}
\mbox{Var$(E_{i})$}=\sigma^{2}(1-h_{ii}),\quad i=1,2,\ldots,n,
\end{equation}
where \(h_{ii}\) is a quantity called the \textit{leverage} which is defined
below. Consequently, in order to check the constant variance
assumption we must standardize the residuals before plotting. We
estimate the standard error of \(E_{i}\) with
\(s_{E_{i}}=s\sqrt{(1-h_{ii})}\) and define the
\textit{standardized residuals} \(R_{i}\), \(i=1,2,\ldots,n\), by
\begin{equation}
R_{i}=\frac{E_{i}}{s\,\sqrt{1-h_{ii}}},\quad i=1,2,\ldots,n.
\end{equation}
For the constant variance assumption we do not need the sign of the
residual so we will plot \(\sqrt{|R_{i}|}\) versus the fitted
values. As we look at a scatterplot of \(\sqrt{|R_{i}|}\) versus
\(\hat{Y}_{i}\) we would expect under the regression assumptions to
see a constant band of observations, indicating no change in the
magnitude of the observed distance from the line. We want to watch out
for a fanning-out of the residuals, or a less common funneling-in of
the residuals. Both patterns indicate a change in the residual
variance and a consequent departure from the regression assumptions,
the first an increase, the second a decrease.

In this case, we plot the standardized residuals versus the fitted
values. The graph may be seen in Figure~\ref{fig:std-resids-fitted-cars}. For these data there does appear to
be somewhat of a slight fanning-out of the residuals.


<<std-resids-fitted-cars, echo=FALSE, fig=TRUE, include=FALSE, height=4,width=5>>=
plot(cars.lm, which = 3)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-std-resids-fitted-cars}
\end{center}
\caption[Standardized residuals against the fitted values for the \texttt{cars} data.]{{\small Plot of standardized residuals against the fitted values for the \texttt{cars} data. Used for checking the constant variance assumption. Watch out for any fanning out (or in) of the dots; hopefully they fall in a constant band.}}
\label{fig:std-resids-fitted-cars}
\end{figure}



\subsubsection{Testing the Constant Variance Assumption}

We will use the Breusch-Pagan test to decide whether the variance of
the residuals is nonconstant. The null hypothesis is that the variance
is the same for all observations, and the alternative hypothesis is
that the variance is not the same for all observations. The test
statistic is found by fitting a linear model to the centered squared
residuals,
\begin{equation}
W_{i} = E_{i}^{2} - \frac{SSE}{n}, \quad i=1,2,\ldots,n.
\end{equation}

By default the same explanatory variables are used in the new model
which produces fitted values \(\hat{W}_{i}\), \(i=1,2,\ldots,n\). The
Breusch-Pagan test statistic in \textsf{R} is then calculated with
\begin{equation}
BP=n\sum_{i=1}^{n}\hat{W}_{i}^{2}\div\sum_{i=1}^{n}W_{i}^{2}.
\end{equation}
We reject the null hypothesis if \(BP\) is too large, which happens
when the explained variation i the new model is large relative to the
unexplained variation in the original model.  We do it in
R with the \texttt{bptest} function from the \texttt{lmtest} package
\cite{lmtest}.

<<echo=TRUE>>=
library(lmtest)
bptest(cars.lm)
@


For these data we would not reject the null hypothesis at the
\(\alpha=0.05\) level. There is relatively weak evidence against the
assumption of constant variance.

\subsection{Independence Assumption} \label{sub:independence-assumption}

One of the strongest of the regression assumptions is the one
regarding independence. Departures from the independence assumption
are often exhibited by correlation (or autocorrelation, literally,
self-correlation) present in the residuals. There can be positive or
negative correlation.

Positive correlation is displayed by positive residuals followed by
positive residuals, and negative residuals followed by negative
residuals. Looking from left to right, this is exhibited by a cyclical
feature in the residual plots, with long sequences of positive
residuals being followed by long sequences of negative ones.

On the other hand, negative correlation implies positive residuals
followed by negative residuals, which are then followed by positive
residuals, \textit{etc}. Consequently, negatively correlated residuals
are often associated with an alternating pattern in the residual
plots. We examine the residual plot in Figure~\ref{fig:resids-fitted-cars}. There is no obvious cyclical wave
pattern or structure to the residual plot.


<<resids-fitted-cars, echo=FALSE, fig=TRUE, include=FALSE, height=2.8,width=5>>=
plot(cars.lm, which = 1)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-resids-fitted-cars}
\end{center}
\caption[Residuals versus fitted values for the \texttt{cars} data.]{{\small Plot of the residuals versus the fitted values for the \texttt{cars} data. Used for checking the independence assumption. Watch out for any patterns or structure; hopefully the points are randomly scattered on the plot.}}
\label{fig:resids-fitted-cars}
\end{figure}



\subsubsection{Testing the Independence Assumption}

We may statistically test whether there is evidence of autocorrelation
in the residuals with the Durbin-Watson test. The test is based on the
statistic
\begin{equation}
D=\frac{\sum_{i=2}^{n}(E_{i}-E_{i-1})^{2}}{\sum_{j=1}^{n}E_{j}^{2}}.
\end{equation}
Exact critical values are difficult to obtain, but \textsf{R} will
calculate the \textit{p}-value to great accuracy. It is performed with the
\texttt{dwtest} function from the \texttt{lmtest} package \cite{lmtest}. We will
conduct a two sided test that the correlation is not zero, which is
not the default (the default is to test that the autocorrelation is
positive).

<<echo=TRUE>>=
dwtest(cars.lm, alternative = "two.sided")
@

In this case we do not reject the null hypothesis at the
\(\alpha=0.05\) significance level; there is very little evidence of
nonzero autocorrelation in the residuals.

\subsection{Remedial Measures}

We often find problems with our model that suggest that at least one
of the three regression assumptions is violated. What do we do then?
There are many measures at the statistician's disposal, and we mention
specific steps one can take to improve the model under certain types
of violation.

\begin{description}
\item [{Mean response is not linear}] We can directly modify the model to
     better approximate the mean response. In particular, perhaps a
     polynomial regression function of the form \[ \mu(x) =
     \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{1}^{2} \] would be
     appropriate. Alternatively, we could have a function of the form
     \[ \mu(x)=\beta_{0}\mathrm{e}^{\beta_{1}x}.  \] Models like these
     are studied in nonlinear regression courses.
\item [{Error variance is not constant}] Sometimes a transformation of the
     dependent variable will take care of the problem. There is a
     large class of them called \textit{Box-Cox transformations}. They take
     the form
     \begin{equation}
     Y^{\ast}=Y^{\lambda},
     \end{equation}
     where \(\lambda\) is a constant. (The method proposed by Box and
     Cox will determine a suitable value of \(\lambda\) automatically
     by maximum likelihood). The class contains the transformations
     \begin{alignat*}{1} \lambda=2,\quad & Y^{\ast}=Y^{2}\\
       \lambda=0.5,\quad & Y^{\ast}=\sqrt{Y}\\ \lambda=0,\quad &
       Y^{\ast}=\ln\: Y\\ \lambda=-1,\quad & Y^{\ast}=
       1/Y \end{alignat*} Alternatively, we can use the method of
     \textit{weighted least squares}. This is studied in more detail
     in later classes.
\item [{Error distribution is not normal}]  The same transformations for
     stabilizing the variance are equally appropriate for smoothing
     the residuals to a more Gaussian form. In fact, often we will
     kill two birds with one stone.
\item [{Errors are not independent}] There is a large class of
     autoregressive models to be used in this situation which occupy
     the latter part of a chapter on time series.
\end{description}

\section{Other Diagnostic Tools} \label{sec:other-diagnostic-tools-slr}

There are two types of observations with which we must be especially
careful:

\begin{description}
\item [{Influential observations}] are those that have a substantial effect
     on our estimates, predictions, or inferences. A small change in
     an influential observation is followed by a large change in the
     parameter estimates or inferences.
\item [{Outlying observations}] are those that fall fall far from the rest
     of the data. They may be indicating a lack of fit for our
     regression model, or they may just be a mistake or typographical
     error that should be corrected. Regardless, special attention
     should be given to these observations. An outlying observation
     may or may not be influential.
\end{description}

We will discuss outliers first because the notation builds
sequentially in that order.

\subsection{Outliers}
There are three ways that an observation \((x_{i},y_{i})\) might be
identified as an outlier: it can have an \(x_{i}\) value which falls
far from the other \(x\) values, it can have a \(y_{i}\) value which
falls far from the other \(y\) values, or it can have both its
\(x_{i}\) and \(y_{i}\) values falling far from the other \(x\) and
\(y\) values.

\subsection{Leverage}
Leverage statistics are designed to identify observations which have
\(x\) values that are far away from the rest of the data. In the
simple linear regression model the leverage of \(x_{i}\) is denoted by
\(h_{ii}\) and defined by
\begin{equation}
h_{ii}=\frac{1}{n}+\frac{(x_{i}-\overline{x})^{2}}{\sum_{k=1}^{n}(x_{k}-\overline{x})^{2}},\quad i=1,2,\ldots,n.
\end{equation}
The formula has a nice interpretation in the SLR model: if the
distance from \(x_{i}\) to \(\overline{x}\) is large relative to the
other \(x\)'s then \(h_{ii}\) will be close to 1.

Leverages have nice mathematical properties; for example, they satisfy
\begin{equation}
\label{eq:slr-leverage-between}
0\leq h_{ii}\leq1,
\end{equation}
and their sum is
\begin{eqnarray}
\label{eq:slr-average-leverage}
\sum_{i=1}^{n}h_{ii} & = & \sum_{i=1}^{n}\left[\frac{1}{n}+\frac{(x_{i}-\overline{x})^{2}}{\sum_{k=1}^{n}(x_{k}-\overline{x})^{2}}\right],\\
 & = & \frac{n}{n}+\frac{\sum_{i}(x_{i}-\overline{x})^{2}}{\sum_{k}(x_{k}-\overline{x})^{2}},\\
 & = & 2.
\end{eqnarray}

A rule of thumb is to consider leverage values to be large if they are
more than double their average size (which is \(2/n\) according to
Equation \eqref{eq:slr-average-leverage}). So leverages larger than \(4/n\)
are suspect. Another rule of thumb is to say that values bigger than
0.5 indicate high leverage, while values between 0.3 and 0.5 indicate
moderate leverage.

\subsection{Standardized and Studentized Deleted Residuals}

We have already encountered the \textit{standardized residuals} \(r_{i}\) in
Section~\ref{sub:constant-variance-assumption}; they are merely residuals that
have been divided by their respective standard deviations:
\begin{equation}
R_{i}=\frac{E_{i}}{S\sqrt{1-h_{ii}}},\quad i=1,2,\ldots,n.
\end{equation}
Values of \(|R_{i}| > 2\) are extreme and suggest that the observation
has an outlying \(y\)-value.

Now delete the \(i^{\mathrm{th}}\) case and fit the regression
function to the remaining \(n - 1\) cases, producing a fitted value
\(\hat{Y}_{(i)}\) with \textit{deleted residual}
\(D_{i}=Y_{i}-\hat{Y}_{(i)}\). It is shown in later classes that
\begin{equation}
\mbox{Var $(D_{i})$}=\frac{S_{(i)}^{2}}{1-h_{ii}},\quad i=1,2,\ldots,n,
\end{equation}
so that the \textit{studentized deleted residuals} \(t_{i}\) defined by
\begin{equation}
\label{eq:slr-studentized-deleted-resids}
t_{i}=\frac{D_{i}}{S_{(i)}/(1-h_{ii})},\quad i=1,2,\ldots,n,
\end{equation}
have a \(\mathsf{t}(\mathtt{df}=n-3)\) distribution and we compare
observed values of \(t_{i}\) to this distribution to decide whether or
not an observation is extreme.

The folklore in regression classes is that a test based on the
statistic in Equation \eqref{eq:slr-studentized-deleted-resids} can be
too liberal. A rule of thumb is if we suspect an observation to be an
outlier \textit{before} seeing the data then we say it is
significantly outlying if its two-tailed \(p\)-value is less than
\(\alpha\), but if we suspect an observation to be an outlier
\textit{after} seeing the data then we should only say it is
significantly outlying if its two-tailed \(p\)-value is less than
\(\alpha/n\). The latter rule of thumb is called the
\textit{Bonferroni approach} and can be overly conservative for large
data sets. The responsible statistician should look at the data and
use his/her best judgement, in every case.

\subsubsection{How to do it with \textsf{R}}

We can calculate the standardized residuals with the \texttt{rstandard}
function. The input is the \texttt{lm} object, which is \texttt{cars.lm}.

<<echo=TRUE>>=
sres <- rstandard(cars.lm)
sres[1:5]
@


We can find out which observations have studentized residuals larger
than two with the command

<<echo=TRUE>>=
sres[which(abs(sres) > 2)]
@


In this case, we see that observations 23, 35, and 49 are potential
outliers with respect to their \(y\)-value.  We can compute the
studentized deleted residuals with \texttt{rstudent}:

<<echo=TRUE>>=
sdelres <- rstudent(cars.lm)
sdelres[1:5]
@


We should compare these values with critical values from a
\(\mathsf{t}(\mathtt{df}=n-3)\) distribution, which in this case is
\(\mathsf{t}(\mathtt{df}=50-3=47)\). We can calculate a 0.005 quantile
and check with

<<echo=TRUE>>=
t0.005 <- qt(0.005, df = 47, lower.tail = FALSE)
sdelres[which(abs(sdelres) > t0.005)]
@

This means that observations 23 and 49 have a large studentized
deleted residual. The leverages can be found with the \texttt{hatvalues}
function:

<<echo=TRUE>>=
leverage <- hatvalues(cars.lm)
leverage[which(leverage > 4/50)]
@


Here we see that observations 1, 2, and 50 have leverages bigger than
double their mean value. These observations would be considered
outlying with respect to their \(x\) value (although they may or may
not be influential).

\subsection{Influential Observations}

\subsubsection{\(DFBETAS\) and \(DFFITS\)}

Any time we do a statistical analysis, we are confronted with the
variability of data. It is always a concern when an observation plays
too large a role in our regression model, and we would not like or
procedures to be overly influenced by the value of a single
observation. Hence, it becomes desirable to check to see how much our
estimates and predictions would change if one of the observations were
not included in the analysis. If an observation changes the
estimates/predictions a large amount, then the observation is
influential and should be subjected to a higher level of scrutiny.

We measure the change in the parameter estimates as a result of
deleting an observation with \(DFBETAS\). The \(DFBETAS\) for the
intercept \(b_{0}\) are given by
\begin{equation}
(DFBETAS)_{0(i)}=\frac{b_{0}-b_{0(i)}}{S_{(i)}\sqrt{\frac{1}{n}+\frac{\overline{x}^{2}}{\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}}},\quad i=1,2,\ldots,n.
\end{equation}
and the \(DFBETAS\) for the slope \(b_{1}\) are given by
\begin{equation}
(DFBETAS)_{1(i)}=\frac{b_{1}-b_{1(i)}}{S_{(i)}\left[\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}\right]^{-1/2}},\quad i=1,2,\ldots,n.
\end{equation}
See Section~\ref{sec:residual-analysis-mlr} for a better way to write
these. The signs of the \(DFBETAS\) indicate whether the coefficients
would increase or decrease as a result of including the
observation. If the \(DFBETAS\) are large, then the observation has a
large impact on those regression coefficients. We label observations
as suspicious if their \(DFBETAS\) have magnitude greater 1 for small
data or \(2/\sqrt{n}\) for large data sets.  We can calculate the
\(DFBETAS\) with the \texttt{dfbetas} function (some output has been
omitted):

<<echo=TRUE>>=
dfb <- dfbetas(cars.lm)
head(dfb)
@


We see that the inclusion of the first observation slightly increases
the \texttt{Intercept} and slightly decreases the coefficient on \texttt{speed}.

We can measure the influence that an observation has on its fitted
value with \(DFFITS\). These are calculated by deleting an
observation, refitting the model, recalculating the fit, then
standardizing. The formula is
\begin{equation}
(DFFITS)_{i}=\frac{\hat{Y_{i}}-\hat{Y}_{(i)}}{S_{(i)}\sqrt{h_{ii}}},\quad i=1,2,\ldots,n.
\end{equation}
The value represents the number of standard deviations of
\(\hat{Y_{i}}\) that the fitted value \(\hat{Y_{i}}\) increases or
decreases with the inclusion of the \(i^{\textrm{th}}\)
observation. We can compute them with the \texttt{dffits} function.

<<echo=TRUE>>=
dff <- dffits(cars.lm)
dff[1:5]
@

A rule of thumb is to flag observations whose \(DFFIT\) exceeds one in
absolute value, but there are none of those in this data set.

\subsubsection{Cook's Distance}

The \(DFFITS\) are good for measuring the influence on a single fitted
value, but we may want to measure the influence an observation has on
all of the fitted values simultaneously. The statistics used for
measuring this are Cook's distances which may be
calculated\footnote{Cook's distances are actually defined by a different
formula than the one shown. The formula in Equation
\eqref{eq:slr-cooks-distance} is algebraically equivalent to the
defining formula and is, in the author's opinion, more transparent.} by the formula
\begin{equation}
\label{eq:slr-cooks-distance}
D_{i}=\frac{E_{i}^{2}}{(p+1)S^{2}}\cdot\frac{h_{ii}}{(1-h_{ii})^{2}},\quad i=1,2,\ldots,n.
\end{equation}
It shows that Cook's distance depends both on the residual \(E_{i}\)
and the leverage \(h_{ii}\) and in this way \(D_{i}\) contains
information about outlying \(x\) and \(y\) values.

To assess the significance of \(D\), we compare to quantiles of an
\(\mathsf{f}(\mathtt{df1}=2,\,\mathtt{df2}=n-2)\) distribution. A rule
of thumb is to classify observations falling higher than the
\(50^{\mathrm{th}}\) percentile as being extreme.


\subsubsection{How to do it with \textsf{R}}

We can calculate the Cook's Distances with the \texttt{cooks.distance}
function.

<<echo=TRUE>>=
cooksD <- cooks.distance(cars.lm)
cooksD[1:4]
@


We can look at a plot of the Cook's distances with the command
\texttt{plot(cars.lm, which = 4)}.


<<cooks-distance-cars, echo=FALSE, fig=TRUE, include=FALSE, height=2.8,width=5>>=
plot(cars.lm, which = 4)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-cooks-distance-cars}
\end{center}
\caption[Cook's distances for the \texttt{cars} data.]{{\small Cook's distances for the \texttt{cars} data. Used for checking for influential and/our outlying observations. Values with large Cook's distance merit further investigation.}}
\label{fig:cooks-distance-cars}
\end{figure}


Observations with the largest Cook's D values are labeled, hence we
see that observations 23, 39, and 49 are suspicious. However, we need
to compare to the quantiles of an
\(\mathsf{f}(\mathtt{df1} = 2, \,\mathtt{df2} = 48)\) distribution:

<<echo=TRUE>>=
F0.50 <- qf(0.5, df1 = 2, df2 = 48)
any(cooksD > F0.50)
@


We see that with this data set there are no observations with extreme
Cook's distance, after all.

\subsection{All Influence Measures Simultaneously}

We can display the result of diagnostic checking all at once in one
table, with potentially influential points displayed. We do it with
the command \texttt{influence.measures(cars.lm)}:

<<echo=TRUE, eval=FALSE>>=
influence.measures(cars.lm)
@

The output is a huge matrix display, which we have omitted in the
interest of brevity. A point is identified if it is classified to be
influential with respect to any of the diagnostic measures. Here we
see that observations 2, 11, 15, and 18 merit further investigation.

We can also look at all diagnostic plots at once with the commands

<<echo=TRUE, eval=FALSE>>=
plot(cars.lm)
@

The \texttt{par} command is used so that \(2\times 2 = 4\) plots will be
shown on the same display. The diagnostic plots for the \texttt{cars} data
are shown in Figure~\ref{fig:diagnostic-plots-cars}.



<<diagnostic-plots-cars, echo=FALSE, fig=TRUE, include=FALSE, width=5>>=
par(mfrow = c(2,2))
plot(cars.lm)
par(mfrow = c(1,1))
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-diagnostic-plots-cars}
\end{center}
\caption[ Diagnostic plots for the \texttt{cars} data.]{{\small Diagnostic plots for the \texttt{cars} data. The residuals versus fitted plot in the upper left can be used to assess the independence assumption.  The q-q plot in the upper right is used to assess the normality assumption.  The Scale-Location plot on the lower left is used to assess the constant variance assumption, and the Residuals-Leverage plot can be used to identify influential observations.}}
\label{fig:diagnostic-plots-cars}
\end{figure}




We have discussed all of the plots except the last, which is possibly
the most interesting. It shows Residuals vs. Leverage, which will
identify outlying \(y\) values versus outlying \(x\) values. Here we
see that observation 23 has a high residual, but low leverage, and it
turns out that observations 1 and 2 have relatively high leverage but
low/moderate leverage (they are on the right side of the plot, just
above the horizontal line). Observation 49 has a large residual with a
comparatively large leverage.

We can identify the observations with the \texttt{identify} command; it
allows us to display the observation number of dots on the
plot. First, we plot the graph, then we call \texttt{identify}:

<<echo=TRUE, eval=FALSE>>=
plot(cars.lm, which = 5)          # std'd resids vs lev plot
identify(leverage, sres, n = 4)   # identify 4 points
@

The graph with the identified points is omitted (but the plain plot is
shown in the bottom right corner of Figure~\ref{fig:diagnostic-plots-cars}). Observations 1 and 2 fall on the far right
side of the plot, near the horizontal axis.


\section{Chapter Exercises}




\begin{Exercise}[label=exr:anova-equality]
Prove the ANOVA equality, Equation \eqref{eq:anovaeq}. \textit{Hint:}
show that
\begin{equation}
\sum_{i=1}^{n}(Y_{i}-\hat{Y_{i}})(\hat{Y_{i}}-\overline{Y})=0.
\end{equation}
\end{Exercise}





\begin{Exercise}[label=exr:find-mles-slr]
Solve the following system of equations for
\(\beta_{1}\) and \(\beta_{0}\) to find the MLEs for slope and
intercept in the simple linear regression model.
\begin{eqnarray*}
n\beta_{0}+\beta_{1}\sum_{i=1}^{n}x_{i} & = & \sum_{i=1}^{n}y_{i}\\
\beta_{0}\sum_{i=1}^{n}x_{i}+\beta_{1}\sum_{i=1}^{n}x_{i}^{2} & = & \sum_{i=1}^{n}x_{i}y_{i}
\end{eqnarray*}
\end{Exercise}





\begin{Exercise}[label=exr:show-alternate-slope-formula]
Show that the formula given in
Equation \eqref{eq:sample-correlation-formula} is equivalent to
\begin{equation}
\hat{\beta}_{1} =
\frac{\sum_{i=1}^{n}x_{i}y_{i}-\left.\left(\sum_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}y_{i}\right)\right/
n}{\sum_{i=1}^{n}x_{i}^{2}-\left.\left(\sum_{i=1}^{n}x_{i}\right)^{2}\right/
n}.
\end{equation}
\end{Exercise}




\chapter{Multiple Linear Regression} 
\label{cha:multiple-linear-regression}


<<echo=FALSE, include=FALSE>>=
# This chapter's package dependencies
library(scatterplot3d)
library(lattice)
@

We know a lot about simple linear regression models, and a next step
is to study multiple regression models that have more than one
independent (explanatory) variable. In the discussion that follows we
will assume that we have \(p\) explanatory variables, where \(p > 1\).

The language is phrased in matrix terms -- for two reasons. First, it
is quicker to write and (arguably) more pleasant to read. Second, the
matrix approach will be required for later study of the subject; the
reader might as well be introduced to it now.

Most of the results are stated without proof or with only a cursory
justification. Those yearning for more should consult an advanced text
in linear regression for details, such as \textit{Applied Linear
  Regression Models} \cite{Neter1996} or \textit{Linear Models: Least
  Squares and Alternatives} \cite{Rao1999}.

\paragraph{What do I want them to know?}

\begin{itemize}
\item the basic MLR model, and how it relates to the SLR
\item how to estimate the parameters and use those estimates to make
  predictions
\item basic strategies to determine whether or not the model is doing a
  good job
\item a few thoughts about selected applications of the MLR, such as
  polynomial, interaction, and dummy variable models
\item some of the uses of residuals to diagnose problems
\item hints about what will be coming later
\end{itemize}

\section{The Multiple Linear Regression Model} \label{sec:the-mlr-model}

The first thing to do is get some better notation. We will write
\begin{equation}
\mathbf{Y}_{\mathrm{n}\times1}=
\begin{bmatrix}y_{1}\\
y_{2}\\
\vdots\\
y_{n}
\end{bmatrix},
\quad \mbox{and}\quad \mathbf{X}_{\mathrm{n}\times(\mathrm{p}+1)}=
\begin{bmatrix}1 & x_{11} & x_{21} & \cdots & x_{p1}\\
1 & x_{12} & x_{22} & \cdots & x_{p2}\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
1 & x_{1n} & x_{2n} & \cdots & x_{pn}
\end{bmatrix}.
\end{equation}
The vector \(\mathbf{Y}\) is called the \textit{response vector}
\index{response vector} and the matrix \(\mathbf{X}\) is called the
\textit{model matrix} \index{model matrix}. As in Chapter~\ref{cha:simple-linear-regression}, the most general assumption that
relates \(\mathbf{Y}\) to \(\mathbf{X}\) is
\begin{equation}
\mathbf{Y}=\mu(\mathbf{X})+\upepsilon,
\end{equation}
where \(\mu\) is some function (the \textit{signal}) and \(\upepsilon\) is
the \textit{noise} (everything else). We usually impose some structure
on \(\mu\) and \(\upepsilon\). In particular, the standard multiple
linear regression model \index{model!multiple linear regression}
assumes
\begin{equation}
\mathbf{Y}=\mathbf{X}\upbeta+\upepsilon,
\end{equation}
where the parameter vector \(\upbeta\) looks like
\begin{equation}
\upbeta_{(\mathrm{p}+1)\times1}=\begin{bmatrix}\beta_{0} & \beta_{1} & \cdots & \beta_{p}\end{bmatrix}^{\mathrm{T}},
\end{equation}
and the random vector
\(\upepsilon_{\mathrm{n}\times1}=\begin{bmatrix}\epsilon_{1} &
\epsilon_{2} & \cdots & \epsilon_{n}\end{bmatrix}^{\mathrm{T}}\) is
assumed to be distributed
\begin{equation}
\upepsilon\sim\mathsf{mvnorm}\left(\mathtt{mean}=\mathbf{0}_{\mathrm{n}\times1},\,\mathtt{sigma}=\sigma^{2}\mathbf{I}_{\mathrm{n}\times\mathrm{n}}\right).
\end{equation}

The assumption on \(\upepsilon\) is equivalent to the assumption that
\(\epsilon_{1}\), \(\epsilon_{2}\), \ldots, \(\epsilon_{n}\) are IID
\(\mathsf{norm}(\mathtt{mean}=0,\,\mathtt{sd}=\sigma)\). It is a
linear model because the quantity
\(\mu(\mathbf{X})=\mathbf{X}\upbeta\) is linear in the parameters
\(\beta_{0}\), \(\beta_{1}\), \ldots, \(\beta_{p}\). It may be helpful to
see the model in expanded form; the above matrix formulation is
equivalent to the more lengthy
\begin{equation}
Y_{i}=\beta_{0}+\beta_{1}x_{1i}+\beta_{2}x_{2i}+\cdots+\beta_{p}x_{pi}+\epsilon_{i},\quad i=1,2,\ldots,n.
\end{equation}



\begin{example}[Girth, Height, and Volume for Black Cherry trees.]
\index{Data sets!trees@\texttt{trees}} Measurements were made of the
girth, height, and volume of timber in 31 felled black cherry
trees. Note that girth is the diameter of the tree (in inches)
measured at 4 ft 6 in above the ground. The variables are
\end{example}

\begin{enumerate}
\item \texttt{Girth}: tree diameter in inches (denoted \(x_{1}\))
\item \texttt{Height}: tree height in feet (\(x_{2}\)).
\item \texttt{Volume}: volume of the tree in cubic feet. (\(y\))
\end{enumerate}

The data are in the \texttt{datasets} package \cite{datasets} and are already
on the search path; they can be viewed with

<<echo=TRUE>>=
head(trees)
@

Let us take a look at a visual display of the data. For multiple
variables, instead of a simple scatterplot we use a scatterplot matrix
which is made with the \texttt{splom} function in the \texttt{lattice}
package \cite{lattice} as shown below. The plot is shown in Figure~\ref{fig:splom-trees}.


<<splom-trees, echo=FALSE, fig=TRUE, include=FALSE, width=5>>=
splom(trees)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-splom-trees}
\end{center}
\caption{{\small A scatterplot matrix of the \texttt{trees} data.}}
\label{fig:splom-trees}
\end{figure}

The dependent (response) variable \texttt{Volume} is listed in the
first row of the scatterplot matrix. Moving from left to right, we see
an approximately linear relationship between \texttt{Volume} and the
independent (explanatory) variables \texttt{Height} and
\texttt{Girth}. A first guess at a model for these data might be
\begin{equation}
Y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\epsilon,
\end{equation}
in which case the quantity
\(\mu(x_{1},x_{2})=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}\) would
represent the mean value of \(Y\) at the point \((x_{1},x_{2})\).

\subsection{What does it mean?}

The interpretation is simple. The intercept \(\beta_{0}\) represents
the mean \texttt{Volume} when all other independent variables are
zero. The parameter \(\beta_{i}\) represents the change in mean
\texttt{Volume} when there is a unit increase in \(x_{i}\), while the
other independent variable is held constant. For the \texttt{trees}
data, \(\beta_{1}\) represents the change in average \texttt{Volume}
as \texttt{Girth} increases by one unit when the \texttt{Height} is
held constant, and \(\beta_{2}\) represents the change in average
\texttt{Volume} as \texttt{Height} increases by one unit when the
\texttt{Girth} is held constant.


In simple linear regression, we had one independent variable and our
linear regression surface was 1D, simply a line. In multiple
regression there are many independent variables and so our linear
regression surface will be many-D\ldots in general, a hyperplane. But
when there are only two explanatory variables the hyperplane is just
an ordinary plane and we can look at it with a 3D scatterplot.

One way to do this is with the \textsf{R} Commander in the \texttt{Rcmdr}
package \cite{Rcmdr}. It has a 3D scatterplot option under the
\texttt{Graphs} menu. It is especially great because the resulting
graph is dynamic; it can be moved around with the mouse, zoomed,
\textit{etc}. But that particular display does not translate well to a
printed book.

Another way to do it is with the \texttt{scatterplot3d} function in the
\texttt{scatterplot3d} package \cite{scatterplot3d}. The code follows, and
the result is shown in Figure~\ref{fig:3D-scatterplot-trees}.


<<3D-scatterplot-trees, echo=FALSE, fig=TRUE, include=FALSE, width=5>>=
s3d <- with(trees, scatterplot3d(Girth, Height, Volume,
            pch = 16, highlight.3d = TRUE, angle = 60))
attach(trees)
  my.lm <- lm(Volume ~ Girth + Height)
  s3d$plane3d(my.lm, lty.box = "solid")
detach(trees)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-3D-scatterplot-trees}
\end{center}
\caption{{\small A 3D scatterplot with regression plane for the \texttt{trees} data.}}
\label{fig:3D-scatterplot-trees}
\end{figure}



Looking at the graph we see that the data points fall close to a plane
in three dimensional space. (The plot looks remarkably good. In the
author's experience it is rare to see points fit so well to the plane
without some additional work.)

\section{Estimation and Prediction} \label{sec:estimation-and-prediction-mlr}

\subsection{Parameter estimates} \label{sub:mlr-parameter-estimates}

We will proceed exactly like we did in Section~\ref{sec:slr-estimation}. We know
\begin{equation}
\upepsilon\sim\mathsf{mvnorm}\left(\mathtt{mean}=\mathbf{0}_{\mathrm{n}\times1},\,\mathtt{sigma}=\sigma^{2}\mathbf{I}_{\mathrm{n}\times\mathrm{n}}\right),
\end{equation}
which means that \(\mathbf{Y}=\mathbf{X}\upbeta+\upepsilon\) has an \(\mathsf{mvnorm}\left(\mathtt{mean}=\mathbf{X}\upbeta,\,\mathtt{sigma}=\sigma^{2}\mathbf{I}_{\mathrm{n}\times\mathrm{n}}\right)\) distribution. Therefore, the likelihood function \index{likelihood function} is
\begin{equation}
L(\upbeta,\sigma)=\frac{1}{2\pi^{n/2}\sigma}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(\mathbf{Y}-\mathbf{X}\upbeta\right)^{\mathrm{T}}\left(\mathbf{Y}-\mathbf{X}\upbeta\right)\right\}.
\end{equation}

To \textit{maximize} the likelihood \index{maximum likelihood} in
\(\upbeta\), we need to \textit{minimize} the quantity
\(g(\upbeta)=\left(\mathbf{Y}-\mathbf{X}\upbeta\right)^{\mathrm{T}}\left(\mathbf{Y}-\mathbf{X}\upbeta\right)\). We
do this by differentiating \(g\) with respect to \(\upbeta\). (It may
be a good idea to brush up on the material in Appendices~\ref{sec:linear-algebra} and~\ref{sec:multivariable-calculus}.)
First we will rewrite \(g\):
\begin{equation}
g(\upbeta)=\mathbf{Y}^{\mathrm{T}}\mathbf{Y}-\mathbf{Y}^{\mathrm{T}}\mathbf{X}\upbeta-\upbeta^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\mathbf{Y}+\upbeta^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\mathbf{X}\upbeta,
\end{equation}
which can be further simplified to
\(g(\upbeta)=\mathbf{Y}^{\mathrm{T}}\mathbf{Y}-2\upbeta^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\mathbf{Y}+\upbeta^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\mathbf{X}\upbeta\)
since \(\upbeta^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\mathbf{Y}\) is
\(1\times1\) and thus equal to its transpose. Now we differentiate to
get
\begin{equation}
\frac{\partial g}{\partial\upbeta}=\mathbf{0}-2\mathbf{X}^{\mathrm{T}}\mathbf{Y}+2\mathbf{X}^{\mathrm{T}}\mathbf{X}\upbeta,
\end{equation}
since \(\mathbf{X}^{\mathrm{T}}\mathbf{X}\) is symmetric. Setting the
derivative equal to the zero vector yields the so called ``normal
equations'' \index{normal equations}
\begin{equation}
\mathbf{X}^{\mathrm{T}}\mathbf{X}\upbeta=\mathbf{X}^{\mathrm{T}}\mathbf{Y}.
\end{equation}

In the case that \(\mathbf{X}^{\mathrm{T}}\mathbf{X}\) is
invertible\footnote{We can find solutions of the normal equations even when
\(\mathbf{X}^{\mathrm{T}}\mathbf{X}\) is not of full rank, but the
topic falls outside the scope of this book. The interested reader can
consult an advanced text such as Rao \cite{Rao1999}.}, we may solve the equation for \(\upbeta\) to
get the maximum likelihood estimator of \(\upbeta\) which we denote by
\(\mathbf{b}\):
\begin{equation}
\label{eq:b-formula-matrix}
\mathbf{b}=\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}\mathbf{Y}.
\end{equation}





\begin{rem}[]
The formula in Equation \eqref{eq:b-formula-matrix} is convenient for
mathematical study but is inconvenient for numerical
computation. Researchers have devised much more efficient algorithms
for the actual calculation of the parameter estimates, and we do not
explore them here.
\end{rem}



\begin{rem}[]
We have only found a critical value, and have not actually shown that
the critical value is a minimum. We omit the details and refer the
interested reader to \cite{Rao1999}.
\end{rem}


\subsubsection{How to do it with \textsf{R}}

We do all of the above just as we would in simple linear
regression. The powerhouse is the \texttt{lm} \index{lm@\texttt{lm}}
function. Everything else is based on it. We separate explanatory
variables in the model formula by a plus sign.

<<echo=FALSE>>=
fit <- lm(Volume ~ Girth + Height, data = trees)
@

<<echo=TRUE>>=
trees.lm <- lm(Volume ~ Girth + Height, data = trees)
trees.lm
@

We see from the output that for the \texttt{trees} data our parameter
estimates are \[ \mathbf{b}=\begin{bmatrix}-58.0 & 4.7 &
0.3\end{bmatrix}, \] and consequently our estimate of the mean
response is \(\hat{\mu}\) given by
\begin{alignat}{1}
\hat{\mu}(x_{1},x_{2}) = & \ b_{0} + b_{1} x_{1} + b_{2}x_{2},\\ \approx & -58.0 + 4.7 x_{1} + 0.3 x_{2}.
\end{alignat}

We could see the entire model matrix \(\mathbf{X}\) with the
\texttt{model.matrix} \index{model.matrix@\texttt{model.matrix}}
function, but in the interest of brevity we only show the first few
rows.

<<echo=TRUE>>=
head(model.matrix(trees.lm))
@


\subsection{Point Estimates of the Regression Surface} \label{sub:mlr-point-est-regsurface}

The parameter estimates \(\mathbf{b}\) make it easy to find the fitted
values \index{fitted values}, \(\hat{\mathbf{Y}}\). We write them
individually as \(\hat{Y}_{i}\), \(i=1,2,\ldots,n\), and recall that
they are defined by
\begin{eqnarray}
\hat{Y}_{i} & = & \hat{\mu}(x_{1i},x_{2i}),\\
 & = & b_{0}+b_{1}x_{1i}+b_{2}x_{2i},\quad i=1,2,\ldots,n.
\end{eqnarray}
They are expressed more compactly by the matrix equation
\begin{equation}
\hat{\mathbf{Y}}=\mathbf{X}\mathbf{b}.
\end{equation}
From Equation \eqref{eq:b-formula-matrix} we know that
\(\mathbf{b}=\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}\mathbf{Y}\),
so we can rewrite
\begin{eqnarray}
\hat{\mathbf{Y}} & = & \mathbf{X}\left[\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}\mathbf{Y}\right],\\
 & = & \mathbf{H}\mathbf{Y},
\end{eqnarray}
where
\(\mathbf{H}=\mathbf{X}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}\)
is appropriately named \textit{the hat matrix} \index{hat matrix} because it
``puts the hat on \(\mathbf{Y}\)''. The hat matrix is very important in
later courses. Some facts about \(\mathbf{H}\) are

\begin{itemize}
\item \(\mathbf{H}\) is a symmetric square matrix, of dimension
  \(\mathrm{n}\times\mathrm{n}\).
\item The diagonal entries \(h_{ii}\) satisfy \(0\leq h_{ii}\leq1\)
  (compare to Equation \eqref{eq:slr-leverage-between}).
\item The trace is \(\mathrm{tr}(\mathbf{H})=p\).
\item \(\mathbf{H}\) is \textit{idempotent} (also known as a
  \textit{projection matrix}) which means that
  \(\mathbf{H}^{2}=\mathbf{H}\). The same is true of
  \(\mathbf{I}-\mathbf{H}\).
\end{itemize}

Now let us write a column vector
\(\mathbf{x}_{0}=(x_{10},x_{20})^{\mathrm{T}}\) to denote given values
of the explanatory variables \texttt{Girth ==} \(x_{10}\) and \texttt{Height ==}
\(x_{20}\). These values may match those of the collected data, or
they may be completely new values not observed in the original data
set. We may use the parameter estimates to find
\(\hat{Y}(\mathbf{x}_{0})\), which will give us
\begin{enumerate}
\item an estimate of \(\mu(\mathbf{x}_{0})\), the mean value of a future
   observation at \(\mathbf{x}_{0}\), and
\item a prediction for \(Y(\mathbf{x}_{0})\), the actual value of a
   future observation at \(\mathbf{x}_{0}\).
\end{enumerate}
We can represent \(\hat{Y}(\mathbf{x}_{0})\) by the matrix equation
\begin{equation}
\label{eq:mlr-single-yhat-matrix}
\hat{Y}(\mathbf{x}_{0})=\mathbf{x}_{0}^{\mathrm{T}}\mathbf{b},
\end{equation}
which is just a fancy way to write
\begin{equation}
\hat{Y}(x_{10},x_{20})=b_{0}+b_{1}x_{10}+b_{2}x_{20}.
\end{equation}



\begin{example}[]
If we wanted to predict the average volume of black cherry trees that
have \texttt{Girth = 15} in and are \texttt{Height = 77} ft tall then we would use
the estimate
\end{example}

\begin{alignat*}{1}
\hat{\mu}(15,\,77)= & -58+4.7(15)+0.3(77),\\
\approx & 35.6\text{ft}^{3}.
\end{alignat*}

We would use the same estimate \(\hat{Y}=35.6\) to predict the
measured \texttt{Volume} of another black cherry tree -- yet to be observed
-- that has \texttt{Girth = 15} in and is \texttt{Height = 77} ft tall.


\subsubsection{How to do it with \textsf{R}}

The fitted values are stored inside \texttt{trees.lm} and may be accessed
with the \texttt{fitted} function. We only show the first five fitted values.

<<echo=TRUE>>=
fitted(trees.lm)[1:5]
@


The syntax for general prediction does not change much from simple
linear regression. The computations are done with the \texttt{predict}
function as described below.

The only difference from SLR is in the way we tell \textsf{R} the
values of the explanatory variables for which we want predictions. In
SLR we had only one independent variable but in MLR we have many (for
the \texttt{trees} data we have two). We will store values for the
independent variables in the data frame \texttt{new}, which has two columns
(one for each independent variable) and three rows (we shall make
predictions at three different locations).

<<echo=TRUE>>=
new <- data.frame(Girth = c(9.1, 11.6, 12.5),
                  Height = c(69, 74, 87))
@

We can view the locations at which we will predict:

<<echo=TRUE>>=
new
@


We continue just like we would have done in SLR.

<<echo=TRUE>>=
predict(trees.lm, newdata = new)
@

<<echo=FALSE, include=FALSE>>=
treesFIT <- round(predict(trees.lm, newdata = new), 1)
@


\textbf{Example.} Using the \texttt{trees} data,

\begin{enumerate}
\item Report a point estimate of the mean \texttt{Volume} of a tree of \texttt{Girth}
   9.1 in and \texttt{Height} 69 ft.  The fitted value for \(x_{1}=9.1\) and
   \(x_{2} = 69\) is \Sexpr{treesFIT[1]}, so a point estimate
   would be \Sexpr{treesFIT[1]} cubic feet.
\item Report a point prediction for and a 95\% prediction interval for the
   \texttt{Volume} of a hypothetical tree of \texttt{Girth} 12.5 in and \texttt{Height} 87
   ft.  The fitted value for \(x_{1} = 12.5\) and \(x_{2} = 87\) is
   \Sexpr{treesFIT[3]}, so a point prediction for the \texttt{Volume}
   is \Sexpr{treesFIT[3]} cubic feet.
\end{enumerate}

\subsection{Mean Square Error and Standard Error} \label{sub:mlr-mse-se}

The residuals are given by
\begin{equation}
\mathbf{E}=\mathbf{Y}-\hat{\mathbf{Y}}=\mathbf{Y}-\mathbf{H}\mathbf{Y}=(\mathbf{I}-\mathbf{H})\mathbf{Y}.
\end{equation}
Now we can use Theorem~\ref{thm:mvnorm-dist-matrix-prod} to see that the
residuals are distributed
\begin{equation}
\mathbf{E}\sim\mathsf{mvnorm}(\mathtt{mean}=\mathbf{0},\,\mathtt{sigma}=\sigma^{2}(\mathbf{I}-\mathbf{H})),
\end{equation}
since
\((\mathbf{I}-\mathbf{H})\mathbf{X}\upbeta=\mathbf{X}\upbeta-\mathbf{X}\upbeta=\mathbf{0}\)
and
\((\mathbf{I}-\mathbf{H})\,(\sigma^{2}\mathbf{I})\,(\mathbf{I}-\mathbf{H})^{\mathrm{T}}=\sigma^{2}(\mathbf{I}-\mathbf{H})^{2}=\sigma^{2}(\mathbf{I}-\mathbf{H})\). Thesum
of squared errors \(SSE\) is just
\begin{equation}
SSE=\mathbf{E}^{\mathrm{T}}\mathbf{E}=\mathbf{Y}^{\mathrm{T}}(\mathbf{I}-\mathbf{H})(\mathbf{I}-\mathbf{H})\mathbf{Y}=\mathbf{Y}^{\mathrm{T}}(\mathbf{I}-\mathbf{H})\mathbf{Y}.
\end{equation}
Recall that in SLR we had two parameters (\(\beta_{0}\) and
\(\beta_{1}\)) in our regression model and we estimated \(\sigma^{2}\)
with \(s^{2}=SSE/(n-2)\). In MLR, we have \(p+1\) parameters in our
regression model and we might guess that to estimate \(\sigma^{2}\) we
would use the \textit{mean square error} \(S^{2}\) defined by
\begin{equation}
S^{2}=\frac{SSE}{n-(p+1)}.
\end{equation}
That would be a good guess. The \textit{residual standard error} is
\(S=\sqrt{S^{2}}\).

\subsubsection{How to do it with \textsf{R}}

The residuals are also stored with \texttt{trees.lm} and may be accessed with
the \texttt{residuals} function. We only show the first five residuals.

<<echo=TRUE>>=
residuals(trees.lm)[1:5]
@

The \texttt{summary} function output (shown later) lists the \texttt{Residual
Standard Error} which is just \(S=\sqrt{S^{2}}\). It is stored in the
\texttt{sigma} component of the \texttt{summary} object.

<<echo=TRUE>>=
treesumry <- summary(trees.lm)
treesumry$sigma
@

For the \texttt{trees} data we find \(s \approx\) \Sexpr{round(treesumry$sigma,3)}.

\subsection{Interval Estimates of the Parameters} \label{sub:mlr-interval-est-params}

We showed in Section~\ref{sub:mlr-parameter-estimates} that
\(\mathbf{b}=\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}\mathbf{Y}\),
which is really just a big matrix -- namely
\(\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}\)
-- multiplied by \(\mathbf{Y}\). It stands to reason that the sampling
distribution of \(\mathbf{b}\) would be intimately related to the
distribution of \(\mathbf{Y}\), which we assumed to be
\begin{equation}
\mathbf{Y}\sim\mathsf{mvnorm}\left(\mathtt{mean}=\mathbf{X}\upbeta,\,\mathtt{sigma}=\sigma^{2}\mathbf{I}\right).
\end{equation}
Now recall Theorem~\ref{thm:mvnorm-dist-matrix-prod} that we said we were
going to need eventually (the time is now). That proposition
guarantees that
\begin{equation}
\label{eq:distn-b-mlr}
\mathbf{b}\sim\mathsf{mvnorm}\left(\mathtt{mean}=\upbeta,\,\mathtt{sigma}=\sigma^{2}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\right),
\end{equation}
since
\begin{equation}
\mathbb{E}\mathbf{b}=\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}(\mathbf{X}\upbeta)=\upbeta,
\end{equation}
and
\begin{equation}
\mbox{Var}(\mathbf{b})=\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm{T}}(\sigma^{2}\mathbf{I})\mathbf{X}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}=\sigma^{2}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1},
\end{equation}
the first equality following because the matrix
\(\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\) is symmetric.

There is a lot that we can glean from Equation \eqref{eq:distn-b-mlr}. First,
it follows that the estimator \(\mathbf{b}\) is unbiased (see Section~\ref{sec:point-estimation}). Second, the variances of \(b_{0}\), \(b_{1}\),
\ldots, \(b_{n}\) are exactly the diagonal elements of
\(\sigma^{2}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\),
which is completely known except for that pesky parameter
\(\sigma^{2}\). Third, we can estimate the standard error of \(b_{i}\)
(denoted \(S_{b_{i}}\)) with the mean square error \(S\) (defined in
the previous section) multiplied by the corresponding diagonal element
of \(\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\). Finally,
given estimates of the standard errors we may construct confidence
intervals for \(\beta_{i}\) with an interval that looks like
\begin{equation}
b_{i}\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-p-1)S_{b_{i}}.
\end{equation}
The degrees of freedom for the Student's \(t\) distribution\footnote{We are taking great leaps over the mathematical
details. In particular, we have yet to show that \(s^{2}\) has a
chi-square distribution and we have not even come close to showing
that \(b_{i}\) and \(s_{b_{i}}\) are independent. But these are
entirely outside the scope of the present book and the reader may rest
assured that the proofs await in later classes. See C.R. Rao for more.}
are the same as the denominator of \(S^{2}\).


\subsubsection{How to do it with \textsf{R}}

To get confidence intervals for the parameters we need only use
\texttt{confint} \index{confint@\texttt{confint}}:

<<echo=TRUE>>=
confint(trees.lm)
@

<<echo=FALSE, include=FALSE>>=
treesPAR <- round(confint(trees.lm), 1)
@

For example, using the calculations above we say that for the
regression model \texttt{Volume ~ Girth + Height} we are 95\% confident that
the parameter \(\beta_{1}\) lies somewhere in the interval
\Sexpr{treesPAR[2,1]} to \Sexpr{treesPAR[2,2]}.

\subsection{Confidence and Prediction Intervals}

We saw in Section~\ref{sub:mlr-point-est-regsurface} how to make
point estimates of the mean value of additional observations and
predict values of future observations, but how good are our estimates?
We need confidence and prediction intervals to gauge their accuracy,
and lucky for us the formulas look similar to the ones we saw in SLR.

In Equation \eqref{eq:mlr-single-yhat-matrix} we wrote $\hat{Y}(\mathbf{x}_0)=\mathbf{x}_0^{\mathrm{T}}\mathbf{b}$, and
in Equation \eqref{eq:distn-b-mlr} we saw that
\begin{equation}
\mathbf{b}\sim\mathsf{mvnorm}\left(\mathtt{mean} = \upbeta,\,\mathtt{sigma}=\sigma^{2}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\right).
\end{equation}
The following is therefore immediate from Theorem~\ref{thm:mvnorm-dist-matrix-prod}:
\begin{equation}
\hat{Y}(\mathbf{x}_{0})\sim\mathsf{mvnorm}\left(\mathtt{mean}=\mathbf{x}_{0}^{\mathrm{T}}\upbeta,\,\mathtt{sigma}=\sigma^{2}\mathbf{x}_{0}^{\mathrm{T}}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{x}_{0}\right).
\end{equation}
It should be no surprise that confidence intervals for the mean value
of a future observation at the location
\(\mathbf{x}_{0}=\begin{bmatrix}x_{10} & x_{20} & \ldots &
x_{p0}\end{bmatrix}^{\mathrm{T}}\) are given by
\begin{equation}
\hat{Y}(\mathbf{x}_{0})\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-p-1)\, S\sqrt{\mathbf{x}_{0}^{\mathrm{T}}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{x}_{0}}.
\end{equation}
Intuitively,
\(\mathbf{x}_{0}^{\mathrm{T}}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{x}_{0}\)
measures the distance of \(\mathbf{x}_{0}\) from the center of the
data. The degrees of freedom in the Student's \(t\) critical value are
\(n-(p+1)\) because we need to estimate \(p+1\) parameters.

Prediction intervals for a new observation at \(\mathbf{x}_{0}\) are
given by
\begin{equation}
\hat{Y}(\mathbf{x}_{0})\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-p-1)\, S\sqrt{1+\mathbf{x}_{0}^{\mathrm{T}}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\mathbf{x}_{0}}.
\end{equation}
The prediction intervals are wider than the confidence intervals, just as in Section~\ref{sub:slr-interval-est-regline}.

\subsubsection{How to do it with \textsf{R}}

The syntax is identical to that used in SLR, with the proviso that we
need to specify values of the independent variables in the data frame
\texttt{new} as we did in Section~\ref{sub:slr-interval-est-regline} (which we
repeat here for illustration).

<<echo=TRUE>>=
new <- data.frame(Girth = c(9.1, 11.6, 12.5),
                  Height = c(69, 74, 87))
@

Confidence intervals are given by

<<echo=TRUE>>=
predict(trees.lm, newdata = new, interval = "confidence")
@

<<echo=FALSE, include=FALSE>>=
treesCI <- round(predict(trees.lm, newdata = new, interval = "confidence"), 1)
@

Prediction intervals are given by

<<echo=TRUE>>=
predict(trees.lm, newdata = new, interval = "prediction")
@

<<echo=FALSE, include=FALSE>>=
treesPI <- round(predict(trees.lm, newdata = new, interval = "prediction"), 1)
@

As before, the interval type is decided by the \texttt{interval} argument and
the default confidence level is 95\% (which can be changed with the
\texttt{level} argument).

\textbf{Example.} Using the \texttt{trees} data,

\begin{enumerate}
\item Report a 95\% confidence interval for the mean \texttt{Volume} of a tree of
   \texttt{Girth} 9.1 in and \texttt{Height} 69 ft. The 95\% CI is given by
   \Sexpr{treesCI[1,2]} to \Sexpr{treesCI[1,3]}, so with 95\%
   confidence the mean \texttt{Volume} lies somewhere between
   \Sexpr{treesCI[1, 2]} cubic feet and \Sexpr{treesCI[1,3]} cubic feet.
\item Report a 95\% prediction interval for the \texttt{Volume} of a hypothetical
   tree of \texttt{Girth} 12.5 in and \texttt{Height} 87 ft. The 95\% prediction
   interval is given by \Sexpr{treesCI[3,2]} to
   \Sexpr{treesCI[3,3]}, so with 95\% confidence we may assert that
   the hypothetical \texttt{Volume} of a tree of \texttt{Girth} 12.5 in and \texttt{Height}
   87 ft would lie somewhere between \Sexpr{treesCI[3, 2]}
   cubic feet and \Sexpr{treesCI[3,3]} feet.
\end{enumerate}

\section{Model Utility and Inference} \label{sec:model-utility-and-mlr}


\subsection{Multiple Coefficient of Determination}

We saw in Section~\ref{sub:mlr-mse-se} that the error sum of squares \(SSE\) can be conveniently written in MLR as
\begin{equation}
\label{eq:mlr-sse-matrix}
SSE=\mathbf{Y}^{\mathrm{T}}(\mathbf{I}-\mathbf{H})\mathbf{Y}.
\end{equation}
It turns out that there are equally convenient formulas for the total sum of squares \(SSTO\) and the regression sum of squares \(SSR\). They are:
\begin{alignat}{1}
\label{eq:mlr-ssto-matrix}
SSTO= & \mathbf{Y}^{\mathrm{T}}\left(\mathbf{I}-\frac{1}{n}\mathbf{J}\right)\mathbf{Y}
\end{alignat}
and
\begin{alignat}{1}
\label{eq:mlr-ssr-matrix}
SSR= & \mathbf{Y}^{\mathrm{T}}\left(\mathbf{H}-\frac{1}{n}\mathbf{J}\right)\mathbf{Y}.
\end{alignat}
(The matrix \(\mathbf{J}\) is defined in Appendix~\ref{sec:linear-algebra}.) Immediately from Equations
\eqref{eq:mlr-sse-matrix}, \eqref{eq:mlr-ssto-matrix}, and
\eqref{eq:mlr-ssr-matrix} we get the \textit{Anova Equality}
\begin{equation}
SSTO=SSE+SSR.
\end{equation}
(See Exercise~\ref{exr:anova-equality}.) We define the
\textit{multiple coefficient of determination} by the formula
\begin{equation}
R^{2}=1-\frac{SSE}{SSTO}.
\end{equation}

We interpret \(R^{2}\) as the proportion of total variation that is
explained by the multiple regression model. In MLR we must be careful,
however, because the value of \(R^{2}\) can be artificially inflated
by the addition of explanatory variables to the model, regardless of
whether or not the added variables are useful with respect to
prediction of the response variable. In fact, it can be proved that
the addition of a single explanatory variable to a regression model
will increase the value of \(R^{2}\), \textit{no matter how worthless} the
explanatory variable is. We could model the height of the ocean tides,
then add a variable for the length of cheetah tongues on the Serengeti
plain, and our \(R^{2}\) would inevitably increase.

This is a problem, because as the philosopher, Occam, once said:
``causes should not be multiplied beyond necessity''. We address the
problem by penalizing \(R^{2}\) when parameters are added to the
model. The result is an \textit{adjusted} \(R^{2}\) which we denote by
\(\overline{R}^{2}\).
\begin{equation}
\overline{R}^{2}=\left(R^{2}-\frac{p}{n-1}\right)\left(\frac{n-1}{n-p-1}\right).
\end{equation}
It is good practice for the statistician to weigh both \(R^{2}\) and
\(\overline{R}^{2}\) during assessment of model utility. In many cases
their values will be very close to each other. If their values differ
substantially, or if one changes dramatically when an explanatory
variable is added, then (s)he should take a closer look at the
explanatory variables in the model.

\subsubsection{How to do it with \textsf{R}}
For the \texttt{trees} data, we can get \(R^{2}\) and \(\overline{R}^{2}\)
from the \texttt{summary} output or access the values directly by name as
shown (recall that we stored the \texttt{summary} object in \texttt{treesumry}).

<<echo=TRUE>>=
treesumry$r.squared
@


<<echo=TRUE>>=
treesumry$adj.r.squared
@

High values of \(R^{2}\) and $\overline{R}^2$ such as these
indicate that the model fits very well, which agrees with what we saw
in Figure~\ref{fig:3D-scatterplot-trees}.

\subsection{Overall F-Test} \label{sub:mlr-overall-f-test}

Another way to assess the model's utility is to to test the hypothesis
\[ H_{0}:\beta_{1}=\beta_{2}=\cdots=\beta_{p}=0\mbox{ versus
}H_{1}:\mbox{ at least one $\beta_{i}\neq0$}.  \] The idea is that if
all \(\beta_{i}\)'s were zero, then the explanatory variables
\(X_{1},\ldots,X_{p}\) would be worthless predictors for the response
variable \(Y\). We can test the above hypothesis with the overall
\(F\) statistic, which in MLR is defined by
\begin{equation}
F=\frac{SSR/p}{SSE/(n-p-1)}.
\end{equation}
When the regression assumptions hold and under \(H_{0}\), it can be
shown that
\(F\sim\mathsf{f}(\mathtt{df1}=p,\,\mathtt{df2}=n-p-1)\). We reject
\(H_{0}\) when \(F\) is large, that is, when the explained variation
is large relative to the unexplained variation.

\subsubsection{How to do it with \textsf{R}}

The overall \(F\) statistic and its associated \textit{p}-value is listed at
the bottom of the \texttt{summary} output, or we can access it directly by
name; it is stored in the \texttt{fstatistic} component of the \texttt{summary}
object.

<<echo=TRUE>>=
treesumry$fstatistic
@

For the \texttt{trees} data, we see that \( F = \) \Sexpr{treesumry$fstatistic[1]} with a \textit{p}-value =< \texttt{2.2e-16}. Consequently
we reject \(H_{0}\), that is, the data provide strong evidence that
not all \(\beta_{i}\)'s are zero.

\subsection{Student's t Tests} \label{sub:mlr-students-t-tests}

We know that
\begin{equation}
\mathbf{b}\sim\mathsf{mvnorm}\left(\mathtt{mean}=\upbeta,\,\mathtt{sigma}=\sigma^{2}\left(\mathbf{X}^{\mathrm{T}}\mathbf{X}\right)^{-1}\right)
\end{equation}
and we have seen how to test the hypothesis \(H_{0}:\beta_{1}=\beta_{2}=\cdots=\beta_{p}=0\), but let us now consider the test
\begin{equation}
H_{0}:\beta_{i}=0\mbox{ versus }H_{1}:\beta_{i}\neq0,
\end{equation}
where \(\beta_{i}\) is the coefficient for the \(i^{\textrm{th}}\) independent variable. We test the hypothesis by calculating a statistic, examining its null distribution, and rejecting
\(H_{0}\) if the \textit{p}-value is small. If \(H_{0}\) is rejected, then we conclude that there is a significant relationship between
\(Y\) and \(x_{i}\) \textit{in the regression model}
\(Y\sim(x_{1},\ldots,x_{p})\). This last part of the sentence is very important because the significance of the variable
\(x_{i}\) sometimes depends on the presence of other independent variables in the model\footnote{In other words, a variable might be highly significant one moment but then fail to be significant when another variable is added to the model. When this happens it often indicates a problem with the explanatory variables, such as \textit{multicollinearity}. See Section~\ref{sub:multicollinearity}.}.

To test the hypothesis we go to find the sampling distribution of \(
b_{i} \), the estimator of the corresponding parameter \(\beta_{i}\), when the null hypothesis is true. We saw in Section~\ref{sub:mlr-interval-est-params} that
\begin{equation}
T_{i}=\frac{b_{i}-\beta_{i}}{S_{b_{i}}}
\end{equation}
has a Student's \(t\) distribution with \(n-(p+1)\) degrees of
freedom. (Remember, we are estimating \(p+1\) parameters.)
Consequently, under the null hypothesis \(H_{0}:\beta_{i}=0\) the
statistic \(t_{i}=b_{i}/S_{b_{i}}\) has a
\(\mathsf{t}(\mathtt{df}=n-p-1)\) distribution.

\subsubsection{How to do it with \textsf{R}}

The Student's \(t\) tests for significance of the individual
explanatory variables are shown in the \texttt{summary} output.

<<echo=TRUE>>=
treesumry
@

We see from the \textit{p-values} that there is a significant linear
relationship between \texttt{Volume} and \texttt{Girth} and between
\texttt{Volume} and \texttt{Height} in the regression model
\texttt{Volume ~ Girth + Height}. Further, it appears that the
\texttt{Intercept} is significant in the aforementioned model.

\section{Polynomial Regression} \label{sec:polynomial-regression}

\subsection{Quadratic Regression Model}

In each of the previous sections we assumed that \(\mu\) was a linear function of the explanatory variables. For example, in SLR we assumed that \(\mu(x)=\beta_{0}+\beta_{1}x\), and in our previous MLR examples we assumed
\(\mu(x_{1},x_{2}) = \beta_{0}+\beta_{1}x_{1} + \beta_{2}x_{2}\). In every case the scatterplots indicated that our assumption was
reasonable. Sometimes, however, plots of the data suggest that the linear model is incomplete and should be modified.


<<scatterplot-volume-girth-trees, echo=FALSE, fig=TRUE, include=FALSE, height=3.25,width=5>>=
plot(Volume ~ Girth, data = trees)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-scatterplot-volume-girth-trees}
\end{center}
\caption{{\small Scatterplot of \texttt{Volume} versus \texttt{Girth} for the \texttt{trees} data.}}
\label{fig:scatterplot-volume-girth-trees}
\end{figure}

For example, let us examine a scatterplot of \texttt{Volume} versus \texttt{Girth} a little more closely. See Figure~\ref{fig:scatterplot-volume-girth-trees}. There might be a slight
curvature to the data; the volume curves ever so slightly upward as the girth increases. After looking at the plot we might try to capture the curvature with a mean response such as
\begin{equation}
\mu(x_{1})=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{1}^{2}.
\end{equation}
The model associated with this choice of \(\mu\) is
\begin{equation}
Y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{1}^{2}+\epsilon.
\end{equation}
The regression assumptions are the same. Almost everything indeed is the same. In fact, it is still called a ``linear regression model'', since the mean response \(\mu\) is linear \textit{in the parameters} \(\beta_{0}\), \(\beta_{1}\), and \(\beta_{2}\).

\textit{However, there is one important difference.} When we introduce the squared variable in the model we inadvertently also introduce strong dependence between the terms which can cause significant numerical problems when it comes time to calculate the parameter estimates. Therefore, we should usually rescale the independent variable to have mean zero (and even variance one if we wish) \textit{before} fitting the model. That is, we replace the \(x_{i}\)'s with \(x_{i}-\overline{x}\) (or \((x_{i}-\overline{x})/s\)) before fitting the model\footnote{Rescaling the data gets the job done but a better way to avoid the multicollinearity introduced by the higher order terms is with \textit{orthogonal polynomials}, whose coefficients are chosen just right so that the polynomials are not correlated with each other. This is beginning to linger outside the scope of this book, however, so we will content ourselves with a brief mention and
  then stick with the rescaling approach in the discussion that
  follows. A nice example of orthogonal polynomials in action can be run with \texttt{example(cars)}.}.


\subsubsection{How to do it with \textsf{R}}

There are multiple ways to fit a quadratic model to the variables
\texttt{Volume} and \texttt{Girth} using \textsf{R}.

\begin{enumerate}
\item One way would be to square the values for \texttt{Girth} and save them in a vector \texttt{Girthsq}. Next, fit the linear model \texttt{Volume ~ Girth + Girthsq}.
\item A second way would be to use the \textit{insulate} function in \textsf{R}, denoted by \texttt{I}: \texttt{Volume ~ Girth + I(Girth\^2)} The second method is shorter than the first but the end result is the same. And once we calculate and store the fitted model (in, say, \texttt{treesquad.lm}) all of the previous comments regarding \textsf{R} apply.
\item A third and ``right'' way to do it is with orthogonal polynomials: \texttt{Volume ~ poly(Girth, degree = 2)} See \texttt{?poly} and \texttt{?cars} for more information. Note that we can recover the approach in 2 with \texttt{poly(Girth, degree = 2, raw = TRUE)}.
\end{enumerate}



\begin{example}[]
We will fit the quadratic model to the \texttt{trees} data and display the results with \texttt{summary}, being careful to rescale the data before fitting the model. We may rescale the \texttt{Girth} variable to have zero mean and unit variance on-the-fly with the \texttt{scale} function.
\end{example}

<<echo=TRUE>>=
treesquad.lm <- lm(Volume ~ scale(Girth) + I(scale(Girth)^2),
                   data = trees)
summary(treesquad.lm)
@

We see that the \(F\) statistic indicates the overall model including
\texttt{Girth} and \texttt{Girth\^2} is significant. Further, there is strong
evidence that both \texttt{Girth} and \texttt{Girth\^2} are significantly related to
\texttt{Volume}. We may examine a scatterplot together with the fitted
quadratic function using the \texttt{lines} function, which adds a line to the plot tracing the estimated mean response.


<<fitting-the-quadratic, echo=FALSE, fig=TRUE, include=FALSE, height=3.25,width=5>>=
plot(Volume ~ scale(Girth), data = trees)
lines(fitted(treesquad.lm) ~ scale(Girth), data = trees)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-fitting-the-quadratic}
\end{center}
\caption{{\small A quadratic model for the \texttt{trees} data.}}
\label{fig:fitting-the-quadratic}
\end{figure}

The plot is shown in Figure~\ref{fig:fitting-the-quadratic}. Pay
attention to the scale on the \(x\)-axis: it is on the scale of the transformed \texttt{Girth} data and not on the original scale.



\begin{rem}[]
  When a model includes a quadratic term for an independent variable, it is customary to also include the linear term in the model. The principle is called \textit{parsimony}. More generally, if the researcher decides to include \(x^{m}\) as a term in the model, then (s)he should also include all lower order terms \(x\), \(x^{2}\), \ldots,\(x^{m-1}\) in the model.
\end{rem}

We do estimation/prediction the same way that we did in Section~\ref{sub:mlr-point-est-regsurface}, except we do not need a \texttt{Height} column in the dataframe \texttt{new} since the variable is not included in the quadratic model.

<<echo=TRUE>>=
new <- data.frame(Girth = c(9.1, 11.6, 12.5))
predict(treesquad.lm, newdata = new, interval = "prediction")
@

The predictions and intervals are slightly different from what they were previously. Notice that it was not necessary to rescale the \texttt{Girth} prediction data before input to the \texttt{predict} function; the model did the rescaling for us automatically.



\begin{rem}[]
We have mentioned on several occasions that it is important to rescale the explanatory variables for polynomial regression. Watch whathappens if we ignore this advice:
\end{rem}

<<echo=TRUE>>=
summary(lm(Volume ~ Girth + I(Girth^2), data = trees))
@

Now nothing is significant in the model except \texttt{Girth\^2}. We could delete the \texttt{Intercept} and \texttt{Girth} from the model, but the model would no longer be \textit{parsimonious}. A novice may see the output and be confused about how to proceed, while the seasoned statistician recognizes immediately that \texttt{Girth} and \texttt{Girth\^2} are highly correlated (see Section~\ref{sub:multicollinearity}). The only remedy to this ailment is to rescale \texttt{Girth}, which we should have done in the first place.


\textbf{Note:} The \texttt{trees} data do not have any qualitative explanatory variables, so we will construct one for illustrative purposes\footnote{This procedure of replacing a continuous variable by a discrete/qualitative one is called \textit{binning}, and is almost \textit{never} the right thing to do. We are in a bind at this point, however, because we have invested this chapter in the \texttt{trees} data and I do not want to switch mid-discussion. I am currently searching for a data set with pre-existing qualitative variables that also conveys the same points present in the trees data, and when I find it I will update this chapter accordingly.}.  We will leave the \texttt{Girth} variable alone, but we will replace the variable \texttt{Height} by a new variable \texttt{Tall} which indicates whether or not the cherry tree is taller than a certain threshold (which for the sake of argument will be the sample median height of 76\,ft). That is, \texttt{Tall} will be defined
by
\begin{equation} 
\mathtt{Tall} = \begin{cases} \mathtt{yes}, & \mbox{if }\mathtt{Height} > 76,\\ \mathtt{no}, & \mbox{if }\mathtt{Height}\leq 76. \end{cases}
\end{equation}

We can construct \texttt{Tall} very quickly in \textsf{R} with the \texttt{cut} function:

<<echo=TRUE>>=
trees$Tall <- cut(trees$Height, breaks = c(-Inf, 76, Inf),
                  labels = c("no","yes"))
trees$Tall[1:5]
@

Note that \texttt{Tall} is automatically generated to be a factor with the labels in the correct order. See \texttt{?cut} for more.

Once we have \texttt{Tall}, we include it in the regression model just like we would any other variable. It is handled internally in a special way. Define a ``dummy variable'' \texttt{Tallyes} that takes values
\begin{equation} 
\mathtt{Tallyes} = \begin{cases} 1, & \mbox{if
    }\mathtt{Tall}=\mathtt{yes},\\ 0, &
    \mbox{otherwise.} \end{cases} 
    \end{equation} 
    
That is, \texttt{Tallyes} is an \textit{indicator variable} which indicates when a respective tree is tall. The model may now be written as
\begin{equation}
\mathtt{Volume}=\beta_{0}+\beta_{1}\mathtt{Girth}+\beta_{2}\mathtt{Tallyes}+\epsilon.
\end{equation}
Let us take a look at what this definition does to the mean
response. Trees with \texttt{Tall = yes} will have the mean response
\begin{equation}
\mu(\mathtt{Girth})=(\beta_{0}+\beta_{2})+\beta_{1}\mathtt{Girth},
\end{equation}
while trees with \texttt{Tall = no} will have the mean response
\begin{equation}
\mu(\mathtt{Girth})=\beta_{0}+\beta_{1}\mathtt{Girth}.
\end{equation}
In essence, we are fitting two regression lines: one for tall trees, and one for short trees. The regression lines have the same slope but they have different \(y\) intercepts (which are exactly \(|\beta_{2}|\) far apart).

\subsection{How to do it with \textsf{R}}

The important thing is to double check that the qualitative variable in question is stored as a factor. The way to check is with the \texttt{class} command. For example,

<<echo=TRUE>>=
class(trees$Tall)
@

If the qualitative variable is not yet stored as a factor then we may
convert it to one with the \texttt{factor} command. See Section~\ref{sub:qualitative-data}. Other than this we perform MLR as we
normally would.

<<echo=TRUE>>=
treesdummy.lm <- lm(Volume ~ Girth + Tall, data = trees)
summary(treesdummy.lm)
@

From the output we see that all parameter estimates are statistically
significant and we conclude that the mean response differs for trees
with \texttt{Tall = yes} and trees with \texttt{Tall = no}.



\begin{rem}[]
We were somewhat disingenuous when we defined the dummy variable
\texttt{Tallyes} because, in truth, \textsf{R} defines \texttt{Tallyes}
automatically without input from the user\footnote{That is, \textsf{R} by default handles contrasts
according to its internal settings which may be customized by the user
for fine control. Given that we will not investigate contrasts further
in this book it does not serve the discussion to delve into those
settings, either. The interested reader should check \texttt{?contrasts} for
details.}. Indeed, the
author fit the model beforehand and wrote the discussion afterward
with the knowledge of what \textsf{R} would do so that the output
the reader saw would match what (s)he had previously read. The way
that \textsf{R} handles factors internally is part of a much
larger topic concerning \textit{contrasts}, which falls outside the scope of
this book. The interested reader should see Neter et al
\cite{Neter1996} or Fox \cite{Fox1997} for more.
\end{rem}




\begin{rem}[]
In general, if an explanatory variable \texttt{foo} is qualitative with \(n\)
levels \texttt{bar1}, \texttt{bar2}, \ldots, \texttt{barn} then \textsf{R} will by default automatically define \(n-1\) indicator variables in the following way:
\begin{eqnarray*} \mathtt{foobar2} & = & \begin{cases} 1, & \mbox{if }\mathtt{foo}=\mathtt{"bar2"},\\ 0, & \mbox{otherwise.}\end{cases},\,\ldots,\,\mathtt{foobarn}=\begin{cases} 1, & \mbox{if }\mathtt{foo}=\mathtt{"barn"},\\ 0, & \mbox{otherwise.}\end{cases} \end{eqnarray*}
The level \texttt{bar1} is represented by
\(\mathtt{foobar2}=\cdots=\mathtt{foobarn}=0\). We just need to make sure that \texttt{foo} is stored as a factor and \textsf{R} will take care of the rest.
\end{rem}

\subsection{Graphing the Regression Lines}

We can see a plot of the two regression lines with the following mouthful of code.

<<dummy-variable-trees, echo=TRUE, fig=TRUE, include=FALSE, height=3.5,width=5>>=
treesTall <- split(trees, trees$Tall)
treesTall[["yes"]]$Fit <- predict(treesdummy.lm,
                                  treesTall[["yes"]])
treesTall[["no"]]$Fit <- predict(treesdummy.lm,
                                 treesTall[["no"]])
plot(Volume ~ Girth, data = trees)
points(Volume ~ Girth, data = treesTall[["yes"]], pch = 1)
points(Volume ~ Girth, data = treesTall[["no"]], pch = 2)
lines(Fit ~ Girth, data = treesTall[["yes"]])
lines(Fit ~ Girth, data = treesTall[["no"]])
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-dummy-variable-trees}
\end{center}
\caption{{\small A dummy variable model for the \texttt{trees} data.}}
\label{fig:dummy-variable-trees}
\end{figure}


It may look intimidating but there is reason to the madness. First we
\texttt{split} the \texttt{trees} data into two pieces, with groups determined by the \texttt{Tall} variable. Next we add the fitted values to each piece via \texttt{predict}. Then we set up a \texttt{plot} for the variables \texttt{Volume} versus \texttt{Girth}, but we do not plot anything yet (\texttt{type = n}) because we want to use different symbols for the two groups. Next we add \texttt{points} to the plot for the \texttt{Tall = yes} trees and use an open circle for a plot character (\texttt{pch = 1}), followed by \texttt{points} for the \texttt{Tall = no} trees with a triangle character (\texttt{pch = 2}). Finally, we add regression \texttt{lines} to the plot, one for each group.

There are other---shorter---ways to plot regression lines by groups, namely the \texttt{scatterplot} function in the \texttt{car} package \cite{car} and the \texttt{xyplot} function in the \texttt{lattice} package \cite{lattice}. We elected to introduce the reader to the above approach since many advanced plots in \textsf{R} are done in a similar, consecutive fashion.

\section{Partial F Statistic} \label{sec:partial-f-statistic}

We saw in Section~\ref{sub:mlr-overall-f-test} how to test
\(H_{0}:\beta_{0}=\beta_{1}=\cdots=\beta_{p}=0\) with the overall
\(F\) statistic and we saw in Section~\ref{sub:mlr-students-t-tests} how to
test \(H_{0}:\beta_{i}=0\) that a particular coefficient \(\beta_{i}\)
is zero. Sometimes, however, we would like to test whether a certain
part of the model is significant. Consider the regression model
\begin{equation}
Y=\beta_{0}+\beta_{1}x_{1}+\cdots+\beta_{j}x_{j}+\beta_{j+1}x_{j+1}+\cdots+\beta_{p}x_{p}+\epsilon,
\end{equation}
where \(j\geq1\) and \(p\geq2\). Now we wish to test the hypothesis
\begin{equation}
H_{0}:\beta_{j+1}=\beta_{j+2}=\cdots=\beta_{p}=0
\end{equation}
versus the alternative
\begin{equation}
H_{1}:\mbox{at least one of $\beta_{j+1},\ \beta_{j+2},\ ,\ldots,\beta_{p}\neq0$}.
\end{equation}

The interpretation of \(H_{0}\) is that none of the variables
\(x_{j+1}\), \ldots,\(x_{p}\) is significantly related to \(Y\) and the
interpretation of \(H_{1}\) is that at least one of \(x_{j+1}\),
\ldots,\(x_{p}\) is significantly related to \(Y\). In essence, for this
hypothesis test there are two competing models under consideration:
\begin{align}
\mbox{the full model:} & \quad y=\beta_{0}+\beta_{1}x_{1}+\cdots+\beta_{p}x_{p}+\epsilon,\\
\mbox{the reduced model:} & \quad y=\beta_{0}+\beta_{1}x_{1}+\cdots+\beta_{j}x_{j}+\epsilon,
\end{align}

Of course, the full model will always explain the data \textit{better} than the reduced model, but does the full model explain the data \textit{significantly better} than the reduced model? This question is exactly what the partial \(F\) statistic is designed to answer.

We first calculate \(SSE_{f}\), the unexplained variation in the full model, and \(SSE_{r}\), the unexplained variation in the reduced model. We base our test on the difference \(SSE_{r}-SSE_{f}\) which measures the reduction in unexplained variation attributable to the variables \(x_{j+1}\),\ldots, \(x_{p}\). In the full model there are \(p+1\) parameters and in the reduced model there are \(j+1\) parameters, which gives a difference of \(p-j\) parameters (hence degrees of freedom). The partial F statistic is
\begin{equation}
F=\frac{(SSE_{r}-SSE_{f})/(p-j)}{SSE_{f}/(n-p-1)}.
\end{equation}
It can be shown when the regression assumptions hold under \(H_{0}\) that the partial \(F\) statistic has an \(\mathsf{f}(\mathtt{df1}=p-j,\,\mathtt{df2}=n-p-1)\) distribution. We calculate the \(p\)-value of the observed partial \(F\) statistic and reject \(H_{0}\) if the \(p\)-value is small.

\subsection{How to do it with \textsf{R}}

The key ingredient above is that the two competing models are \textit{nested}
in the sense that the reduced model is entirely contained within the complete model. The way to test whether the improvement is significant is to compute \texttt{lm} objects both for the complete model and the reduced model then compare the answers with the \texttt{anova} function.



\begin{example}[Diameter, Height and Volume for Black Cherry Trees]
For the \texttt{trees} data, let us fit a polynomial regression model and for
the sake of argument we will ignore our own good advice and fail to
rescale the explanatory variables.
\end{example}

<<echo=TRUE>>=
treesfull.lm <- lm(Volume ~ Girth + I(Girth^2) + Height +
                   I(Height^2), data = trees)
summary(treesfull.lm)
@

In this ill-formed model nothing is significant except \texttt{Girth} and
\texttt{Girth\^2}. Let us continue down this path and suppose that we would
like to try a reduced model which contains nothing but \texttt{Girth} and
\texttt{Girth\^2} (not even an \texttt{Intercept}). Our two models are now
\begin{align*}
\mbox{the full model:} & \quad Y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{1}^{2}+\beta_{3}x_{2}+\beta_{4}x_{2}^{2}+\epsilon,\\
\mbox{the reduced model:} & \quad Y=\beta_{1}x_{1}+\beta_{2}x_{1}^{2}+\epsilon,
\end{align*}
We fit the reduced model with \texttt{lm} and store the results:

<<echo=TRUE>>=
treesreduced.lm <- lm(Volume ~ -1 + Girth + I(Girth^2), data = trees)
@

To delete the intercept from the model we used \texttt{-1} in the model
formula. Next we compare the two models with the \texttt{anova} function. The
convention is to list the models from smallest to largest.

<<echo=TRUE>>=
anova(treesreduced.lm, treesfull.lm)
@

We see from the output that the complete model is highly significant compared to the model that does not incorporate \texttt{Height} or the \texttt{Intercept}. We wonder (with our tongue in our cheek) if the \texttt{Height\^2} term in the full model is causing all of the trouble. We
will fit an alternative reduced model that only deletes \texttt{Height\^2}.

<<echo=TRUE>>=
treesreduced2.lm <- lm(Volume ~ Girth + I(Girth^2) + Height,
                       data = trees)
anova(treesreduced2.lm, treesfull.lm)
@

In this case, the improvement to the reduced model that is attributable to \texttt{Height\^2} is not significant, so we can delete \texttt{Height\^2} from the model with a clear conscience. We notice that the \textit{p}-value for this latest partial \(F\) test is 0.8865, which seems to be remarkably close to the \textit{p}-value we saw for the univariate \textit{t} test of \texttt{Height\^2} at the beginning of this example. In fact, the \textit{p-values} are \textit{exactly} the same. Perhaps now we gain some insight into the true meaning of the univariate tests.


\section{Residual Analysis and Diagnostic Tools} 
\label{sec:residual-analysis-mlr}

We encountered many, many diagnostic measures for simple linear
regression in Sections~\ref{sec:residual-analysis-slr} and
\ref{sec:other-diagnostic-tools-slr}. All of these are valid in
multiple linear regression, too, but there are some slight changes
that we need to make for the multivariate case. We list these below,
and apply them to the trees example.

\begin{description}
\item [{Shapiro-Wilk, Breusch-Pagan, Durbin-Watson:}] unchanged from SLR,
     but we are now equipped to talk about the Shapiro-Wilk test
     statistic for the residuals. It is defined by the formula
     \begin{equation}
     W=\frac{\mathbf{a}^{\mathrm{T}}\mathbf{E}^{\ast}}{\mathbf{E}^{\mathrm{T}}\mathbf{E}},
     \end{equation}
     where \(\mathbf{E}^{\ast}\) is the sorted residuals and
     \(\mathbf{a}_{1\times\mathrm{n}}\) is defined by
     \begin{equation}
     \mathbf{a}=\frac{\mathbf{m}^{\mathrm{T}}\mathbf{V}^{-1}}{\sqrt{\mathbf{m}^{\mathrm{T}}\mathbf{V}^{-1}\mathbf{V}^{-1}\mathbf{m}}},
     \end{equation}
     where \(\mathbf{m}_{\mathrm{n}\times1}\) and
     \(\mathbf{V}_{\mathrm{n}\times\mathrm{n}}\) are the mean and
     covariance matrix, respectively, of the order statistics from an
     \(\mathsf{mvnorm}\left(\mathtt{mean}=\mathbf{0},\,\mathtt{sigma}=\mathbf{I}\right)\)
     distribution.
\item [{Leverages:}] are defined to be the diagonal entries of the hat
                matrix \(\mathbf{H}\) (which is why we called them
                \(h_{ii}\) in Section~\ref{sub:mlr-point-est-regsurface}).
                The sum of the
                leverages is \(\mbox{tr}(\mathbf{H})=p+1\). One rule
                of thumb considers a leverage extreme if it is larger
                than double the mean leverage value, which is
                \(2(p+1)/n\), and another rule of thumb considers
                leverages bigger than 0.5 to indicate high leverage,
                while values between 0.3 and 0.5 indicate moderate
                leverage.
\item [{Standardized residuals:}] unchanged. Considered extreme if
     \(|R_{i}|>2\).
\item [{Studentized residuals:}] compared to a
     \(\mathsf{t}(\mathtt{df}=n-p-2)\) distribution.
\item [{DFBETAS:}] The formula is generalized to
   \begin{equation}
   (DFBETAS)_{j(i)}=\frac{b_{j}-b_{j(i)}}{S_{(i)}\sqrt{c_{jj}}},\quad j=0,\ldots p,\ i=1,\ldots,n,
   \end{equation}
   where \(c_{jj}\) is the \(j^{\mathrm{th}}\) diagonal entry of
   \((\mathbf{X}^{\mathrm{T}}\mathbf{X})^{-1}\). Values
   larger than one for small data sets or \(2/\sqrt{n}\)
   for large data sets should be investigated.
\item [{DFFITS:}] unchanged. Larger than one in absolute value is
             considered extreme.
\item [{Cook's D:}] compared to an \(\mathsf{f}(\mathtt{df1} = p +
               1,\,\mathtt{df2} = n - p - 1)\)
               distribution. Observations falling higher than the
               50\(^{\textrm{th}}\) percentile are extreme.  Note that
               plugging the value \(p=1\) into the formulas will
               recover all of the ones we saw in Chapter~\ref{cha:simple-linear-regression}.
\end{description}

\section{Additional Topics} 
\label{sec:additional-topics-mlr}

\subsection{Nonlinear Regression}

We spent the entire chapter talking about the \texttt{trees} data, and all of
our models looked like \texttt{Volume ~ Girth + Height} or a variant of this
model. But let us think again: we know from elementary school that the
volume of a rectangle is \(V=lwh\) and the volume of a cylinder (which
is closer to what a black cherry tree looks like) is
\begin{equation}
V=\pi r^{2}h\quad \mbox{or}\quad V=4\pi dh,
\end{equation}
where \(r\) and \(d\) represent the radius and diameter of the tree,
respectively. With this in mind, it would seem that a more appropriate
model for \(\mu\) might be
\begin{equation}
\label{eq:trees-nonlin-reg}
\mu(x_{1},x_{2})=\beta_{0}x_{1}^{\beta_{1}}x_{2}^{\beta_{2}},
\end{equation}
where \(\beta_{1}\) and \(\beta_{2}\) are parameters to adjust for the
fact that a black cherry tree is not a perfect cylinder.

How can we fit this model? The model is not linear in the parameters any more, so our linear regression methods will not work\ldots or will they? In the \texttt{trees} example we may take the logarithm of both sides of Equation \eqref{eq:trees-nonlin-reg} to get
\begin{equation}
\mu^{\ast}(x_{1},x_{2})=\ln\left[\mu(x_{1},x_{2})\right]=\ln\beta_{0}+\beta_{1}\ln x_{1}+\beta_{2}\ln x_{2},
\end{equation}
and this new model \(\mu^{\ast}\) is linear in the parameters
\(\beta_{0}^{\ast}=\ln\beta_{0}\), \(\beta_{1}^{\ast}=\beta_{1}\) and
\(\beta_{2}^{\ast}=\beta_{2}\). We can use what we have learned to fit
a linear model \texttt{log(Volume) ~ log(Girth) + log(Height)}, and
everything will proceed as before, with one exception: we will need to
be mindful when it comes time to make predictions because the model
will have been fit on the log scale, and we will need to transform our
predictions back to the original scale (by exponentiating with \texttt{exp})
to make sense.

<<echo=TRUE>>=
treesNonlin.lm <- lm(log(Volume) ~ log(Girth) +
                       log(Height), data = trees)
summary(treesNonlin.lm)
@

This is our best model yet (judging by \(R^{2}\) and \(\overline{R}^{2}\)), all of the parameters are significant, it is simpler than the quadratic or interaction models, and it even makes theoretical sense. It rarely gets any better than that.

We may get confidence intervals for the parameters, but remember that it is usually better to transform back to the original scale for interpretation purposes:

<<echo=TRUE>>=
exp(confint(treesNonlin.lm))
@

(Note that we did not update the row labels of the matrix to show that we exponentiated and so they are misleading as written.) We do predictions just as before. Remember to transform the response variable back to the original scale after prediction.

<<echo=TRUE>>=
new <- data.frame(Girth = c(9.1, 11.6, 12.5),
                  Height = c(69, 74, 87))
exp(predict(treesNonlin.lm, newdata = new,
            interval = "confidence"))
@


The predictions and intervals are slightly different from those calculated earlier, but they are close. Note that we did not need to transform the \texttt{Girth} and \texttt{Height} arguments in the dataframe \texttt{new}. All transformations are done for us automatically.

\subsection{Real Nonlinear Regression}

We saw with the \texttt{trees} data that a nonlinear model might be more appropriate for the data based on theoretical considerations, and we were lucky because the functional form of \(\mu\) allowed us to take logarithms to transform the nonlinear model to a linear one. The same trick will not work in other circumstances, however. We need techniques to fit general models of the form
\begin{equation}
\mathbf{Y}=\mu(\mathbf{X})+\epsilon,
\end{equation}
where \(\mu\) is some crazy function that does not lend itself to linear transformations.

There are a host of methods to address problems like these which are studied in advanced regression classes. The interested reader should see Neter \textit{et al} \cite{Neter1996} or Tabachnick and Fidell \cite{Tabachnick2006}.

It turns out that John Fox has posted an Appendix to his book \cite{Fox2002} which discusses some of the methods and issues associated with nonlinear regression; see \url{http://cran.r-project.org/doc/contrib/Fox-Companion/appendix.html} for more.  Here is an example of how it works, based on a question from R-help.

<<echo=TRUE>>=
# fake data
set.seed(1)
x <- seq(from = 0, to = 1000, length.out = 200)
y <- 1 + 2*(sin((2*pi*x/360) - 3))^2 + rnorm(200, sd = 2)
# plot(x, y)
acc.nls <- nls(y ~ a + b*(sin((2*pi*x/360) - c))^2,
               start = list(a = 0.9, b = 2.3, c = 2.9))
summary(acc.nls)
#plot(x, fitted(acc.nls))
@

\subsection{Multicollinearity} \label{sub:multicollinearity}

A multiple regression model exhibits \textit{multicollinearity} when
two or more of the explanatory variables are substantially correlated
with each other. We can measure multicollinearity by having one of the
explanatory play the role of ``dependent variable'' and regress it on
the remaining explanatory variables. The the \(R^{2}\) of the
resulting model is near one, then we say that the model is
multicollinear or shows multicollinearity.

Multicollinearity is a problem because it causes instability in the
regression model. The instability is a consequence of redundancy in
the explanatory variables: a high \(R^{2}\) indicates a strong
dependence between the selected independent variable and the
others. The redundant information inflates the variance of the
parameter estimates which can cause them to be statistically
insignificant when they would have been significant otherwise. To wit,
multicollinearity is usually measured by what are called \textit{variance
inflation factors}.

Once multicollinearity has been diagnosed there are several approaches
to remediate it. Here are a couple of important ones.

\begin{description}
\item [{Principal Components Analysis.}] This approach casts out two or
     more of the original explanatory variables and replaces them with
     new variables, derived from the original ones, that are by design
     uncorrelated with one another. The redundancy is thus eliminated
     and we may proceed as usual with the new variables in
     hand. Principal Components Analysis is important for other
     reasons, too, not just for fixing multicollinearity problems.
\item [{Ridge Regression.}] The idea of this approach is to replace the
     original parameter estimates with a different type of parameter
     estimate which is more stable under multicollinearity. The
     estimators are not found by ordinary least squares but rather a
     different optimization procedure which incorporates the variance
     inflation factor information.
\end{description}

We decided to omit a thorough discussion of multicollinearity because
we are not equipped to handle the mathematical details. Perhaps the
topic will receive more attention in a later edition.

\begin{itemize}
\item What to do when data are not normal: Bootstrap (see Chapter~\ref{cha:resampling-methods}).
\end{itemize}



\section{Chapter Exercises}


\textbf{Exercise.} Use Equations \eqref{eq:mlr-sse-matrix},
\eqref{eq:mlr-ssto-matrix}, and \eqref{eq:mlr-ssr-matrix} to prove the
Anova Equality: \[ SSTO = SSE + SSR.\]



\chapter{Nonparametric Statistics} \label{cha:nonparametric-statistics}

\paragraph{What do I want them to know?}

\begin{itemize}
\item that a whole ocean of nonparametric inference exists outside of the frog's parametric well,
\item the nonparametric counterpart to each of the standard parametric procedures we devoted so much time to learning,
\item where to find the actual formulas, definitions, and tie-break rules associated with these methods that are so often used, but too often dismissed by hand-waving,
\item how to actually get some of this done with \textsf{R}.
\end{itemize}


<<echo=FALSE, include=FALSE>>=
# Preliminary code to load before start
# This chapter's package dependencies
library(mvtnorm)
@

\section{Single Sample}

\subsection{Sign Test (Fisher, 1925)}

<<echo = FALSE>>=
set.seed(42)
@

The model is
\[
Z_{i} = \theta + e_{i},\quad i=1,2,\ldots,n,
\]
where

\begin{itemize}
\item \(e_{1} \ldots e_{n}\) are mutually independent
\item each \(e\) comes from a continuous population (but not necessarily the same one) with median zero, that is \(\mathbb{P}(e_{i} < 0) = \mathbb{P}(e_{i} > 0) = 1/2\).
\end{itemize}

\subsubsection{Test statistic}

For a specified value \(\theta = \theta_{0}\), let
\[
\psi_{i}=\begin{cases}
1, & \text{if }Z_{i}>\theta_{0},\\
0, & \text{if }Z_{i}<\theta_{0}.
\end{cases}
\]
Then
\[
B = \sum_{i=1}^{n} \psi_{i} = \mbox{number of \(Z_{i}\)'s falling above \(\theta_{0}\)}.
\]


\subsubsection{Hypothesis tests}

Let \(b_{\alpha,n,1/2}\) satisfy the equation \(\mathbb{P}\left(B \geq b_{\alpha,n,1/2}\right) = \alpha\), where \[B \sim \mathtt{binom(size = n, prob = 0.5)}\].  This distribution is implemented in \textsf{R} with the \texttt{pbinom} function.

\begin{table}
\begin{center}
\begin{tabular}{lll}
\(H_{0}\) & \(H_{a}\) & Rejection Region\\
\hline
\(\theta = \theta_{0}\) & \(\theta > \theta_{0}\) & \(B \geq b_{\alpha,n,1/2}\)\\
\(\theta = \theta_{0}\) & \(\theta < \theta_{0}\) & \(B \leq (n - b_{\alpha,n,1/2})\)\\
\(\theta = \theta_{0}\) & \(\theta \neq \theta_{0}\) & \(B \geq b_{\alpha/2,n,1/2}\) or \(B \leq (n - b_{\alpha/2,n,1/2})\)\\
\end{tabular}
\end{center}
\caption{Hypothesis tests, population median, small samples.}
\label{tab:pop-median-small}
\end{table}


\subsubsection{Large sample approximation}

If \(n\) is large then use \(B^{\ast}\) instead defined by
\[
B^{\ast} = \frac{B - (n/2)}{\sqrt{n/4}}  \overset{\cdot}{\sim} \mathtt{norm(mean = 0, sd = 1)}
\]

\subsubsection{How to handle ties}
Discard any values \(Z_{i} = \theta_{0}\) and decrease \(n\) by the number of discards.

\subsubsection{Confidence Interval}

For a \(100(1-\alpha)\%\) confidence interval for \(\theta\) first find an integer \(C_{\alpha}\) from \texttt{pbinom(size = n, prob = 0.5)} that satisfies
\[
\mathbb{P}\left\{C_{\alpha} \leq  B \leq \left(n - C_{\alpha}\right)\right\} \geq 1 - \alpha.
\]
Order \(Z_{(1)} \leq Z_{(2)} \leq \cdots \leq Z_{(n)}\) and set
\[
\theta_{L} = Z_{\left(C_\alpha\right)}\quad \mbox{and}\quad \theta_{U} = Z_{\left(n + 1 - C_{\alpha}\right)}.
\]
Then
\[
\mathbb{P}\left(\theta_{L} \leq  \theta \leq \theta_{U}\right) \geq 1 - \alpha.
\]
For large samples, \[C_{\alpha}\approx \frac{n}{2} - z_{\alpha/2}\sqrt{\frac{n}{4}}\].

Note that the \(\mathtt{binom(size = n, prob = 0.5)}\) distribution is symmetric and the method is such that all of our confidence intervals look like \([Z_{(k)},\ Z_{(n-k)}]\) for some \(k\); there are a limited number of such intervals (about \(n/2\) of them).  Consequently there is a limited list of confidence levels possible to achieve using the above method.  The confidence level for \([Z_{(k)},\ Z_{(n-k)}]\) is
\[
\mathbb{P}\left\{k \leq  B \leq (n - k)\right\} = 1 - 2\sum_{i=0}^{k-1}{n \choose i}\frac{1}{2^{n}}.
\]
For a specified confidence level \(1-\alpha\) we are obliged to take the confidence interval available with the next-highest confidence level.


\subsubsection{Point estimate}

\[
\tilde{\theta} = \text{median}\left\{Z_{i},\ i=1,\ldots,n\right\}
\]


\subsubsection{How to do it with \textsf{R}}

First we generate a simple random sample of size 13 from a nonnormal continuous distribution (not symmetric) for the sake of argument.  Note that the population median for this distribution is \texttt{log(2)} \(\approx 0.7\).
<<>>=
z <- rexp(13)
z
@

Then we perform a test of \(H_{0}:\theta = 1\) against \(H_{a}:\theta \neq 1\).  Note that the alternative hypothesis is true in this example.

<<>>=
sum(z > 1)
@

We use this number of ``successes'' to plug into the \texttt{binom.test} function.  Our test is about the median, so \texttt{p = 0.50}.
<<>>=
binom.test(x = 4, n = 13, p = 0.5)
@

<<echo = FALSE>>=
TMP <- binom.test(x = 4, n = 13, p = 0.5)
@

Here we fail to reject the null hypothesis at significance level \(\alpha = 0.05\). The \texttt{sample estimate} given above is for the probability of success, which is not the parameter of interest to us. Instead we calculate our point estimate for \(\theta\) by hand with
<<>>=
median(z)
@

The confidence interval reported above isn't the one we want, either.  We must also compute that by hand, and as we said, there are only so many confidence intervals for \(\theta\) available to us. Those for this example are displayed in the table below.

<<>>=
data.frame(lwr = sort(z)[1:5],
           upr = sort(z, decreasing=TRUE)[1:5],
           clevel = 1 - 2*pbinom(5 - 5:1, size = 13, prob = 0.5))
@

<<echo = FALSE>>=
TMP <- data.frame(lwr = sort(z)[1:5],
           upr = sort(z, decreasing=TRUE)[1:5],
           clevel = 1 - 2*pbinom(5 - (5:1), size = 13, prob = 0.5))
TMP <- round(TMP,3)
@

If we want 95\% confidence interval for \(\theta\) then we take the \Sexpr{100*TMP[3,3]} \% interval which runs from approximately \Sexpr{TMP[3,1]} to \Sexpr{TMP[3,2]}.

\subsection{Wilcoxon Signed Rank (one sample)}

<<echo = FALSE>>=
set.seed(42)
@

The model is
\[
Z_{i} = \theta + e_{i},\quad i=1,2,\ldots,n,
\]
where

\begin{itemize}
\item \(e_{1}, \ldots, e_{n}\) are mutually independent
\item each \(e\) has a continuous distribution (not necessarily the same one) that is symmetric about zero
\end{itemize}

\subsubsection{Test statistic}

For a specified value \(\theta = \theta_{0}\), the test statistic \(V\) is determined by the following procedure.

\begin{enumerate}
\item Let \(R_{i}\) denote the rank of \(\vert Z_{i} - \theta_{0}\vert\) in the ordered (least to greatest) list of \(\vert Z_{1} - \theta_{0}\vert,\ \vert Z_{2} - \theta_{0}\vert,\ldots \vert Z_{n} - \theta_{0}\vert\).

\item Let
\[
\psi_{i}=\begin{cases}
1, & \text{if }Z_{i}>\theta_{0},\\
0, & \text{if }Z_{i}<\theta_{0}.
\end{cases}
\]
Then the \textit{positive signed rank} of \(Z_{i}\) is \(R_{i}\psi_{i}\) and
\[
V = \sum_{i=1}^{n}R_{i}\psi_{i} = \mbox{sum of positive signed ranks}.
\]
\end{enumerate}

\subsubsection{Hypothesis tests}

Let \(v_{\alpha,n}\) satisfy the equation
\(\mathbb{P}\left(V \geq v_{\alpha,n}\right) = \alpha\), where the
probability is calculated under the assumption
\(\theta = \theta_{0} = 0\).  This distribution is implemented in \textsf{R}
with the \texttt{psignrank} function.

\begin{table}
\begin{center}
\begin{tabular}{lll}
\(H_{0}\) & \(H_{a}\) & Rejection Region\\
\hline
\(\theta = \theta_{0}\) & \(\theta > \theta_{0}\) & \(V \geq v_{\alpha,n}\)\\
\(\theta = \theta_{0}\) & \(\theta < \theta_{0}\) & \(V \leq \left[ \frac{n(n+1)}{2} - v_{\alpha,n}\right]\)\\
\(\theta = \theta_{0}\) & \(\theta \neq \theta_{0}\) & \(V \geq v_{\alpha/2,n}\) or \(V \leq \left[\frac{n(n+1)}{2} - v_{\alpha/2,n}\right]\)\\
\end{tabular}
\end{center}
\caption{Hypothesis tests, population median, small sample.}
\label{tab:pop-median-small-V}
\end{table}

\subsubsection{Large samples}

If \(n\) is large then use \(V^{\ast}\) instead defined by
\[
V^{\ast} = \frac{V-\frac{n(n+1)}{4}}{\sqrt{\frac{n(n+1)(2n+1)}{24}}} \overset{\cdot}{\sim} \mathtt{norm(mean = 0, sd = 1)}
\]

\subsubsection{How to handle ties}

\begin{itemize}
\item discard any values \(Z_{i} = \theta_{0}\) and decrease \(n\) by the number of discards
\item If there are ties among nondiscarded \(Z_{i}\)'s then use average ranks to compute \(V\)
\item with large samples, use average ranks and modified variance
  \[
  \text{Var}_{\theta_{0}}\left(V\right)=\frac{1}{24}\left[n(n+1)(2n+1)- \frac{1}{2}\sum_{j=1}^{g}t_{j}(t_{j} - 1)(t_{j}+1)\right],
  \]
  where \(g\) is the number of groups and \(t_{j}\) is the size of group \(j\) (if there are no ties then \(g = n\) and \(t_{j} \equiv 1\)).
\end{itemize}

\paragraph{Comments}

\begin{itemize}
\item \(V\) was introduced by Wilcoxon (1945)
\item Pratt (1959) said what to do with zero observations and tied ranks
\end{itemize}

\subsubsection{Confidence Interval}

For a \(100(1-\alpha)\%\) confidence interval for \(\theta\) first find an integer \(C_{\alpha} \) from \texttt{psignrank} that satisfies
\[
\mathbb{P}\left( C_{\alpha} \leq  V \leq \left[\frac{n(n+1)}{2} - C_{\alpha}\right] \right) = 1 - \alpha
\]
Let
\[
W_{(1)} \leq W_{(2)} \leq \cdots \leq W_{(M)}
\]
be the ordered values of \(\frac{Z_{i} + Z_{j}}{2}\), \(i \leq j \), where \(M=n(n + 1)/2\).

Set
\[
\theta_{L} = W_{\left(C_\alpha\right)}\quad \mbox{and}\quad \theta_{U} = W_{\left(M + 1 - C_{\alpha}\right)}.
\]
Then
\[
\mathbb{P}\left(\theta_{L} \leq  \theta \leq \theta_{U}\right) = 1 - \alpha.
\]

As with the Sign Test, the nature of the confidence interval procedure implies that not all possible confidence levels are achievable, particularly if the sample size is very small.

\subsubsection{Point estimate}

The Hodges-Lehmann estimator is the sample \textit{pseudomedian} given by
\[
\hat{\theta} = \text{median}\left\{\frac{Z_{i} + Z_{j}}{2},\ i \leq j  \right\}.
\]

The pseudomedian of a distribution \(F\) is the median of the distribution of \((U+V)/2\), where \(U\) and \(V\) are independent and identically distributed according to \(F\).  If \(F\) is symmetric then the pseudomedian and the median are the same.


\subsubsection{How to do it with \textsf{R}}

First we generate a simple random sample of size 15 from a nonnormal continuous symmetric distribution with population median 0 for the sake of argument.
<<>>=
z = rcauchy(15)
z
@

Then we perform a test of \(H_{0}:\theta = 4\) against \(H_{a}:\theta \neq 4\).  (Note that the alternative hypothesis is true in this example.)

<<>>=
wilcox.test(z, mu = 4, conf.int = TRUE)
@

<<echo = FALSE>>=
TMP <- wilcox.test(z, mu = 4, conf.int = TRUE)
@

Here we reject the null hypothesis at significance level \(\alpha = 0.05\), the Hodges-Lehmann estimator is \(\hat{\theta} \approx \Sexpr{round(TMP$estimate,3)}\), and the confidence interval is approximately \Sexpr{round(TMP$conf.int[1],3)} to  \Sexpr{round(TMP$conf.int[2],3)}.

Watch what happens if we try the same thing with a ridiculously small sample of size three:

<<>>=
wilcox.test(rcauchy(3), mu = 4, conf.int = TRUE)
@




\section{Two Samples}

\subsection{Wilcoxon Rank Sum (independent samples)}

We observe \(X_{1},\ldots,X_{m}\) and \(Y_{1},\ldots,Y_{n}\). The model is
\[
\begin{aligned}
X_{i} & = e_{i} + \Delta, & i = 1,\ldots,m,\\
Y_{j} & = e_{m+j},        & j = 1,\ldots,n.
\end{aligned}
\]
where

\begin{itemize}
\item \(\Delta\) is an unknown ``location shift''
\item \(\Delta = \mu_{x} - \mu_{y}\) where the \(\mu\)'s are the medians and if
  the means exist then \(\Delta\) also equals the difference in means.
\item The \(e\)'s are a \(SRS(N)\) from a continuous population distribution, where \(N = m + n\).
\end{itemize}

\subsubsection{Comments}

Closely related to the \textit{Mann-Whitney} test.

\subsubsection{Test statistic}

For a specified value \(\Delta=\Delta_{0}\), the test statistic \(W\) is determined by the following procedure.

\begin{enumerate}
\item Sort the \(N=m+n\) observations \((X_{1}-\Delta_{0}),\ldots,(X_{m}-\Delta_{0})\) and \(Y_{1},\ldots,Y_{n}\) in increasing order and let \(R_{i}\) denote the rank of \((X_{i}-\Delta_{0})\), for \(i=1,\ldots,m\).
\item Set \[W = \sum_{i = 1}^{m} R_{i} = \mbox{sum of ranks assigned to } (X_{1}-\Delta_{0}),\ldots,(X_{m}-\Delta_{0}).\]
\end{enumerate}

\subsubsection{Hypothesis tests}

Let \(w_{\alpha,m,n}\) satisfy the equation
\(\mathbb{P}\left(W \geq w_{\alpha,m,n}\right) = \alpha\), where the
probability is calculated under the assumption
\(\Delta = \Delta_{0} = 0\).  This distribution is implemented in \textsf{R}
with the \texttt{pwilcox} function.

\begin{table}
\begin{center}
\begin{tabular}{lll}
\(H_{0}\) & \(H_{a}\) & Rejection Region\\
\hline
\(\Delta = \Delta_{0}\) & \(\Delta > \Delta_{0}\) & \(W \geq w_{\alpha,m,n}\)\\
\(\Delta = \Delta_{0}\) & \(\Delta < \Delta_{0}\) & \(W \leq \left[m(m+n+1) - w_{\alpha,m,n} \right]\)\\
\(\Delta = \Delta_{0}\) & \(\Delta \neq \Delta_{0}\) & \(W \geq w_{\alpha/2,m,n}\) or \(W \leq \left[m(m+n+1) - w_{\alpha/2,m,n} \right]\)\\
\end{tabular}
\end{center}
\caption{Hypothesis tests, difference in population medians, small sample.}
\label{tab:diff-med-small}
\end{table}

\subsubsection{Large samples}

When \(N = m + n\) is large use the statistic \(W^{\ast}\) instead defined by
\[
W^{\ast} = \frac{W - [m(m + n + 1)]/2}{[mn(m + n + 1)/12]^{1/2}} \overset{\cdot}{\sim} \mathtt{norm(mean = 0, sd = 1)}.
\]

\subsubsection{How to handle ties}

Use average ranks.  For the large sample approximation we replace \(\text{Var}_{\Delta_{0}}(W)\) above with
\[
\text{Var}_{\Delta_{0}}(W)=\frac{mn}{12}\left\{m+n+1-\frac{\sum_{j=1}^{g}t_{j}(t_{j}^{2}-1)}{(m+n)(m+n-1)}\right\},
\]
where \(g\) is the number of ``groups'' and \(t_{j}\) is the number of observations in group \(j\) (if there are no ties then \(g=n\) and \(t_{j} \equiv 1\)).

\subsubsection{Confidence Interval}

For a \(100(1-\alpha)\%\) confidence interval for \(\Delta\) first find an integer \(C_{\alpha}\) from \texttt{pwilcox} that satisfies
\[
\mathbb{P}\left( \left[\frac{m(m+1)}{2} + C_{\alpha}\right] \leq  W \leq \left[\frac{m(2n+m+1)}{2} - C_{\alpha}\right] \right) = 1 - \alpha.
\]
Let
\[
U_{(1)} \leq U_{(2)} \leq \cdots \leq U_{(mn)}
\]
be the ordered values of \((X_{i} - Y_{j})\), \(i = 1,\ldots,m\) and \(j=1,\ldots,n\) and set
\[
\Delta_{L} = U_{\left(C_\alpha\right)}\quad \mbox{and}\quad \Delta_{U} = U_{\left(mn + 1 - C_{\alpha}\right)}.
\]
Then
\[
\mathbb{P}\left(\Delta_{L} \leq  \Delta \leq \Delta_{U}\right) = 1 - \alpha.
\]

As with the other procedures, particularly with small sample sizes some confidence levels are not achievable.

\subsubsection{Point estimate}

The Hodges-Lehmann estimator is
\[
\hat{\Delta} = \text{median}\{(X_{i} - Y_{j}),\ i = 1,\ldots,m,\ j = 1,\ldots,n \}
\]

\subsubsection{Note}

For rules about what \textsf{R} considers ``large samples'' see \texttt{?pwilcox}.

\subsubsection{How to do it with \textsf{R}}

<<echo = FALSE>>=
set.seed(42)
@

First we generate independent samples from a nonnormal continuous
distribution, say, from an Exponential distribution (it could have
been anything continuous, really), and let's shift the \texttt{x} data up by
\(\Delta = 3\).

<<>>=
x <- rexp(15) + 3
y <- rexp(9)
@

Note that the samples are not the same size.  We will test the hypothesis \(H_{0}:\Delta = 2\) against a two-sided alternative.  Note that the alternative hypothesis is true in this example.

<<>>=
wilcox.test(x, y, mu = 2, conf.int = TRUE)
@

<<echo = FALSE>>=
TMP <- wilcox.test(x, y, mu = 2, conf.int = TRUE)
@

Here we reject the null hypothesis (barely) at significance level \(\alpha = 0.05\), the Hodges-Lehmann estimator is \(\hat{\Delta} \approx \Sexpr{round(TMP$estimate,3)}\), and with 95\% confidence \(\Delta\) is covered by the interval \Sexpr{floor(1000*TMP$conf.int[1])/1000} to  \Sexpr{ceiling(1000*TMP$conf.int[2])/1000}.


\subsection{Wilcoxon Signed Rank (paired samples)}

We observe \(X_{1},\ldots,X_{n}\) and \(Y_{1},\ldots,Y_{n}\), where \(X_{i}\) is paired or matched to \(Y_{i}\). Define \(Z_{i} = X_{i} - Y_{i}\) and continue as in the one sample problem. The model is
\[
Z_{i} = (X_{i} - Y_{i}) = \theta + e_{i},\quad i=1,2,\ldots,n,
\]
where

\begin{itemize}
\item \(e_{1}, \ldots, e_{n}\) are mutually independent
\item each \(e\) has a continuous distribution (not necessarily the same one) that is symmetric about zero.

\subsubsection{Comments}
\item if \(X_{i}\) and \(Y_{i}\) come from populations differing
  only in location then \(Z_{i}\) comes from a population symmetric about
  zero, and the same is true under more general conditions, see Hollander and Wolfe.
\item we don't assume that \(X_{i}\) and \(Y_{i}\) are independent, and in practice they are often dependent
\end{itemize}

\subsubsection{Test statistic, Hypothesis tests, Confidence Interval, Point estimate}

Same as in the one sample problem.

\subsubsection{How to do it with \textsf{R}}

In Section~\ref{BLANK} we studied the \texttt{sleep} data, which are paired (see Example~\ref{BLANK}), and there we noticed that the distribution of differences was not normal  We conducted the parametric test anyway, for illustrative purposes, but perhaps a nonparametric test would have been more appropriate.

Translating to the language of this section, we will test the hypothesis \(H_{0}:\theta = 0\) against a two-sided alternative.

<<>>=
wilcox.test(extra ~ group, data = sleep, paired = TRUE, 
            exact = FALSE, conf.int = TRUE)
@

<<echo = FALSE>>=
TMP <- wilcox.test(extra ~ group, data = sleep, paired = TRUE, 
            exact = FALSE, conf.int = TRUE)
@

\noindent
Here we reject the null hypothesis at significance level \(\alpha = 0.05\), the Hodges-Lehmann estimator is \(\hat{\theta} \approx \Sexpr{round(TMP$estimate,3)}\), and with 95\% confidence \(\theta\) is covered by the interval \Sexpr{floor(1000*TMP$conf.int[1])/1000} to  \Sexpr{ceiling(1000*TMP$conf.int[2])/1000}.  Note that we set \texttt{exact = FALSE} because there were ties among the 10 differences.

The \(p\)-value here is bigger than it was with the Paired \(t\)-test, but still highly significant, so in this case the nonparametric test came to the same conclusion as the parametric test, which is an encouraging thought.


\section{Three or More Samples}

\subsection{Kruskal Wallis Rank Sum Test}

We observe \(k \geq 3\) samples
\[
\left\{X_{11},\ldots,X_{n_{1}1}\right\},\  \left\{X_{12},\ldots,X_{n_{2}2}\right\},\ldots, \  \left\{X_{1k},\ldots,X_{n_{k}k}\right\},
\]
and the model is
\[
X_{ij} = \mu + \tau_{j} + e_{ij}, \quad j = 1,\ldots,k, \quad i = 1,\ldots,n_{j},
\]
where

\begin{itemize}
\item \(N = \sum n_{j}\) is the total sample size
\item \(\mu\) is the grand mean
\item \(\tau_{j}\) is the effect of treatment \(j\)
\item \(\sum \tau_{j} = 0\)
\item \(e_{ij}\)'s are a \(SRS(N)\) from a continuous population distribution (presumably nonnormal)
\end{itemize}

\subsubsection{Test statistic}

In short, we sort all \(N\) observations from least to greatest and compute the ordinary ANOVA test statistic on the ranks.  In symbols, denote the rank of \(X_{ij}\) by \(r_{ij}\) and set
\[
R_{j} = \sum_{i=1}^{n_{j}}r_{ij}, \quad R_{\cdot j} = \frac{R_{j}}{n_{j}}, \quad R_{\cdot\cdot}= \frac{N+1}{2}.
\]
The Kruskal-Wallis test statistic is \(H\) defined by
\[
\begin{aligned}
H & = \frac{12}{N(N+1)} \sum_{j = 1}^{k} n_{j} \left(R_{\cdot j} - R_{\cdot\cdot} \right)^{2}, \\
  & = \left(\frac{12}{N(N+1)}\sum_{j=1}^{k} \frac{R_{j}^{2}}{n_{j}}  \right) - 3(N+1).
\end{aligned}
\]

The null distribution of \(H\) is computationally intensive to compute
except for relatively small sample sizes.  The asymptotic distribution of
\(H\) as \(N \to \infty\) is \(\mathtt{chisq(df = k - 1)}\), and this asymptotic distribution is what \textsf{R} uses to compute/report \textit{p}-values.

\subsubsection{Hypothesis test}

The null hypothesis is \[H_{0}: \tau_{1} = \cdots = \tau_{k},\] and the alternative is
\[H_{a}: \mbox{at least one \(\tau_{j}\) differs from the others.} \]

We reject \(H_{0}\) if \(H \geq \chi^2_{\alpha}(\mathtt{df = 1})\).

\subsubsection{How to handle ties}

Use average ranks and instead of \(H\) use \(H^{\ast}\)
defined by
\[
H^{\ast} = \frac{H}{1 - \left.\sum_{j=1}^{g}\left(t_{j}^{3} - t_{j}\right) \right/\left(N^{3}-N\right)}.
\]
where \(g\) is the number of groups and \(t_{j}\) is the size of group \(j\) (if there are no ties then \(g = n\) and \(t_{j} \equiv 1\)).

\begin{example}[Effectiveness of Insect Sprays]
The \texttt{InsectSprays} data records counts of insects treated with insecticides labeled \texttt{"A"} through \texttt{"F"}.  In Exercise~\ref{BLANK} the reader is asked to check the normality of \texttt{count} by \texttt{spray}, and .
\end{example}


\subsubsection{How to do it with \textsf{R}}

We perform a test that the location parameters are all the same with
the \texttt{kruskal.test} function.  The \textit{p}-value returned is
based on the asymptotic chi-squared distribution.

<<>>=
kruskal.test(count ~ spray, data = InsectSprays)
@

Here we reject the null hypothesis that all treatment effects are the
same.



\section{Chapter Exercises}




\begin{Exercise}[]
Use the methods of Exercise~\ref{BLANK} to check normality of the \texttt{count} variable in the \texttt{InsectSprays} data, by groups determined by the \texttt{spray} variable.  Is a nonparametric procedure warranted in this case?  Explain.
\end{Exercise}




\begin{Exercise}[]
In Exercise~\ref{BLANK} we conducted an standard parametric ANOVA of the \texttt{PlantGrowth} data. This time, conduct an appropriate nonparametric alternative.  Check all assumptions.  How do the analyses differ?  Which procedure would you recommend for the \texttt{PlantGrowth} data, and why?
\end{Exercise}




\chapter{Resampling Methods} \label{cha:resampling-methods}

<<echo=FALSE, include=FALSE>>=
# This chapter's package dependencies
library(boot)
library(coin)
@

Computers have changed the face of statistics. Their quick
computational speed and flawless accuracy, coupled with large data
sets acquired by the researcher, make them indispensable for many
modern analyses. In particular, resampling methods (due in large part
to Bradley Efron) have gained prominence in the modern statistician's
repertoire. We first look at a classical problem to get some insight
why.

I have used \textit{Statistical Computing with \textsf{R}} by Rizzo
\cite{Rizzo2008} and I recommend it to those looking for a more
advanced treatment with additional topics. I believe that
\textit{Monte Carlo Statistical Methods} by Robert and Casella
\cite{Robert2004} has an edition that integrates \textsf{R} into the narrative.

\paragraph{What do I want them to know?}

\begin{itemize}
\item basic philosophy of resampling and why it is important
\item resampling for standard errors and confidence intervals
\item resampling for hypothesis tests (permutation tests)
\end{itemize}

\section{Introduction} \label{sec:introduction-resampling}

\begin{description}
\item [{Classical question:}] Given a population of interest, how may
  we effectively learn some of its salient features, \textit{e.g.},
  the population's mean? One way is through representative random
  sampling. Given a random sample, we summarize the information
  contained therein by calculating a reasonable statistic,
  \textit{e.g.}, the sample mean. Given a value of a statistic, how do
  we know whether that value is significantly different from that
  which was expected? We don't; we look at the \textit{sampling
    distribution} of the statistic, and we try to make probabilistic
  assertions based on a confidence level or other consideration. For
  example, we may find ourselves saying things like, ``With 95\%
  confidence, the true population mean is greater than zero''.
\item [{Problem:}] Unfortunately, in most cases the sampling
  distribution is \textit{unknown}. Thus, in the past, in efforts to
  say something useful, statisticians have been obligated to place
  some restrictive assumptions on the underlying population. For
  example, if we suppose that the population has a normal
  distribution, then we can say that the distribution of
  \(\overline{X}\) is normal, too, with the same mean (and a smaller
  standard deviation). It is then easy to draw conclusions, make
  inferences, and go on about our business.
\item [{Alternative:}] We don't know what the underlying population
  distributions is, so let us \textit{estimate} it, just like we would
  with any other parameter. The statistic we use is the
  \textit{empirical CDF}, that is, the function that places mass
  \(1/n\) at each of the observed data points \(x_{1},\ldots,x_{n}\)
  (see Section~\ref{sec:empirical-distribution}). As the sample size
  increases, we would expect the approximation to get better and
  better (with IID observations, it does, and there is a wonderful
  theorem by Glivenko and Cantelli that proves it). And now that we
  have an (estimated) population distribution, it is easy to find the
  sampling distribution of any statistic we like: just \textit{sample}
  from the empirical CDF many, many times, calculate the statistic
  each time, and make a histogram. Done! Of course, the number of
  samples needed to get a representative histogram is prohibitively
  large\ldots human beings are simply too slow (and clumsy) to do this
  tedious procedure.
\end{description}

Fortunately, computers are very skilled at doing simple, repetitive
tasks very quickly and accurately. So we employ them to give us a
reasonable idea about the sampling distribution of our statistic, and
we use the generated sampling distribution to guide our inferences and
draw our conclusions. If we would like to have a better approximation
for the sampling distribution (within the confines of the information
contained in the original sample), we merely tell the computer to
sample more. In this (restricted) sense, we are limited only by our
current computational speed and pocket book.

In short, here are some of the benefits that the advent of resampling
methods has given us:

\begin{description}
\item [{Fewer assumptions.}]  We are no longer required to assume the
     population is normal or the sample size is large (though, as
     before, the larger the sample the better).
\item [{Greater accuracy.}] Many classical methods are based on rough upper
     bounds or Taylor expansions. The bootstrap procedures can be
     iterated long enough to give results accurate to several decimal
     places, often beating classical approximations.
\item [{Generality.}]  Resampling methods are easy to understand and apply
                 to a large class of seemingly unrelated
                 procedures. One no longer needs to memorize long
                 complicated formulas and algorithms.
\end{description}



\begin{rem}[]
  Due to the special structure of the empirical CDF, to get an IID
  sample we just need to take a random sample of size \(n\), with
  replacement, from the observed data \(x_{1},\ldots,x_{n}\). Repeats
  are expected and acceptable. Since we already sampled to get the
  original data, the term \textit{resampling} is used to describe the
  procedure.
\end{rem}

\subsection{General bootstrap procedure}

The above discussion leads us to the following general procedure to
approximate the sampling distribution of a statistic
\(S=S(x_{1},x_{2},\ldots,x_{n})\) based on an observed simple random
sample \(\mathbf{x}=(x_{1},x_{2},\ldots,x_{n})\) of size \(n\):

\begin{enumerate}
\item Create many many samples
  \(\mathbf{x}_{1}^{\ast}, \ldots, \mathbf{x}_{M}^{\ast}\), called
  \textit{resamples}, by sampling with replacement from the data.
\item Calculate the statistic of interest
   \(S(\mathbf{x}_{1}^{\ast}),\ldots,S(\mathbf{x}_{M}^{\ast})\) for
   each resample. The distribution of the resample statistics is
   called a \textit{bootstrap distribution}.
\item The bootstrap distribution gives information about the sampling
   distribution of the original statistic \(S\). In particular, the
   bootstrap distribution gives us some idea about the center, spread,
   and shape of the sampling distribution of \(S\).
\end{enumerate}

\section{Bootstrap Standard Errors} \label{sec:bootstrap-standard-errors}

Since the bootstrap distribution gives us information about a
statistic's sampling distribution, we can use the bootstrap
distribution to estimate properties of the statistic. We will
illustrate the bootstrap procedure in the special case that the
statistic \(S\) is a standard error.



\begin{example}[Standard error of the mean]
\label{exm:bootstrap-se-mean-examp}
In this example we illustrate the bootstrap by estimating the standard error
of the sample meanand we will do it in the special case that the
underlying population is
\(\mathsf{norm}(\mathtt{mean}=3,\,\mathtt{sd}=1)\).
\end{example}

Of course, we do not really need a bootstrap distribution here because
from Section~\ref{sec:sampling-from-normal-dist} we know that
\(\overline{X}\sim\mathsf{norm}(\mathtt{mean}=3,\,\mathtt{sd}=1/\sqrt{n})\),
but we proceed anyway to investigate how the bootstrap performs when
we know what the answer should be ahead of time.

We will take a random sample of size \(n=25\) from the
population. Then we will \textit{resample} the data 1000 times to get 1000
resamples of size 25. We will calculate the sample mean of each of the
resamples, and will study the data distribution of the 1000 values of
\(\overline{x}\).

<<echo=TRUE>>=
srs <- rnorm(25, mean = 3)
resamps <- replicate(1000, sample(srs, 25, TRUE),
                     simplify = FALSE)
xbarstar <- sapply(resamps, mean, simplify = TRUE)
@

A histogram of the 1000 values of \(\overline{x}\) is shown in Figure~\ref{fig:bootstrap-se-mean}, and was produced by the following code.

<<bootstrap-se-mean, echo=TRUE, fig=TRUE, include=FALSE, height=3.25,width=5>>=
hist(xbarstar, breaks = 40, prob = TRUE, xlim=c(2,3.6))
curve(dnorm(x, 3, 0.2), add = TRUE) # overlay true normal density
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-bootstrap-se-mean}
\end{center}
\caption[Bootstrapping the standard error of the mean, simulated data.]{{\small Bootstrapping the standard error of the mean, simulated data. The original data were 25 observations generated from a \(\mathsf{norm}(\mathtt{mean}=3,\,\mathtt{sd}=1)\) distribution. We next resampled, with replacement, generating 1000 resamples, each of size 25, and calculated the sample mean for each resample. A histogram of the 1000 values of \(\overline{x}\) is shown above. Also shown (with a solid line) is the true sampling distribution of \(\overline{X}\), which is a \(\mathsf{norm}(\mathtt{mean}=3,\,\mathtt{sd}=0.2)\) distribution. Note that the histogram is centered at the sample mean of the original data (\(\overline{x}=\Sexpr{round(mean(xbarstar),1)}\)), while the true sampling distribution is centered at the true value of \(\mu=3\). The shape and spread of the histogram is similar to the shape and spread of the true sampling distribution.}}
\label{fig:bootstrap-se-mean}
\end{figure}




We have overlain what we know to be the true sampling distribution of
\(\overline{X}\), namely, a
\(\mathsf{norm}(\mathtt{mean}=3,\,\mathtt{sd}=1/\sqrt{25})\)
distribution. The histogram matches the true sampling distribution
pretty well with respect to shape and spread\ldots but notice how the
histogram is off-center a little bit. This is not a coincidence -- in
fact, it can be shown that the mean of the bootstrap distribution is
exactly the mean of the original sample, that is, the value of the
statistic that we originally observed. Let us calculate the mean of
the bootstrap distribution and compare it to the mean of the original
sample:

<<echo=TRUE>>=
mean(xbarstar)
mean(srs)
mean(xbarstar) - mean(srs)
@

Notice how close the two values are. The difference between them is an
estimate of how biased the original statistic is, the so-called
\textit{bootstrap estimate of bias}. Since the estimate is small we would
expect our original statistic (\(\overline{X}\)) to have small bias,
but this is no surprise to us because we already knew from Section~\ref{sec:simple-random-samples} that \(\overline{X}\) is an unbiased estimator
of the population mean.

Now back to our original problem, we would like to estimate the
standard error of \(\overline{X}\). Looking at the histogram, we see
that the spread of the bootstrap distribution is similar to the spread
of the sampling distribution. Therefore, it stands to reason that we
could estimate the standard error of \(\overline{X}\) with the sample
standard deviation of the resample statistics. Let us try and see.

<<echo=TRUE>>=
sd(xbarstar)
@


We know from theory that the true standard error is
\(1/\sqrt{25}=0.20\). Our bootstrap estimate is not very far from the
theoretical value.



\begin{rem}[]
What would happen if we take more resamples? Instead of 1000
resamples, we could increase to, say, 2000, 3000, or even
4000\ldots would it help? The answer is both yes and no. Keep in mind
that with resampling methods there are two sources of randomness: that
from the original sample, and that from the subsequent resampling
procedure. An increased number of resamples would reduce the variation
due to the second part, but would do nothing to reduce the variation
due to the first part.

We only took an original sample of size \(n=25\), and resampling more
and more would never generate more information about the population
than was already there. In this sense, the statistician is limited by
the information contained in the original sample.
\end{rem}



\begin{example}[Standard error of the median]
\label{exm:bootstrap-se-median}
We look at one where we do not know the
answer ahead of time. This example uses the \texttt{rivers}
\index{Data sets!rivers@\texttt{rivers}} data set. Recall
the stemplot that we made for these data which shows them to be markedly right-skewed, so a natural estimate of center would be the sample median. Unfortunately, its sampling distribution falls out of our reach. We use the bootstrap to help us with this problem, and the modifications to the last example
are trivial.
\end{example}

<<echo=TRUE>>=
resamps <- replicate(1000, sample(rivers, 141, TRUE),
                     simplify = FALSE)
medstar <- sapply(resamps, median, simplify = TRUE)
sd(medstar)
@


<<bootstrapping-se-median, echo=FALSE, fig=TRUE, include=FALSE, height=3.5,width=5>>=
hist(medstar, breaks = 40, prob = TRUE)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-bootstrapping-se-median}
\end{center}
\caption[Bootstrapping the standard error of the median.]{{\small Bootstrapping the standard error of the median for the \texttt{rivers} data.}}
\label{fig:bootstrapping-se-median}
\end{figure}



The graph is shown in Figure~\ref{fig:bootstrapping-se-median}, and
was produced by the following code.

<<echo=TRUE, eval=FALSE>>=
hist(medstar, breaks = 40, prob = TRUE)
@

<<echo=TRUE>>=
median(rivers)
mean(medstar)
mean(medstar) - median(rivers)
@


\begin{example}[The \texttt{boot} package in \textsf{R}]
It turns out that there are many bootstrap procedures and commands
already built into base \textsf{R}, in the \texttt{boot}
package. Further, inside the \texttt{boot} package \cite{boot} there is
even a function called \texttt{boot}.\index{boot@\texttt{boot}} The basic syntax is of the form: \texttt{boot(data, statistic, R)}.
\end{example}


Here, \texttt{data} is a vector (or matrix) containing the data to be resampled, \texttt{statistic} is a defined function, \textit{of two arguments}, that tells which statistic should be computed, and the parameter R specifies how many resamples should be taken.

For the standard error of the mean (Example~\ref{exm:bootstrap-se-mean-examp}):

<<echo=TRUE>>=
mean_fun <- function(x, indices) mean(x[indices])
library(boot)
boot(data = srs, statistic = mean_fun, R = 1000)
@

For the standard error of the median (Example~\ref{exm:bootstrap-se-median}):

<<echo=TRUE>>=
median_fun <- function(x, indices) median(x[indices])
boot(data = rivers, statistic = median_fun, R = 1000)
@

We notice that the output from both methods of estimating the standard
errors produced similar results. In fact, the \texttt{boot} procedure is to
be preferred since it invisibly returns much more information (which
we will use later) than our naive script and it is much quicker in its
computations.



\begin{rem}[]
Some things to keep in mind about the bootstrap:
\begin{itemize}
\item For many statistics, the bootstrap distribution closely resembles
  the sampling distribution with respect to spread and shape. However,
  the bootstrap will not have the same center as the true sampling
  distribution. While the sampling distribution is centered at the
  population mean (plus any bias), the bootstrap distribution is
  centered at the original value of the statistic (plus any bias). The
  \texttt{boot} function gives an empirical estimate of the bias of the
  statistic as part of its output.
\item We tried to estimate the standard error, but we could have (in
  principle) tried to estimate something else. Note from the previous
  remark, however, that it would be useless to estimate the population
  mean \(\mu\) using the bootstrap since the mean of the bootstrap
  distribution is the observed \(\overline{x}\).
\item You don't get something from nothing. We have seen that we can take
  a random sample from a population and use bootstrap methods to get a
  very good idea about standard errors, bias, and the like. However,
  one must not get lured into believing that by doing some random
  resampling somehow one gets more information about the parameters
  than that which was contained in the original sample. Indeed, there
  is some uncertainty about the parameter due to the randomness of the
  original sample, and there is even more uncertainty introduced by
  resampling. One should think of the bootstrap as just another
  estimation method, nothing more, nothing less.
\end{itemize}
\end{rem}

\section{Bootstrap Confidence Intervals} 
\label{sec:bootstrap-confidence-intervals}


\subsection{Percentile Confidence Intervals}

As a first try, we want to obtain a 95\% confidence interval for a
parameter. Typically the statistic we use to estimate the parameter is
centered at (or at least close by) the parameter; in such cases a 95\%
confidence interval for the parameter is nothing more than a 95\%
confidence interval for the statistic. And to find a 95\% confidence
interval for the statistic we need only go to its sampling
distribution to find an interval that contains 95\% of the area. (The
most popular choice is the equal-tailed interval with 2.5\% in each
tail.)

This is incredibly easy to accomplish with the bootstrap. We need only to take a bunch of bootstrap resamples, order them, and choose the \(\alpha/2\)th and \((1-\alpha)\)th percentiles. There is a function \texttt{boot.ci} \index{boot.ci@\texttt{boot.ci}} in \textsf{R} already created to do just this. Note that in order to use the function \texttt{boot.ci} we must first run the \texttt{boot} function and save the output in a variable, for example, \texttt{data.boot}. We then plug \texttt{data.boot} into the function \texttt{boot.ci}.




\begin{example}[Percentile interval for the expected value of the median]
\label{exm:percentile-interval-median-first}
We will try the naive approach where we generate the resamples and calculate the percentile interval by hand.
\end{example}


<<echo=TRUE>>=
btsamps <- replicate(2000, sample(stack.loss, 21, TRUE),
                     simplify = FALSE)
thetast <- sapply(btsamps, median, simplify = TRUE)
mean(thetast)
median(stack.loss)
quantile(thetast, c(0.025, 0.975))
@




\begin{example}[Confidence interval for expected value of the median, second try]
\label{}
Now we will do it the right way with the \texttt{boot} function.
\end{example}

<<echo=TRUE>>=
med_fun <- function(x, ind) median(x[ind])
med_boot <- boot(stack.loss, med_fun, R = 2000)
boot.ci(med_boot, type = c("perc", "norm", "bca"))
@


\subsection{Student's t intervals (``normal intervals'')}

The idea is to use confidence intervals that we already know and let
the bootstrap help us when we get into trouble. We know that a
\(100(1-\alpha)\%\) confidence interval for the mean of a \(SRS(n)\)
from a normal distribution is
\begin{equation}
\overline{X}\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)\frac{S}{\sqrt{n}},
\end{equation}
where \(\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)\) is the appropriate
critical value from Student's \(t\) distribution, and we remember that
an estimate for the standard error of \(\overline{X}\) is
\(S/\sqrt{n}\). Of course, the estimate for the standard error will
change when the underlying population distribution is not normal, or
when we use a statistic more complicated than \(\overline{X}\). In
those situations the bootstrap will give us quite reasonable estimates
for the standard error. And as long as the sampling distribution of
our statistic is approximately bell-shaped with small bias, the
interval
\begin{equation}
\mbox{statistic}\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)*\mathrm{SE}(\mbox{statistic})
\end{equation}
 will have approximately \(100(1-\alpha)\%\) confidence of containing
 \(\mathbb{E}(\mathrm{statistic})\).



\begin{example}[]
We will use the t-interval method to find the bootstrap CI for the
median. We have looked at the bootstrap distribution; it appears to be
symmetric and approximately mound shaped. Further, we may check that
the bias is approximately 40, which on the scale of these data is
practically negligible. Thus, we may consider looking at the
\(t\)-intervals. Note that, since our sample is so large, instead of
\(t\)-intervals we will essentially be using \(z\)-intervals.
\end{example}

We see that, considering the scale of the data, the confidence
intervals compare with each other quite well.



\begin{rem}[]
We have seen two methods for bootstrapping confidence intervals for a
statistic. Which method should we use? If the bias of the bootstrap
distribution is small and if the distribution is close to normal, then
the percentile and \(t\)-intervals will closely agree. If the
intervals are noticeably different, then it should be considered
evidence that the normality and bias conditions are not met. In this
case, \textit{neither} interval should be used.
\end{rem}

\begin{itemize}
\item \(BC_{a}\): bias-corrected and accelerated
    \begin{itemize}
    \item transformation invariant
    \item more correct and accurate
    \item not monotone in coverage level?
    \end{itemize}
\item \(t\)-intervals
    \begin{itemize}
    \item more natural
    \item numerically unstable
    \end{itemize}
\item Can do things like transform scales, compute confidence intervals,
  and then transform back.
\item Studentized bootstrap confidence intervals where is the Studentized
  version of is the order statistic of the simulation
\end{itemize}

\section{Resampling in Hypothesis Tests} 
\label{sec:resampling-in-hypothesis}

The classical two-sample problem can be stated as follows: given two
groups of interest, we would like to know whether these two groups are
significantly different from one another or whether the groups are
reasonably similar. The standard way to decide is to

\begin{enumerate}
\item Go collect some information from the two groups and calculate an
   associated statistic, for example,
   \(\overline{X}_{1}-\overline{X}_{2}\).
\item Suppose that there is no difference in the groups, and find the
   distribution of the statistic in 1.
\item Locate the observed value of the statistic with respect to the
   distribution found in 2. A value in the main body of the
   distribution is not spectacular, it could reasonably have occurred
   by chance. A value in the tail of the distribution is unlikely, and
   hence provides evidence \textit{against} the null hypothesis that the
   population distributions are the same.
\end{enumerate}


Of course, we usually compute a \textit{p}-value, defined to be the
probability of the observed value of the statistic or more extreme
when the null hypothesis is true. Small \(p\)-values are evidence
against the null hypothesis. It is not immediately obvious how to use
resampling methods here, so we discuss an example.

\subsubsection{Procedure}

\begin{enumerate}
\item Randomly resample 10 scores from the combined scores of \texttt{x1} and
   \texttt{x2}, and assign then to the \texttt{x1} group. The rest will then be in
   the \texttt{x2} group. Calculate the difference in (re)sampled means, and
   store that value.
 \item Repeat this procedure many, many times and draw a histogram of
   the resampled statistics, called the \textit{permutation
     distribution}. Locate the observed difference 10.9 on the
   histogram to get the \(p\)-value. If the \(p\)-value is small, then
   we consider that evidence against the hypothesis that the groups
   are the same.
\end{enumerate}



\begin{rem}[]
In calculating the permutation test \textit{p-value}, the formula is essentially the proportion of resample statistics that are greater than or equal to the observed value. Of course, this is merely an \textit{estimate} of the true \(p\)-value. As it turns out, an adjustment of \(+1\) to both the numerator and denominator of the proportion improves the performance of the estimated \(p\)-value, and this adjustment is implemented in the \texttt{ts.perm} function.
\end{rem}

The \texttt{coin} package implements a permutation test with the \texttt{oneway\_test} function.

<<echo=TRUE>>=
library(coin)
oneway_test(len ~ supp, data = ToothGrowth)
@


\subsection{Comparison with the Two Sample \(t\)-test}

We know from Chapter~\ref{cha:hypothesis-testing} to use the two-sample
\(t\)-test to tell whether there is an improvement as a result of
taking the intervention class. Note that the \(t\)-test assumes normal
underlying populations, with unknown variance, and small sample
\(n=10\). What does the \(t\)-test say? Below is the output.

<<echo=TRUE>>=
t.test(len ~ supp, data = ToothGrowth, alt = "greater",
       var.equal = TRUE)
@


<<>>=
A <- show(oneway_test(len ~ supp, data = ToothGrowth))
B <- t.test(len ~ supp, data = ToothGrowth, alt = "greater",
            var.equal = TRUE)
@

The \(p\)-value for the \(t\)-test was \Sexpr{round(B$p.value, 3)}, while
the permutation test \(p\)-value was \Sexpr{round(A$p.value, 3)}. Note
that there is an underlying normality assumption for the \(t\)-test,
which isn't present in the permutation test. If the normality
assumption may be questionable, then the permutation test would be
more reasonable. We see what can happen when using a test in a
situation where the assumptions are not met: smaller \(p\)-values. In
situations where the normality assumptions are not met, for example,
small sample scenarios, the permutation test is to be preferred. In
particular, if accuracy is very important then we should use the
permutation test.



\begin{rem}[]
Here are some things about permutation tests to keep in mind.

\begin{itemize}
\item While the permutation test does not require normality of the
  populations (as contrasted with the \(t\)-test), nevertheless it
  still requires that the two groups are exchangeable; see Section~\ref{sec:exchangeable-random-variables}. In particular, this means that they
  must be identically distributed under the null hypothesis. They must
  have not only the same means, but they must also have the same
  spread, shape, and everything else. This assumption may or may not
  be true in a given example, but it will rarely cause the \(t\)-test
  to outperform the permutation test, because even if the sample
  standard deviations are markedly different it does not mean that the
  population standard deviations are different. In many situations the
  permutation test will also carry over to the \(t\)-test.
\item If the distribution of the groups is close to normal, then the
  \(t\)-test \(p\)-value and the bootstrap \(p\)-value will be
  approximately equal. If they differ markedly, then this should be
  considered evidence that the normality assumptions do not hold.
\item The generality of the permutation test is such that one can use all
  kinds of statistics to compare the two groups. One could compare the
  difference in variances or the difference in (just about
  anything). Alternatively, one could compare the ratio of sample
  means, \(\overline{X}_{1}/\overline{X}_{2}\). Of course, under the
  null hypothesis this last quantity should be near 1.
\item Just as with the bootstrap, the answer we get is subject to
  variability due to the inherent randomness of resampling from the
  data. We can make the variability as small as we like by taking
  sufficiently many resamples. How many? If the conclusion is very
  important (that is, if lots of money is at stake), then take
  thousands. For point estimation problems typically, \(R=1000\)
  resamples, or so, is enough. In general, if the true \(p\)-value is
  \(p\) then the standard error of the estimated \(p\)-value is
  \(\sqrt{p(1-p)/R}\). You can choose \(R\) to get whatever accuracy
  desired.
\end{itemize}
\end{rem}

\begin{itemize}
\item Other possible testing designs:
    \begin{itemize}
    \item Matched Pairs Designs.
    \item Relationship between two variables.
    \end{itemize}
\end{itemize}


\section{Chapter Exercises}

\chapter{Categorical Data Analysis}  
\label{cha:categorical-data-analysis}

\paragraph{What do I want them to know?}

\begin{itemize}
\item that there are many, many, many ways to estimate a population proportion, and how to do it if needed,
\item how to work with two-way tables, how to enter, manipulate, reorder, generally not be afraid of them,
\item multiple valid ways to visually display categorical data, and how to find patterns/structure in them that aren't always obvious,
\item the greatest hits of categorical inference, such as the Chi-square family of tests, the Fisher Exact Test, 
\item be exposed so some of the finer points of categorical inference, so they will know what to study more deeply somewhere else.

\end{itemize}



<<echo=FALSE, include=FALSE>>=
set.seed(42)
library(binom)
library(prob)
library(reshape)
library(vcd)
@

<<echo=FALSE, include=FALSE, message=FALSE>>=
Dataset = structure(list(School = structure(c(3L, 2L, 1L, 4L, 3L, 2L, 1L,
4L, 3L, 2L, 1L, 4L), .Label = c("Adequate", "Good", "Most desirable",
"Undesirable"), class = "factor"), Rating = structure(c(2L, 2L,
2L, 2L, 1L, 1L, 1L, 1L, 3L, 3L, 3L, 3L), .Label = c("Average",
"Outstanding", "Poor"), class = "factor"), Frequency = c(21,
3, 14, 10, 20, 25, 8, 7, 4, 36, 2, 6)), row.names = c(NA, -12L
), .Names = c("School", "Rating", "Frequency"), class = "data.frame")

library(prob)
A = gen2wayTable(pmatrix = matrix(c(1,3,4,2), nrow = 2),
                 addmargins = FALSE,
                 as.df = TRUE,
                 dmnames = list(gender = c("female","male"),
                                politics = c("dem", "rep")))
B = gen2wayTable(pmatrix = matrix(c(1,3,6,2,4,5), nrow = 2),
                 addmargins = FALSE,
                 as.df = TRUE,
                 dmnames = list(gender = c("female","male"),
                                politics = c("dem", "ind", "rep")))
C = genIndepTable(prow = 1:2, pcol = 1:3,
                 addmargins = FALSE,
                 as.df = TRUE,
                 dmnames = list(gender = c("female","male"),
                                politics = c("dem", "ind", "rep")))
@


\section{Inference for One Proportion}

For this section we observe a simple random sample of size \(n\) of successes and failures (that is, Bernoulli trials), and the parameter of interest is \(\pi = \mathbb{P}(\text{Success})\) on any one trial.  

\subsection{Asymptotic, or Wald interval (Laplace, 1812)}
In Section~\ref{sec:confidence-intervals-proportions} we derived the following interval. Let \(y = \text{number of successes}\) and \(n = \text{sample size}\), and set \(\hat{\pi}= y/n\).  The \(100(1-\alpha)\%\) confidence interval for \(\pi\) is
\begin{equation}
\hat{\pi} \pm z_{\alpha/2}\sqrt{\frac{\hat{\pi}(1 - \hat{\pi})}{n}}.
\end{equation}

<<>>=
library(binom)
binom.confint(2, 10, methods = "asymptotic")
@

\noindent
This interval has poor performance unless \(n\) is very large.  The coverage probability often falls below \(1 - \alpha\) and can fall very low when \(\pi\) is near 0 or 1.

\subsection{Wilson, or Score interval (E. B. Wilson, 1927)}

We derived this interval, too, in Section~\ref{sec:confidence-intervals-proportions}. Again take \(\hat{\pi}= y/n\).  The confidence interval is
\begin{equation}
   \left.\left\{\left(\hat{\pi}+\frac{z_{\alpha/2}^{2}}{2n}\right)\pm z_{\alpha/2}\sqrt{\frac{\hat{\pi}(1-\hat{\pi})}{n}+\frac{z_{\alpha/2}^{2}}{(2n)^{2}}}\right\}\right/ \left(1+\frac{z_{\alpha/2}^{2}}{n}\right).
\end{equation}

<<>>=
binom.confint(2, 10, methods = "wilson")
@

\noindent
This is the confidence interval reported from the \texttt{prop.test} function when \texttt{correct = FALSE} (see below). The Wilson interval outperforms the Wald interval, but has poor coverage (less than \(1 - \alpha\)) when \(\pi\) is near 0 or 1.


\subsection{Agresti-Coull interval (1998)}

Let \(\tilde{y} = y + 0.5z_{\alpha/2}^2\) and \(\tilde{n} = n + z_{\alpha/2}^2\), and set \(\tilde{\pi}= \tilde{y}/\tilde{n}\).  The confidence interval is
\begin{equation}
\tilde{\pi} \pm z_{\alpha/2}\sqrt{\frac{\tilde{\pi}(1 - \tilde{\pi})}{n}}.
\end{equation}

<<>>=
binom.confint(2, 10, methods = "ac")
@

\noindent
For a 95\% interval (so \(z_{\alpha/2} \approx 2)\), this is the famous ``add two successes and two failures'' rule of thumb that yields
\begin{equation}
\tilde{\pi} \approx \frac{y + 2}{n + 4}.
\end{equation}

\noindent
This interval performs much better than the Wald interval, and contains the Wilson interval.  It does better than the Wilson interval when \(\pi\) is near 0 or 1.

\subsection{Clopper-Pearson interval (1934)}

The formula for this interval is difficult to write down compactly, but it comes from inverting the exact binomial test which is based on the binomial distribution.

<<>>=
binom.confint(2, 10, methods = "exact")
@

\noindent
This interval always has coverage \(\geq 1 - \alpha\), but it is often too conservative.

\subsection{Comparison}

There are actually 11 different confidence intervals for a population proportion supported in package \texttt{binom}, and they all have their advantages and disadvantages.  Below are all 11 intervals for an observed \texttt{y = 2} successes in \texttt{n = 10} independent trials.

<<>>=
binom.confint(2, 10, methods = "all")
@

\subsection{Hypothesis tests}

Due to the deep connection between confidence intervals and hypothesis tests (see Remark~\ref{rem:ci-hyp-brothers}), we can always construct a confidence interval and see if it covers a null hypothesized value.  So that means the above \texttt{11} intervals correspond to \texttt{11} different hypothesis tests we now have at our disposal. But the confidence interval method does not generate a
\textit{p}-value. Neverthelesss, the \texttt{prop.test} function is also known as the ``Score Test'' and will generate a \textit{p}-value.

<<>>=
prop.test(x = 2, n = 10, p = 0.5, correct = FALSE)
@

\noindent
Notice that the confidence interval reported by \texttt{prop.test}
when \texttt{correct = FALSE} matches the \texttt{wilson} confidence
interval reported above.

\section{Two-way Contingency Tables}

A two-way table looks like this:
<<echo = FALSE>>=
xtabs(Frequency ~ School + Rating, data = Dataset)
@

There are two variables (hence the name): a \textit{row} variable and a \textit{column} variable.  The numbers in the body of the table are counts of observations falling into the respective row/column combinations.

\subsection{Enter by hand}

If you already have a two-way table (say, from a textbook or website)
then you can enter it into the computer manually like this:

<<>>=
matrix(c(1,2,3,4,5,6), nrow = 2,
           dimnames = list(gender = c("female","male"),
                           politics = c("dem","ind","rep")))
@

Technically speaking this is not a \texttt{table} object, it is a \texttt{matrix}, but it will suit our purposes.

\subsection{From a data frame with \texttt{xtabs}}

Most of the datasets from this chapter's exercises look like this after importing:
<<>>=
head(Dataset)
@

The important thing about this is that there are two columns which represent the categorical variables, and then a third column \texttt{Frequency} which gives the counts for each row/column combination.  We make a two-way table from such a data frame like this:

<<>>=
xtabs(Frequency ~ School + Rating, data = Dataset)
@

Sometimes, though, the data frame does not have a \texttt{Frequency} column, and instead it has a bunch of repeated rows, one row for each observation falling in that row/column combination of the table, like this:

<<>>=
head(A)
@

(This is the case we usually have in data that we collect ourselves).  We still use \texttt{xtabs} to make the table, but we leave the left hand side of the formula specification blank:

<<>>=
xtabs(~ politics, data = A)
xtabs(~ gender, data = A)
xtabs(~ gender + politics, data = A)
@

\subsection{From a table with \texttt{xtabs}}

In the examples below we start with a multi-way contingency table (3 or 4 dimensions) and we construct a two-way table for analysis using the same \texttt{xtabs} function.  We just need to remember that when starting with a table we need to add a \texttt{Freq} to the left hand side of the formula specification, like this, for example using the built-in table \texttt{HairEyeColor}:

<<>>=
HairEyeColor
@

Now we can make a two-way table comparing, say, \texttt{Hair} and \texttt{Eye}.

<<>>=
xtabs(Freq ~ Hair + Eye, data = HairEyeColor)
@

Or we could do \texttt{Sex} and \texttt{Eye}.

<<>>=
xtabs(Freq ~ Hair + Eye, data = HairEyeColor)
@

\subsection{Visualization}

The most basic visual display of a two-way table is a bar graph constructed via the \texttt{barplot} function.  Note that in every two-way table you need a \texttt{legend} to denote the values of the second variable. 

<<stacked-bar-plot, echo=TRUE, fig=TRUE, include=FALSE, height=3.25,width=5>>=
x = xtabs(Freq ~ Sex + Eye, data = HairEyeColor)
barplot(x, legend = TRUE)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-stacked-bar-plot}
\end{center}
\vspace{-0.5in} \caption[A stacked bar plot of \texttt{HairEyeColor}.]{{\small A stacked bar plot of \texttt{HairEyeColor}. }}
\label{fig:stacked-bar-plot}
\end{figure}


Next is a side-by-side bar graph which is available with the argument \texttt{beside=TRUE}.

<<sidebyside-bar-plot, echo=TRUE, fig=TRUE, include=FALSE, height=3.25,width=5>>=
barplot(x, beside = TRUE, legend = TRUE)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-sidebyside-bar-plot}
\end{center}
\vspace{-0.5in} \caption[A side-by-side bar plot of \texttt{HairEyeColor}.]{{\small A side-by-side bar plot of \texttt{HairEyeColor}. }}
\label{fig:sidebyside-bar-plot}
\end{figure}

We can swap the row/column variables via the transpose function \texttt{t()}.

<<transpose-bar-plot, echo=TRUE, fig=TRUE, include=FALSE, height=3.25,width=5>>=
barplot(t(x), beside = TRUE, legend = TRUE)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-transpose-bar-plot}
\end{center}
\vspace{-0.5in}
\caption[A transposed side-by-side bar plot of \texttt{HairEyeColor}.]{{\small A transposed side-by-side bar plot of \texttt{HairEyeColor}. }}
\label{fig:transpose-bar-plot}
\end{figure}

A \texttt{mosaic} plot is important and can be used for even more than two-way tables (three-way, multi-way\ldots).

<<>>=
library(vcd)
mosaic(x)
@

If you just have a one-way table then you can make a bar graph and you do not need a \texttt{legend}.

<<>>=
barplot(xtabs(Freq ~ Hair, data = HairEyeColor))
@

Pie graphs are often a bad idea for reasons discussed in class, but if you are hellbent on making one then use the \texttt{pie} function.

<<>>=
pie(xtabs(Freq ~ Hair, data = HairEyeColor))
@


\subsection{Stuff to do with tables}

We can calculate sums in the margin with the \texttt{addmargins} function.

<<>>=
x = xtabs(Freq ~ Eye, data = HairEyeColor)
addmargins(x)
@

The \texttt{prop.table} function will convert a frequency table to a relative frequency table of proportions.

<<>>=
prop.table(x)
@

Now we will do the same things for a two-way table such as this one:
<<>>=
y = xtabs(Freq ~ Hair + Eye, data = HairEyeColor)
y
@

We calculate row sums, column sums, both at once, and a table of proportions.

<<>>=
rowSums(y)
colSums(y)
addmargins(y)
prop.table(y)
@

We can add just the row sums and calculate the row conditional distributions:

<<>>=
addmargins(y, margin = 2)
prop.table(y, margin = 1)
@

Notice the rows sum to one in the above table. We can add just the column sums and calculate the column conditional distributions:

<<>>=
addmargins(y, margin = 1)
prop.table(y, margin = 2)
@

Notice the columns sum to one in this table.

\subsection{Reorder rows and/or columns}

Original table:

<<>>=
z = xtabs(Freq ~ Sex + Hair, data = HairEyeColor)
z
@

Reorder rows:
<<>>=
z[c(2,1), ]
@

Reorder columns:
<<>>=
z[ , c(3,1,4,2)]
@

Reorder rows and columns:

<<>>=
z[c(2,1), c(3,1,4,2)]
@

This is useful for all sorts of things, and one of them is to improve visual displays by ordering categories in decreasing/increasing frequency, like this:

<<>>=
z = xtabs(Freq ~ Hair, data = HairEyeColor)
z
@

We can make improved pie/bar graphs like this (notice the side-by-side display):

<<imppiebar, echo=TRUE, fig=TRUE, include=FALSE, width=5>>=
par(mfrow = c(2,1))
pie(z[c(2,4,1,3)])
barplot(z[c(2,4,1,3)])
par(mfrow = c(1,1))
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-imppiebar}
\end{center}
\caption{{}}
\label{fig:imppiebar}
\end{figure}

There, that's better.

\subsection{Untable}

Maybe you start with a table (that you entered by hand, say), and you would like to have a data frame to analyze with the \textsf{R} Commander or any of our other aforementioned methods.  First convert to a data frame:

<<>>=
Y = as.data.frame(y)
head(Y)
@

Then \texttt{untable} the data frame.  The \texttt{[, -3]} and \texttt{row.names} parts are just there to make the end result look more pretty.

<<>>=
library(reshape)
Ydf = untable(Y, Y$Freq)[ , -3]
row.names(Ydf) <- NULL
head(Ydf)
@

Now we can set \texttt{Ydf} as the \textit{Active Dataset} in the R
Commander and proceed as usual.

\section{Two Proportions}

The model looks like this:

\begin{equation}
\begin{array}{cccc}
 & \text{Success} & \text{Failure} & \text{Total}\\
\text{Group 1} & \pi_{1} & 1 - \pi_{1} & 1 \\
\text{Group 2} & \pi_{2} & 1 - \pi_{2} & 1
\end{array}
\end{equation}

We observe $n_{1}$ observations from Group 1 with $y_{1}$ successes, and we observe $n_{2}$ observations from Group 2 with $y_{2}$ successes, like this:

\begin{equation}
\begin{array}{cccc}
 & \text{Success} & \text{Failure} & \text{Total}\\
\text{Group 1} & y_{1} & n_{1} - y_{1} & n_{1}\\
\text{Group 2} & y_{2} & n_{2} - y_{2} & n_{2}
\end{array}
\end{equation}

We wish to test (for example, two-sided hypotheses):
\[
H_{0}: \pi_{1} = \pi_{2}\text{ versus } H_{a}:\pi_{1}\neq \pi_{2}.
\]

\subsection{Asymptotic test}

Consider the following 2x2 table:
<<>>=
y = xtabs(Freq ~ Sex + Survived, data = Titanic)
y
@

<<>>=
addmargins(y, margin = 2)
@

<<>>=
prop.table(y, margin = 1)
@

We perform the hypothesis test with \texttt{prop.test} just like before with \texttt{correct = FALSE}.

<<>>=
prop.test(y, correct = FALSE)
@

There is a statistically significant difference between the probability of death for the two genders in the \texttt{Titanic} data.

\subsection{Fisher's Exact Test}

The lady in question (Dr. Muriel Bristol) claimed to be able to tell whether the tea or the milk was added first to a cup. Fisher proposed to give her eight cups, four of each variety, in random order. This test calculates the probability of her getting the specific number of cups she correctly identified by chance alone.


<<>>=
LadyTastingTea = matrix(c(4,0,0,4), nrow = 2,
                dimnames = list(pouredFirst = c("milk","tea"),
                guessedFirst = c("milk","tea")))
LadyTastingTea
@

<<>>=
fisher.test(LadyTastingTea, alternative = "greater")
@

These data provide evidence that Dr. Bristol really could tell which was added first.

\subsection{McNemar Test for Matched Pairs}

For this type of problem we have matched pairs of data, for example, we may ask a single respondent on the same survey twice, one before and one after a treatment of some kind.  The hypotheses of interest are
\begin{equation}
H_{0}: \pi_{1+} = \pi_{+1}\text{ versus } H_{a}:\pi_{1+}\neq \pi_{+1}
\end{equation}
which after some algebra is equivalent to the hypotheses
\begin{equation}
H_{0}: \pi_{12} = \pi_{21}\text{ versus } H_{a}:\pi_{12}\neq \pi_{21}.
\end{equation}

The test statistic is
\begin{equation}
\chi^{2}= \frac{n_{12}- n_{21}}{n_{12}+n_{21}},
\end{equation}
and when the null hypothesis is true the statistic has a chi-square distribution with one degree of freedom.  There is a continuity correction proposed by Edwards\footnote{Edwards, A (1948). ``Note on the correction for continuity in testing the significance of the difference between correlated proportions''. Psychometrika. 13:185187. doi:10.1007/bf02289261} that looks like this:
\begin{equation}
\chi^{2}= \frac{\left(\vert n_{12}- n_{21}\vert - 1 \right)^2}{n_{12}+n_{21}},
\end{equation}


The following example data from Agresti were about a panel of 1600 voting-age British citizens.  They were given a survey about whether they approved or disapproved of the Prime Minister's performance, and then those same 1600 people were asked again six months later.

<<>>=
Performance <- matrix(c(794, 86, 150, 570), nrow = 2,
  dimnames = list("1st Survey" = c("Approve", "Disapprove"),
                 "2nd Survey" = c("Approve", "Disapprove")))
Performance
@

We do the test like this:
<<>>=
mcnemar.test(Performance)
@

Here the proportion of those that approve after six months is significantly different from the proportion that approved before.

\section{Chi-square Tests}

All chi-square tests are based on a test statistic \(\chi^{2}\) which takes the general form
\begin{equation}
\chi^2 = \sum_{i} \frac{(O_{i} - E_{i})^2}{E_{i}},
\end{equation}
where \(O_{i}\) is an \textit{observed} count (an entry in the table) and \(E_{i}\) is an \textit{expected} value, that is, the value we would expect if the null hypothesis is true.  When \(H_{0}\) is true the test statistic has an (asymptotic) chi-square distribution with some degrees of freedom (which depends on the specific hypotheses being tested).

\subsection{Chi-square goodness of fit}

For this class of problems we have a univariate categorical variable (like \texttt{gender} or \texttt{politics}) and a hypothesized model for the probabilities of the different categories.  We would like to see how well the model fits the data.

Suppose the variable \(X\) has \(k\) categories, and we will write our hypothesized model this way:
\begin{equation}
\mathbb{P}(X = \text{category }i) = \pi_{i},\ \text{for }i=1,2,\ldots,k.
\end{equation}
We observe a random sample of size \(n\).  Here \(O_{i}\) is the number of observations in category \(i\), so that \(O_{1}+O_{2}+\cdots+O_{k} = n\).  If the model truly holds then we would expect \(E_{i}= n\pi_{i}\) observations in category \(i\).  The test statistic is
\begin{equation}
\chi^2 = \sum_{i = 1}^k \frac{(O_{i} - E_{i})^2}{E_{i}}
\end{equation}
and when \(H_{0}\) is true, the test statistic has a chi-square distribution with $k - 1$ degrees of freedom.

To do the test with \textsf{R} we enter the observed counts in a vector \texttt{observed} (could also be a one-way table) and the hypothesized probabilities in a vector \texttt{probs}:
<<>>=
observed = c(160, 20, 10, 10)
probs = c(0.50, 0.25, 0.10, 0.15)
@

Then perform the test this way:

<<>>=
chisq.test(observed, p = probs)
@

In this case we reject the null hypothesis and the theoretical model does not fit these data very well (at all).

\subsection{Chi-square tests of Independence and Homogeneity}

The model is

\begin{equation}
\begin{array}{cccccc}
 & \text{Response 1} & \text{Response 2} & &  \text{Response }c & \text{Total}\\
\text{Group 1} & \pi_{11} & \pi_{12} & \cdots &  \pi_{1c} &  \pi_{1+} \\
\text{Group 2} & \pi_{21} & \pi_{22} & \cdots &  \pi_{2c} &  \pi_{2+} \\
\vdots & \vdots & \vdots & \ddots &  \vdots &  \vdots \\
\text{Group }r & \pi_{r1} & \pi_{r2} & \cdots &  \pi_{rc} &  \pi_{r+} \\
\text{Total} & \pi_{+1} & \pi_{+2} & \cdots &  \pi_{+c} &  1 \\
\end{array}
\end{equation}

We observe $n_{ij}$ observations from Group $i$ with Response $j$, with $n$ being the total sample size, like this:

\begin{equation}
\begin{array}{cccccc}
 & \text{Response 1} & \text{Response 2} & &  \text{Response }c & \text{Total}\\
\text{Group 1} & n_{11} & n_{12} & \cdots &  n_{1c} &  n_{1+} \\
\text{Group 2} & n_{21} & n_{22} & \cdots &  n_{2c} &  n_{2+} \\
\vdots & \vdots & \vdots & \ddots &  \vdots &  \vdots \\
\text{Group }r & n_{r1} & n_{r2} & \cdots &  n_{rc} &  n_{r+} \\
\text{Total} & n_{+1} & n_{+2} & \cdots &  n_{+c} &  n \\
\end{array}
\end{equation}

Here, the test statistic takes the general form
\begin{equation}
\chi^2 = \sum_{i=1}^{r}\sum_{j=1}^{c} \frac{(O_{ij} - E_{ij})^2}{E_{ij}},
\end{equation}
but the hypotheses are different so the expected counts are calculated differently and the degrees of freedom are now \((r - 1)(c - 1)\) where \(r\) is the number of rows and \(c\) is the number of columns.

The conditions to use this test are
\begin{itemize}
\item no \(E_{ij}\) is less than one, and
\item no more than 20\% of the \(E_{ij}\)'s are less than five.
\end{itemize}
\noindent
If expected counts are too small then we can
\begin{itemize}
\item combine categories provided there is a natural way to combine them, or
\item use exact tests
\end{itemize}

\subsubsection{Independence}

Here we observe a single random sample and we place observations in the table depending on their classification status.  The hypotheses of interest are
\begin{equation}
H_{0}: \pi_{ij} = \pi_{i+}\pi_{+j}\text{ versus } H_{a}:\pi_{ij}\neq \pi_{i+}\pi_{+j},
\end{equation}
for \(i=1,\ldots,r\) and \(j=1,\ldots,c\).
The expected counts are
\begin{equation}
E_{ij} = n\hat{\pi}_{i+}\hat{\pi}_{+j} = n \frac{n_{i+}}{n}\frac{n_{+j}}{n} = \frac{n_{i+}n_{+j}}{n}.
\end{equation}


\subsubsection{Homogeneity}

Here we have a collection of $r$ subpopulations and we observe simple random samples from each subpopulation (it does not matter whether rows or columns represent the different subpopulations).  The hypotheses of interest to us now are are
\begin{equation}
H_{0}: \text{the row distributions } (\pi_{i1}, \pi_{i2},\ldots\pi_{ic}) \text{ are all the same,}
\end{equation}
for \(i=1,\ldots,r\), versus
\begin{equation}
H_{a}: \text{at least one row is different from the others.}
\end{equation}
The expected counts are calculated with the same formula, and the test is conducted the same way, but the interpretation is different.

\subsubsection{How to do the tests}

We have our data from above

<<>>=
x = xtabs(Freq ~ Sex + Eye, data = HairEyeColor)
x
@

We can compare the row distributions with the following sequence of steps.

<<>>=
addmargins(x, margin = 2)
prop.table(x, margin = 1)
@

The row distributions look similar to each other.  Now we compare the
column distributions.

<<>>=
addmargins(x, margin = 1)
prop.table(x, margin = 2)
@

Again, the column distributions look very similar.  We expect the
chi-square test not to be significant.
<<>>=
chisq.test(x)
@

Here is a visual display of same.
<<echo=TRUE, fig=TRUE, include=FALSE, width=5>>=
mosaic(x, shade = TRUE)
@


<<echo=TRUE, fig=TRUE, include=FALSE, width=5>>=
assoc(x, shade = TRUE)
@


Now let us see if hair color is related to eye color.  Here are the data.

<<>>=
y = xtabs(Freq ~ Hair + Eye, data = HairEyeColor)
addmargins(y, margin = 2)
prop.table(y, margin = 1)
@

The row distributions are not similar at all (maybe brown and red, but
not the other rows).

<<>>=
addmargins(y, margin = 1)
prop.table(y, margin = 2)
@

Neither are the column distributions.  We expect the chi-square
statistic to be significant.

<<>>=
chisq.test(y)
@

And here is a visual display of the same.  Blue means higher observed
frequency than would be expected under independence, red means lower
than expected frequency.

<<echo=TRUE, fig=TRUE, include=FALSE, width=5>>=
mosaic(y, shade = TRUE)
@


<<echo=TRUE, fig=TRUE, include=FALSE, width=5>>=
assoc(y, shade = TRUE)
@



\section{Odds and Odds Ratios}

\subsection{Odds of an event}

Odds in favor of \(A\) is
\begin{equation}
\text{Odds in favor of event }A = \frac{\mathbb{P}(A)}{1 - \mathbb{P}(A)}.
\end{equation}

\subsection{Odds ratio}

Here we have an underlying model that looks like this:
\begin{equation}
\begin{array}{cccc}
 & \text{Success} & \text{Failure} & \text{Total}\\
\text{Group 1} & \pi_{11} & \pi_{12} & \pi_{1+}\\
\text{Group 2} & \pi_{21} & \pi_{22} & \pi_{2+}\\
\text{Total} & \pi_{+1} & \pi_{+2} & 1
\end{array}
\end{equation}

\noindent
The \textit{odds ratio} associated with this model is
\begin{equation}
\psi = \frac{\pi_{11}/\pi_{12}}{\pi_{21}/\pi_{22}} = \frac{\pi_{11}\pi_{22}}{\pi_{21}\pi_{12}},
\end{equation}
and it represents the odds in favor of success for Group 1 divided by the odds
in favor of success for Group 2. We observe a sample and construct a sample
contingency table which we write like this:
\begin{equation}
\begin{array}{cccc}
 & \text{Success} & \text{Failure} & \text{Total}\\
\text{Group 1} & n_{11} & n_{12} & n_{1+}\\
\text{Group 2} & n_{21} & n_{22} & n_{2+}\\
\text{Total} & n_{+1} & n_{+2} & n
\end{array}
\end{equation}
Then the \textit{sample odds ratio} (we usually just call it the odds ratio) is
\begin{equation}
\hat{\psi} = \frac{n_{11}/n_{12}}{n_{21}/n_{22}} = \frac{n_{11}n_{22}}{n_{21}n_{12}}.
\end{equation}

We can compute the odds ratio by hand, or use the \texttt{oddsratio}
function in the \texttt{vcd} package.  Here is an example table:

<<>>=
x = xtabs(Freq ~ Sex + Survived, data = Titanic)
x
@

<<>>=
library(vcd)
oddsratio(x, log = FALSE)
@

\noindent
This means that the odds in favor of death were over 10 times higher for males than the odds for females (for the Titanic disaster). We can make a visual display with a \texttt{fourfold} plot.

<<fourfold-titanic, echo=TRUE, fig=TRUE, include=FALSE, height=3.25,width=5>>=
fourfold(x)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-fourfold-titanic}
\end{center}
\caption[Fourfold plot.]{{\small A fourfold plot.}}
\label{fig:fourfold-titanic}
\end{figure}




\subsection{Asymptotic distribution of \(\ln(\hat{\psi})\)}

The distribution of \(\ln(\hat{\psi})\) is asymptotically normal.
\begin{equation}
\ln(\hat{\psi}) \overset{\cdot}{\sim} \mathsf{norm}\left(\mathtt{mean} =\ln(\psi),\,\mathtt{sd} = \sqrt{\frac{1}{n_{11}} + \frac{1}{n_{12}} + \frac{1}{n_{21}} + \frac{1}{n_{22}}}\right),
\end{equation}
for \(n\) sufficiently large.

This information can be used to construct a large sample confidence interval for \(\psi\).

<<>>=
confint(oddsratio(x, log = FALSE), level = 0.95)
@

\section{Cohran-Mantel-Haenszel (CMH) Test Statistic}

For this class of problems we have a binary predictor \(X\) (like
treatment/placebo) and a categorical response \(Y\) (like
success/failure) but we also have another categorical variable \(Z\)
(like democrat, republican, independent\ldots) against which we would
like to control.  We assume that the odds ratios between \(X\) and \(Y\)
are the same at each level of \(Z\), and we would like to know whether
that common odds ratio \(\psi \neq 1\).

The CMH test statistic is
\begin{equation}
\text{CMH} = \frac{\left[ \sum_{k}\left(n_{11k} - \frac{n_{1+k}n_{+1k}}{n_{++k}} \right)\right]^{2}}{\sum_{k} \frac{n_{1+k}n_{2+k}n_{+1k}n_{+2k}}{n_{++k}^{2}(n_{++k} - 1)}}.
\end{equation}

The test statistic has an asymptotic chi-square distribution with one degree of freedom when the common odds ratio \(\psi = 1\).

MH (1959) proposed a continuity correction in their original paper, and if used, the \textit{p}-value better approximates an exact conditional test, but it tends to be conservative (Agresti).

Note that if there is evidence that the odds ratio is different for the different levels of \(Z\), then the CMH test is not appropriate.  We can check for evidence of this with a \texttt{fourfold} plot and/or the \texttt{woolf\_test}, both from package \texttt{vcd}.

The following data are from the Titanic tragedy.

<<>>=
x = xtabs(Freq ~ Sex + Age + Class, data = Titanic)
ftable(x)
@

\noindent
We check for similarity of odds ratios.  Here are the different odds
ratios:

<<>>=
oddsratio(x, log = FALSE)
@

\noindent
The estimates vary, but let us look to see if that variation is significant.

<<>>=
library(vcd)
woolf_test(x)
@


No problems there, and here is a graphical display.

<<fourfold-titanic-2, echo=TRUE, fig=TRUE, include=FALSE, width=5>>=
fourfold(x)
@

\begin{figure}
\begin{center}
\includegraphics{IPSUR-fourfold-titanic-2}
\end{center}
\vspace{-0.5in} \caption[Fourfold plot.]{{\small A fourfold plot.}}
\label{fig:fourfold-titanic-2}
\end{figure}


The graph looks good. We go on to perform the CMH test.
<<>>=
mantelhaen.test(x)
@

The marginal odds ratio of \texttt{Sex} versus \texttt{Age} is significantly
different from one (the common odds ratio is estimated to be
approximately \Sexpr{mantelhaen.test(x)$estimate}).  We estimate the
odds of being a male child at approximately half the odds of being a
female child, consistently across social status.


\appendix

\chapter{R Session Information} \label{cha:r-session-information}

If you ever try to communicate with someone about an error you are having with \textsf{R},
then you should include your session information in the message. Here is how to
do that, and below is what the output looks like.

<<echo=TRUE>>=
sessionInfo()
@



\chapter{GNU Free Documentation License} \label{cha:gnu-free-documentation}

Version 1.3, 3 November 2008

Copyright (C) 2000, 2001, 2002, 2007, 2008 Free Software Foundation,
Inc. \url{http://fsf.org/}

Everyone is permitted to copy and distribute verbatim copies of this
license document, but changing it is not allowed.

\subsubsection{0. PREAMBLE}

The purpose of this License is to make a manual, textbook, or other
functional and useful document ``free'' in the sense of freedom: to
assure everyone the effective freedom to copy and redistribute it,
with or without modifying it, either commercially or noncommercially.
Secondarily, this License preserves for the author and publisher a way
to get credit for their work, while not being considered responsible
for modifications made by others.

This License is a kind of ``copyleft'', which means that derivative
works of the document must themselves be free in the same sense. It
complements the GNU General Public License, which is a copyleft
license designed for free software.

We have designed this License in order to use it for manuals for free
software, because free software needs free documentation: a free
program should come with manuals providing the same freedoms that the
software does. But this License is not limited to software manuals; it
can be used for any textual work, regardless of subject matter or
whether it is published as a printed book. We recommend this License
principally for works whose purpose is instruction or reference.

\subsubsection{1. APPLICABILITY AND DEFINITIONS}

This License applies to any manual or other work, in any medium, that
contains a notice placed by the copyright holder saying it can be
distributed under the terms of this License. Such a notice grants a
world-wide, royalty-free license, unlimited in duration, to use that
work under the conditions stated herein. The ``Document'', below, refers
to any such manual or work. Any member of the public is a licensee,
and is addressed as ``you''. You accept the license if you copy, modify
or distribute the work in a way requiring permission under copyright
law.

A ``Modified Version'' of the Document means any work containing the
Document or a portion of it, either copied verbatim, or with
modifications and/or translated into another language.

A ``Secondary Section'' is a named appendix or a front-matter section of
the Document that deals exclusively with the relationship of the
publishers or authors of the Document to the Document's overall
subject (or to related matters) and contains nothing that could fall
directly within that overall subject. (Thus, if the Document is in
part a textbook of mathematics, a Secondary Section may not explain
any mathematics.) The relationship could be a matter of historical
connection with the subject or with related matters, or of legal,
commercial, philosophical, ethical or political position regarding
them.

The ``Invariant Sections'' are certain Secondary Sections whose titles
are designated, as being those of Invariant Sections, in the notice
that says that the Document is released under this License. If a
section does not fit the above definition of Secondary then it is not
allowed to be designated as Invariant. The Document may contain zero
Invariant Sections. If the Document does not identify any Invariant
Sections then there are none.

The ``Cover Texts'' are certain short passages of text that are listed,
as Front-Cover Texts or Back-Cover Texts, in the notice that says that
the Document is released under this License. A Front-Cover Text may be
at most 5 words, and a Back-Cover Text may be at most 25 words.

A ``Transparent'' copy of the Document means a machine-readable copy,
represented in a format whose specification is available to the
general public, that is suitable for revising the document
straightforwardly with generic text editors or (for images composed of
pixels) generic paint programs or (for drawings) some widely available
drawing editor, and that is suitable for input to text formatters or
for automatic translation to a variety of formats suitable for input
to text formatters. A copy made in an otherwise Transparent file
format whose markup, or absence of markup, has been arranged to thwart
or discourage subsequent modification by readers is not Transparent.
An image format is not Transparent if used for any substantial amount
of text. A copy that is not ``Transparent'' is called ``Opaque''.

Examples of suitable formats for Transparent copies include plain
ASCII without markup, Texinfo input format, LaTeX input format, SGML
or XML using a publicly available DTD, and standard-conforming simple
HTML, PostScript or PDF designed for human modification. Examples of
transparent image formats include PNG, XCF and JPG. Opaque formats
include proprietary formats that can be read and edited only by
proprietary word processors, SGML or XML for which the DTD and/or
processing tools are not generally available, and the
machine-generated HTML, PostScript or PDF produced by some word
processors for output purposes only.

The ``Title Page'' means, for a printed book, the title page itself,
plus such following pages as are needed to hold, legibly, the material
this License requires to appear in the title page. For works in
formats which do not have any title page as such, ``Title Page'' means
the text near the most prominent appearance of the work's title,
preceding the beginning of the body of the text.

The ``publisher'' means any person or entity that distributes copies of
the Document to the public.

A section ``Entitled XYZ'' means a named subunit of the Document whose
title either is precisely XYZ or contains XYZ in parentheses following
text that translates XYZ in another language. (Here XYZ stands for a
specific section name mentioned below, such as ``Acknowledgements'',
``Dedications'', ``Endorsements'', or ``History''.) To ``Preserve the Title''
of such a section when you modify the Document means that it remains a
section ``Entitled XYZ'' according to this definition.

The Document may include Warranty Disclaimers next to the notice which
states that this License applies to the Document. These Warranty
Disclaimers are considered to be included by reference in this
License, but only as regards disclaiming warranties: any other
implication that these Warranty Disclaimers may have is void and has
no effect on the meaning of this License.

\subsubsection{2. VERBATIM COPYING}

You may copy and distribute the Document in any medium, either
commercially or noncommercially, provided that this License, the
copyright notices, and the license notice saying this License applies
to the Document are reproduced in all copies, and that you add no
other conditions whatsoever to those of this License. You may not use
technical measures to obstruct or control the reading or further
copying of the copies you make or distribute. However, you may accept
compensation in exchange for copies. If you distribute a large enough
number of copies you must also follow the conditions in section 3.

You may also lend copies, under the same conditions stated above, and
you may publicly display copies.

\subsubsection{3. COPYING IN QUANTITY}

If you publish printed copies (or copies in media that commonly have
printed covers) of the Document, numbering more than 100, and the
Document's license notice requires Cover Texts, you must enclose the
copies in covers that carry, clearly and legibly, all these Cover
Texts: Front-Cover Texts on the front cover, and Back-Cover Texts on
the back cover. Both covers must also clearly and legibly identify you
as the publisher of these copies. The front cover must present the
full title with all words of the title equally prominent and visible.
You may add other material on the covers in addition. Copying with
changes limited to the covers, as long as they preserve the title of
the Document and satisfy these conditions, can be treated as verbatim
copying in other respects.

If the required texts for either cover are too voluminous to fit
legibly, you should put the first ones listed (as many as fit
reasonably) on the actual cover, and continue the rest onto adjacent
pages.

If you publish or distribute Opaque copies of the Document numbering
more than 100, you must either include a machine-readable Transparent
copy along with each Opaque copy, or state in or with each Opaque copy
a computer-network location from which the general network-using
public has access to download using public-standard network protocols
a complete Transparent copy of the Document, free of added material.
If you use the latter option, you must take reasonably prudent steps,
when you begin distribution of Opaque copies in quantity, to ensure
that this Transparent copy will remain thus accessible at the stated
location until at least one year after the last time you distribute an
Opaque copy (directly or through your agents or retailers) of that
edition to the public.

It is requested, but not required, that you contact the authors of the
Document well before redistributing any large number of copies, to
give them a chance to provide you with an updated version of the
Document.

\subsubsection{4. MODIFICATIONS}

You may copy and distribute a Modified Version of the Document under
the conditions of sections 2 and 3 above, provided that you release
the Modified Version under precisely this License, with the Modified
Version filling the role of the Document, thus licensing distribution
and modification of the Modified Version to whoever possesses a copy
of it. In addition, you must do these things in the Modified Version:

-   A. Use in the Title Page (and on the covers, if any) a title
    distinct from that of the Document, and from those of previous
    versions (which should, if there were any, be listed in the
    History section of the Document). You may use the same title as a
    previous version if the original publisher of that version
    gives permission.
-   B. List on the Title Page, as authors, one or more persons or
    entities responsible for authorship of the modifications in the
    Modified Version, together with at least five of the principal
    authors of the Document (all of its principal authors, if it has
    fewer than five), unless they release you from this requirement.
-   C. State on the Title page the name of the publisher of the
    Modified Version, as the publisher.
-   D. Preserve all the copyright notices of the Document.
-   E. Add an appropriate copyright notice for your modifications
    adjacent to the other copyright notices.
-   F. Include, immediately after the copyright notices, a license
    notice giving the public permission to use the Modified Version
    under the terms of this License, in the form shown in the
    Addendum below.
-   G. Preserve in that license notice the full lists of Invariant
    Sections and required Cover Texts given in the Document's
    license notice.
-   H. Include an unaltered copy of this License.
-   I. Preserve the section Entitled ``History'', Preserve its Title,
    and add to it an item stating at least the title, year, new
    authors, and publisher of the Modified Version as given on the
    Title Page. If there is no section Entitled ``History'' in the
    Document, create one stating the title, year, authors, and
    publisher of the Document as given on its Title Page, then add an
    item describing the Modified Version as stated in the
    previous sentence.
-   J. Preserve the network location, if any, given in the Document
    for public access to a Transparent copy of the Document, and
    likewise the network locations given in the Document for previous
    versions it was based on. These may be placed in the ``History''
    section. You may omit a network location for a work that was
    published at least four years before the Document itself, or if
    the original publisher of the version it refers to
    gives permission.
-   K. For any section Entitled ``Acknowledgements'' or ``Dedications'',
    Preserve the Title of the section, and preserve in the section all
    the substance and tone of each of the contributor acknowledgements
    and/or dedications given therein.
-   L. Preserve all the Invariant Sections of the Document, unaltered
    in their text and in their titles. Section numbers or the
    equivalent are not considered part of the section titles.
-   M. Delete any section Entitled ``Endorsements''. Such a section may
    not be included in the Modified Version.
-   N. Do not retitle any existing section to be Entitled
    ``Endorsements'' or to conflict in title with any Invariant Section.
-   O. Preserve any Warranty Disclaimers.

If the Modified Version includes new front-matter sections or
appendices that qualify as Secondary Sections and contain no material
copied from the Document, you may at your option designate some or all
of these sections as invariant. To do this, add their titles to the
list of Invariant Sections in the Modified Version's license notice.
These titles must be distinct from any other section titles.

You may add a section Entitled ``Endorsements'', provided it contains
nothing but endorsements of your Modified Version by various
partiesfor example, statements of peer review or that the text has
been approved by an organization as the authoritative definition of a
standard.

You may add a passage of up to five words as a Front-Cover Text, and a
passage of up to 25 words as a Back-Cover Text, to the end of the list
of Cover Texts in the Modified Version. Only one passage of
Front-Cover Text and one of Back-Cover Text may be added by (or
through arrangements made by) any one entity. If the Document already
includes a cover text for the same cover, previously added by you or
by arrangement made by the same entity you are acting on behalf of,
you may not add another; but you may replace the old one, on explicit
permission from the previous publisher that added the old one.

The author(s) and publisher(s) of the Document do not by this License
give permission to use their names for publicity for or to assert or
imply endorsement of any Modified Version.

\subsubsection{5. COMBINING DOCUMENTS}

You may combine the Document with other documents released under this
License, under the terms defined in section 4 above for modified
versions, provided that you include in the combination all of the
Invariant Sections of all of the original documents, unmodified, and
list them all as Invariant Sections of your combined work in its
license notice, and that you preserve all their Warranty Disclaimers.

The combined work need only contain one copy of this License, and
multiple identical Invariant Sections may be replaced with a single
copy. If there are multiple Invariant Sections with the same name but
different contents, make the title of each such section unique by
adding at the end of it, in parentheses, the name of the original
author or publisher of that section if known, or else a unique number.
Make the same adjustment to the section titles in the list of
Invariant Sections in the license notice of the combined work.

In the combination, you must combine any sections Entitled ``History''
in the various original documents, forming one section Entitled
``History''; likewise combine any sections Entitled ``Acknowledgements'',
and any sections Entitled ``Dedications''. You must delete all sections
Entitled ``Endorsements''.

\subsubsection{6. COLLECTIONS OF DOCUMENTS}

You may make a collection consisting of the Document and other
documents released under this License, and replace the individual
copies of this License in the various documents with a single copy
that is included in the collection, provided that you follow the rules
of this License for verbatim copying of each of the documents in all
other respects.

You may extract a single document from such a collection, and
distribute it individually under this License, provided you insert a
copy of this License into the extracted document, and follow this
License in all other respects regarding verbatim copying of that
document.

\subsubsection{7. AGGREGATION WITH INDEPENDENT WORKS}

A compilation of the Document or its derivatives with other separate
and independent documents or works, in or on a volume of a storage or
distribution medium, is called an ``aggregate'' if the copyright
resulting from the compilation is not used to limit the legal rights
of the compilation's users beyond what the individual works permit.
When the Document is included in an aggregate, this License does not
apply to the other works in the aggregate which are not themselves
derivative works of the Document.

If the Cover Text requirement of section 3 is applicable to these
copies of the Document, then if the Document is less than one half of
the entire aggregate, the Document's Cover Texts may be placed on
covers that bracket the Document within the aggregate, or the
electronic equivalent of covers if the Document is in electronic form.
Otherwise they must appear on printed covers that bracket the whole
aggregate.

\subsubsection{8. TRANSLATION}

Translation is considered a kind of modification, so you may
distribute translations of the Document under the terms of section 4.
Replacing Invariant Sections with translations requires special
permission from their copyright holders, but you may include
translations of some or all Invariant Sections in addition to the
original versions of these Invariant Sections. You may include a
translation of this License, and all the license notices in the
Document, and any Warranty Disclaimers, provided that you also include
the original English version of this License and the original versions
of those notices and disclaimers. In case of a disagreement between
the translation and the original version of this License or a notice
or disclaimer, the original version will prevail.

If a section in the Document is Entitled ``Acknowledgements'',
``Dedications'', or ``History'', the requirement (section 4) to Preserve
its Title (section 1) will typically require changing the actual
title.

\subsubsection{9. TERMINATION}

You may not copy, modify, sublicense, or distribute the Document
except as expressly provided under this License. Any attempt otherwise
to copy, modify, sublicense, or distribute it is void, and will
automatically terminate your rights under this License.

However, if you cease all violation of this License, then your license
from a particular copyright holder is reinstated (a) provisionally,
unless and until the copyright holder explicitly and finally
terminates your license, and (b) permanently, if the copyright holder
fails to notify you of the violation by some reasonable means prior to
60 days after the cessation.

Moreover, your license from a particular copyright holder is
reinstated permanently if the copyright holder notifies you of the
violation by some reasonable means, this is the first time you have
received notice of violation of this License (for any work) from that
copyright holder, and you cure the violation prior to 30 days after
your receipt of the notice.

Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License. If your rights have been terminated and not permanently
reinstated, receipt of a copy of some or all of the same material does
not give you any rights to use it.

\subsubsection{10. FUTURE REVISIONS OF THIS LICENSE}

The Free Software Foundation may publish new, revised versions of the
GNU Free Documentation License from time to time. Such new versions
will be similar in spirit to the present version, but may differ in
detail to address new problems or concerns. See
\url{http://www.gnu.org/copyleft/}.

Each version of the License is given a distinguishing version number.
If the Document specifies that a particular numbered version of this
License ``or any later version'' applies to it, you have the option of
following the terms and conditions either of that specified version or
of any later version that has been published (not as a draft) by the
Free Software Foundation. If the Document does not specify a version
number of this License, you may choose any version ever published (not
as a draft) by the Free Software Foundation. If the Document specifies
that a proxy can decide which future versions of this License can be
used, that proxy's public statement of acceptance of a version
permanently authorizes you to choose that version for the Document.

\subsubsection{11. RELICENSING}

``Massive Multiauthor Collaboration Site'' (or ``MMC Site'') means any
World Wide Web server that publishes copyrightable works and also
provides prominent facilities for anybody to edit those works. A
public wiki that anybody can edit is an example of such a server. A
``Massive Multiauthor Collaboration'' (or ``MMC'') contained in the site
means any set of copyrightable works thus published on the MMC site.

``CC-BY-SA'' means the Creative Commons Attribution-Share Alike 3.0
license published by Creative Commons Corporation, a not-for-profit
corporation with a principal place of business in San Francisco,
California, as well as future copyleft versions of that license
published by that same organization.

``Incorporate'' means to publish or republish a Document, in whole or in
part, as part of another Document.

An MMC is ``eligible for relicensing'' if it is licensed under this
License, and if all works that were first published under this License
somewhere other than this MMC, and subsequently incorporated in whole
or in part into the MMC, (1) had no cover texts or invariant sections,
and (2) were thus incorporated prior to November 1, 2008.

The operator of an MMC Site may republish an MMC contained in the site
under CC-BY-SA on the same site at any time before August 1, 2009,
provided the MMC is eligible for relicensing.

\subsection{ADDENDUM: How to use this License for your documents}

To use this License in a document you have written, include a copy of
the License in the document and put the following copyright and
license notices just after the title page:

        Copyright (C)  YEAR  YOUR NAME.
        Permission is granted to copy, distribute and/or
        modify this document under the terms of the GNU
        Free Documentation License, Version 1.3 or any
        later version published by the Free Software
        Foundation; with no Invariant Sections, no
        Front-Cover Texts, and no Back-Cover Texts.
        A copy of the license is included in the
        section entitled ``GNU Free Documentation License''.

If you have Invariant Sections, Front-Cover Texts and Back-Cover
Texts, replace the ``with \ldots Texts.'' line with this:

        with the Invariant Sections being LIST THEIR
        TITLES, with the Front-Cover Texts being LIST,
        and with the Back-Cover Texts being LIST.

If you have Invariant Sections without Cover Texts, or some other
combination of the three, merge those two alternatives to suit the
situation.

If your document contains nontrivial examples of program code, we
recommend releasing these examples in parallel under your choice of
free software license, such as the GNU General Public License, to
permit their use in free software.


\chapter{History} \label{cha:history}

\begin{tabular}{ll}
\textbf{Title:} & Introduction to Probability and Statistics Using
                    \textsf{R}, Third Edition\tabularnewline
\textbf{Year:} & 2021\tabularnewline
\textbf{Authors:} & G.~Jay Kerns\tabularnewline
\textbf{Publisher:} & G.~Jay Kerns\tabularnewline
\textbf{Title:} & Introduction to Probability and Statistics Using
                    \textsf{R}, Fourth Edition\tabularnewline
\textbf{Year:} & 2020\tabularnewline
\textbf{Authors:} & G.~Jay Kerns\tabularnewline
\textbf{Publisher:} & G.~Jay Kerns\tabularnewline
\textbf{Title:} & Introduction to Probability and Statistics Using
                  \textsf{R}, Second Edition\tabularnewline
\textbf{Year:} & 2014\tabularnewline
\textbf{Authors:} & G.~Jay Kerns\tabularnewline
\textbf{Publisher:} & G.~Jay Kerns\tabularnewline
\textbf{Title:} & Introduction to Probability and Statistics Using
                  \textsf{R}, First Edition\tabularnewline
\textbf{Year:} & 2010\tabularnewline
\textbf{Authors:} & G.~Jay Kerns\tabularnewline
\textbf{Publisher:} & G.~Jay Kerns\tabularnewline
\end{tabular}


\chapter{Data} \label{cha:data}



This appendix is a reference of sorts regarding some of the data
structures a statistician is likely to encounter. We discuss their
salient features and idiosyncrasies.

\section{Data Structures} \label{sec:data-structures}

\subsection{Vectors}

See the ``Vectors and Assignment'' section of \textit{An Introduction to}
\(\mathsf{R}\). A vector is an ordered sequence of elements, such as
numbers, characters, or logical values, and there may be \texttt{NA}'s
present. We usually make vectors with the assignment operator \texttt{<-}.

<<echo=TRUE>>=
x <- c(3, 5, 9)
@

Vectors are atomic in the sense that if you try to mix and match
elements of different modes then all elements will be coerced to the
most convenient common mode.

<<echo=TRUE>>=
y <- c(3, "5", TRUE)
@

In the example all elements were coerced to \textit{character}
mode. We can test whether a given object is a vector with
\texttt{is.vector} and can coerce an object (if possible) to a vector
with \texttt{as.vector}.

\subsection{Matrices and Arrays}

See the ``Arrays and Matrices'' section of \textit{An Introduction to}
\(\mathsf{R}\). Loosely speaking, a matrix is a vector that has been
reshaped into rectangular form, and an array is a multidimensional
matrix. Strictly speaking, it is the other way around: an array is a
data vector with a dimension attribute (\texttt{dim}), and a matrix is the
special case of an array with only two dimensions. We can construct a
matrix with the \texttt{matrix} function.

<<echo=TRUE>>=
matrix(letters[1:6], nrow = 2, ncol = 3)
@

Notice the order of the matrix entries, which shows how the matrix is
populated by default. We can change this with the \texttt{byrow} argument:

<<echo=TRUE>>=
matrix(letters[1:6], nrow = 2, ncol = 3, byrow = TRUE)
@

We can test whether a given object is a matrix with \texttt{is.matrix} and
can coerce an object (if possible) to a matrix with \texttt{as.matrix}. As a
final example watch what happens when we mix and match types in the
first argument:

<<echo=TRUE>>=
matrix(c(1,"2",NA, FALSE), nrow = 2, ncol = 3)
@

Notice how all of the entries were coerced to character for the final
result (except \texttt{NA}). Also notice how the four values were
\textit{recycled} to fill up the six entries of the matrix.

The standard arithmetic operations work element-wise with matrices.

<<echo=TRUE>>=
A <- matrix(1:6, 2, 3)
B <- matrix(2:7, 2, 3)
A + B
A # B
@

If you want the standard definition of matrix multiplication then use
the \texttt{\%*\%} function. If we were to try \texttt{A \%*\% B} we would get an error
because the dimensions do not match correctly, but for fun, we could
transpose \texttt{B} to get conformable matrices. The transpose function \texttt{t}
only works for matrices (and data frames).

<<echo=TRUE>>=
# try(A %*% B)     # gives an error
A %*% t(B)         # this is alright
@

To get the ordinary matrix inverse use the \texttt{solve} function:

<<echo=TRUE>>=
solve(A %*% t(B))     # input matrix must be square
@

Arrays more general than matrices, and some functions (like transpose)
do not work for the more general array. Here is what an array looks
like:

<<echo=TRUE>>=
array(LETTERS[1:24], dim = c(3,4,2))
@

We can test with \texttt{is.array} and may coerce with \texttt{as.array}.

\subsection{Data Frames}

A data frame is a rectangular array of information with a special
status in \(\mathsf{R}\). It is used as the fundamental data structure
by many of the modeling functions. It is like a matrix in that all of
the columns must be the same length, but it is more general than a
matrix in that columns are allowed to have different modes.

<<echo=TRUE>>=
x <- c(1.3, 5.2, 6)
y <- letters[1:3]
z <- c(TRUE, FALSE, TRUE)
A <- data.frame(x, y, z)
A
@

Notice the \texttt{names} on the columns of \texttt{A}. We can change those with the
\texttt{names} function.

<<echo=TRUE>>=
names(A) <- c("Fred","Mary","Sue")
A
@

Basic command is \texttt{data.frame}. You can test with \texttt{is.data.frame} and
you can coerce with \texttt{as.data.frame}.

\subsection{Lists}
A list is more general than a data frame.

\subsection{Tables}
The word ``table'' has a special meaning in \(\mathsf{R}\). More
precisely, a contingency table is an object of class \texttt{table} which is
an array.

Suppose you have a contingency table and would like to do descriptive
or inferential statistics on it. The default form of the table is
usually inconvenient to use unless we are working with a function
specially tailored for tables. Here is how to transform your data to a
more manageable form, namely, the raw data used to make the table.

First, we coerce the table to a data frame with:

<<echo=TRUE>>=
A <- as.data.frame(Titanic)
head(A)
@

Note that there are as many preliminary columns of \texttt{A} as there are
dimensions to the table. The rows of \texttt{A} contain every possible
combination of levels from each of the dimensions. There is also a
\texttt{Freq} column, which shows how many observations there were at that
particular combination of levels.

The form of \texttt{A} is often sufficient for our purposes, but more often
we need to do more work: we would usually like to repeat each row of
\texttt{A} exactly the number of times shown in the \texttt{Freq} column. The
\texttt{reshape} package \cite{reshape} has the function \texttt{untable} designed
for that very purpose:

<<echo=TRUE>>=
B <- with(A, untable(A, Freq))
head(B)
@

Now, this is more like it. Note that we slipped in a call to the
\texttt{with} function, which was done to make the call to \texttt{untable} more
pretty; we could just as easily have done

<<echo=TRUE, eval=FALSE>>=
untable(TitanicDF, A$Freq)
@

The only fly in the ointment is the lingering \texttt{Freq} column which has
repeated values that do not have any meaning any more. We could just
ignore it, but it would be better to get rid of the meaningless column
so that it does not cause trouble later. While we are at it, we could
clean up the \texttt{rownames}, too.

<<echo=TRUE>>=
C <- B[, -5]
rownames(C) <- 1:dim(C)[1]
head(C)
@


\subsection{More about Tables}
Suppose you want to make a table that looks like this:

There are at least two ways to do it.

\begin{itemize}
\item Using a matrix:

<<echo=TRUE>>=
tab <- matrix(1:6, nrow = 2, ncol = 3)
rownames(tab) <- c('first', 'second')
colnames(tab) <- c('A', 'B', 'C')
  tab  # Counts
@

\begin{itemize}
\item note that the columns are filled in consecutively by default. If
     you want to fill the data in by rows then do \texttt{byrow = TRUE} in
     the \texttt{matrix} command.
\end{itemize}

\item Using a dataframe

<<echo=TRUE>>=
p <- c("milk","tea")
g <- c("milk","tea")
catgs <- expand.grid(poured = p, guessed = g)
cnts <- c(3, 1, 1, 3)
D <- cbind(catgs, count = cnts)
xtabs(count ~ poured + guessed, data = D)
@

\begin{itemize}
\item again, the data are filled in column-wise.
\item the object is a dataframe
\item if you want to store it as a table then do \texttt{A <- xtabs(count ~ poured + guessed, data = D)}
\end{itemize}
\end{itemize}

\section{Importing Data} \label{sec:importing-a-data}

Statistics is the study of data, so the statistician's first step is
usually to obtain data from somewhere or another and read them into
\(\mathsf{R}\). In this section we describe some of the most common
sources of data and how to get data from those sources into a running
\(\mathsf{R}\) session.

For more information please refer to the \(\mathsf{R}\) \textit{Data
  Import/Export Manual}, \cite{rstatenv} and \textit{An Introduction to}
\(\mathsf{R}\), \cite{Venables2010}.

\subsection{Data in Packages}

There are many data sets stored in the \texttt{datasets} package
\cite{datasets} of base \(\mathsf{R}\). To see a list of them all
issue the command \texttt{data(package = "datasets")}. The output is omitted
here because the list is so long. The names of the data sets are
listed in the left column. Any data set in that list is already on the
search path by default, which means that a user can use it immediately
without any additional work.

There are many other data sets available in the thousands of
contributed packages. To see the data sets available in those packages
that are currently loaded into memory issue the single command
\texttt{data()}. If you would like to see all of the data sets that are
available in all packages that are installed on your computer (but not
necessarily loaded), issue the command

<<echo=TRUE, eval=FALSE>>=
data(package = .packages(all.available = TRUE))
@

To load the data set \texttt{foo} in the contributed package \texttt{bar} issue the
commands \texttt{library("bar")} followed by \texttt{data(foo)}, or just the single
command

<<echo=TRUE, eval=FALSE>>=
data(foo, package = "bar")
@

\subsection{Text Files}
Many sources of data are simple text files. The entries in the file
are separated by delimeters such as TABS (tab-delimeted), commas
(comma separated values, or \texttt{.csv}, for short) or even just white
space (no special name). A lot of data on the Internet are stored with
text files, and even if they are not, a person can copy-paste
information from a web page to a text file, save it on the computer,
and read it into \(\mathsf{R}\).

\subsection{Other Software Files}
Often the data set of interest is stored in some other, proprietary,
format by third-party software such as Minitab, SAS, or SPSS. The
\texttt{foreign} package \cite{foreign} supports import/conversion from many
of these formats. Please note, however, that data sets from other
software sometimes have properties with no direct analogue in
\(\mathsf{R}\). In those cases the conversion process may lose some
information which will need to be reentered manually from within
\(\mathsf{R}\). See the \textit{Data Import/Export Manual}.

As an example, suppose the data are stored in the SPSS file \texttt{foo.sav}
which the user has copied to the working directory; it can be imported
with the commands

<<echo=TRUE, eval=FALSE>>=
library("foreign")
read.spss("foo.sav")
@

See \texttt{?read.spss} for the available options to customize the file
import. Note that the \(\mathsf{R}\) Commander will import many of the
common file types with a menu driven interface.

\subsection{Importing a Data Frame}

The basic command is \texttt{read.table}.

\section{Creating New Data Sets} \label{sec:creating-new-data}

- Using \texttt{c}
- Using \texttt{scan}
- Using the \(\mathsf{R}\) Commander.

\section{Editing Data} \label{sec:editing-data-sets}

\subsection{Editing Data Values}
\subsection{Inserting Rows and Columns}
\subsection{Deleting Rows and Columns}
\subsection{Sorting Data}

We can sort a vector with the \texttt{sort} function. Normally we have a data
frame of several columns (variables) and many, many rows
(observations). The goal is to shuffle the rows so that they are
ordered by the values of one or more columns. This is done with the
\texttt{order} function.

For example, we may sort all of the rows of the \texttt{Puromycin} data (in
ascending order) by the variable \texttt{conc} with the following:

<<echo=TRUE>>=
Tmp <- Puromycin[order(Puromycin$conc), ]
head(Tmp)
@

We can accomplish the same thing with the command

<<echo=TRUE, eval=FALSE>>=
with(Puromycin, Puromycin[order(conc), ])
@

We can sort by more than one variable. To sort first by \texttt{state} and
next by \texttt{conc} do

<<echo=TRUE, eval=FALSE>>=
with(Puromycin, Puromycin[order(state, conc), ])
@

If we would like to sort a numeric variable in descending order then
we put a minus sign in front of it.

<<echo=TRUE>>=
Tmp <- with(Puromycin, Puromycin[order(-conc), ])
head(Tmp)
@

If we would like to sort by a character (or factor) in decreasing
order then we can use the \texttt{xtfrm} function which produces a numeric
vector in the same order as the character vector.

<<echo=TRUE>>=
Tmp <- with(Puromycin, Puromycin[order(-xtfrm(state)), ])
head(Tmp)
@


\section{Exporting Data} \label{sec:Exporting-a-Data}

The basic function is \texttt{write.table}. The \texttt{MASS} package \cite{MASS}
also has a \texttt{write.matrix} function.

\section{Reshaping Data} \label{sec:Reshaping-a-Data}
- Aggregation
- Convert Tables to data frames and back

\texttt{rbind}, \texttt{cbind}
\texttt{ab[order(ab[,1]),]}
\texttt{complete.cases}
\texttt{aggregate}
\texttt{stack}


\chapter{Mathematical Machinery} 
\label{cha:mathematical-machinery}

This appendix houses many of the standard definitions and theorems
that are used at some point during the narrative. It is targeted for
someone reading the book who forgets the precise definition of
something and would like a quick reminder of an exact statement. No
proofs are given, and the interested reader should consult a good text
on Calculus (say, Stewart \cite{Stewart2008} or Apostol
\cite{Apostol1967}, \cite{ApostolI1967}), Linear Algebra (say, Strang
\cite{Strang1988} and Magnus \cite{Magnus1999}), Real Analysis (say,
Folland \cite{Folland1999}, or Carothers \cite{Carothers2000}), or
Measure Theory (Billingsley \cite{Billingsley1995}, Ash
\cite{Ash2000}, Resnick \cite{Resnick1999}) for details.

\section{Set Algebra} 
\label{sec:the-algebra-of}

We denote sets by capital letters, \(A\), \(B\), \(C\), \textit{etc}. The
letter \(S\) is reserved for the sample space, also known as the
universe or universal set, the set which contains all possible
elements. The symbol \(\emptyset\) represents the empty set, the set
with no elements.

\subsection{Set Union, Intersection, and Difference}

Given subsets \(A\) and \(B\), we may manipulate them in an algebraic
fashion. To this end, we have three set operations at our disposal:
union, intersection, and difference. Below is a table summarizing the
pertinent information about these operations.

\begin{table}
\begin{centering}
\begin{tabular}{|l|c|l|l|}
\hline
Name          & Denoted              & Defined by elements   & \textsf{R} syntax \tabularnewline \hline
Union         & $A\cup B$            & in $A$ or $B$ or both & \texttt{union(A, B)} \tabularnewline
Intersection  & $A\cap B$            & in both $A$ and $B$   & \texttt{intersect(A, B)} \tabularnewline
Difference    & $A$\textbackslash $B$  & in $A$ but not in $B$ & \texttt{setdiff(A, B)} \tabularnewline
Complement    & $A^{c}$               & in $S$ but not in $A$ & \texttt{setdiff(S, A)} \tabularnewline
\end{tabular}
\par\end{centering}
\caption{Set operations}
\label{tab:set-operations}
\end{table}

\subsection{Identities and Properties}

\begin{enumerate}
\item \(A\cup\emptyset=A,\quad A\cap\emptyset=\emptyset\)
\item \(A\cup S=S,\quad A\cap S=A\)
\item \(A\cup A^{c}=S\), \(A\cap A^{c}=\emptyset\)
\item \((A{}^{c})^{c}=A\)
\item The Commutative Property:
   \begin{equation}
   A \cup B = B\cup A,\quad A\cap B = B\cap A
   \end{equation}
\item The Associative Property:
   \begin{equation}
   (A\cup B)\cup C=A\cup(B\cup C),\quad (A\cap B)\cap C=A\cap(B\cap C)
   \end{equation}
\item The Distributive Property:
   \begin{equation}
   A\cup(B\cap C)=(A\cup B)\cap(A\cup C),\quad A\cap(B\cup C)=(A\cap B)\cup(A\cap C)
   \end{equation}
\item DeMorgan's Laws
   \begin{equation}
   (A\cup B)^{c}=A^{c}\cap B^{c}\quad \mbox{and}\quad (A\cap B)^{c}=A^{c}\cup B^{c},
   \end{equation}
   or more generally,
   \begin{equation}
   \left(\bigcup_{\alpha}A_{\alpha}\right)^{c}=\bigcap_{\alpha}A_{\alpha}^{c},\quad \mbox{and}\quad \left(\bigcap_{\alpha}A_{\alpha}\right)^{c}=\bigcup_{\alpha}A_{\alpha}^{c}
   \end{equation}
\end{enumerate}

\section{Differential and Integral Calculus} \label{sec:differential-and-integral}

A function \(f\) of one variable is said to be one-to-one if no two
distinct \(x\) values are mapped to the same \(y=f(x)\) value. To show
that a function is one-to-one we can either use the horizontal line
test or we may start with the equation \(f(x_{1}) = f(x_{2})\) and use
algebra to show that it implies \(x_{1} = x_{2}\).

\subsection{Limits and Continuity}

\begin{defn}[]
Let \(f\) be a function defined on some open interval that contains
the number \(a\), except possibly at \(a\) itself. Then we say the
\textit{limit of} \(f(x)\) \textit{as} \(x\) \textit{approaches} \(a\) \textit{is} \(L\), and we
write
\begin{equation}
\lim_{x \to a}f(x) = L,
\end{equation}
if for every \(\epsilon > 0\) there exists a number \(\delta > 0\) such that \(0 < |x-a| < \delta\) implies \(|f(x) - L| < \epsilon\).
\end{defn}



\begin{defn}[]
A function \(f\) is \textit{continuous at a number} \(a\) if
\begin{equation}
\lim_{x \to a} f(x) = f(a).
\end{equation}
The function \(f\) is \textit{right-continuous at the number} \(a\) if
\(\lim_{x\to a^{+}}f(x)=f(a)\), and \textit{left-continuous} at \(a\) if
\(\lim_{x\to a^{-}}f(x)=f(a)\). Finally, the function \(f\) is
\textit{continuous on an interval} \(I\) if it is continuous at every number
in the interval.
\end{defn}

\subsection{Differentiation}

\begin{defn}[]
The \textit{derivative of a function} \(f\) \textit{at a number} \(a\), denoted by
\(f'(a)\), is
\begin{equation}
f'(a)=\lim_{h\to0}\frac{f(a+h)-f(a)}{h},
\end{equation}
provided this limit exists.  A function is \textit{differentiable at} \(a\)
if \(f'(a)\) exists. It is \textit{differentiable on an open interval}
\((a,b)\) if it is differentiable at every number in the interval.
\end{defn}

\subsubsection{Differentiation Rules}

In the table that follows, \(f\) and \(g\) are differentiable
functions and \(c\) is a constant.

\begin{table}[H]
\begin{centering}
\begin{tabular}{|c|c|c|}
\hline 
 &  & \tabularnewline
$\frac{\mathrm{d}}{\mathrm{d} x}c=0$ & $\frac{\mathrm{d}}{\mathrm{d} x}x^{n}=nx^{n-1}$ & $(cf)'=cf'$\tabularnewline
 &  & \tabularnewline
$(f\pm g)'=f'\pm g'$ & $(fg)'=f'g+fg'$ & $\left(\frac{f}{g}\right)'=\frac{f'g-fg'}{g^{2}}$\tabularnewline
 &  & \tabularnewline
\hline
\end{tabular}
\par\end{centering}
\caption{Differentiation rules\textbf{\label{tab:differentiation-rules}}}
\end{table}



\begin{thm}[Chain Rule]
If \(f\) and \(g\) are both differentiable and \(F=f\circ g\) is the
composite function defined by \(F(x)=f[g(x)]\), then \(F\) is
differentiable and \(F'(x) = f'[ g(x) ] \cdot g'(x)\).
\end{thm}

\subsubsection{Useful Derivatives}

\begin{table}[H]
\begin{centering}
\begin{tabular}{|c|c|c|}
\hline 
 &  & \tabularnewline
$\frac{\mathrm{d}}{\mathrm{d} x}\mathrm{e}^{x}=\mathrm{e}^{x}$ & $\frac{\mathrm{d}}{\mathrm{d} x}\ln x=x^{-1}$ & $\frac{\mathrm{d}}{\mathrm{d} x}\sin x=\cos x$\tabularnewline
 &  & \tabularnewline
$\frac{\mathrm{d}}{\mathrm{d} x}\cos x=-\sin x$ & $\frac{\mathrm{d}}{\mathrm{d} x}\tan x=\sec^{2}x$ & $\frac{\mathrm{d}}{\mathrm{d} x}\tan^{-1}x=(1+x^{2})^{-1}$$ $\tabularnewline
 &  & \tabularnewline
\hline
\end{tabular}
\par\end{centering}
\caption{Some derivatives\textbf{\label{tab:useful-derivatives}}}
\end{table}

\subsection{Optimization}

\begin{defn}[]
A \textit{critical number} of the function \(f\) is a value \(x^{\ast}\) for
which \(f'(x^{\ast})=0\) or for which \(f'(x^{\ast})\) does not exist.
\end{defn}


\begin{thm}[First Derivative Test]
\label{thm:first-derivative-test}
If \(f\) is differentiable and if
\(x^{\ast}\) is a critical number of \(f\) and if \(f'(x)\geq0\) for
\(x\leq x^{\ast}\) and \(f'(x)\leq0\) for \(x\geq x^{\ast}\), then
\(x^{\ast}\) is a local maximum of \(f\). If \(f'(x)\leq0\) for
\(x\leq x^{\ast}\) and \(f'(x)\geq0\) for \(x\geq x^{\ast}\) , then
\(x^{\ast}\) is a local minimum of \(f\).
\end{thm}




\begin{thm}[Second Derivative Test]
If \(f\) is twice differentiable and if \(x^{\ast}\) is a critical
number of \(f\), then \(x^{\ast}\) is a local maximum of \(f\) if
\(f''(x^{\ast})<0\) and \(x^{\ast}\) is a local minimum of \(f\) if
\(f''(x^{\ast})>0\).
\end{thm}

\subsection{Integration}

As it turns out, there are all sorts of things called ``integrals'',
each defined in its own idiosyncratic way. There are \textit{Riemann}
integrals, \textit{Lebesgue} integrals, variants of these called
\textit{Stieltjes} integrals, \textit{Daniell} integrals, \textit{Ito}
integrals, and the list continues. Given that this is an introductory
book, we will use the Riemannian integral with the caveat that the
Riemann integral is \textit{not} the integral that will be used in
more advanced study.



\begin{defn}[]
Let \(f\) be defined on \([a,b]\), a closed interval of the real
line. For each \(n\), divide \([a,b]\) into subintervals
\([x_{i},x_{i+1}]\), \(i=0,1,\ldots,n-1\), of length \(\Delta
x_{i}=(b-a)/n\) where \(x_{0}=a\) and \(x_{n}=b\), and let
\(x_{i}^{\ast}\) be any points chosen from the respective
subintervals. Then the \textit{definite integral} of \(f\) from \(a\) to
\(b\) is defined by
\begin{equation}
\int_{a}^{b}f(x)\,\mathrm{d} x=\lim_{n\to\infty}\sum_{i=0}^{n-1}f(x_{i}^{\ast})\,\Delta x_{i},
\end{equation}
provided the limit exists, and in that case, we say that \(f\) is
\textit{integrable} from \(a\) to \(b\).
\end{defn}



\begin{thm}[The Fundamental Theorem of Calculus]
Suppose \(f\) is continuous on \([a,b]\). Then

\begin{enumerate}
\item the function \(g\) defined by \(g(x)=\int_{a}^{x}f(t)\:\mathrm{d} t\), \(a\leq x\leq b\), is continuous on \([a,b]\) and differentiable on \((a,b)\) with \(g'(x)=f(x)\).

\item \(\int_{a}^{b}f(x)\,\mathrm{d} x=F(b)-F(a)\), where \(F\) is any \textit{antiderivative} of \(f\), that is, any function \(F\) satisfying \(F'=f\).
\end{enumerate}
\end{thm}

\subsubsection{Change of Variables}

\begin{thm}[]
If \(g\) is a differentiable function whose range is the interval
\([a,b]\) and if both \(f\) and \(g'\) are continuous on the range of
\(u = g(x)\), then
\begin{equation}
\int_{g(a)}^{g(b)}f(u)\:\mathrm{d} u=\int_{a}^{b}f[g(x)]\: g'(x)\:\mathrm{d} x.
\end{equation}
\end{thm}

\subsubsection{Useful Integrals}

\begin{table}[H]
\begin{centering}
\begin{tabular}{|c|c|c|}
\hline 
 &  & \tabularnewline
$\int x^{n}\,\mathrm{d} x=x^{n+1}/(n+1),\ n\neq-1$ & $\int\mathrm{e}^{x}\,\mathrm{d} x=\mathrm{e}^{x}$ & $\int x^{-1}\,\mathrm{d} x=\ln|x|$\tabularnewline
 &  & \tabularnewline
$\int\tan x\:\mathrm{d} x=\ln|\sec x|$ & $\int a^{x}\,\mathrm{d} x=a^{x}/\ln a$ & $\int(x^{2}+1)^{-1}\,\mathrm{d} x=\tan^{-1}x$\tabularnewline
 &  & \tabularnewline
\hline
\end{tabular}
\par\end{centering}
\caption{Some integrals (constants of integration omitted)\textbf{\label{tab:useful-integrals}}}
\end{table}
\subsubsection{Integration by Parts}

\begin{equation}
\int u\:\mathrm{d} v=uv-\int v\:\mathrm{d} u
\end{equation}

\begin{thm}[L'H\^ opital's Rule]
Suppose \(f\) and \(g\) are differentiable and \(g'(x)\neq0\) near
\(a\), except possibly at \(a\). Suppose that the limit
\begin{equation}
\lim_{x\to a}\frac{f(x)}{g(x)}
\end{equation}
is an indeterminate form of type \(\frac{0}{0}\) or
\(\infty/\infty\). Then
\begin{equation}
\lim_{x\to a}\frac{f(x)}{g(x)}=\lim_{x\to a}\frac{f'(x)}{g'(x)},
\end{equation}
provided the limit on the right-hand side exists or is infinite.
\end{thm}

\subsubsection{Improper Integrals}

If \(\int_{a}^{t}f(x)\mathrm{d} x\) exists for every number \(t\geq
a\), then we define
\begin{equation}
\int_{a}^{\infty}f(x)\,\mathrm{d} x=\lim_{t\to\infty}\int_{a}^{t}f(x)\,\mathrm{d} x,
\end{equation}
provided this limit exists as a finite number, and in that case we say
that \(\int_{a}^{\infty}f(x)\,\mathrm{d} x\) is
\textit{convergent}. Otherwise, we say that the improper integral is
\textit{divergent}.

If \(\int_{t}^{b}f(x)\,\mathrm{d} x\) exists for every number \(t\leq
b\), then we define
\begin{equation}
\int_{-\infty}^{b}f(x)\,\mathrm{d} x=\lim_{t\to-\infty}\int_{t}^{b}f(x)\,\mathrm{d} x,
\end{equation}
provided this limit exists as a finite number, and in that case we say
that \(\int_{-\infty}^{b}f(x)\,\mathrm{d} x\) is
\textit{convergent}. Otherwise, we say that the improper integral is
\textit{divergent}.

If both \(\int_{a}^{\infty}f(x)\,\mathrm{d} x\) and
\(\int_{-\infty}^{a}f(x)\,\mathrm{d} x\) are convergent, then we
define
\begin{equation}
\int_{-\infty}^{\infty}f(x)\,\mathrm{d} x=\int_{-\infty}^{a}f(x)\,\mathrm{d} x+\int_{a}^{\infty}f(x)\mathrm{d} x,
\end{equation}
and we say that \(\int_{-\infty}^{\infty}f(x)\,\mathrm{d} x\) is
\textit{convergent}. Otherwise, we say that the improper integral is
\textit{divergent}.

\section{Sequences and Series} \label{sec:sequences-and-series}


A \textit{sequence} is an ordered list of numbers,
\(a_{1}, a_{2}, a_{3}, \ldots, a_{n} = \left(a_{k}\right)_{k=1}^{n}\).
A sequence may be finite or infinite. In the latter case we write
\(a_{1}, a_{2}, a_{3}, \ldots =\left(a_{k}\right)_{k=1}^{\infty}\). We
say that \textit{the infinite sequence} \(\left(a_{k}\right)_{k=1}^{\infty}\)
\textit{converges to the finite limit} L, and we write
\begin{equation}
\lim_{k\to\infty}a_{k} = L,
\end{equation}
if for every \(\epsilon > 0\) there exists an integer \(N \geq 1\)
such that \(|a_{k} - L| < \epsilon\) for all \(k \geq N\). We say that
\textit{the infinite sequence} \(\left(a_{k}\right)_{k=1}^{\infty}\)
\textit{diverges to} \(+\infty\) (or \(-\infty\)) if for every \(M\geq0\)
there exists an integer \(N\geq1\) such that \(a_{k} \geq M\) for all
\(k \geq N\) (or \(a_{k} \leq - M\) for all \(k \geq N\)).

\subsection{Finite Series}

\begin{equation}
\label{eq:gauss-series}
\sum_{k=1}^{n}k=1+2+\cdots+n=\frac{n(n+1)}{2}
\end{equation}
\begin{equation}
\label{eq:gauss-series-sq}
\sum_{k=1}^{n}k^{2}=1^{2}+2^{2}+\cdots+n^{2}=\frac{n(n+1)(2n+3)}{6}
\end{equation}

\subsubsection{The Binomial Series}
\begin{equation}
\label{eq:binom-series}
\sum_{k=0}^{n}{n \choose k}\, a^{n-k}b^{k}=(a+b)^{n}
\end{equation}

\subsection{Infinite Series}

Given an infinite sequence of numbers
\(a_{1}, a_{2},a_{3},\ldots =\left(a_{k}\right)_{k=1}^{\infty}\),
let \(s_{n}\) denote the
\textit{partial sum} of the first \(n\) terms:
\begin{equation}
s_{n}=\sum_{k=1}^{n}a_{k}=a_{1}+a_{2}+\cdots+a_{n}.
\end{equation}
If the sequence \(\left(s_{n}\right)_{n=1}^{\infty}\) converges to a
finite number \(S\) then we say that the infinite series
\(\sum_{k}a_{k}\) is \textit{convergent} and write
\begin{equation}
\sum_{k=1}^{\infty}a_{k}=S.
\end{equation}
Otherwise we say the infinite series is \textit{divergent}.

\subsection{Rules for Series}

Let \(\left(a_{k}\right)_{k=1}^{\infty}\) and
\(\left(b_{k}\right)_{k=1}^{\infty}\) be infinite sequences and let
\(c\) be a constant.

\begin{equation}
\sum_{k=1}^{\infty}ca_{k}=c\sum_{k=1}^{\infty}a_{k}
\end{equation}
\begin{equation}
\sum_{k=1}^{\infty}(a_{k}\pm b_{k})=\sum_{k=1}^{\infty}a_{k}\pm\sum_{k=1}^{\infty}b_{k}
\end{equation}

In both of the above the series on the left is convergent if the
series on the right is (are) convergent.

\subsubsection{The Geometric Series}
\begin{equation}
\label{eq:geom-series}
\sum_{k=0}^{\infty} x^{k} = \frac{1}{1 - x},\quad |x| < 1.
\end{equation}

\subsubsection{The Exponential Series}
\begin{equation}
\label{eq:exp-series}
\sum_{k=0}^{\infty}\frac{x^{k}}{k!} = \mathrm{e}^{x},\quad -\infty < x < \infty.
\end{equation}

Other Series
\begin{equation}
\label{eq:negbin-series}
\sum_{k=0}^{\infty}{m+k-1 \choose m-1}x^{k}=\frac{1}{(1-x)^{m}},\quad |x|<1.
\end{equation}

\begin{equation}
\label{eq:log-series}
-\sum_{k=1}^{\infty}\frac{x^{n}}{n}=\ln(1-x),\quad |x| < 1.
\end{equation}
\begin{equation}
\label{eq:binom-series-infinite}
\sum_{k=0}^{\infty}{n \choose k}x^{k}=(1+x)^{n},\quad |x| < 1.
\end{equation}

\subsection{Taylor Series}

If the function \(f\) has a \textit{power series} representation at the point
\(a\) with radius of convergence \(R>0\), that is, if
\begin{equation}
f(x)=\sum_{k=0}^{\infty}c_{k}(x-a)^{k},\quad |x - a| < R,
\end{equation}
for some constants \(\left(c_{k}\right)_{k=0}^{\infty}\), then \(c_{k}\) must be
\begin{equation}
c_{k}=\frac{f^{(k)}(a)}{k!},\quad k=0,1,2,\ldots
\end{equation}
Furthermore, the function \(f\) is differentiable on the open interval
\((a-R,\, a+R)\) with
\begin{equation}
f'(x)=\sum_{k=1}^{\infty}kc_{k}(x-a)^{k-1},\quad |x-a| < R,
\end{equation}
\begin{equation}
\int f(x)\,\mathrm{d} x=C+\sum_{k=0}^{\infty}c_{k}\frac{(x-a)^{k+1}}{k+1},\quad |x-a| < R,
\end{equation}
in which case both of the above series have radius of convergence
\(R\).

\section{The Gamma Function} \label{sec:the-gamma-function}


The \textit{Gamma function} \(\Gamma\) will be defined in this book according
to the formula
\begin{equation}
\Gamma(\alpha)=\int_{0}^{\infty}x^{\alpha-1}\mathrm{e}^{-x}\:\mathrm{d} x,\quad \mbox{for }\alpha > 0.
\end{equation}



\begin{fact}[]
Properties of the Gamma Function:

\begin{itemize}
\item \(\Gamma(\alpha)=(\alpha - 1)\Gamma(\alpha - 1)\) for any \(\alpha >
  1\), and so \(\Gamma(n)=(n-1)!\) for any positive integer \(n\).
\item \(\Gamma(1/2)=\sqrt{\pi}\).
\end{itemize}
\end{fact}

\section{Linear Algebra} \label{sec:linear-algebra}

\subsection{Matrices}

A \textit{matrix} is an ordered array of numbers or expressions; typically we
write \(\mathbf{A}=\begin{pmatrix}a_{ij}\end{pmatrix}\) or
\(\mathbf{A}=\begin{bmatrix}a_{ij}\end{bmatrix}\). If \(\mathbf{A}\)
has \(m\) rows and \(n\) columns then we write
\begin{equation}
\mathbf{A}_{\mathrm{m}\times\mathrm{n}}=\begin{bmatrix}a_{11} & a_{12} & \cdots & a_{1n}\\
a_{21} & a_{22} & \cdots & a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{m1} & a_{m2} & \cdots & a_{mn}\end{bmatrix}.
\end{equation}
The \textit{identity matrix} \(\mathbf{I}_{\mathrm{n}\times\mathrm{n}}\) is
an \(\mathrm{n}\times\mathrm{n}\) matrix with zeros everywhere except
for 1's along the main diagonal:
\begin{equation}
\mathbf{I}_{\mathrm{n}\times\mathrm{n}}=\begin{bmatrix}1 & 0 & \cdots & 0\\
0 & 1 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & 1\end{bmatrix}.
\end{equation}
and the matrix with ones everywhere is denoted
\(\mathbf{J}_{\mathrm{n}\times\mathrm{n}}\):
\begin{equation}
\mathbf{J}_{\mathrm{n}\times\mathrm{n}}=\begin{bmatrix}1 & 1 & \cdots & 1\\
1 & 1 & \cdots & 1\\
\vdots & \vdots & \ddots & \vdots\\
1 & 1 & \cdots & 1\end{bmatrix}.
\end{equation}

A \textit{vector} is a matrix with one of the dimensions equal to one, such
as \(\mathbf{A}_{\mathrm{m}\times1}\) (a column vector) or
\(\mathbf{A}_{\mathrm{1}\times\mathrm{n}}\) (a row vector). The \textit{zero
vector} \(\mathbf{0}_{\mathrm{n}\times1}\) is an \(\mathrm{n}\times1\)
matrix of zeros:
\begin{equation}
\mathbf{0}_{\mathrm{n}\times1}=\begin{bmatrix}0 & 0 & \cdots & 0\end{bmatrix}^{\mathrm{T}}.
\end{equation}

The \textit{transpose} of a matrix
\(\mathbf{A}=\begin{pmatrix}a_{ij}\end{pmatrix}\) is the matrix
\(\mathbf{A}^{\mathrm{T}}=\begin{pmatrix}a_{ji}\end{pmatrix}\), which
is just like \(\mathbf{A}\) except the rows are columns and the
columns are rows. The matrix \(\mathbf{A}\) is said to be \textit{symmetric}
if \(\mathbf{A}^{\mathrm{T}}=\mathbf{A}\). Note that
\(\left(\mathbf{A}\mathbf{B}\right)^{\mathrm{T}}=\mathbf{B}^{\mathrm{T}}\mathbf{A}^{\mathrm{T}}\).

The \textit{trace} of a square matrix \(\mathbf{A}\) is the sum of its
diagonal elements: \(\mathrm{tr}(\mathbf{A})=\sum_{i}a_{ii}\).

The \textit{inverse} of a square matrix
\(\mathbf{A}_{\mathrm{n}\times\mathrm{n}}\) (when it exists) is the
unique matrix denoted \(\mathbf{A}^{-1}\) which satisfies
\(\mathbf{A}\mathbf{A}^{-1}=\mathbf{A}^{-1}\mathbf{A}=\mathbf{I}_{\mathrm{n}\times\mathrm{n}}\). If
\(\mathbf{A}^{-1}\) exists then we say \(\mathbf{A}\) is \textit{invertible},
or \textit{nonsingular}. Note that
\(\left(\mathbf{A}^{\mathrm{T}}\right)^{-1}=\left(\mathbf{A}^{\mathrm{-1}}\right)^{\mathrm{T}}\).



\begin{fact}[]
The inverse of the \(2\times2\) matrix
\begin{equation}
\mathbf{A}=\begin{bmatrix}a & b\\
c & d\end{bmatrix}\quad \mbox{is}\quad \mathbf{A}^{-1}=\frac{1}{ad-bc}\begin{bmatrix}d & -b\\
-c & a\end{bmatrix},
\end{equation}
provided \(ad-bc\neq0\).
\end{fact}

\subsection{Determinants}

\begin{defn}[]
The \textit{determinant} of a square matrix \(\mathbf{A}_{\mathrm{n}\times
n}\) is denoted \(\mathrm{det}(\mathbf{A})\) or \(|\mathbf{A}|\) and
is defined recursively by
\begin{equation}
\mathrm{det}(\mathbf{A})=\sum_{i=1}^{n}(-1)^{i+j}a_{ij}\,\mathrm{det}(\mathbf{M}_{ij}),
\end{equation}
where \(\mathbf{M}_{ij}\) is the submatrix formed by deleting the
\(i^{\mathrm{th}}\) row and \(j^{\mathrm{th}}\) column of
\(\mathbf{A}\). We may choose any fixed \(1\leq j\leq n\) we wish to
compute the determinant; the final result is independent of the \(j\)
chosen.
\end{defn}

\begin{fact}[]
The determinant of the \(2\times2\) matrix
\begin{equation}
\mathbf{A}=\begin{bmatrix}a & b\\
c & d\end{bmatrix}\quad \mbox{is} \quad |\mathbf{A}|=ad-bc.
\end{equation}
\end{fact}

\begin{fact}[]
A square matrix \(\mathbf{A}\) is nonsingular if and only if
\(\mathrm{det}(\mathbf{A})\neq0\).
\end{fact}

\subsection{Positive (Semi)Definite}

If the matrix \(\mathbf{A}\) satisfies
\(\mathbf{x^{\mathrm{T}}}\mathbf{A}\mathbf{x}\geq0\) for all vectors
\(\mathbf{x}\neq\mathbf{0}\), then we say that \(\mathbf{A}\) is
\textit{positive semidefinite}. If strict inequality holds for all
\(\mathbf{x}\neq\mathbf{0}\), then \(\mathbf{A}\) is \textit{positive
definite}. The connection to statistics is that covariance matrices
(see Chapter~\ref{cha:multivariable-distributions}) are always positive
semidefinite, and many of them are even positive definite.

\section{Multivariable Calculus} \label{sec:multivariable-calculus}


\subsection{Partial Derivatives}

If \(f\) is a function of two variables, its \textit{first-order partial
derivatives} are defined by
\begin{equation}
\frac{\partial f}{\partial x}=\frac{\partial}{\partial x}f(x,y)=\lim_{h\to0}\frac{f(x+h,\, y)-f(x,y)}{h}
\end{equation}
and
\begin{equation}
\frac{\partial f}{\partial y}=\frac{\partial}{\partial y}f(x,y)=\lim_{h\to0}\frac{f(x,\, y+h)-f(x,y)}{h},
\end{equation}
provided these limits exist. The \textit{second-order partial derivatives} of
\(f\) are defined by
\begin{equation}
\frac{\partial^{2}f}{\partial x^{2}}=\frac{\partial}{\partial x}\left(\frac{\partial f}{\partial x}\right),\quad \frac{\partial^{2}f}{\partial y^{2}}=\frac{\partial}{\partial y}\left(\frac{\partial f}{\partial y}\right),\quad \frac{\partial^{2}f}{\partial x\partial y}=\frac{\partial}{\partial x}\left(\frac{\partial f}{\partial y}\right),\quad \frac{\partial^{2}f}{\partial y\partial x}=\frac{\partial}{\partial y}\left(\frac{\partial f}{\partial x}\right).
\end{equation}
In many cases (and for all cases in this book) it is true that
\begin{equation}
\frac{\partial^{2}f}{\partial x\partial y}=\frac{\partial^{2}f}{\partial y\partial x}.
\end{equation}

\subsection{Optimization}
An function \(f\) of two variables has a \textit{local maximum} at \((a,b)\)
if \(f(x,y)\geq f(a,b)\) for all points \((x,y)\) near \((a,b)\), that
is, for all points in an open disk centered at \((a,b)\). The number
\(f(a,b)\) is then called a \textit{local maximum value} of \(f\). The
function \(f\) has a \textit{local minimum} if the same thing happens with
the inequality reversed.

Suppose the point \((a,b)\) is a \textit{critical point} of \(f\),
that is, suppose \((a,b)\) satisfies
\begin{equation}
\frac{\partial f}{\partial x}(a,b)=\frac{\partial f}{\partial y}(a,b)=0.
\end{equation}
Further suppose \(\frac{\partial^{2}f}{\partial x^{2}}\) and
\(\frac{\partial^{2}f}{\partial y^{2}}\) are continuous near
\((a,b)\). Let the \textit{Hessian matrix} \(H\) (not to be confused
with the \textit{hat matrix} \(\mathbf{H}\) of Chapter~\ref{cha:multiple-linear-regression}) be defined by
\begin{equation}
H =
\begin{bmatrix}
\frac{\partial^{2}f}{\partial x^{2}} & \frac{\partial^{2}f}{\partial x\partial y}\\
\frac{\partial^{2}f}{\partial y\partial x} & \frac{\partial^{2}f}{\partial y^{2}}
\end{bmatrix}.
\end{equation}
We use the following rules to decide whether \((a,b)\) is an
\textit{extremum} (that is, a local minimum or local maximum) of
\(f\).

\begin{itemize}
\item If \(\mbox{det}(H)>0\) and \(\frac{\partial^{2}f}{\partial
  x^{2}}(a,b)>0\), then \((a,b)\) is a local minimum of \(f\).
\item If \(\mbox{det}(H)>0\) and \(\frac{\partial^{2}f}{\partial
  x^{2}}(a,b)<0\), then \((a,b)\) is a local maximum of \(f\).
\item If \(\mbox{det}(H)<0\), then \((a,b)\) is a \textit{saddle
    point} of \(f\) and so is not an extremum of \(f\).
\item If \(\mbox{det}(H)=0\), then we do not know the status of \((a,b)\);
  it might be an extremum or it might not be.
\end{itemize}

\subsection{Double and Multiple Integrals}
Let \(f\) be defined on a rectangle \(R=[a,b]\times[c,d]\), and for
each \(m\) and \(n\) divide \([a,b]\) (respectively \([c,d]\)) into
subintervals \([x_{j},x_{j+1}]\), \(i=0,1,\ldots,m-1\) (respectively
\([y_{i},y_{i+1}]\)) of length \(\Delta x_{j}=(b-a)/m\) (respectively
\(\Delta y_{i}=(d-c)/n\)) where \(x_{0}=a\) and \(x_{m}=b\) (and
\(y_{0}=c\) and \(y_{n}=d\) ), and let \(x_{j}^{\ast}\)
(\(y_{i}^{\ast}\)) be any points chosen from their respective
subintervals. Then the \textit{double integral} of \(f\) over the
rectangle \(R\) is
\begin{equation}
\iintop_{R}f(x,y)\,\mathrm{d} A=\intop_{c}^{d}\!\!\!\intop_{a}^{b}f(x,y)\,\mathrm{d} x\mathrm{d} y=\lim_{m,n\to\infty}\sum_{i=1}^{n}\sum_{j=1}^{m}f(x_{j}^{\ast},y_{i}^{\ast})\Delta x_{j}\Delta y_{i},
\end{equation}
provided this limit exists. Multiple integrals are defined in the same
way just with more letters and sums.

\subsection{Bivariate and Multivariate Change of Variables}

Suppose we have a transformation\footnote{For our purposes \(T\) is in
  fact the \textit{inverse} of a one-to-one transformation that we are
  initially given. We usually start with functions that map
  \((x,y) \longmapsto (u,v)\), and one of our first tasks is to solve
  for the inverse transformation that maps
  \((u,v)\longmapsto(x,y)\). It is this inverse transformation which
  we are calling \(T\).} \(T\) that maps points \((u,v)\) in a set
\(A\) to points \((x,y)\) in a set \(B\). We typically write
\(x=x(u,v)\) and \(y=y(u,v)\), and we assume that \(x\) and \(y\) have
continuous first-order partial derivatives. We say that \(T\) is
\textit{one-to-one} if no two distinct \((u,v)\) pairs get mapped to the same
\((x,y)\) pair; in this book, all of our multivariate transformations
\(T\) are one-to-one.

The \textit{Jacobian} (pronounced ``yah-KOH-bee-uhn'') of \(T\) is denoted by
\(\partial(x,y)/\partial(u,v)\) and is defined by the determinant of
the following matrix of partial derivatives:
\begin{equation}
\frac{\partial(x,y)}{\partial(u,v)}=\left|
\begin{array}{cc}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v}\\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
\end{array}
\right|=\frac{\partial x}{\partial u}\frac{\partial y}{\partial v}-\frac{\partial x}{\partial v}\frac{\partial y}{\partial u}.
\end{equation}

If the function \(f\) is continuous on \(A\) and if the Jacobian of
\(T\) is nonzero except perhaps on the boundary of \(A\), then
\begin{equation}
\iint_{B}f(x,y)\,\mathrm{d} x\,\mathrm{d} y=\iint_{A}f\left[x(u,v),\, y(u,v)\right]\ \left|\frac{\partial(x,y)}{\partial(u,v)}\right|\mathrm{d} u\,\mathrm{d} v.
\end{equation}

A multivariate change of variables is defined in an analogous way: the
one-to-one transformation \(T\) maps points
\((u_{1},u_{2},\ldots,u_{n})\) to points
\((x_{1},x_{2},\ldots,x_{n})\), the Jacobian is the determinant of the
\(\mathrm{n}\times\mathrm{n}\) matrix of first-order partial
derivatives of \(T\) (lined up in the natural manner), and instead of
a double integral we have a multiple integral over multidimensional
sets \(A\) and \(B\).

\chapter{RcmdrTestDrive Story} \label{cha:rcmdrtestdrive-story}

The goal of \texttt{RcmdrTestDrive} was to have a data set sufficiently rich
in the types of data represented such that a person could load it into
the \(\mathsf{R}\) Commander and be able to explore all of \texttt{Rcmdr}'s
menu options at once. I decided early-on that an efficient way to do
this would be to generate the data set randomly, and later add to the
list of variables as more \texttt{Rcmdr} menu options became
available. Generating the data was easy, but generating a story that
related all of the respective variables proved to be less so.

In the Summer of 2006 I gave a version of the raw data and variable
names to my STAT 3743 Probability and Statistics class and invited
each of them to write a short story linking all of the variables
together in a coherent narrative. No further direction was given.

The most colorful of those I received was written by Jeffery
Cornfield, submitted July 12, 2006, and is included below with his
permission. It was edited slightly by the present author and updated
to respond dynamically to the random generation of \texttt{RcmdrTestDrive};
otherwise, the story has been unchanged.

\subsubsection{Case File: ALU-179 ``Murder Madness in Toon Town''}

\begin{quote}
\textit{WARNING} This file is not for the faint of heart, dear reader,
because it is filled with horrible images that will haunt your
nightmares. If you are weak of stomach, have irritable bowel syndrome,
or are simply paranoid, DO NOT READ FURTHER! Otherwise, read at your
own risk.
\end{quote}

One fine sunny day, Police Chief R. Runner called up the forensics
department at Acme-Looney University. There had been
\Sexpr{dim(RcmdrTestDrive)[1] - 2} murders in the past
\Sexpr{round((dim(RcmdrTestDrive)[1] - 2)/24)} days, approximately
one murder every hour, of many of the local Human workers, shop
keepers, and residents of Toon Town. These alarming rates threatened
to destroy the fragile balance of Toon and Human camaraderie that had
developed in Toon Town.

Professor Twee T. Bird, a world-renowned forensics specialist and a
Czechoslovakian native, received the call. ``Professor, we need your
expertise in this field to identify the pattern of the killer or
killers,'' Chief Runner exclaimed. ``We need to establish a link between
these people to stop this massacre.''

``Yes, Chief Runner, please give me the details of the case,'' Professor
Bird declared with a heavy native accent, (though, for the sake of the
case file, reader, I have decided to leave out the accent due to the
fact that it would obviously drive you -- if you will forgive the pun
-- looney!)

``All prints are wiped clean and there are no identifiable marks on the
bodies of the victims. All we are able to come up with is the
possibility that perhaps there is some kind of alternative method of
which we are unaware. We have sent a secure e-mail with a listing of
all of the victims' \texttt{races}, \texttt{genders}, locations of the bodies, and
the sequential \texttt{order} in which they were killed. We have also
included other information that might be helpful,'' said Chief Runner.

``Thank you very much. Perhaps I will contact my colleague in the
Statistics Department here, Dr. Elmer Fudd-Einstein,'' exclaimed
Professor Bird. ``He might be able to identify a pattern of attack with
mathematics and statistics.''

``Good luck trying to find him, Professor. Last I heard, he had a
bottle of scotch and was in the Hundred Acre Woods hunting rabbits,''
Chief Runner declared in a manner that questioned the beloved doctor's
credibility.

``Perhaps I will take a drive to find him. The fresh air will do me
good.''

\begin{quote}
I will skip ahead, dear reader, for much occurred during this
time. Needless to say, after a fierce battle with a mountain cat that
the Toon-ology Department tagged earlier in the year as ``Sylvester,''
Professor Bird found Dr. Fudd-Einstein and brought him back, with much
bribery of alcohol and the promise of the future slaying of those
``wascally wabbits'' (it would help to explain that Dr. Fudd-Einstein
had a speech impediment which was only worsened during the consumption
of alcohol.)
\end{quote}

Once our two heroes returned to the beautiful Acme-Looney University,
and once Dr. Fudd-Einstein became sober and coherent, they set off to
examine the case and begin solving these mysterious murders.

``First off,'' Dr. Fudd-Einstein explained, ``these people all worked at
the University at some point or another. Also, there also seems to be
a trend in the fact that they all had a \texttt{salary} between
\Sexpr{round(min(RcmdrTestDrive$salary))} and
\Sexpr{round(max(RcmdrTestDrive$salary))} when they retired.''

``That's not really a lot to live off of,'' explained Professor Bird.

``Yes, but you forget that the Looney Currency System works differently
than the rest of the American Currency System. One Looney is
equivalent to Ten American Dollars. Also, these faculty members are
the ones who faced a cut in their salary, as denoted by
\texttt{reduction}. Some of them dropped quite substantially when the
University had to fix that little \textit{faux pas} in the Chemistry
Department. You remember: when Dr. D. Duck tried to create that
Everlasting Elixir?' As a result, these faculty left the
university. Speaking of which, when is his memorial service?'' inquired
Dr. Fudd-Einstein.

``This coming Monday. But if there were all of these killings, how in
the world could one person do it? It just doesn't seem to be possible;
stay up \Sexpr{round((dim(RcmdrTestDrive)[1] - 2)/24)} days
straight and be able to kill all of these people and have the energy
to continue on,'' Professor Bird exclaimed, doubting the guilt of only
one person.

``Perhaps then, it was a group of people, perhaps there was more than
one killer placed throughout Toon Town to commit these crimes. If I
feed in these variables, along with any others that might have a
pattern, the Acme Computer will give us an accurate reading of
suspects, with a scant probability of error. As you know, the Acme
Computer was developed entirely in house here at Acme-Looney
University,'' Dr. Fudd-Einstein said as he began feeding the numbers
into the massive server.

``Hey, look at this,'' Professor Bird exclaimed, ``What's with this
\texttt{before} and \texttt{after} information?''

``Scroll down; it shows it as a note from the coroner's
office. Apparently Toon Town Coroner Marvin -- that strange fellow
from Mars, Pennsylvania -- feels, in his opinion, that given the fact
that the cadavers were either \texttt{smokers} or non-smokers, and given
their personal health, and family medical history, that this was their
life expectancy before contact with cigarettes or second-hand smoke
and after,'' Dr. Fudd-Einstein declared matter-of-factly.

``Well, would race or gender have something to do with it, Elmer?''
inquired Professor Bird.

``Maybe, but I would bet my money on somebody was trying to quiet these
faculty before they made a big ruckus about the secret
money-laundering of Old Man Acme. You know, most people think that is
how the University receives most of its funds, through the mob
families out of Chicago. And I would be willing to bet that these
faculty figured out the connection and were ready to tell the Looney
Police.'' Dr. Fudd-Einstein spoke lower, fearing that somebody would
overhear their conversation.

Dr. Fudd-Einstein then pressed \texttt{Enter} on the keyboard and waited for
the results. The massive computer roared to life\ldots and when I say
roared, I mean it literally \textit{roared}. All the hidden bells, whistles,
and alarm clocks in its secret compartments came out and created such
a loud racket that classes across the university had to come to a
stand-still until it finished computing.

Once it was completed, the computer listed 4 names:



\texttt{SUSPECTS}

\begin{itemize}
\item \texttt{Yosemite Sam ("Looney" Insane Asylum)}
\item \texttt{Wile E. Coyote (deceased)}
\item \texttt{Foghorn Leghorn (whereabouts unknown)}
\item \texttt{Granny (1313 Mockingbird Lane, Toon Town USA)}
\end{itemize}

Dr. Fudd-Einstein and Professor Bird looked on in silence. They could
not believe their eyes. The greatest computer on the Gulf of Mexico
seaboard just released the most obscure results imaginable.

``There seems to be a mistake. Perhaps something is off,'' Professor
Bird asked, still unable to believe the results.

``Not possible; the Acme Computer takes into account every kind of
connection available. It considers affiliations to groups, and
affiliations those groups have to other groups. It checks the FBI,
CIA, British intelligence, NAACP, AARP, NSA, JAG, TWA, EPA, FDA, USWA,
R, MAPLE, SPSS, SAS, and Ben \& Jerry's files to identify
possible links, creating the most powerful computer in the
world\ldots with a tweak of Toon fanaticism,'' Dr. Fudd-Einstein
proclaimed, being a proud co-founder of the Acme Computer Technology.

``Wait a minute, Ben \& Jerry? What would eating ice cream have to do
with anything?'' Professor Bird inquired.

``It is in the works now, but a few of my fellow statistician
colleagues are trying to find a mathematical model to link the type of
ice cream consumed to the type of person they might become. Assassins
always ate vanilla with chocolate sprinkles, a little known fact they
would tell you about Oswald and Booth,'' Dr. Fudd-Einstein declared.

``I've heard about this. My forensics graduate students are trying to
identify car thieves with either rocky road or mint chocolate chip\ldots so
far, the pattern is showing a clear trend with chocolate chip,''
Professor Bird declared.  ``Well, what do we know about these suspects,
Twee?'' Dr. Fudd-Einstein asked.

``Yosemite Sam was locked up after trying to rob that bank in the West
Borough. Apparently his guns were switched and he was sent the Acme
Kids Joke Gun and they blew up in his face. The containers of peroxide
they contained turned all of his facial hair red. Some little child is
running around Toon Town with a pair of .38's to this day.

``Wile E. Coyote was that psychopath working for the Yahtzee - the
fanatics who believed that Toons were superior to Humans. He strapped
sticks of Acme Dynamite to his chest to be a martyr for the cause, but
before he got to the middle of Toon Town, this defective TNT blew him
up. Not a single other person -- Toon or Human -- was even close.

``Foghorn Leghorn is the most infamous Dog Kidnapper of all times. He
goes to the homes of prominent Dog citizens and holds one of their
relatives for ransom. If they refuse to pay, he sends them to the
pound. Either way, they're sure stuck in the dog house,'' Professor
Bird laughed. Dr. Fudd-Einstein didn't seem amused, so Professor Bird
continued.

``Granny is the most beloved alumnus of Acme-Looney University. She was
in the first graduating class and gives graciously each year to the
university. Without her continued financial support, we wouldn't have
the jobs we do. She worked as a parking attendant at the University
lots\ldots wait a minute, take a look at this,'' Professor Bird said as he
scrolled down in the police information. ``Granny's signature is on
each of these faculty members' \texttt{parking} tickets. Kind of odd,
considering the Chief-of-Parking signed each personally. The deceased
had from as few as \Sexpr{min(RcmdrTestDrive$parking)} ticket to as
many as \Sexpr{max(RcmdrTestDrive$parking)}. All tickets were unpaid.

``And look at this, Granny married Old Man Acme after graduation. He
was a resident of Chicago and rumored to be a consigliere to one of
the most prominent crime families in Chicago, the Chuck Jones/Warner
Crime Family,'' Professor Bird read from the screen as a cold feeling
of terror rose from the pit of his stomach.

``Say, don't you live at her house? Wow, you're living under the same
roof as one of the greatest criminals/murderers of all time!''
Dr. Fudd-Einstein said in awe and sarcasm.

``I would never have suspected her, but I guess it makes sense. She is
older, so she doesn't need near the amount of sleep as a younger
person. She has access to all of the vehicles so she can copy license
plate numbers and follow them to their houses. She has the finances to
pay for this kind of massive campaign on behalf of the Mob, and she
hates anyone that even remotely smells like smoke,'' Professor Bird
explained, wishing to have his hit of nicotine at this time.

``Well, I guess there is nothing left to do but to call Police Chief
Runner and have him arrest her,'' Dr. Fudd-Einstein explained as he
began dialing. ``What I can't understand is how in the world the Police
Chief sent me all of this information and somehow seemed to screw it
up.''

``What do you mean?'' inquired Professor Bird.

`` Well, look here. The data file from the Chief's email shows
\Sexpr{dim(RcmdrTestDrive)[1]} murders, but there have only been
\Sexpr{dim(RcmdrTestDrive)[1] - 2}. This doesn't make any
sense. I'll have to straighten it out. Hey, wait a minute. Look at
this, Person number \Sexpr{dim(RcmdrTestDrive)[1] - 1} and Person
number \Sexpr{dim(RcmdrTestDrive)[1]} seem to match our
stats. But how can that be?''

It was at this moment that our two heroes were shot from behind and
fell over the computer, dead. The killer hit \texttt{Delete} on the computer
and walked out slowly (considering they had arthritis) and cackling
loudly in the now quiet computer lab.

And so, I guess my question to you the reader is, did Granny murder
\Sexpr{dim(RcmdrTestDrive)[1]} people, or did the murderer slip
through the cracks of justice? You be the statistician and come to
your own conclusion.

Detective Pyork E.~Pig

\textbf{End File}

\vfill{}


\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{\bibname}


%\cleardoublepage
%\phantomsection
%\addcontentsline{toc}{chapter}{\bibname}

\bibliographystyle{plainurl}
\nocite{*}
\bibliography{IPSUR,Rpackages}
\vfill{}
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{\indexname}
\printindex{}


\end{document}




